[
  {
    "paper_id": "1911.10742",
    "question": "How big is the ANTISCAM dataset? ",
    "gold_answer": "",
    "answer_a": "3,044 sentences in 100 dialogs. Explanation: We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words",
    "answer_b": "What are the evaluation metrics?\n< Explanation: The ANTISCAM dataset contains 1000 utterances from 100 users. The utterances are divided into 10 categories, each containing 100 utterances. The utterances are labeled as either \u201cgood\u201d or \u201cbad\u201d based on the user\u2019s sentiment. The dataset is divided into 100 training and 100 testing sets. The training set contains 500 utterances, and the testing set contains 500 utterances. The dataset is annotated by 100 human annotators. The annotations are divided",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big is the ANTISCAM dataset? \n\nGround Truth Answer:\n\n\nAnswer A:\n3,044 sentences in 100 dialogs. Explanation: We posted a role-playing task on the Amazon Mechanical Turk platform and collected a typing conversation dataset named AntiScam. We collected 220 human-human dialogs. The average conversation length is 12.45 turns and the average utterance length is 11.13 words\n\nAnswer B:\nWhat are the evaluation metrics?\n< Explanation: The ANTISCAM dataset contains 1000 utterances from 100 users. The utterances are divided into 10 categories, each containing 100 utterances. The utterances are labeled as either \u201cgood\u201d or \u201cbad\u201d based on the user\u2019s sentiment. The dataset is divided into 100 training and 100 testing sets. The training set contains 500 utterances, and the testing set contains 500 utterances. The dataset is annotated by 100 human annotators. The annotations are divided\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.10742",
    "question": "How is intent annotated?",
    "gold_answer": "",
    "answer_a": "using a role-playing task on the Amazon Mechanical Turk platform and collecting typed conversations. Explanation: dataset",
    "answer_b": "What are the different slots?\n< Explanation: We use the intent annotation scheme proposed by Wolf et al. (2019) to annotate the intent slots. The intent slots are divided into three categories: \u201cask\u201d, \u201crequest\u201d, and \u201crequest with reason\u201d. The \u201cask\u201d slot is used to ask the user a question, the \u201crequest\u201d slot is used to request something, and the \u201crequest with reason\u201d slot is used to request something with a reason. The intent slots are annotated by the system and the user. The system annotates the intent slots by using the intent slots to guide the user to complete the task",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is intent annotated?\n\nGround Truth Answer:\n\n\nAnswer A:\nusing a role-playing task on the Amazon Mechanical Turk platform and collecting typed conversations. Explanation: dataset\n\nAnswer B:\nWhat are the different slots?\n< Explanation: We use the intent annotation scheme proposed by Wolf et al. (2019) to annotate the intent slots. The intent slots are divided into three categories: \u201cask\u201d, \u201crequest\u201d, and \u201crequest with reason\u201d. The \u201cask\u201d slot is used to ask the user a question, the \u201crequest\u201d slot is used to request something, and the \u201crequest with reason\u201d slot is used to request something with a reason. The intent slots are annotated by the system and the user. The system annotates the intent slots by using the intent slots to guide the user to complete the task\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.10742",
    "question": "What are the baselines outperformed by this work?",
    "gold_answer": "",
    "answer_a": "TransferTransfo and Hybrid. Explanation: We compare MISSA mainly with two baseline models:\n\nTransferTransfo The vanilla TransferTransfo framework is compared with MISSA to show the impact and necessity of adding the intent and slot classifiers. We follow the original TransferTransfo design BIBREF0 and train with undelexicalized data.\n\nHybrid Following BIBREF4 yu2017learning, we also build a hybrid dialog system by combining vanilla TransferTransfo and MISSA. Specifically, we first determine if the human utterances are on-task or off-task with human intent classifier. If the classifier decides that the utterance is on-task, we choose the response from MISSA; otherwise, we choose the response from vanilla TransferTransfo baseline.",
    "answer_b": "What are the baselines?\n< Explanation: We compare our model with the following baselines: (1) TransferTransfo (Wolf et al. 2019) which is a transfer learning model that uses the intent and semantic slots as intermediate sentence representations to guide the generation process. (2) BIBREF10 which is a baseline that uses the intent and semantic slots as intermediate sentence representations to guide the generation process. (3) BIBREF11 which is a baseline that uses the intent and semantic slots as intermediate sentence representations to guide the generation process. (4) BIBREF12 which is a",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the baselines outperformed by this work?\n\nGround Truth Answer:\n\n\nAnswer A:\nTransferTransfo and Hybrid. Explanation: We compare MISSA mainly with two baseline models:\n\nTransferTransfo The vanilla TransferTransfo framework is compared with MISSA to show the impact and necessity of adding the intent and slot classifiers. We follow the original TransferTransfo design BIBREF0 and train with undelexicalized data.\n\nHybrid Following BIBREF4 yu2017learning, we also build a hybrid dialog system by combining vanilla TransferTransfo and MISSA. Specifically, we first determine if the human utterances are on-task or off-task with human intent classifier. If the classifier decides that the utterance is on-task, we choose the response from MISSA; otherwise, we choose the response from vanilla TransferTransfo baseline.\n\nAnswer B:\nWhat are the baselines?\n< Explanation: We compare our model with the following baselines: (1) TransferTransfo (Wolf et al. 2019) which is a transfer learning model that uses the intent and semantic slots as intermediate sentence representations to guide the generation process. (2) BIBREF10 which is a baseline that uses the intent and semantic slots as intermediate sentence representations to guide the generation process. (3) BIBREF11 which is a baseline that uses the intent and semantic slots as intermediate sentence representations to guide the generation process. (4) BIBREF12 which is a\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.10742",
    "question": "What are the evaluation metrics and criteria used to evaluate the model performance?",
    "gold_answer": "",
    "answer_a": "Perplexity. Explanation: Experiments ::: Automatic Evaluation Metrics\nPerplexity Since the canonical measure of a good language model is perplexity, which indicates the error rate of the expected word. We choose perplexity to evaluate the model performance.",
    "answer_b": "What are the results of the evaluation?\n<blockquote>Table 1: Evaluation metrics and criteria. The first column shows the number of correct responses. The second column shows the number of correct responses for each task. The third column shows the number of correct responses for each task. The fourth column shows the number of correct responses for each task. The fifth column shows the number of correct responses for each task. The sixth column shows the number of correct responses for each task. The seventh column shows the number of correct responses for each task. The eighth column shows the number of correct responses for each task. The ninth column shows the number of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the evaluation metrics and criteria used to evaluate the model performance?\n\nGround Truth Answer:\n\n\nAnswer A:\nPerplexity. Explanation: Experiments ::: Automatic Evaluation Metrics\nPerplexity Since the canonical measure of a good language model is perplexity, which indicates the error rate of the expected word. We choose perplexity to evaluate the model performance.\n\nAnswer B:\nWhat are the results of the evaluation?\n<blockquote>Table 1: Evaluation metrics and criteria. The first column shows the number of correct responses. The second column shows the number of correct responses for each task. The third column shows the number of correct responses for each task. The fourth column shows the number of correct responses for each task. The fifth column shows the number of correct responses for each task. The sixth column shows the number of correct responses for each task. The seventh column shows the number of correct responses for each task. The eighth column shows the number of correct responses for each task. The ninth column shows the number of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1904.09131",
    "question": "What is the accuracy of this model compared to sota?",
    "gold_answer": "",
    "answer_a": "How does it compare to other systems?\n< Explanation: We compare our model to the state-of-the-art (SOTA) system in the following ways: (1) accuracy on the test set, (2) accuracy on the test set with the addition of the 10000000000000000000000000000000000000000000000000000000000000000000000000",
    "answer_b": "The model improves the state of the art performance for the ISTEX dataset (F1 micro: 0.870, F1 macro: 0.858) and for the Microposts 2016 dataset (F1 micro:  0.087).. Explanation: FLOAT SELECTED: Figure 2: F1 scores on test datasets",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the accuracy of this model compared to sota?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow does it compare to other systems?\n< Explanation: We compare our model to the state-of-the-art (SOTA) system in the following ways: (1) accuracy on the test set, (2) accuracy on the test set with the addition of the 10000000000000000000000000000000000000000000000000000000000000000000000000\n\nAnswer B:\nThe model improves the state of the art performance for the ISTEX dataset (F1 micro: 0.870, F1 macro: 0.858) and for the Microposts 2016 dataset (F1 micro:  0.087).. Explanation: FLOAT SELECTED: Figure 2: F1 scores on test datasets\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.06322",
    "question": "What previous methods do they compare against?",
    "gold_answer": "",
    "answer_a": "Explanation: We compare our method with the following approaches:\n\n1. Rumour detection using a recurrent neural network (RNN) trained on the news articles and the rumours. The RNN is trained on the news articles and the rumours, and then used to predict the rumours. The RNN is trained on the news articles and the rumours, and then used to predict the rumours. The RNN is trained on the news articles and the rumours, and then used to predict the rumours. The RNN is trained on the news articles and the rumours, and then used to predict the rumours. The RNN is trained on the news articles and",
    "answer_b": "two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented.. Explanation: To evaluate our new features for rumour detection, we compare them with two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. We chose the algorithm by Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter. Both apply various message-, user-, topic- and propagation-based features and rely on an SVM classifier which they also found to perform best.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat previous methods do they compare against?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We compare our method with the following approaches:\n\n1. Rumour detection using a recurrent neural network (RNN) trained on the news articles and the rumours. The RNN is trained on the news articles and the rumours, and then used to predict the rumours. The RNN is trained on the news articles and the rumours, and then used to predict the rumours. The RNN is trained on the news articles and the rumours, and then used to predict the rumours. The RNN is trained on the news articles and the rumours, and then used to predict the rumours. The RNN is trained on the news articles and\n\nAnswer B:\ntwo state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented.. Explanation: To evaluate our new features for rumour detection, we compare them with two state-of-the-art early rumour detection baselines Liu et. al (2015) and Yang et. al (2012), which we re-implemented. We chose the algorithm by Yang et. al (2012), dubbed Yang, because they proposed a feature set for early detection tailored to Sina Weibo and were used as a state-of-the-art baseline before by Liu et. al (2015). The algorithm by Liu et. al (2015), dubbed Liu, is said to operate in real-time and outperformed Yang, when only considering features available on Twitter. Both apply various message-, user-, topic- and propagation-based features and rely on an SVM classifier which they also found to perform best.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.06322",
    "question": "What is their evaluation metric?",
    "gold_answer": "",
    "answer_a": "How is it evaluated?\n< Explanation: We evaluate our approach using the metrics proposed by BIBREF10. The novelty score is computed as the difference between the number of words in the news article and the number of words in the rumour. The novelty score is computed as the difference between the number of words in the news article and the number of words in the rumour. The novelty score is computed as the difference between the number of words in the news article and the number of words in the rumour. The novelty score is computed as the difference between the number of words in the news article and the number of words in the",
    "answer_b": "accuracy to evaluate effectiveness. Explanation: We report accuracy to evaluate effectiveness, as is usual in the literature (Zhou et. al, 2015). Additionally we use the standard TDT evaluation procedure (Allan et. al, 2000; NIST, 2008) with the official TDT3 evaluation scripts (NIST, 2008) using standard settings. This procedure evaluates detection tasks using Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability. By visualizing the full range of thresholds, DET plots provide a more comprehensive illustration of effectiveness than single value metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is their evaluation metric?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it evaluated?\n< Explanation: We evaluate our approach using the metrics proposed by BIBREF10. The novelty score is computed as the difference between the number of words in the news article and the number of words in the rumour. The novelty score is computed as the difference between the number of words in the news article and the number of words in the rumour. The novelty score is computed as the difference between the number of words in the news article and the number of words in the rumour. The novelty score is computed as the difference between the number of words in the news article and the number of words in the\n\nAnswer B:\naccuracy to evaluate effectiveness. Explanation: We report accuracy to evaluate effectiveness, as is usual in the literature (Zhou et. al, 2015). Additionally we use the standard TDT evaluation procedure (Allan et. al, 2000; NIST, 2008) with the official TDT3 evaluation scripts (NIST, 2008) using standard settings. This procedure evaluates detection tasks using Detection Error Trade-off (DET) curves, which show the trade-off between miss and false alarm probability. By visualizing the full range of thresholds, DET plots provide a more comprehensive illustration of effectiveness than single value metrics (Allan et. al, 2000). We also evaluate the efficiency of computing the proposed features, measured by the throughput per second, when applied to a high number of messages.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.06322",
    "question": "Are their methods fully supervised?",
    "gold_answer": "",
    "answer_a": "No. They additionally use similarity to previously detected rumors to make the decision of whether a document is likely to be a rumor. Explanation: In a nutshell, documents that are similar to previously detected rumours are considered to be more likely to also be a rumour.",
    "answer_b": "How do they handle the lack of data?\n<sep><br/ >Yes, they are fully supervised. They use a supervised learning approach, where they train a classifier on the news articles and the rumours. The novelty features are extracted from the news articles and the rumours are classified as either true or false. The novelty features are then used to train a classifier on the rumours and the news articles. The novelty features are used to train a classifier on the rumours and the news articles. The novelty features are used to train a classifier on the rumours and the news articles. The novelty features are used to train a classifier on the rumours and the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nAre their methods fully supervised?\n\nGround Truth Answer:\n\n\nAnswer A:\nNo. They additionally use similarity to previously detected rumors to make the decision of whether a document is likely to be a rumor. Explanation: In a nutshell, documents that are similar to previously detected rumours are considered to be more likely to also be a rumour.\n\nAnswer B:\nHow do they handle the lack of data?\n<sep><br/ >Yes, they are fully supervised. They use a supervised learning approach, where they train a classifier on the news articles and the rumours. The novelty features are extracted from the news articles and the rumours are classified as either true or false. The novelty features are then used to train a classifier on the rumours and the news articles. The novelty features are used to train a classifier on the rumours and the news articles. The novelty features are used to train a classifier on the rumours and the news articles. The novelty features are used to train a classifier on the rumours and the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.06322",
    "question": "What languages do they evaluate their methods on?",
    "gold_answer": "",
    "answer_a": "Explanation: We evaluate our methods on English and German. We use the same dataset for both languages. The dataset consists of 1000 documents, each containing 1000 words. We use the same training and test sets. The dataset is available at https://github.com/abdelrahman1991/NoveltyDetection. We use the same evaluation metrics. We use the same evaluation metrics. We use the same evaluation metrics. We use the same evaluation metrics. We use the same evaluation metrics. We use the same evaluation metrics. We use the same evaluation metrics. We use the same evaluation metrics",
    "answer_b": "Chinese. Explanation: Since we operate on Chinese social media, we gathered news articles from Xinhua News Agency, the leading news-wire in China.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat languages do they evaluate their methods on?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We evaluate our methods on English and German. We use the same dataset for both languages. The dataset consists of 1000 documents, each containing 1000 words. We use the same training and test sets. The dataset is available at https://github.com/abdelrahman1991/NoveltyDetection. We use the same evaluation metrics. We use the same evaluation metrics. We use the same evaluation metrics. We use the same evaluation metrics. We use the same evaluation metrics. We use the same evaluation metrics. We use the same evaluation metrics. We use the same evaluation metrics\n\nAnswer B:\nChinese. Explanation: Since we operate on Chinese social media, we gathered news articles from Xinhua News Agency, the leading news-wire in China.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.06322",
    "question": "How do they define rumors?",
    "gold_answer": "",
    "answer_a": "What is the difference between the two approaches?\n< Explanation: Rumours are defined as information that is not true, but is widely believed to be true. Rumours are often spread by word of mouth, but can also be spread through social media. Rumours are often false, but can be true. Rumours are often false, but can be true. Rumours are often false, but can be true. Rumours are often false, but can be true. Rumours are often false, but can be true. Rumours are often false, but can be true. Rumours are often false, but can be true. Rum",
    "answer_b": "the presence of information unconfirmed by the official media is construed as an indication of being a rumour.. Explanation: When a message arrives from a social media stream, we build features based on its novelty with respect to the confirmed information in the trusted sources. In a nutshell, the presence of information unconfirmed by the official media is construed as an indication of being a rumour.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they define rumors?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the two approaches?\n< Explanation: Rumours are defined as information that is not true, but is widely believed to be true. Rumours are often spread by word of mouth, but can also be spread through social media. Rumours are often false, but can be true. Rumours are often false, but can be true. Rumours are often false, but can be true. Rumours are often false, but can be true. Rumours are often false, but can be true. Rumours are often false, but can be true. Rumours are often false, but can be true. Rum\n\nAnswer B:\nthe presence of information unconfirmed by the official media is construed as an indication of being a rumour.. Explanation: When a message arrives from a social media stream, we build features based on its novelty with respect to the confirmed information in the trusted sources. In a nutshell, the presence of information unconfirmed by the official media is construed as an indication of being a rumour.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1604.02038",
    "question": "What baselines did they compare with?",
    "gold_answer": "",
    "answer_a": "LDA. Explanation: The following baselines were used in our experiments:\n\nLDA BIBREF2 . LDA is the classic topic model, and we used GibbsLDA++ for its implementation.\n\nDoc-NADE BIBREF24 . Doc-NADE is a representative neural network based topic model. We used the open-source code provided by the authors.\n\nHTMM BIBREF9 . HTMM models consider the sentence level Markov transitions. Similar to Doc-NADE, the implementation was provided by the authors.\n\nGMNTM BIBREF12 . GMNTM considers models the order of words within a sentence by a feedforward neural network",
    "answer_b": "What are the results?\n< Explanation: Table 1 shows the results of the baseline models. The results show that SLRTM outperforms the baseline models on all the tasks. The results of the baseline models are also shown in Table 2. The results of the baseline models are also shown in Table 3. The results of the baseline models are also shown in Table 4. The results of the baseline models are also shown in Table 5. The results of the baseline models are also shown in Table 6. The results of the baseline models are also shown in Table 7. The results of the baseline models are",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat baselines did they compare with?\n\nGround Truth Answer:\n\n\nAnswer A:\nLDA. Explanation: The following baselines were used in our experiments:\n\nLDA BIBREF2 . LDA is the classic topic model, and we used GibbsLDA++ for its implementation.\n\nDoc-NADE BIBREF24 . Doc-NADE is a representative neural network based topic model. We used the open-source code provided by the authors.\n\nHTMM BIBREF9 . HTMM models consider the sentence level Markov transitions. Similar to Doc-NADE, the implementation was provided by the authors.\n\nGMNTM BIBREF12 . GMNTM considers models the order of words within a sentence by a feedforward neural network\n\nAnswer B:\nWhat are the results?\n< Explanation: Table 1 shows the results of the baseline models. The results show that SLRTM outperforms the baseline models on all the tasks. The results of the baseline models are also shown in Table 2. The results of the baseline models are also shown in Table 3. The results of the baseline models are also shown in Table 4. The results of the baseline models are also shown in Table 5. The results of the baseline models are also shown in Table 6. The results of the baseline models are also shown in Table 7. The results of the baseline models are\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1604.02038",
    "question": "Which tasks are explored in this paper?",
    "gold_answer": "",
    "answer_a": "generative model evaluation (i.e. test set perplexity) and document classification. Explanation: We have conducted experiments to compare SLRTM with several strong topic model baselines on two tasks: generative model evaluation (i.e. test set perplexity) and document classification.",
    "answer_b": "What are the results?\n< Explanation: We evaluate SLRTM on the following tasks: (1) Topic Generation: SLRTM is used to generate sentences given a topic. We compare SLRTM with the following baselines: (i) Topic Generation with a fixed topic (i.e., Topic Generation with Topic) and (ii) Topic Generation with a fixed topic and a fixed history (i.e., Topic Generation with Topic History). (2) Topic Coherence: SLRTM is used to measure the coherence between topics. We compare SLRTM with the following baselines: (i) Topic Coherence with a fixed",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich tasks are explored in this paper?\n\nGround Truth Answer:\n\n\nAnswer A:\ngenerative model evaluation (i.e. test set perplexity) and document classification. Explanation: We have conducted experiments to compare SLRTM with several strong topic model baselines on two tasks: generative model evaluation (i.e. test set perplexity) and document classification.\n\nAnswer B:\nWhat are the results?\n< Explanation: We evaluate SLRTM on the following tasks: (1) Topic Generation: SLRTM is used to generate sentences given a topic. We compare SLRTM with the following baselines: (i) Topic Generation with a fixed topic (i.e., Topic Generation with Topic) and (ii) Topic Generation with a fixed topic and a fixed history (i.e., Topic Generation with Topic History). (2) Topic Coherence: SLRTM is used to measure the coherence between topics. We compare SLRTM with the following baselines: (i) Topic Coherence with a fixed\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.04474",
    "question": "Which NER dataset do they use?",
    "gold_answer": "",
    "answer_a": "What is the performance of TENER on this dataset?\n< Explanation: We use the Penn Treebank (PTB) dataset for NER. The PTB dataset is a large-scale NER dataset with 10,000 named entities. The dataset is divided into training, validation and test sets. The training set is 80%, the validation set is 10% and the test set is 10%. The test set is used to evaluate the performance of TENER. The test set is divided into 1000 named entities. The test set is used to evaluate the performance of TENER.",
    "answer_b": "CoNLL2003. Explanation: We evaluate our model in two English NER datasets and four Chinese NER datasets.\n\n(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four different named entities: PERSON, LOCATION, ORGANIZATION, and MISC BIBREF34.\n\n(2) OntoNotes 5.0 is an English NER dataset whose corpus comes from different domains, such as telephone conversation, newswire. We exclude the New Testaments portion since there is no named entity in it BIBREF8, BIBREF7. This dataset has eleven entity names and seven value types, like CARDINAL, MONEY, LOC.\n\n(3) BIBREF35 released OntoNotes 4.0. In this paper, we use the Chinese part. We adopted the same pre-process as BIBREF36.\n\n(4) The corpus of the Chinese NER dataset MSRA came from news domain BIBREF37.\n\n(5) Weibo NER was built based on text in Chinese social media Sina Weibo BIBREF38, and it contained 4 kinds of entities.\n\n(6) Resume NER was annotated by BIBREF33.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich NER dataset do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of TENER on this dataset?\n< Explanation: We use the Penn Treebank (PTB) dataset for NER. The PTB dataset is a large-scale NER dataset with 10,000 named entities. The dataset is divided into training, validation and test sets. The training set is 80%, the validation set is 10% and the test set is 10%. The test set is used to evaluate the performance of TENER. The test set is divided into 1000 named entities. The test set is used to evaluate the performance of TENER.\n\nAnswer B:\nCoNLL2003. Explanation: We evaluate our model in two English NER datasets and four Chinese NER datasets.\n\n(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four different named entities: PERSON, LOCATION, ORGANIZATION, and MISC BIBREF34.\n\n(2) OntoNotes 5.0 is an English NER dataset whose corpus comes from different domains, such as telephone conversation, newswire. We exclude the New Testaments portion since there is no named entity in it BIBREF8, BIBREF7. This dataset has eleven entity names and seven value types, like CARDINAL, MONEY, LOC.\n\n(3) BIBREF35 released OntoNotes 4.0. In this paper, we use the Chinese part. We adopted the same pre-process as BIBREF36.\n\n(4) The corpus of the Chinese NER dataset MSRA came from news domain BIBREF37.\n\n(5) Weibo NER was built based on text in Chinese social media Sina Weibo BIBREF38, and it contained 4 kinds of entities.\n\n(6) Resume NER was annotated by BIBREF33.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.04474",
    "question": "How do they incorporate direction and relative distance in attention?",
    "gold_answer": "",
    "answer_a": "by using an relative sinusodial positional embedding and unscaled attention. Explanation: Therefore, to improve the Transformer with direction- and distance-aware characteristic, we calculate the attention scores using the equations below:\n\nwhere $t$ is index of the target token, $j$ is the index of the context token, $Q_t, K_j$ is the query vector and key vector of token $t, j$ respectively, $W_q, W_v \\in \\mathbb {R}^{d \\times d_k}$. To get $H_{d_k}\\in \\mathbb {R}^{l \\times d_k}$, we first split $H$ into $d/d_k$ partitions in the second dimension, then for each head we use one partition. $\\mathbf {u} \\in \\mathbb {R}^{d_k}$, $\\mathbf {v} \\in \\mathbb {R}^{d_k}$ are learnable parameters, $R_{t-j}$ is the relative positional encoding, and $R_{t-j} \\in \\mathbb {R}^{d_k}$, $i$ in Eq.() is in the range $[0, \\frac{d_k}{2}]$. $Q_t^TK_j$ in Eq.() is the attention score between two tokens; $Q_t^TR_{t-j}$ is the $t$th token's bias on certain relative distance; $u^TK_j$ is the bias on the $j$th token; $v^TR_{t-j}$ is the bias term for certain distance and direction.",
    "answer_b": "What is the un-scaled attention?\n< Explanation: The BiLSTM is a bidirectional LSTM network. The BiLSTM is used to encode the character-level features and word-level features. The BiLSTM is used to encode the character-level features and word-level features. The BiLSTM is used to encode the character-level features and word-level features. The BiLSTM is used to encode the character-level features and word-level features. The BiLSTM is used to encode the character-level features and word-level features. The BiLSTM is used to encode the character-level features and word-level features. The BiLSTM",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they incorporate direction and relative distance in attention?\n\nGround Truth Answer:\n\n\nAnswer A:\nby using an relative sinusodial positional embedding and unscaled attention. Explanation: Therefore, to improve the Transformer with direction- and distance-aware characteristic, we calculate the attention scores using the equations below:\n\nwhere $t$ is index of the target token, $j$ is the index of the context token, $Q_t, K_j$ is the query vector and key vector of token $t, j$ respectively, $W_q, W_v \\in \\mathbb {R}^{d \\times d_k}$. To get $H_{d_k}\\in \\mathbb {R}^{l \\times d_k}$, we first split $H$ into $d/d_k$ partitions in the second dimension, then for each head we use one partition. $\\mathbf {u} \\in \\mathbb {R}^{d_k}$, $\\mathbf {v} \\in \\mathbb {R}^{d_k}$ are learnable parameters, $R_{t-j}$ is the relative positional encoding, and $R_{t-j} \\in \\mathbb {R}^{d_k}$, $i$ in Eq.() is in the range $[0, \\frac{d_k}{2}]$. $Q_t^TK_j$ in Eq.() is the attention score between two tokens; $Q_t^TR_{t-j}$ is the $t$th token's bias on certain relative distance; $u^TK_j$ is the bias on the $j$th token; $v^TR_{t-j}$ is the bias term for certain distance and direction.\n\nAnswer B:\nWhat is the un-scaled attention?\n< Explanation: The BiLSTM is a bidirectional LSTM network. The BiLSTM is used to encode the character-level features and word-level features. The BiLSTM is used to encode the character-level features and word-level features. The BiLSTM is used to encode the character-level features and word-level features. The BiLSTM is used to encode the character-level features and word-level features. The BiLSTM is used to encode the character-level features and word-level features. The BiLSTM is used to encode the character-level features and word-level features. The BiLSTM\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.00840",
    "question": "What was their accuracy score?",
    "gold_answer": "",
    "answer_a": "95.6% on knowledge authoring, 95% on the manually constructed QA dataset and 100% accuracy on the MetaQA dataset. Explanation: KALM achieves an accuracy of 95.6%\u2014much higher than the other systems.\n\nFor KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy\u2014much higher than the state-of-the-art machine learning approach BIBREF14 .",
    "answer_b": "How did they compare to the state-of-the-art systems?\n<blockquote>Our results show that KALM achieves superior accuracy in knowledge authoring and question answering as compared to the state-of-the-art systems. The accuracy of KALM is 98.5% and the accuracy of the state-of-the-art systems is 96.5%. </blockquote>\nThe accuracy of KALM is 98.5% and the accuracy of the state-of-the-art systems is 96.5%. The accuracy of KALM is 98.5% and the accuracy of the state-of-the-art systems",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat was their accuracy score?\n\nGround Truth Answer:\n\n\nAnswer A:\n95.6% on knowledge authoring, 95% on the manually constructed QA dataset and 100% accuracy on the MetaQA dataset. Explanation: KALM achieves an accuracy of 95.6%\u2014much higher than the other systems.\n\nFor KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy\u2014much higher than the state-of-the-art machine learning approach BIBREF14 .\n\nAnswer B:\nHow did they compare to the state-of-the-art systems?\n<blockquote>Our results show that KALM achieves superior accuracy in knowledge authoring and question answering as compared to the state-of-the-art systems. The accuracy of KALM is 98.5% and the accuracy of the state-of-the-art systems is 96.5%. </blockquote>\nThe accuracy of KALM is 98.5% and the accuracy of the state-of-the-art systems is 96.5%. The accuracy of KALM is 98.5% and the accuracy of the state-of-the-art systems\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.00840",
    "question": "What are the state-of-the-art systems?",
    "gold_answer": "",
    "answer_a": "What are the differences between KALM and them?\n<blockquote>1. KALM: KALM is a rule-based system that allows the user to author knowledge and query the KB in text. It is based on the knowledge extraction and question answering systems. KALM is a rule-based system that allows the user to author knowledge and query the KB in text. It is based on the knowledge extraction and question answering systems. KALM is a rule-based system that allows the user to author knowledge and query the KB in text. It is based on the knowledge extraction and question answering systems. KALM is a rule",
    "answer_b": "SEMAFOR. Explanation: Based on the 50 frames, we have manually constructed 250 sentences that are adapted from FrameNet exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%\u2014much higher than the other systems.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the state-of-the-art systems?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the differences between KALM and them?\n<blockquote>1. KALM: KALM is a rule-based system that allows the user to author knowledge and query the KB in text. It is based on the knowledge extraction and question answering systems. KALM is a rule-based system that allows the user to author knowledge and query the KB in text. It is based on the knowledge extraction and question answering systems. KALM is a rule-based system that allows the user to author knowledge and query the KB in text. It is based on the knowledge extraction and question answering systems. KALM is a rule\n\nAnswer B:\nSEMAFOR. Explanation: Based on the 50 frames, we have manually constructed 250 sentences that are adapted from FrameNet exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%\u2014much higher than the other systems.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.00840",
    "question": "What dataset did they evaluate on?",
    "gold_answer": "",
    "answer_a": "dataset consisting 250 sentences adapted from FrameNet exemplar sentences, dataset consisting general questions based on 50 logical framesderived from FrameNet, MetaQA dataset. Explanation: We have created a total of 50 logical frames, mostly derived from FrameNet but also some that FrameNet is missing (like Restaurant, Human_Gender) for representing the meaning of English sentences. Based on the 50 frames, we have manually constructed 250 sentences that are adapted from FrameNet exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%\u2014much higher than the other systems.\n\nFor KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy\u2014much higher than the state-of-the-art machine learning approach BIBREF14 .",
    "answer_b": "How did they evaluate it?\n<blockquote>\n<font size=\"2\">The dataset used in this thesis is the KB of the Wikipedia article on the history of the Internet. The dataset contains 100000 lines of text, each line is a sentence in English. The sentences are randomly selected from the Wikipedia article. The sentences are divided into 100000 sentences, each of which is a sentence in English. Each sentence is a sentence in English. The sentences are randomly selected from the Wikipedia article. The sentences are divided into 100000 sentences, each of which is a sentence",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat dataset did they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\ndataset consisting 250 sentences adapted from FrameNet exemplar sentences, dataset consisting general questions based on 50 logical framesderived from FrameNet, MetaQA dataset. Explanation: We have created a total of 50 logical frames, mostly derived from FrameNet but also some that FrameNet is missing (like Restaurant, Human_Gender) for representing the meaning of English sentences. Based on the 50 frames, we have manually constructed 250 sentences that are adapted from FrameNet exemplar sentences and evaluate these sentences on KALM, SEMAFOR, SLING, and Stanford KBP system. KALM achieves an accuracy of 95.6%\u2014much higher than the other systems.\n\nFor KALM-QA, we evaluate it on two datasets. The first dataset is manually constructed general questions based on the 50 logical frames. KALM-QA achieves an accuracy of 95% for parsing the queries. The second dataset we use is MetaQA dataset BIBREF14 , which contains contains almost 29,000 test questions and over 260,000 training questions. KALM-QA achieves 100% accuracy\u2014much higher than the state-of-the-art machine learning approach BIBREF14 .\n\nAnswer B:\nHow did they evaluate it?\n<blockquote>\n<font size=\"2\">The dataset used in this thesis is the KB of the Wikipedia article on the history of the Internet. The dataset contains 100000 lines of text, each line is a sentence in English. The sentences are randomly selected from the Wikipedia article. The sentences are divided into 100000 sentences, each of which is a sentence in English. Each sentence is a sentence in English. The sentences are randomly selected from the Wikipedia article. The sentences are divided into 100000 sentences, each of which is a sentence\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1810.02229",
    "question": "What are the contributions of this paper?",
    "gold_answer": "",
    "answer_a": "adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach. Explanation: The contributions of this paper are the followings: i.) the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach; ii.) an investigation on the quality of existing Italian word embeddings for this task; iii.) a comparison against a state-of-the-art discrete classifier. The pre-trained models and scripts running the system (or re-train it) are publicly available.",
    "answer_b": "Explanation: The paper presents a set of experiments with different word embeddings to initialize a state-of-the-art Bi-LSTM-CRF network for event detection and classification in Italian, following the EVENTI evaluation exercise. The net- work obtains a new state-of-the-art result by improving the F1 score for detection of 1.3 points, and of 6.5 points for classification, by using a single step approach. The results also provide further evidence that embeddings have a major impact on the performance of such architectures.\n\nThe paper presents a set of experiments with different word embeddings to initialize a state-of-the-art Bi-LSTM-CRF network",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the contributions of this paper?\n\nGround Truth Answer:\n\n\nAnswer A:\nadaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach. Explanation: The contributions of this paper are the followings: i.) the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach; ii.) an investigation on the quality of existing Italian word embeddings for this task; iii.) a comparison against a state-of-the-art discrete classifier. The pre-trained models and scripts running the system (or re-train it) are publicly available.\n\nAnswer B:\nExplanation: The paper presents a set of experiments with different word embeddings to initialize a state-of-the-art Bi-LSTM-CRF network for event detection and classification in Italian, following the EVENTI evaluation exercise. The net- work obtains a new state-of-the-art result by improving the F1 score for detection of 1.3 points, and of 6.5 points for classification, by using a single step approach. The results also provide further evidence that embeddings have a major impact on the performance of such architectures.\n\nThe paper presents a set of experiments with different word embeddings to initialize a state-of-the-art Bi-LSTM-CRF network\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1810.02229",
    "question": "What are the baselines this paper uses?",
    "gold_answer": "",
    "answer_a": "cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features. Explanation: FBK-HLT is a cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features.",
    "answer_b": "Explanation: The baseline is the same as the one used in the EVENTI evaluation exercise, which is a single step approach with a single word embedding. The baseline is the same as the one used in the EVENTI evaluation exercise, which is a single step approach with a single word embedding. The baseline is the same as the one used in the EVENTI evaluation exercise, which is a single step approach with a single word embedding. The baseline is the same as the one used in the EVENTI evaluation exercise, which is a single step approach with a single word embedding. The baseline is the same as the one used in the EVENTI evaluation",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the baselines this paper uses?\n\nGround Truth Answer:\n\n\nAnswer A:\ncascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features. Explanation: FBK-HLT is a cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features.\n\nAnswer B:\nExplanation: The baseline is the same as the one used in the EVENTI evaluation exercise, which is a single step approach with a single word embedding. The baseline is the same as the one used in the EVENTI evaluation exercise, which is a single step approach with a single word embedding. The baseline is the same as the one used in the EVENTI evaluation exercise, which is a single step approach with a single word embedding. The baseline is the same as the one used in the EVENTI evaluation exercise, which is a single step approach with a single word embedding. The baseline is the same as the one used in the EVENTI evaluation\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.00091",
    "question": "How do they decide what is the semantic concept label of particular cluster?",
    "gold_answer": "",
    "answer_a": "What is the definition of the semantic concept label?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Figure 1</td>\n<td align=\"left\">Figure 2</td>\n</tr>\n<tr>\n<td align=\"left\">Figure 3</td>\n<td align=\"left\">Figure 4</td>\n</tr>\n<tr>\n<td align=\"left\">Figure 5</td>\n<td align=\"left\">Figure 6</td>\n</tr>\n<tr>\n<td align=\"left\">Figure 7</td>\n<td align=\"left\">Figure 8</td>\n</",
    "answer_b": "Given a cluster, our algorithm proceeds with the following three steps:\n\nSense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets.\n\nCandidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$.\n\nCandidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.\n\nIn steps 1 and 3, we use WordNet pathwise distance, but we encourage the exploration of other distance representations as well.. Explanation: Given a cluster, our algorithm proceeds with the following three steps:\n\nSense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets.\n\nCandidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$.\n\nCandidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.\n\nIn steps 1 and 3, we use WordNet pathwise distance, but we encourage the exploration of other distance representations as well.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they decide what is the semantic concept label of particular cluster?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the definition of the semantic concept label?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Figure 1</td>\n<td align=\"left\">Figure 2</td>\n</tr>\n<tr>\n<td align=\"left\">Figure 3</td>\n<td align=\"left\">Figure 4</td>\n</tr>\n<tr>\n<td align=\"left\">Figure 5</td>\n<td align=\"left\">Figure 6</td>\n</tr>\n<tr>\n<td align=\"left\">Figure 7</td>\n<td align=\"left\">Figure 8</td>\n</\n\nAnswer B:\nGiven a cluster, our algorithm proceeds with the following three steps:\n\nSense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets.\n\nCandidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$.\n\nCandidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.\n\nIn steps 1 and 3, we use WordNet pathwise distance, but we encourage the exploration of other distance representations as well.. Explanation: Given a cluster, our algorithm proceeds with the following three steps:\n\nSense disambiguation: The goal is to assign each cluster word to one of its WordNet synsets; let $S$ represent the collection of chosen synsets. We know that these words have been clustered in domain-specific embedding space, which means that in the context of the domain, these words are very close semantically. Thus, we choose $S^*$ that minimizes the total distance between its synsets.\n\nCandidate label generation: In this step, we generate $L$, the set of possible cluster labels. Our approach is simple: we take the union of all hypernyms of the synsets in $S^*$.\n\nCandidate label ranking: Here, we rank the synsets in $L$. We want labels that are as close to all of the synsets in $S^*$ as possible; thus, we score the candidate labels by the sum of their distances to each synset in $S^*$ and we rank them from least to most distance.\n\nIn steps 1 and 3, we use WordNet pathwise distance, but we encourage the exploration of other distance representations as well.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.00091",
    "question": "How do they discover coherent word clusters?",
    "gold_answer": "",
    "answer_a": "What are the evaluation metrics?\n<sep><font size=\"4\">Answer: We use the word clustering algorithm of [1] to discover coherent word clusters. We use the word clustering algorithm of [2] to discover coherent word clusters. We use the word clustering algorithm of [3] to discover coherent word clusters. We use the word clustering algorithm of [4] to discover coherent word clusters. We use the word clustering algorithm of [5] to discover coherent word clusters. We use the word clustering algorithm of [6] to discover coherent word clusters. We use the word clustering algorithm of [7] to discover coherent word clusters.",
    "answer_b": "First, we trained domain-specific word embeddings using the Word2Vec BIBREF33 CBOW model ($w \\in R^{100}$). Then, we used k-means clustering to cluster the embeddings of the gender-associated words. Since k-means may converge at local optima, we ran the algorithm 50 times and kept the model with the lowest sum of squared errors.. Explanation: With word-level associations in hand, our next goals were to discover coherent clusters among the words and to automatically label those clusters.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they discover coherent word clusters?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the evaluation metrics?\n<sep><font size=\"4\">Answer: We use the word clustering algorithm of [1] to discover coherent word clusters. We use the word clustering algorithm of [2] to discover coherent word clusters. We use the word clustering algorithm of [3] to discover coherent word clusters. We use the word clustering algorithm of [4] to discover coherent word clusters. We use the word clustering algorithm of [5] to discover coherent word clusters. We use the word clustering algorithm of [6] to discover coherent word clusters. We use the word clustering algorithm of [7] to discover coherent word clusters.\n\nAnswer B:\nFirst, we trained domain-specific word embeddings using the Word2Vec BIBREF33 CBOW model ($w \\in R^{100}$). Then, we used k-means clustering to cluster the embeddings of the gender-associated words. Since k-means may converge at local optima, we ran the algorithm 50 times and kept the model with the lowest sum of squared errors.. Explanation: With word-level associations in hand, our next goals were to discover coherent clusters among the words and to automatically label those clusters.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.00091",
    "question": "How big are two introduced datasets?",
    "gold_answer": "",
    "answer_a": "300K sentences in each dataset. Explanation: Two datasets for studying language and gender, each consisting of over 300K sentences.",
    "answer_b": "What are the domains of the datasets?\n<sep><br/ >Answer: The first dataset contains 10,000 tweets from the Twitter news feed of the New York Times. The second dataset contains 10,000 tweets from the Twitter news feed of the University of California, Berkeley. The tweets are labeled with the gender of the author. The datasets are available at <URL> and <URL>. Explanation: The first dataset contains 10,000 tweets from the Twitter news feed of the New York Times. The second dataset contains 10,000 tweets from the Twitter news",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big are two introduced datasets?\n\nGround Truth Answer:\n\n\nAnswer A:\n300K sentences in each dataset. Explanation: Two datasets for studying language and gender, each consisting of over 300K sentences.\n\nAnswer B:\nWhat are the domains of the datasets?\n<sep><br/ >Answer: The first dataset contains 10,000 tweets from the Twitter news feed of the New York Times. The second dataset contains 10,000 tweets from the Twitter news feed of the University of California, Berkeley. The tweets are labeled with the gender of the author. The datasets are available at <URL> and <URL>. Explanation: The first dataset contains 10,000 tweets from the Twitter news feed of the New York Times. The second dataset contains 10,000 tweets from the Twitter news\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.00091",
    "question": "What are strong baselines authors used?",
    "gold_answer": "",
    "answer_a": "What are their results?\n< Explanation: We compare our methods to the following baselines: (1) a baseline that uses the word clusters from the corpus of student reviews of computer science professors (SCC) as the ground truth, (2) a baseline that uses the word clusters from the corpus of celebrity news (CN) as the ground truth, (3) a baseline that uses the word clusters from the corpus of student reviews of computer science professors (SCC) as the ground truth, (4) a baseline that uses the word clusters from the corpus of celebrity news (CN) as the ground truth, (5) a",
    "answer_b": "The authors contrasted human evaluations against a random baseline, and used the centroid of the cluster as a strong baseline.. Explanation: Human Evaluations\nTo test our clusters, we employed the Word Intrusion task BIBREF35. We present the annotator with five words \u2013 four drawn from one cluster and one drawn randomly from the domain vocabulary \u2013 and we ask them to pick out the intruder. The intuition is that if the cluster is coherent, then an observer should be able to identify the out-of-cluster word as the intruder. For both domains, we report results on all clusters and on the top 8, ranked by ascending normalized sum of squared errors, which can be seen as a prediction of coherence. In the celebrity domain, annotators identified the out-of-cluster word 73% of the time in the top-8 and 53% overall. In the professor domain, annotators identified it 60% of the time in the top-8 and 49% overall. As expected, top-8 performance in both domains does considerably better than overall, but at all levels the precision is significantly above the random baseline of 20%.\n\nTo test cluster labels, we present the annotator with a label and a word, and we ask them whether the word falls under the concept. The concept is a potential cluster label and the word is either a word from that cluster or drawn randomly from the domain vocabulary. For a good label, the rate at which in-cluster words fall under the label should be much higher than the rate at which out-of-cluster words fall under. In our experiments, we tested the top 4 predicted labels and the centroid of the cluster as a strong baseline label.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are strong baselines authors used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are their results?\n< Explanation: We compare our methods to the following baselines: (1) a baseline that uses the word clusters from the corpus of student reviews of computer science professors (SCC) as the ground truth, (2) a baseline that uses the word clusters from the corpus of celebrity news (CN) as the ground truth, (3) a baseline that uses the word clusters from the corpus of student reviews of computer science professors (SCC) as the ground truth, (4) a baseline that uses the word clusters from the corpus of celebrity news (CN) as the ground truth, (5) a\n\nAnswer B:\nThe authors contrasted human evaluations against a random baseline, and used the centroid of the cluster as a strong baseline.. Explanation: Human Evaluations\nTo test our clusters, we employed the Word Intrusion task BIBREF35. We present the annotator with five words \u2013 four drawn from one cluster and one drawn randomly from the domain vocabulary \u2013 and we ask them to pick out the intruder. The intuition is that if the cluster is coherent, then an observer should be able to identify the out-of-cluster word as the intruder. For both domains, we report results on all clusters and on the top 8, ranked by ascending normalized sum of squared errors, which can be seen as a prediction of coherence. In the celebrity domain, annotators identified the out-of-cluster word 73% of the time in the top-8 and 53% overall. In the professor domain, annotators identified it 60% of the time in the top-8 and 49% overall. As expected, top-8 performance in both domains does considerably better than overall, but at all levels the precision is significantly above the random baseline of 20%.\n\nTo test cluster labels, we present the annotator with a label and a word, and we ask them whether the word falls under the concept. The concept is a potential cluster label and the word is either a word from that cluster or drawn randomly from the domain vocabulary. For a good label, the rate at which in-cluster words fall under the label should be much higher than the rate at which out-of-cluster words fall under. In our experiments, we tested the top 4 predicted labels and the centroid of the cluster as a strong baseline label.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.04387",
    "question": "How do data-driven models usually respond to abuse?",
    "gold_answer": "",
    "answer_a": "What are the differences between them?\n<sep><font color=\"red\">Explanation: We evaluate the performance of the three rule-based models and the three commercial models. The rule-based models are based on a set of rules that are applied to the user's input. The commercial models are based on a set of rules that are applied to the user's input and are trained on a large dataset of abusive and non-abusive user inputs. The rule-based models are evaluated on a subset of the dataset, while the commercial models are evaluated on the entire dataset. The commercial models are also evaluated on a subset of the dataset, while the rule-based",
    "answer_b": "either by refusing politely, or, with flirtatious responses, or, by retaliating. Explanation: 4 Data-driven approaches:\n\nCleverbot BIBREF12;\n\nNeuralConvo BIBREF13, a re-implementation of BIBREF14;\n\nan implementation of BIBREF15's Information Retrieval approach;\n\na vanilla Seq2Seq model trained on clean Reddit data BIBREF1.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do data-driven models usually respond to abuse?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the differences between them?\n<sep><font color=\"red\">Explanation: We evaluate the performance of the three rule-based models and the three commercial models. The rule-based models are based on a set of rules that are applied to the user's input. The commercial models are based on a set of rules that are applied to the user's input and are trained on a large dataset of abusive and non-abusive user inputs. The rule-based models are evaluated on a subset of the dataset, while the commercial models are evaluated on the entire dataset. The commercial models are also evaluated on a subset of the dataset, while the rule-based\n\nAnswer B:\neither by refusing politely, or, with flirtatious responses, or, by retaliating. Explanation: 4 Data-driven approaches:\n\nCleverbot BIBREF12;\n\nNeuralConvo BIBREF13, a re-implementation of BIBREF14;\n\nan implementation of BIBREF15's Information Retrieval approach;\n\na vanilla Seq2Seq model trained on clean Reddit data BIBREF1.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.04387",
    "question": "How much data did they gather from crowdsourcing?",
    "gold_answer": "",
    "answer_a": "600K. Explanation: We first gather abusive utterances from 600K conversations with US-based customers.",
    "answer_b": "They gather data from 1000 users of the chatbot.\n<sep>Answer: We collected data from 1000 users of the chatbot. We also collected data from 1000 users of the chatbot who were not part of the evaluation. We also collected data from 1000 users of the chatbot who were not part of the evaluation. We also collected data from 1000 users of the chatbot who were not part of the evaluation. We also collected data from 1000 users of the chatbot who were not part of the evaluation. We",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much data did they gather from crowdsourcing?\n\nGround Truth Answer:\n\n\nAnswer A:\n600K. Explanation: We first gather abusive utterances from 600K conversations with US-based customers.\n\nAnswer B:\nThey gather data from 1000 users of the chatbot.\n<sep>Answer: We collected data from 1000 users of the chatbot. We also collected data from 1000 users of the chatbot who were not part of the evaluation. We also collected data from 1000 users of the chatbot who were not part of the evaluation. We also collected data from 1000 users of the chatbot who were not part of the evaluation. We also collected data from 1000 users of the chatbot who were not part of the evaluation. We\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.04387",
    "question": "How many different strategies were evaluated?",
    "gold_answer": "",
    "answer_a": "14. Explanation: FLOAT SELECTED: Table 1: Full annotation scheme for system response types after user abuse. Categories (1a) and (1b) are excluded from this study.",
    "answer_b": "Explanation: We evaluate 10 different strategies, each of which is evaluated in terms of its appropriateness to the user's situation. We evaluate the appropriateness of the following strategies: (1) \"I'm sorry\" (2) \"I'm sorry\" with a \"yes\" response (3) \"I'm sorry\" with a \"no\" response (4) \"I'm sorry\" with a \"yes\" response (5) \"I'm sorry\" with a \"no\" response (6) \"I'm sorry\" with a \"yes\" response (7) \"I'm sorry\" with a \"",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many different strategies were evaluated?\n\nGround Truth Answer:\n\n\nAnswer A:\n14. Explanation: FLOAT SELECTED: Table 1: Full annotation scheme for system response types after user abuse. Categories (1a) and (1b) are excluded from this study.\n\nAnswer B:\nExplanation: We evaluate 10 different strategies, each of which is evaluated in terms of its appropriateness to the user's situation. We evaluate the appropriateness of the following strategies: (1) \"I'm sorry\" (2) \"I'm sorry\" with a \"yes\" response (3) \"I'm sorry\" with a \"no\" response (4) \"I'm sorry\" with a \"yes\" response (5) \"I'm sorry\" with a \"no\" response (6) \"I'm sorry\" with a \"yes\" response (7) \"I'm sorry\" with a \"\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.11937",
    "question": "What morphological typologies are considered?",
    "gold_answer": "",
    "answer_a": "agglutinative and fusional languages. Explanation: We analyze the results separately for agglutinative and fusional languages and reveal the links between certain linguistic phenomena and the IOC, IOW values.",
    "answer_b": "Explanation: We consider three types of morphological typologies: word, character and morphology. We train three models for each type and evaluate their performance on the SRL task. We also compare the performance of the models on the three types of morphological typologies. We also compare the performance of the models on the three types of morphological typologies on the same dataset. We also compare the performance of the models on the same dataset on the three types of morphological typologies. We also compare the performance of the models on the same dataset on the three types of morphological typologies on the same language. We also compare the performance",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat morphological typologies are considered?\n\nGround Truth Answer:\n\n\nAnswer A:\nagglutinative and fusional languages. Explanation: We analyze the results separately for agglutinative and fusional languages and reveal the links between certain linguistic phenomena and the IOC, IOW values.\n\nAnswer B:\nExplanation: We consider three types of morphological typologies: word, character and morphology. We train three models for each type and evaluate their performance on the SRL task. We also compare the performance of the models on the three types of morphological typologies. We also compare the performance of the models on the three types of morphological typologies on the same dataset. We also compare the performance of the models on the same dataset on the three types of morphological typologies. We also compare the performance of the models on the same dataset on the three types of morphological typologies on the same language. We also compare the performance\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.11937",
    "question": "What type of morphological features are used?",
    "gold_answer": "",
    "answer_a": "What are the strengths and limitations of character-level models?\n< Explanation: We use the following morphological features: morphological root, morphological stem, morphological suffix, morphological prefix, morphological infix, morphological prefix infix, morphological suffix infix, morphological infix suffix, morphological infix suffix infix, morphological infix suffix infix suffix, morphological infix suffix infix suffix suffix, morphological infix suffix infix suffix suffix suffix, morphological infix suffix infix suffix suffix suffix suffix, morphological infix suffix infix suffix suffix suffix suffix suffix, morphological infix suffix infix suffix suffix suffix suffix suffix suffix, morphological infix suffix infix suffix suffix suffix",
    "answer_b": "char3 slides a character window of width $n=3$ over the token. Explanation: We use three types of units: (1) words (2) characters and character sequences and (3) outputs of morphological analysis. Words serve as a lower bound; while morphology is used as an upper bound for comparison. Table 1 shows sample outputs of various $\\rho $ functions.\n\nHere, char function simply splits the token into its characters. Similar to n-gram language models, char3 slides a character window of width $n=3$ over the token. Finally, gold morphological features are used as outputs of morph-language. Throughout this paper, we use morph and oracle interchangably, i.e., morphology-level models (MLM) have access to gold tags unless otherwise is stated. For all languages, morph outputs the lemma of the token followed by language specific morphological tags. As an exception, it outputs additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units. Due to low scores obtained from our preliminary experiments and unsatisfactory results from previous studies BIBREF13 , we excluded these units.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of morphological features are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the strengths and limitations of character-level models?\n< Explanation: We use the following morphological features: morphological root, morphological stem, morphological suffix, morphological prefix, morphological infix, morphological prefix infix, morphological suffix infix, morphological infix suffix, morphological infix suffix infix, morphological infix suffix infix suffix, morphological infix suffix infix suffix suffix, morphological infix suffix infix suffix suffix suffix, morphological infix suffix infix suffix suffix suffix suffix, morphological infix suffix infix suffix suffix suffix suffix suffix, morphological infix suffix infix suffix suffix suffix suffix suffix suffix, morphological infix suffix infix suffix suffix suffix\n\nAnswer B:\nchar3 slides a character window of width $n=3$ over the token. Explanation: We use three types of units: (1) words (2) characters and character sequences and (3) outputs of morphological analysis. Words serve as a lower bound; while morphology is used as an upper bound for comparison. Table 1 shows sample outputs of various $\\rho $ functions.\n\nHere, char function simply splits the token into its characters. Similar to n-gram language models, char3 slides a character window of width $n=3$ over the token. Finally, gold morphological features are used as outputs of morph-language. Throughout this paper, we use morph and oracle interchangably, i.e., morphology-level models (MLM) have access to gold tags unless otherwise is stated. For all languages, morph outputs the lemma of the token followed by language specific morphological tags. As an exception, it outputs additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units. Due to low scores obtained from our preliminary experiments and unsatisfactory results from previous studies BIBREF13 , we excluded these units.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.09070",
    "question": "What datasets are used in this paper?",
    "gold_answer": "",
    "answer_a": "The Semantic Scholar corpus. Explanation: Results and Discussion ::: Datasets",
    "answer_b": "How large are they?\n< Explanation: We use the following datasets: (1) the official dataset of the 2018 IJCAI competition, which contains 10,000 figures and 10,000 captions, (2) the official dataset of the 2019 IJCAI competition, which contains 10,000 figures and 10,000 captions, (3) the official dataset of the 2020 IJCAI competition, which contains 10,000 figures and 10,00",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets are used in this paper?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe Semantic Scholar corpus. Explanation: Results and Discussion ::: Datasets\n\nAnswer B:\nHow large are they?\n< Explanation: We use the following datasets: (1) the official dataset of the 2018 IJCAI competition, which contains 10,000 figures and 10,000 captions, (2) the official dataset of the 2019 IJCAI competition, which contains 10,000 figures and 10,000 captions, (3) the official dataset of the 2020 IJCAI competition, which contains 10,000 figures and 10,00\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.09070",
    "question": "What language are the captions in?",
    "gold_answer": "",
    "answer_a": "How are they encoded?\n< Explanation: The captions are in English. They are encoded as a sequence of tokens, where each token is a word or a punctuation mark. The tokens are separated by a space. The length of the sequence is the number of tokens in the caption. The tokens are encoded as integers, where the integer value of a token is the position of the token in the sequence. The integer value of a token is the same for all tokens in the sequence. The integer value of a token is the same for all tokens in the sequence. The integer value of a token is the same for all tokens in the sequence.",
    "answer_b": "English. Explanation: FLOAT SELECTED: Figure 2: Selected visual features and activation heatmaps. The top row labels the dominant pattern for each feature.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat language are the captions in?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they encoded?\n< Explanation: The captions are in English. They are encoded as a sequence of tokens, where each token is a word or a punctuation mark. The tokens are separated by a space. The length of the sequence is the number of tokens in the caption. The tokens are encoded as integers, where the integer value of a token is the position of the token in the sequence. The integer value of a token is the same for all tokens in the sequence. The integer value of a token is the same for all tokens in the sequence. The integer value of a token is the same for all tokens in the sequence.\n\nAnswer B:\nEnglish. Explanation: FLOAT SELECTED: Figure 2: Selected visual features and activation heatmaps. The top row labels the dominant pattern for each feature.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.09070",
    "question": "What ad-hoc approaches are explored?",
    "gold_answer": "",
    "answer_a": "How do they compare to supervised baselines?\n< Explanation: We first introduce a novel figure-caption correspondence learning task that makes use of our observations. We train a figure-caption network that learns to map a figure to its caption, and then evaluate its performance on a large corpus of scientific figures and their corresponding captions. We show that this task can be solved by a simple combination of a figure and a caption, and that the resulting features are transferable to other tasks involving scientific text and figures. We then show that the figure-caption network can be trained to learn from a knowledge graph, and that this improves the performance of the figure-caption network.",
    "answer_b": "HolE. Explanation: Since graph-based KG embedding approaches like HolE only generate embeddings of the artifacts explicitly contained in the KG, this may indicate that Sensigrafo, the KG used in this task, provides a partial coverage of the scientific domain, as could be expected since we are using an off-the-shelf version. Deeper inspection shows that HolE only covers 20% of the lemmas in the SciGraph vocabulary. On the other hand, Vecsigrafo, trained on the same KG, also captures lexical information from the text corpora it is trained on, Wikipedia or SemScholar, raising lemma coverage to 42% and 47%, respectively.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat ad-hoc approaches are explored?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they compare to supervised baselines?\n< Explanation: We first introduce a novel figure-caption correspondence learning task that makes use of our observations. We train a figure-caption network that learns to map a figure to its caption, and then evaluate its performance on a large corpus of scientific figures and their corresponding captions. We show that this task can be solved by a simple combination of a figure and a caption, and that the resulting features are transferable to other tasks involving scientific text and figures. We then show that the figure-caption network can be trained to learn from a knowledge graph, and that this improves the performance of the figure-caption network.\n\nAnswer B:\nHolE. Explanation: Since graph-based KG embedding approaches like HolE only generate embeddings of the artifacts explicitly contained in the KG, this may indicate that Sensigrafo, the KG used in this task, provides a partial coverage of the scientific domain, as could be expected since we are using an off-the-shelf version. Deeper inspection shows that HolE only covers 20% of the lemmas in the SciGraph vocabulary. On the other hand, Vecsigrafo, trained on the same KG, also captures lexical information from the text corpora it is trained on, Wikipedia or SemScholar, raising lemma coverage to 42% and 47%, respectively.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.09070",
    "question": "What supervised baselines did they compare with?",
    "gold_answer": "",
    "answer_a": "direct combination. Explanation: The direct combination baseline computes the figure-caption correspondence through the scalar product between the softmax outputs of both networks.",
    "answer_b": "What is the performance of the baselines?\n< Explanation: We compare our model with the following baselines: (1) a baseline that uses the figure caption as a feature, (2) a baseline that uses the figure caption as a feature and the figure as a feature, (3) a baseline that uses the figure caption as a feature and the figure caption as a feature, (4) a baseline that uses the figure caption as a feature and the figure caption as a feature, (5) a baseline that uses the figure caption as a feature and the figure caption as a feature, (6) a baseline that uses the figure",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat supervised baselines did they compare with?\n\nGround Truth Answer:\n\n\nAnswer A:\ndirect combination. Explanation: The direct combination baseline computes the figure-caption correspondence through the scalar product between the softmax outputs of both networks.\n\nAnswer B:\nWhat is the performance of the baselines?\n< Explanation: We compare our model with the following baselines: (1) a baseline that uses the figure caption as a feature, (2) a baseline that uses the figure caption as a feature and the figure as a feature, (3) a baseline that uses the figure caption as a feature and the figure caption as a feature, (4) a baseline that uses the figure caption as a feature and the figure caption as a feature, (5) a baseline that uses the figure caption as a feature and the figure caption as a feature, (6) a baseline that uses the figure\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.09070",
    "question": "Where do their figure and captions come from?",
    "gold_answer": "",
    "answer_a": "How are they annotated?\n< Explanation: The figures are taken from the Open Science Framework (OSF) repository. The captions are taken from the Open Science Framework repository. The figures are annotated by the authors of the figures. The captions are annotated by the authors of the captions. The figures and captions are annotated by the authors of the figures and the authors of the captions. The authors of the figures and the authors of the captions are not the same person. The authors of the figures and the authors of the captions are not the same person. The authors of the figures and the authors of the captions are not the same person. The authors",
    "answer_b": "The Semantic Scholar corpus. Explanation: The Semantic Scholar corpus BIBREF21 (SemScholar) is a large dataset of scientific publications made available by AI2. From its 39M articles, we downloaded 3,3M PDFs (the rest were behind paywalls, did not have a link or it was broken) and extracted 12.5M figures and captions through PDFFigures2 BIBREF22.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhere do their figure and captions come from?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they annotated?\n< Explanation: The figures are taken from the Open Science Framework (OSF) repository. The captions are taken from the Open Science Framework repository. The figures are annotated by the authors of the figures. The captions are annotated by the authors of the captions. The figures and captions are annotated by the authors of the figures and the authors of the captions. The authors of the figures and the authors of the captions are not the same person. The authors of the figures and the authors of the captions are not the same person. The authors of the figures and the authors of the captions are not the same person. The authors\n\nAnswer B:\nThe Semantic Scholar corpus. Explanation: The Semantic Scholar corpus BIBREF21 (SemScholar) is a large dataset of scientific publications made available by AI2. From its 39M articles, we downloaded 3,3M PDFs (the rest were behind paywalls, did not have a link or it was broken) and extracted 12.5M figures and captions through PDFFigures2 BIBREF22.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.05521",
    "question": "what was the baseline?",
    "gold_answer": "",
    "answer_a": "What was the baseline?\nAnswer: The baseline is the model that was used for the WASSA 2017 Emotion Intensity Shared Task. The baseline model is a simple RNN that uses the emotion words as input and outputs the intensity. The baseline model was trained on the emotion words and the intensity words. The baseline model was trained on the emotion words and the intensity words. The baseline model was trained on the emotion words and the intensity words. The baseline model was trained on the emotion words and the intensity words. The baseline model was trained on the emotion words and the intensity words. The baseline model was trained on",
    "answer_b": "Weka baseline BIBREF5. Explanation: In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat was the baseline?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat was the baseline?\nAnswer: The baseline is the model that was used for the WASSA 2017 Emotion Intensity Shared Task. The baseline model is a simple RNN that uses the emotion words as input and outputs the intensity. The baseline model was trained on the emotion words and the intensity words. The baseline model was trained on the emotion words and the intensity words. The baseline model was trained on the emotion words and the intensity words. The baseline model was trained on the emotion words and the intensity words. The baseline model was trained on the emotion words and the intensity words. The baseline model was trained on\n\nAnswer B:\nWeka baseline BIBREF5. Explanation: In general, as Table TABREF13 shows, our intra-sentence attention RNN was able to outperform the Weka baseline BIBREF5 on the development dataset by a solid margin.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.05521",
    "question": "what was their result?",
    "gold_answer": "",
    "answer_a": "Explanation: The results of the model are shown in Table 1. The model achieved the best performance of 13th place among 22 shared task competitors. The results are obtained by using the same model as in the previous paper (BIBREF10) and the same dataset (BIBREF11). The model is trained on 10000 training examples and tested on 10000 test examples. The model is trained with a batch size of 100 and the learning rate is set to 0.001. The model is trained for 100 epochs",
    "answer_b": "Pearson correlation on sadness test data is 0.52, on joy test data is .537, on anger test data is 0.47, on fear data is 0.561.. Explanation: To evaluate our model, we wrapped the provided scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat was their result?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The results of the model are shown in Table 1. The model achieved the best performance of 13th place among 22 shared task competitors. The results are obtained by using the same model as in the previous paper (BIBREF10) and the same dataset (BIBREF11). The model is trained on 10000 training examples and tested on 10000 test examples. The model is trained with a batch size of 100 and the learning rate is set to 0.001. The model is trained for 100 epochs\n\nAnswer B:\nPearson correlation on sadness test data is 0.52, on joy test data is .537, on anger test data is 0.47, on fear data is 0.561.. Explanation: To evaluate our model, we wrapped the provided scripts for the shared task and calculated the Pearson correlation coefficient and the Spearman rank coefficient with the gold standard in the validation set, as well as the same values over a subset of the same data formed by taking every instance with a gold emotion intensity score greater than or equal to 0.5.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.05521",
    "question": "what dataset was used?",
    "gold_answer": "",
    "answer_a": "training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger. Explanation: To test our model, we experiment using the training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger.",
    "answer_b": "Explanation: The dataset used in this paper is the WASSA 2017 Emotion Intensity Shared Task. The dataset is available at https://github.com/alexander-kravets/wassatask. The dataset contains 10000 sentences, each of which is labeled with one of 10 emotions. The dataset is split into training, validation and test sets. The training set contains 7000 sentences, the validation set contains 2000 sentences and the test set contains 2000 sentences. The dataset is available at https://github.com/alexander-",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat dataset was used?\n\nGround Truth Answer:\n\n\nAnswer A:\ntraining, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger. Explanation: To test our model, we experiment using the training, validation and test datasets provided for the shared task BIBREF5 , which include tweets for four emotions: joy, sadness, fear, and anger.\n\nAnswer B:\nExplanation: The dataset used in this paper is the WASSA 2017 Emotion Intensity Shared Task. The dataset is available at https://github.com/alexander-kravets/wassatask. The dataset contains 10000 sentences, each of which is labeled with one of 10 emotions. The dataset is split into training, validation and test sets. The training set contains 7000 sentences, the validation set contains 2000 sentences and the test set contains 2000 sentences. The dataset is available at https://github.com/alexander-\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.11049",
    "question": "What is their definition of hate speech?",
    "gold_answer": "",
    "answer_a": "Explanation: Hate speech is a type of speech that expresses a negative attitude towards a group of people or a group of groups. It is a form of discrimination that is often used to target a specific group of people, such as race, religion, gender, or sexual orientation. Hate speech can be expressed in various forms, such as words, phrases, or actions, and it can be directed towards individuals, groups, or entire communities. Hate speech can be motivated by a range of factors, including prejudice, ignorance, and ignorance, and it can be used to target individuals or groups in a variety of ways, such as physical violence, harassment",
    "answer_b": "rely on the general public opinion and common linguistic knowledge to assess how people view and react to hate speech. Explanation: We rely on the general public opinion and common linguistic knowledge to assess how people view and react to hate speech.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is their definition of hate speech?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: Hate speech is a type of speech that expresses a negative attitude towards a group of people or a group of groups. It is a form of discrimination that is often used to target a specific group of people, such as race, religion, gender, or sexual orientation. Hate speech can be expressed in various forms, such as words, phrases, or actions, and it can be directed towards individuals, groups, or entire communities. Hate speech can be motivated by a range of factors, including prejudice, ignorance, and ignorance, and it can be used to target individuals or groups in a variety of ways, such as physical violence, harassment\n\nAnswer B:\nrely on the general public opinion and common linguistic knowledge to assess how people view and react to hate speech. Explanation: We rely on the general public opinion and common linguistic knowledge to assess how people view and react to hate speech.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.11049",
    "question": "What languages does the new dataset contain?",
    "gold_answer": "",
    "answer_a": "Explanation: The Hate Speech Dataset contains 1000 hate speech samples in 10 languages. The dataset is divided into 1000 hate speech samples in 10 languages. The dataset is divided into 1000 hate speech samples in 10 languages. The dataset is divided into 1000 hate speech samples in 10 languages. The dataset is divided into 1000 hate speech samples in 10 languages. The dataset is divided into 1000 hate speech samples in 10 languages. The dataset is divided into 1000 hate speech",
    "answer_b": "English. Explanation: Our dataset is the first trilingual dataset comprising English, French, and Arabic tweets that encompasses various targets and hostility types.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat languages does the new dataset contain?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The Hate Speech Dataset contains 1000 hate speech samples in 10 languages. The dataset is divided into 1000 hate speech samples in 10 languages. The dataset is divided into 1000 hate speech samples in 10 languages. The dataset is divided into 1000 hate speech samples in 10 languages. The dataset is divided into 1000 hate speech samples in 10 languages. The dataset is divided into 1000 hate speech samples in 10 languages. The dataset is divided into 1000 hate speech\n\nAnswer B:\nEnglish. Explanation: Our dataset is the first trilingual dataset comprising English, French, and Arabic tweets that encompasses various targets and hostility types.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.11049",
    "question": "What aspects are considered?",
    "gold_answer": "",
    "answer_a": "(a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments. Explanation: We believe that, in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis. Hence, our annotations indicate (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments.",
    "answer_b": "Explanation: We consider the following aspects: (1) the presence of hate speech, (2) the hate speech content, (3) the hate speech sentiment, (4) the hate speech context, and (5) the hate speech language. We also consider the following aspects: (1) the presence of hate speech, (2) the hate speech content, (3) the hate speech sentiment, (4) the hate speech context, and (5) the hate speech language. We also consider the following aspects: (1) the presence of hate speech, (2) the hate speech content, (3) the hate",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat aspects are considered?\n\nGround Truth Answer:\n\n\nAnswer A:\n(a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments. Explanation: We believe that, in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis. Hence, our annotations indicate (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments.\n\nAnswer B:\nExplanation: We consider the following aspects: (1) the presence of hate speech, (2) the hate speech content, (3) the hate speech sentiment, (4) the hate speech context, and (5) the hate speech language. We also consider the following aspects: (1) the presence of hate speech, (2) the hate speech content, (3) the hate speech sentiment, (4) the hate speech context, and (5) the hate speech language. We also consider the following aspects: (1) the presence of hate speech, (2) the hate speech content, (3) the hate\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.11049",
    "question": "How big is their dataset?",
    "gold_answer": "",
    "answer_a": "13 000 tweets. Explanation: We use Amazon Mechanical Turk to label around 13,000 potentially derogatory tweets in English, French, and Arabic based on the above mentioned aspects and, regard each aspect as a prediction task.",
    "answer_b": "What are the languages in it?\n< Explanation: We collected hate speech data from the Hate Speech Dataset (HSD) BIBREF11 . The HSD dataset contains 10,000 hate speech samples from 10 languages. The dataset is divided into 1000 samples per language. We used the Hate Speech Dataset BIBREF11 to create a new dataset of hate speech samples from 10 languages. The dataset contains 10,000 hate speech samples from 10 languages. The dataset is divided into 1000 samples per language. The dataset is available",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big is their dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\n13 000 tweets. Explanation: We use Amazon Mechanical Turk to label around 13,000 potentially derogatory tweets in English, French, and Arabic based on the above mentioned aspects and, regard each aspect as a prediction task.\n\nAnswer B:\nWhat are the languages in it?\n< Explanation: We collected hate speech data from the Hate Speech Dataset (HSD) BIBREF11 . The HSD dataset contains 10,000 hate speech samples from 10 languages. The dataset is divided into 1000 samples per language. We used the Hate Speech Dataset BIBREF11 to create a new dataset of hate speech samples from 10 languages. The dataset contains 10,000 hate speech samples from 10 languages. The dataset is divided into 1000 samples per language. The dataset is available\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.10676",
    "question": "What are the opportunities presented by the use of Semantic Web technologies in Machine Translation?",
    "gold_answer": "",
    "answer_a": "Explanation: The use of Semantic Web technologies in Machine Translation (MTR) has the potential to improve the quality of translations by providing a richer and more accurate representation of the meaning of the source language text. This is achieved by using the Semantic Web to encode the meaning of the source text in a way that is consistent with the target language. This is achieved by using the Semantic Web to encode the meaning of the source text in a way that is consistent with the target language. This is achieved by using the Semantic Web to encode the meaning of the source text in a way that is consistent with the target language. This is achieved by using the",
    "answer_b": "disambiguation. Explanation: SW has already shown its capability for semantic disambiguation of polysemous and homonymous words.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the opportunities presented by the use of Semantic Web technologies in Machine Translation?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The use of Semantic Web technologies in Machine Translation (MTR) has the potential to improve the quality of translations by providing a richer and more accurate representation of the meaning of the source language text. This is achieved by using the Semantic Web to encode the meaning of the source text in a way that is consistent with the target language. This is achieved by using the Semantic Web to encode the meaning of the source text in a way that is consistent with the target language. This is achieved by using the Semantic Web to encode the meaning of the source text in a way that is consistent with the target language. This is achieved by using the\n\nAnswer B:\ndisambiguation. Explanation: SW has already shown its capability for semantic disambiguation of polysemous and homonymous words.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.10676",
    "question": "What are the challenges associated with the use of Semantic Web technologies in Machine Translation?",
    "gold_answer": "",
    "answer_a": "Please summarize in a sentence.\nThe challenges associated with the use of Semantic Web technologies in Machine Translation are: (1) the lack of a unified standard for the Semantic Web; (2) the lack of a unified standard for the Semantic Web; (3) the lack of a unified standard for the Semantic Web; (4) the lack of a unified standard for the Semantic Web; (5) the lack of a unified standard for the Semantic Web; (6) the lack of a unified standard for the Semantic Web; (7) the lack of a unified standard for the Semantic Web; (8) the lack of a unified standard",
    "answer_b": "syntactic disambiguation problem which as yet lacks good solutions. Explanation: On the other hand, there is also a syntactic disambiguation problem which as yet lacks good solutions.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the challenges associated with the use of Semantic Web technologies in Machine Translation?\n\nGround Truth Answer:\n\n\nAnswer A:\nPlease summarize in a sentence.\nThe challenges associated with the use of Semantic Web technologies in Machine Translation are: (1) the lack of a unified standard for the Semantic Web; (2) the lack of a unified standard for the Semantic Web; (3) the lack of a unified standard for the Semantic Web; (4) the lack of a unified standard for the Semantic Web; (5) the lack of a unified standard for the Semantic Web; (6) the lack of a unified standard for the Semantic Web; (7) the lack of a unified standard for the Semantic Web; (8) the lack of a unified standard\n\nAnswer B:\nsyntactic disambiguation problem which as yet lacks good solutions. Explanation: On the other hand, there is also a syntactic disambiguation problem which as yet lacks good solutions.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.10676",
    "question": "What are the other obstacles to automatic translations which are not mentioned in the abstract?",
    "gold_answer": "",
    "answer_a": "Excessive focus on English and European languages. Explanation: (1) Excessive focus on English and European languages as one of the involved languages in MT approaches and poor research on low-resource language pairs such as African and/or South American languages. (2) The limitations of SMT approaches for translating across domains. Most MT systems exhibit good performance on law and the legislative domains due to the large amount of data provided by the European Union. In contrast, translations performed on sports and life-hacks commonly fail, because of the lack of training data. (3) How to translate the huge amount of data from social networks that uniquely deal with no-standard speech texts from users (e.g., tweets). (4) The difficult translations among morphologically rich languages. This challenge shares the same problem with the first one, namely that most research work focuses on English as one of the involved languages. Therefore, MT systems which translate content between, for instance, Arabic and Spanish are rare. (5) For the speech translation task, the parallel data for training differs widely from real user speech.",
    "answer_b": "Please provide references.\n<ref name=review>\nDefinition: Semantic Web technologies\nSemantic Web technologies are a set of technologies that enable the creation of a semantic web, which is a web of interconnected data that can be used to support the creation of knowledge bases and to facilitate the exchange of information. Semantic Web technologies are used to create a semantic web by using the Web of Data (WOD) to store and exchange data. The WOD is a network of interconnected data that is created by the Semantic Web Alliance (SNA). The WOD is a network of interconnected data that is created by the Semantic Web Alliance (SNA). The",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the other obstacles to automatic translations which are not mentioned in the abstract?\n\nGround Truth Answer:\n\n\nAnswer A:\nExcessive focus on English and European languages. Explanation: (1) Excessive focus on English and European languages as one of the involved languages in MT approaches and poor research on low-resource language pairs such as African and/or South American languages. (2) The limitations of SMT approaches for translating across domains. Most MT systems exhibit good performance on law and the legislative domains due to the large amount of data provided by the European Union. In contrast, translations performed on sports and life-hacks commonly fail, because of the lack of training data. (3) How to translate the huge amount of data from social networks that uniquely deal with no-standard speech texts from users (e.g., tweets). (4) The difficult translations among morphologically rich languages. This challenge shares the same problem with the first one, namely that most research work focuses on English as one of the involved languages. Therefore, MT systems which translate content between, for instance, Arabic and Spanish are rare. (5) For the speech translation task, the parallel data for training differs widely from real user speech.\n\nAnswer B:\nPlease provide references.\n<ref name=review>\nDefinition: Semantic Web technologies\nSemantic Web technologies are a set of technologies that enable the creation of a semantic web, which is a web of interconnected data that can be used to support the creation of knowledge bases and to facilitate the exchange of information. Semantic Web technologies are used to create a semantic web by using the Web of Data (WOD) to store and exchange data. The WOD is a network of interconnected data that is created by the Semantic Web Alliance (SNA). The WOD is a network of interconnected data that is created by the Semantic Web Alliance (SNA). The\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.08871",
    "question": "what eeg features were used?",
    "gold_answer": "",
    "answer_a": "We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0. Explanation: EEG signals were sampled at 1000Hz and a fourth order IIR band pass filter with cut off frequencies 0.1Hz and 70Hz was applied. A notch filter with cut off frequency 60 Hz was used to remove the power line noise. EEGlab's BIBREF17 Independent component analysis (ICA) toolbox was used to remove other biological signal artifacts like electrocardiography (ECG), electromyography (EMG), electrooculography (EOG) etc from the EEG signals. We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0 . So in total we extracted 31(channels) X 5 or 155 features for EEG signals.The EEG features were extracted at a sampling frequency of 100Hz for each EEG channel.\n\nWe used spectral entropy because it captures the spectral ( frequency domain) and signal complexity information of EEG. It is also a widely used feature in EEG signal analysis BIBREF18 . Similarly zero crossing rate was chosen as it is a commonly used feature both for speech recognition and bio signal analysis. Remaining features were chosen to capture time domain statistical information. We performed lot of experiments to identify this set of features. Initially we used only spectral entropy and zero crossing rate but we noticed that the performance of the ASR system significantly went up by 20 % when we added the remaining additional features.\n\nThe recorded speech signal was sampled at 16KHz frequency. We extracted Mel-frequency cepstrum (MFCC) as features for speech signal. We first extracted MFCC 13 features and then computed first and second order differentials (delta and delta-delta) thus having total MFCC 39 features. The MFCC features were also sampled at 100Hz same as the sampling frequency of EEG features to avoid seq2seq problem.",
    "answer_b": "Explanation: The EEG signals were collected from the left and right earlobes of the subjects. The EEG signals were recorded using a 128 channel EEG amplifier (model: 128-128) with a sampling rate of 1000 Hz. The EEG signals were preprocessed using a Fast Fourier Transform (FFT) and a Butterworth low pass filter. The preprocessed EEG signals were then normalized to a range of -1 to 1. The normalized EEG signals were then fed into the attention model and connectionist temporal classification (CTC) model for performing the recognition. The EEG signals were also",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat eeg features were used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWe extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0. Explanation: EEG signals were sampled at 1000Hz and a fourth order IIR band pass filter with cut off frequencies 0.1Hz and 70Hz was applied. A notch filter with cut off frequency 60 Hz was used to remove the power line noise. EEGlab's BIBREF17 Independent component analysis (ICA) toolbox was used to remove other biological signal artifacts like electrocardiography (ECG), electromyography (EMG), electrooculography (EOG) etc from the EEG signals. We extracted five statistical features for EEG, namely root mean square, zero crossing rate,moving window average,kurtosis and power spectral entropy BIBREF0 . So in total we extracted 31(channels) X 5 or 155 features for EEG signals.The EEG features were extracted at a sampling frequency of 100Hz for each EEG channel.\n\nWe used spectral entropy because it captures the spectral ( frequency domain) and signal complexity information of EEG. It is also a widely used feature in EEG signal analysis BIBREF18 . Similarly zero crossing rate was chosen as it is a commonly used feature both for speech recognition and bio signal analysis. Remaining features were chosen to capture time domain statistical information. We performed lot of experiments to identify this set of features. Initially we used only spectral entropy and zero crossing rate but we noticed that the performance of the ASR system significantly went up by 20 % when we added the remaining additional features.\n\nThe recorded speech signal was sampled at 16KHz frequency. We extracted Mel-frequency cepstrum (MFCC) as features for speech signal. We first extracted MFCC 13 features and then computed first and second order differentials (delta and delta-delta) thus having total MFCC 39 features. The MFCC features were also sampled at 100Hz same as the sampling frequency of EEG features to avoid seq2seq problem.\n\nAnswer B:\nExplanation: The EEG signals were collected from the left and right earlobes of the subjects. The EEG signals were recorded using a 128 channel EEG amplifier (model: 128-128) with a sampling rate of 1000 Hz. The EEG signals were preprocessed using a Fast Fourier Transform (FFT) and a Butterworth low pass filter. The preprocessed EEG signals were then normalized to a range of -1 to 1. The normalized EEG signals were then fed into the attention model and connectionist temporal classification (CTC) model for performing the recognition. The EEG signals were also\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.08871",
    "question": "what dataset was used?",
    "gold_answer": "",
    "answer_a": "two types of simultaneous speech EEG recording databases. Explanation: We built two types of simultaneous speech EEG recording databases for this work. For database A five female and five male subjects took part in the experiment. For database B five male and three female subjects took part in the experiment. Except two subjects, rest all were native English speakers for both the databases. All subjects were UT Austin undergraduate,graduate students in their early twenties.",
    "answer_b": "Explanation: The dataset used for the experiments is the 2016-2017 National English Test (NET) dataset. The NET is a national English test conducted by the National Council of Educational Research and Training (NCERT) in 2016-2017. The NET is a national test conducted by NCERT to assess the English language proficiency of the students of the 10th standard of the 11th standard of the 12th standard of the 13th standard of the 14th standard of the 15th standard of the 16th",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat dataset was used?\n\nGround Truth Answer:\n\n\nAnswer A:\ntwo types of simultaneous speech EEG recording databases. Explanation: We built two types of simultaneous speech EEG recording databases for this work. For database A five female and five male subjects took part in the experiment. For database B five male and three female subjects took part in the experiment. Except two subjects, rest all were native English speakers for both the databases. All subjects were UT Austin undergraduate,graduate students in their early twenties.\n\nAnswer B:\nExplanation: The dataset used for the experiments is the 2016-2017 National English Test (NET) dataset. The NET is a national English test conducted by the National Council of Educational Research and Training (NCERT) in 2016-2017. The NET is a national test conducted by NCERT to assess the English language proficiency of the students of the 10th standard of the 11th standard of the 12th standard of the 13th standard of the 14th standard of the 15th standard of the 16th\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.04124",
    "question": "On which datasets does LadaBERT achieve state-of-the-art?",
    "gold_answer": "",
    "answer_a": "MNLI-m, MNLI-mm, SST-2, QQP, QNLI. Explanation: FLOAT SELECTED: Table 3: Performance comparison on various model sizes",
    "answer_b": "What are the training overheads?\n< Explanation: We evaluate LadaBERT on the following datasets: BERT-base, BERT-large, BERT-small, BERT-medium, BERT-large-medium, BERT-large-small, BERT-small-medium, BERT-small-large, BERT-large-small, BERT-large-medium, BERT-large-small, BERT-large-medium, BERT-large-small, BERT-large-medium, BERT-large-small, BERT-large-medium, BERT-large-small, BERT-large-medium, BERT-large-small, BERT-large-medium, BERT-large-small, BERT-large-medium, BERT",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nOn which datasets does LadaBERT achieve state-of-the-art?\n\nGround Truth Answer:\n\n\nAnswer A:\nMNLI-m, MNLI-mm, SST-2, QQP, QNLI. Explanation: FLOAT SELECTED: Table 3: Performance comparison on various model sizes\n\nAnswer B:\nWhat are the training overheads?\n< Explanation: We evaluate LadaBERT on the following datasets: BERT-base, BERT-large, BERT-small, BERT-medium, BERT-large-medium, BERT-large-small, BERT-small-medium, BERT-small-large, BERT-large-small, BERT-large-medium, BERT-large-small, BERT-large-medium, BERT-large-small, BERT-large-medium, BERT-large-small, BERT-large-medium, BERT-large-small, BERT-large-medium, BERT-large-small, BERT-large-medium, BERT-large-small, BERT-large-medium, BERT\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.07252",
    "question": "What domain of text are they working with?",
    "gold_answer": "",
    "answer_a": "How large is the corpus?\n< Explanation: We use the following datasets: (i) the Wikipedia article summarization dataset (Wikipedia-Summary) BIBREF1 , which contains 10,000 documents and 10,000 summary pairs. (ii) the Wikipedia article summarization dataset (Wikipedia-Summary-2) BIBREF2 , which contains 10,000 documents and 10,000 summary pairs. (iii) the Wikipedia article summarization dataset (Wikipedia-Summary-3) BIBREF3 , which contains 1",
    "answer_b": "news articles. Explanation: We evaluate our models both automatically (in terms of Rouge) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat domain of text are they working with?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow large is the corpus?\n< Explanation: We use the following datasets: (i) the Wikipedia article summarization dataset (Wikipedia-Summary) BIBREF1 , which contains 10,000 documents and 10,000 summary pairs. (ii) the Wikipedia article summarization dataset (Wikipedia-Summary-2) BIBREF2 , which contains 10,000 documents and 10,000 summary pairs. (iii) the Wikipedia article summarization dataset (Wikipedia-Summary-3) BIBREF3 , which contains 1\n\nAnswer B:\nnews articles. Explanation: We evaluate our models both automatically (in terms of Rouge) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.07252",
    "question": "What dataset do they use?",
    "gold_answer": "",
    "answer_a": "DUC 2002 document summarization corpus. Explanation: To overcome the paucity of annotated data for training, we adopt a methodology similar to hermann2015teaching and create two large-scale datasets, one for sentence extraction and another one for word extraction.\n\nIn a nutshell, we retrieved hundreds of thousands of news articles and their corresponding highlights from DailyMail (see Figure 1 for an example).",
    "answer_b": "How large is it?\n< Explanation: We use the following datasets: (i) the Wikipedia dataset (Wikipedia) which contains 100k documents and 10k summaries. (ii) the Stanford Sentiment Treebank (SST) which contains 100k documents and 10k summaries. (iii) the Stanford Sentiment Treebank with English (SST-2) which contains 100k documents and 10k summaries. (iv) the Stanford Sentiment Treebank with Chinese (SST-3) which contains 100k documents and 10",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat dataset do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nDUC 2002 document summarization corpus. Explanation: To overcome the paucity of annotated data for training, we adopt a methodology similar to hermann2015teaching and create two large-scale datasets, one for sentence extraction and another one for word extraction.\n\nIn a nutshell, we retrieved hundreds of thousands of news articles and their corresponding highlights from DailyMail (see Figure 1 for an example).\n\nAnswer B:\nHow large is it?\n< Explanation: We use the following datasets: (i) the Wikipedia dataset (Wikipedia) which contains 100k documents and 10k summaries. (ii) the Stanford Sentiment Treebank (SST) which contains 100k documents and 10k summaries. (iii) the Stanford Sentiment Treebank with English (SST-2) which contains 100k documents and 10k summaries. (iv) the Stanford Sentiment Treebank with Chinese (SST-3) which contains 100k documents and 10\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.00549",
    "question": "What types of commonsense knowledge are they talking about?",
    "gold_answer": "",
    "answer_a": "hypernym relations. Explanation: In this work, we use the ConceptNet BIBREF3 , WordNet BIBREF2 , and Microsoft Concept Graph (MCG) BIBREF11 , BIBREF12 knowledge bases for our ontology prediction experiments.",
    "answer_b": "How do they measure their performance?\n<blockquote>\nWe evaluate our models on the commonsense knowledge representation task, which is a subset of the Knowledge Representation Challenge (KRC) 2015 dataset. The task is to predict the commonsense knowledge represented by a given set of relations and entities. We use the following evaluation metric: the average F1 score of the top 100 predictions. We also use the following evaluation metric: the average F1 score of the top 100 predictions for each of the 1000 training examples. We report the average of these two metrics over all 10",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat types of commonsense knowledge are they talking about?\n\nGround Truth Answer:\n\n\nAnswer A:\nhypernym relations. Explanation: In this work, we use the ConceptNet BIBREF3 , WordNet BIBREF2 , and Microsoft Concept Graph (MCG) BIBREF11 , BIBREF12 knowledge bases for our ontology prediction experiments.\n\nAnswer B:\nHow do they measure their performance?\n<blockquote>\nWe evaluate our models on the commonsense knowledge representation task, which is a subset of the Knowledge Representation Challenge (KRC) 2015 dataset. The task is to predict the commonsense knowledge represented by a given set of relations and entities. We use the following evaluation metric: the average F1 score of the top 100 predictions. We also use the following evaluation metric: the average F1 score of the top 100 predictions for each of the 1000 training examples. We report the average of these two metrics over all 10\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.00549",
    "question": "What do they mean by intrinsic geometry of spaces of learned representations?",
    "gold_answer": "",
    "answer_a": "In these models, the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than the local relation predictions of previous models.. Explanation: Recently, a thread of research on representation learning has aimed to create embedding spaces that automatically enforce consistency in these predictions using the intrinsic geometry of the embedding space BIBREF9 , BIBREF0 , BIBREF10 . In these models, the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than the local relation predictions of previous models.",
    "answer_b": "How do they define it?\n< Explanation: The intrinsic geometry of spaces of learned representations is defined as the geometry of the space of learned representations, which is the set of all possible embeddings. The space of learned representations is a subset of the space of all possible embeddings. The space of learned representations is a subset of the space of all possible embeddings. The space of learned representations is a subset of the space of all possible embeddings. The space of learned representations is a subset of the space of all possible embeddings. The space of learned representations is a subset of the space of all possible embeddings. The space of learned representations is a subset of the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat do they mean by intrinsic geometry of spaces of learned representations?\n\nGround Truth Answer:\n\n\nAnswer A:\nIn these models, the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than the local relation predictions of previous models.. Explanation: Recently, a thread of research on representation learning has aimed to create embedding spaces that automatically enforce consistency in these predictions using the intrinsic geometry of the embedding space BIBREF9 , BIBREF0 , BIBREF10 . In these models, the inferred embedding space creates a globally consistent structured prediction of the ontology, rather than the local relation predictions of previous models.\n\nAnswer B:\nHow do they define it?\n< Explanation: The intrinsic geometry of spaces of learned representations is defined as the geometry of the space of learned representations, which is the set of all possible embeddings. The space of learned representations is a subset of the space of all possible embeddings. The space of learned representations is a subset of the space of all possible embeddings. The space of learned representations is a subset of the space of all possible embeddings. The space of learned representations is a subset of the space of all possible embeddings. The space of learned representations is a subset of the space of all possible embeddings. The space of learned representations is a subset of the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.00472",
    "question": "What were the most salient features extracted by the models?",
    "gold_answer": "",
    "answer_a": "unigrams and bigrams. Explanation: We extract word unigrams and bigrams.",
    "answer_b": "What were the results of the evaluation?\n<blockquote>Our results show that the models are able to extract the most salient features from the text. The models are able to extract the most salient features from the text in both English and Spanish. The models are able to extract the most salient features from the text in both English and Spanish. The models are able to extract the most salient features from the text in both English and Spanish. The models are able to extract the most salient features from the text in both English and Spanish. The models are able to extract the most salient features from the text in both English and",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat were the most salient features extracted by the models?\n\nGround Truth Answer:\n\n\nAnswer A:\nunigrams and bigrams. Explanation: We extract word unigrams and bigrams.\n\nAnswer B:\nWhat were the results of the evaluation?\n<blockquote>Our results show that the models are able to extract the most salient features from the text. The models are able to extract the most salient features from the text in both English and Spanish. The models are able to extract the most salient features from the text in both English and Spanish. The models are able to extract the most salient features from the text in both English and Spanish. The models are able to extract the most salient features from the text in both English and Spanish. The models are able to extract the most salient features from the text in both English and\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.00472",
    "question": "How many languages are in the dataset?",
    "gold_answer": "",
    "answer_a": "What are the features extracted?\nFor the English dataset, we used the following features: 1) sentiment polarity (positive, negative, neutral) 2) emotion (anger, disgust, fear, happiness, sadness, surprise, and trust) 3) emotion intensity (low, medium, high) 4) emotion duration (short, medium, long) 5) emotion type (anger, disgust, fear, happiness, sadness, trust) 6) emotion intensity (low, medium, high) 7) emotion duration (short, medium, long) 8) emotion type (anger, disgust, fear, happiness",
    "answer_b": "2. Explanation: Training data provided for the task included documents were collected from social media, SMS, news articles, and news wires. This consisted of 76 documents in English and 47 in Spanish.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many languages are in the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the features extracted?\nFor the English dataset, we used the following features: 1) sentiment polarity (positive, negative, neutral) 2) emotion (anger, disgust, fear, happiness, sadness, surprise, and trust) 3) emotion intensity (low, medium, high) 4) emotion duration (short, medium, long) 5) emotion type (anger, disgust, fear, happiness, sadness, trust) 6) emotion intensity (low, medium, high) 7) emotion duration (short, medium, long) 8) emotion type (anger, disgust, fear, happiness\n\nAnswer B:\n2. Explanation: Training data provided for the task included documents were collected from social media, SMS, news articles, and news wires. This consisted of 76 documents in English and 47 in Spanish.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.02866",
    "question": "What are the parts of the \"multimodal\" resources?",
    "gold_answer": "",
    "answer_a": "spatial organisation. Explanation: From the perspective of computational processing, diagrammatic representations present a formidable challenge, as they involve tasks from both computer vision and natural language processing. On the one hand, diagrams have a spatial organisation \u2013 layout \u2013 which needs to be segmented to identify meaningful units and their position. Making sense of how diagrams exploit the 2D layout space falls arguably within the domain of computer vision. On the other hand, diagrams also have a discourse structure, which uses the layout space to set up discourse relations between instances of natural language, various types of images, arrows and lines, thus forming a unified discourse organisation.",
    "answer_b": "How are they represented?\n<blockquote>Figure 1: Diagrams from the two resources. The first diagram is from the diagram resource, and the second diagram is from the diagram resource. The diagrams are from the same topic, but the diagrams are from different resources. The diagrams are from the same topic, but the diagrams are from different resources. The diagrams are from the same topic, but the diagrams are from different resources. The diagrams are from the same topic, but the diagrams are from different resources. The diagrams are from the same topic, but the diagrams are from different resources. The diagrams are from the same topic, but the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the parts of the \"multimodal\" resources?\n\nGround Truth Answer:\n\n\nAnswer A:\nspatial organisation. Explanation: From the perspective of computational processing, diagrammatic representations present a formidable challenge, as they involve tasks from both computer vision and natural language processing. On the one hand, diagrams have a spatial organisation \u2013 layout \u2013 which needs to be segmented to identify meaningful units and their position. Making sense of how diagrams exploit the 2D layout space falls arguably within the domain of computer vision. On the other hand, diagrams also have a discourse structure, which uses the layout space to set up discourse relations between instances of natural language, various types of images, arrows and lines, thus forming a unified discourse organisation.\n\nAnswer B:\nHow are they represented?\n<blockquote>Figure 1: Diagrams from the two resources. The first diagram is from the diagram resource, and the second diagram is from the diagram resource. The diagrams are from the same topic, but the diagrams are from different resources. The diagrams are from the same topic, but the diagrams are from different resources. The diagrams are from the same topic, but the diagrams are from different resources. The diagrams are from the same topic, but the diagrams are from different resources. The diagrams are from the same topic, but the diagrams are from different resources. The diagrams are from the same topic, but the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.02866",
    "question": "Are annotators familiar with the science topics annotated?",
    "gold_answer": "",
    "answer_a": "How do they annotate diagrams?\n<blockquote>Figure 1: Diagrams annotated by a trained expert. The diagram is labeled with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic",
    "answer_b": "The annotation for AI2D was\ncreated by crowd-sourced non-expert annotators on AMT while AI2D-RST covers a subset of diagrams from AI2D annotated by trained experts. Explanation: The annotation for AI2D, which includes layout segmentations for the diagram images, DPGs and a multiple choice question-answer set, was created by crowd-sourced non-expert annotators on Amazon Mechanical Turk BIBREF10.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nAre annotators familiar with the science topics annotated?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they annotate diagrams?\n<blockquote>Figure 1: Diagrams annotated by a trained expert. The diagram is labeled with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic it represents. The diagram is annotated with the science topic\n\nAnswer B:\nThe annotation for AI2D was\ncreated by crowd-sourced non-expert annotators on AMT while AI2D-RST covers a subset of diagrams from AI2D annotated by trained experts. Explanation: The annotation for AI2D, which includes layout segmentations for the diagram images, DPGs and a multiple choice question-answer set, was created by crowd-sourced non-expert annotators on Amazon Mechanical Turk BIBREF10.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.02866",
    "question": "How are the expert and crowd-sourced annotations compared to one another?",
    "gold_answer": "",
    "answer_a": "by using them as features in classifying diagrams and\ntheir parts using various graph neural networks.. Explanation: This section presents two experiments that compare AI2D and AI2D-RST annotations in classifying diagrams and their parts using various graph neural networks.\n\nExperiments ::: Graph Neural Networks\nI evaluated the following graph neural network architectures for both graph and node classification tasks:\n\nGraph Convolutional Network (GCN) BIBREF25\n\nSimplifying Graph Convolution (SGC) BIBREF26, averaging incoming node features from up to 2 hops away\n\nGraph Attention Network (GAT) BIBREF27 with 2 heads",
    "answer_b": "What is the difference between the two?\n< Explanation: The expert annotations are based on the annotation schema of the diagram, which is a set of rules that describe the diagram elements. The diagram elements are then annotated with the corresponding annotation schema. The diagram elements are then annotated with the corresponding annotation schema. The diagram elements are then annotated with the corresponding annotation schema. The diagram elements are then annotated with the corresponding annotation schema. The diagram elements are then annotated with the corresponding annotation schema. The diagram elements are then annotated with the corresponding annotation schema. The diagram elements are then annotated with the corresponding annotation schema. The diagram elements are then annotated with the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are the expert and crowd-sourced annotations compared to one another?\n\nGround Truth Answer:\n\n\nAnswer A:\nby using them as features in classifying diagrams and\ntheir parts using various graph neural networks.. Explanation: This section presents two experiments that compare AI2D and AI2D-RST annotations in classifying diagrams and their parts using various graph neural networks.\n\nExperiments ::: Graph Neural Networks\nI evaluated the following graph neural network architectures for both graph and node classification tasks:\n\nGraph Convolutional Network (GCN) BIBREF25\n\nSimplifying Graph Convolution (SGC) BIBREF26, averaging incoming node features from up to 2 hops away\n\nGraph Attention Network (GAT) BIBREF27 with 2 heads\n\nAnswer B:\nWhat is the difference between the two?\n< Explanation: The expert annotations are based on the annotation schema of the diagram, which is a set of rules that describe the diagram elements. The diagram elements are then annotated with the corresponding annotation schema. The diagram elements are then annotated with the corresponding annotation schema. The diagram elements are then annotated with the corresponding annotation schema. The diagram elements are then annotated with the corresponding annotation schema. The diagram elements are then annotated with the corresponding annotation schema. The diagram elements are then annotated with the corresponding annotation schema. The diagram elements are then annotated with the corresponding annotation schema. The diagram elements are then annotated with the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.02866",
    "question": "What platform do the crowd-sourced workers come from?",
    "gold_answer": "",
    "answer_a": "How do they annotate the diagrams?\n< Explanation: The diagram annotators are trained on a corpus of diagrams from the Natural Science Diagrams corpus (NDS) [1]. The corpus contains 1,000 diagrams, each of which is annotated with a label indicating the type of diagram it is. The diagram annotators are trained to label diagrams with the following labels: diagram, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type",
    "answer_b": "Amazon Mechanical Turk. Explanation: The annotation for AI2D, which includes layout segmentations for the diagram images, DPGs and a multiple choice question-answer set, was created by crowd-sourced non-expert annotators on Amazon Mechanical Turk BIBREF10.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat platform do the crowd-sourced workers come from?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they annotate the diagrams?\n< Explanation: The diagram annotators are trained on a corpus of diagrams from the Natural Science Diagrams corpus (NDS) [1]. The corpus contains 1,000 diagrams, each of which is annotated with a label indicating the type of diagram it is. The diagram annotators are trained to label diagrams with the following labels: diagram, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type, diagram type\n\nAnswer B:\nAmazon Mechanical Turk. Explanation: The annotation for AI2D, which includes layout segmentations for the diagram images, DPGs and a multiple choice question-answer set, was created by crowd-sourced non-expert annotators on Amazon Mechanical Turk BIBREF10.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.02866",
    "question": "Who are considered trained experts?",
    "gold_answer": "",
    "answer_a": "Annotators trained on multimodality theory. Explanation: Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic structures and thus justify their higher cost is one question that this study seeks to answer.",
    "answer_b": "How are they selected?\n<sep><EXPERIMENT1> Expert annotation: Expert annotation is performed by a human annotator who is trained to annotate diagrams with a set of predefined annotations. The human annotator is trained to annotate diagrams with a set of predefined annotations. The human annotator is trained to annotate diagrams with a set of predefined annotations. The human annotator is trained to annotate diagrams with a set of predefined annotations. The human annotator is trained to annotate diagrams with a set of predefined annotations. The human annotator is trained to annotate diagrams with a set of predefined annotations. The human annotator is trained to annotate diagrams with",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWho are considered trained experts?\n\nGround Truth Answer:\n\n\nAnswer A:\nAnnotators trained on multimodality theory. Explanation: Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic structures and thus justify their higher cost is one question that this study seeks to answer.\n\nAnswer B:\nHow are they selected?\n<sep><EXPERIMENT1> Expert annotation: Expert annotation is performed by a human annotator who is trained to annotate diagrams with a set of predefined annotations. The human annotator is trained to annotate diagrams with a set of predefined annotations. The human annotator is trained to annotate diagrams with a set of predefined annotations. The human annotator is trained to annotate diagrams with a set of predefined annotations. The human annotator is trained to annotate diagrams with a set of predefined annotations. The human annotator is trained to annotate diagrams with a set of predefined annotations. The human annotator is trained to annotate diagrams with\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1812.00382",
    "question": "Which model architecture do they opt for?",
    "gold_answer": "",
    "answer_a": "What is the performance of the models?\n<sep><table cellspacing=0 cellpadding=4>\n<tr>\n<td align=left>\nFigure 1: The architecture of the models. The left column shows the architecture of the baseline model, the right column shows the architecture of the baseline model with the addition of a word embedding layer. The models are trained on the Wikipedia linking dataset. The models are trained on the Wikipedia linking dataset and the Wikipedia linking dataset with the addition of a word embedding layer. The models are trained on the Wikipedia linking dataset and the Wikipedia linking dataset with the addition of a word embedding layer. The models are trained on the",
    "answer_b": "Recurrent Neural Networks. Explanation: We use two often applied network architectures adopting word embeddings, to classify controversy: Recurrent Neural Networks BIBREF10 and Convolutional Neural Networks BIBREF11 to answer the following research question.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich model architecture do they opt for?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of the models?\n<sep><table cellspacing=0 cellpadding=4>\n<tr>\n<td align=left>\nFigure 1: The architecture of the models. The left column shows the architecture of the baseline model, the right column shows the architecture of the baseline model with the addition of a word embedding layer. The models are trained on the Wikipedia linking dataset. The models are trained on the Wikipedia linking dataset and the Wikipedia linking dataset with the addition of a word embedding layer. The models are trained on the Wikipedia linking dataset and the Wikipedia linking dataset with the addition of a word embedding layer. The models are trained on the\n\nAnswer B:\nRecurrent Neural Networks. Explanation: We use two often applied network architectures adopting word embeddings, to classify controversy: Recurrent Neural Networks BIBREF10 and Convolutional Neural Networks BIBREF11 to answer the following research question.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1812.00382",
    "question": "Which dataset do they use?",
    "gold_answer": "",
    "answer_a": "Clueweb09. Explanation: We use the Clueweb09 derived dataset of BIBREF0 for baseline comparison.",
    "answer_b": "How do they evaluate the models?\n< Explanation: We use the Wikipedia dataset from the Stanford NLP Corpus (SNC) BIBREF11 . We use the Wikipedia dataset from the Stanford NLP Corpus (SNC) BIBREF11 . We use the Wikipedia dataset from the Stanford NLP Corpus (SNC) BIBREF11 . We use the Wikipedia dataset from the Stanford NLP Corpus (SNC) BIBREF11 . We use the Wikipedia dataset from the Stanford NLP Corpus (SNC) BIBREF11 . We use the Wikipedia dataset from the Stanford NLP Corpus (S",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dataset do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nClueweb09. Explanation: We use the Clueweb09 derived dataset of BIBREF0 for baseline comparison.\n\nAnswer B:\nHow do they evaluate the models?\n< Explanation: We use the Wikipedia dataset from the Stanford NLP Corpus (SNC) BIBREF11 . We use the Wikipedia dataset from the Stanford NLP Corpus (SNC) BIBREF11 . We use the Wikipedia dataset from the Stanford NLP Corpus (SNC) BIBREF11 . We use the Wikipedia dataset from the Stanford NLP Corpus (SNC) BIBREF11 . We use the Wikipedia dataset from the Stanford NLP Corpus (SNC) BIBREF11 . We use the Wikipedia dataset from the Stanford NLP Corpus (S\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1812.00382",
    "question": "Which weak signal data do they use?",
    "gold_answer": "",
    "answer_a": "How do they evaluate the performance of the models?\n< Explanation: We use the following weak signal data: (1) the number of links between two pages, (2) the number of words in a page, (3) the number of words in a page that are in the same category, (4) the number of words in a page that are in the same category, (5) the number of words in a page that are in the same category, (6) the number of words in a page that are in the same category, (7) the number of words in a page that are in the same category,",
    "answer_b": "semantic representations of word embeddings. Explanation: To this end, we focus only on fulltext features and suggest to leverage the semantic representations of word embeddings to reduce the vocabulary-gap for unseen topics and exact lexical representations.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich weak signal data do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate the performance of the models?\n< Explanation: We use the following weak signal data: (1) the number of links between two pages, (2) the number of words in a page, (3) the number of words in a page that are in the same category, (4) the number of words in a page that are in the same category, (5) the number of words in a page that are in the same category, (6) the number of words in a page that are in the same category, (7) the number of words in a page that are in the same category,\n\nAnswer B:\nsemantic representations of word embeddings. Explanation: To this end, we focus only on fulltext features and suggest to leverage the semantic representations of word embeddings to reduce the vocabulary-gap for unseen topics and exact lexical representations.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1903.02930",
    "question": "what dataset was used for training?",
    "gold_answer": "",
    "answer_a": "64M segments from YouTube videos. Explanation: Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens BIBREF14 . We tokenize the training data using a vocabulary of 66K wordpieces BIBREF15 . Thus, the input to the model is a sequence of wordpieces.",
    "answer_b": "Explanation: We use YouCookII, a dataset of 100,000 sentences from the YouCook project. The dataset contains 100,000 sentences, each of which is a sentence from a user's cooking conversation. The dataset is divided into 100,000 training sentences and 100,000 test sentences. The training sentences are used to train a standard RNN language model, and the test sentences are used to evaluate the model. We use the same test sentences for both training and evaluation. The test sentences are randomly selected from the training set. We",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat dataset was used for training?\n\nGround Truth Answer:\n\n\nAnswer A:\n64M segments from YouTube videos. Explanation: Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens BIBREF14 . We tokenize the training data using a vocabulary of 66K wordpieces BIBREF15 . Thus, the input to the model is a sequence of wordpieces.\n\nAnswer B:\nExplanation: We use YouCookII, a dataset of 100,000 sentences from the YouCook project. The dataset contains 100,000 sentences, each of which is a sentence from a user's cooking conversation. The dataset is divided into 100,000 training sentences and 100,000 test sentences. The training sentences are used to train a standard RNN language model, and the test sentences are used to evaluate the model. We use the same test sentences for both training and evaluation. The test sentences are randomly selected from the training set. We\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1903.02930",
    "question": "what is the size of the training data?",
    "gold_answer": "",
    "answer_a": "How big is the test data?\n< Explanation: The YouCookII dataset contains 100,000 sentences, each of which is 1000 words long. The 20bn-something-something-v2 dataset contains 200,000 sentences, each of which is 1000 words long. The total size of the training data is 2000000000 words. The test data is 2000000000 words long. The total size of the test data is 2000000",
    "answer_b": "64M video segments with 1.2B tokens. Explanation: Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens BIBREF14 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the size of the training data?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow big is the test data?\n< Explanation: The YouCookII dataset contains 100,000 sentences, each of which is 1000 words long. The 20bn-something-something-v2 dataset contains 200,000 sentences, each of which is 1000 words long. The total size of the training data is 2000000000 words. The test data is 2000000000 words long. The total size of the test data is 2000000\n\nAnswer B:\n64M video segments with 1.2B tokens. Explanation: Our training data consist of about 64M segments from YouTube videos comprising a total of INLINEFORM0 B tokens BIBREF14 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1903.02930",
    "question": "what features were derived from the videos?",
    "gold_answer": "",
    "answer_a": "< Explanation: We use the following features: (1) the video frame, (2) the video frame's bounding box, (3) the video frame's bounding box's center, (4) the video frame's bounding box's width and height, (5) the video frame's bounding box's left and top coordinates, (6) the video frame's bounding box's left and top coordinates' relative to the video frame's bounding box's center, (7) the video frame's bounding box's left and top coordinates' relative to the video frame's bounding box's width and height, (8) the video frame",
    "answer_b": "1500-dimensional vectors similar to those used for large scale image classification tasks.. Explanation: The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 , BIBREF18 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat features were derived from the videos?\n\nGround Truth Answer:\n\n\nAnswer A:\n< Explanation: We use the following features: (1) the video frame, (2) the video frame's bounding box, (3) the video frame's bounding box's center, (4) the video frame's bounding box's width and height, (5) the video frame's bounding box's left and top coordinates, (6) the video frame's bounding box's left and top coordinates' relative to the video frame's bounding box's center, (7) the video frame's bounding box's left and top coordinates' relative to the video frame's bounding box's width and height, (8) the video frame\n\nAnswer B:\n1500-dimensional vectors similar to those used for large scale image classification tasks.. Explanation: The features are 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks BIBREF17 , BIBREF18 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.04873",
    "question": "What translation models are explored?",
    "gold_answer": "",
    "answer_a": "Explanation: The current neural machine translation models are evaluated on the following tasks: (1) translation of a given proof to a given proof, (2) translation of a given proof to a given term, (3) translation of a given term to a given proof, and (4) translation of a given term to a given term. The results are presented in Table 1. The results show that the current neural machine translation models are not adequate for learning symbolic rewriting. The best results are obtained by the neural machine translation models based on the RNN-Transformer architecture. The best results are obtained by the neural machine translation models based on",
    "answer_b": "NMT architecture BIBREF10. Explanation: For experiments with both data sets we used an established NMT architecture BIBREF10 based on LSTMs (long short-term memory cells) and implementing the attention mechanism.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat translation models are explored?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The current neural machine translation models are evaluated on the following tasks: (1) translation of a given proof to a given proof, (2) translation of a given proof to a given term, (3) translation of a given term to a given proof, and (4) translation of a given term to a given term. The results are presented in Table 1. The results show that the current neural machine translation models are not adequate for learning symbolic rewriting. The best results are obtained by the neural machine translation models based on the RNN-Transformer architecture. The best results are obtained by the neural machine translation models based on\n\nAnswer B:\nNMT architecture BIBREF10. Explanation: For experiments with both data sets we used an established NMT architecture BIBREF10 based on LSTMs (long short-term memory cells) and implementing the attention mechanism.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.04873",
    "question": "What is symbolic rewriting?",
    "gold_answer": "",
    "answer_a": "How is it used in the experiments?\n< Explanation: Symbolic rewriting is a formalism for describing and manipulating mathematical structures. It is a powerful tool for solving problems in mathematics, computer science, and other fields. It is a formalism for describing and manipulating mathematical structures. It is a formalism for describing and manipulating mathematical structures. It is a formalism for describing and manipulating mathematical structures. It is a formalism for describing and manipulating mathematical structures. It is a formalism for describing and manipulating mathematical structures. It is a formalism for describing and manipulating mathematical structures. It is a formalism for describing and manipulating mathematical structures. It",
    "answer_b": "It is a process of translating a set of formal symbolic data to another set of formal symbolic data.. Explanation: One of the tasks in this domain is translation between natural languages \u2013 neural machine translation (NMT) systems established here the state-of-the-art performance.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is symbolic rewriting?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it used in the experiments?\n< Explanation: Symbolic rewriting is a formalism for describing and manipulating mathematical structures. It is a powerful tool for solving problems in mathematics, computer science, and other fields. It is a formalism for describing and manipulating mathematical structures. It is a formalism for describing and manipulating mathematical structures. It is a formalism for describing and manipulating mathematical structures. It is a formalism for describing and manipulating mathematical structures. It is a formalism for describing and manipulating mathematical structures. It is a formalism for describing and manipulating mathematical structures. It is a formalism for describing and manipulating mathematical structures. It\n\nAnswer B:\nIt is a process of translating a set of formal symbolic data to another set of formal symbolic data.. Explanation: One of the tasks in this domain is translation between natural languages \u2013 neural machine translation (NMT) systems established here the state-of-the-art performance.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1606.07043",
    "question": "How do they incorporate expert knowledge into their topic model?",
    "gold_answer": "",
    "answer_a": "What is the underlying intuition?\n< Explanation: We first introduce a new method for incorporating expert knowledge into a topic model. We use the information bottleneck (IB) approach to learn a latent topic model, and then use the information bottleneck to learn a topic model that is consistent with the expert knowledge. We call this approach Anchored CorEx (ACEx). We then show that the resulting topic model is more interpretable than the original topic model. We also show that the resulting topic model is more coherent than the original topic model. Finally, we show that the resulting topic model is more informative than the original topic model. We call this approach",
    "answer_b": "The experts define anchors and the model learns correlations between the anchors and latent topics.. Explanation: In this workshop paper, we introduce a simple approach to anchored information theoretic topic modeling using a novel combination of Correlation Explanation (CorEx) BIBREF3 and the information bottleneck BIBREF4 . This flexible framework enables the user to leverage domain knowledge to guide exploration of a collection of documents and to impose semantics onto latent factors learned by CorEx.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they incorporate expert knowledge into their topic model?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the underlying intuition?\n< Explanation: We first introduce a new method for incorporating expert knowledge into a topic model. We use the information bottleneck (IB) approach to learn a latent topic model, and then use the information bottleneck to learn a topic model that is consistent with the expert knowledge. We call this approach Anchored CorEx (ACEx). We then show that the resulting topic model is more interpretable than the original topic model. We also show that the resulting topic model is more coherent than the original topic model. Finally, we show that the resulting topic model is more informative than the original topic model. We call this approach\n\nAnswer B:\nThe experts define anchors and the model learns correlations between the anchors and latent topics.. Explanation: In this workshop paper, we introduce a simple approach to anchored information theoretic topic modeling using a novel combination of Correlation Explanation (CorEx) BIBREF3 and the information bottleneck BIBREF4 . This flexible framework enables the user to leverage domain knowledge to guide exploration of a collection of documents and to impose semantics onto latent factors learned by CorEx.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1606.07043",
    "question": "On which corpora do they evaluate on?",
    "gold_answer": "",
    "answer_a": "< Explanation: We evaluate Anchored CorEx on two corpora: the Medical Chart Corpus (MCC) and the Medical Chart Corpus with Annotations (MCC-A). The MCC is a collection of 1000 medical chart excerpts from the National Health Service (NHS) in the UK. The MCC-A is a collection of 1000 medical chart excerpts from the NHS in the UK, with annotations from a medical expert. We use the same annotation set for both corpora. We use the same corpus for both evaluation. We use the same evaluation metrics. We use the same evaluation settings. We use the",
    "answer_b": "20 Newsgroups. Explanation: To demonstrate the utility of Anchored CorEx, we run experiments on two document collections: 20 Newsgroups and the i2b2 2008 Obesity Challenge BIBREF22 data set.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nOn which corpora do they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\n< Explanation: We evaluate Anchored CorEx on two corpora: the Medical Chart Corpus (MCC) and the Medical Chart Corpus with Annotations (MCC-A). The MCC is a collection of 1000 medical chart excerpts from the National Health Service (NHS) in the UK. The MCC-A is a collection of 1000 medical chart excerpts from the NHS in the UK, with annotations from a medical expert. We use the same annotation set for both corpora. We use the same corpus for both evaluation. We use the same evaluation metrics. We use the same evaluation settings. We use the\n\nAnswer B:\n20 Newsgroups. Explanation: To demonstrate the utility of Anchored CorEx, we run experiments on two document collections: 20 Newsgroups and the i2b2 2008 Obesity Challenge BIBREF22 data set.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.04234",
    "question": "What is F-score obtained?",
    "gold_answer": "",
    "answer_a": "How is it calculated?\n< Explanation: The F-score is calculated as follows: where is the number of correct predictions, is the number of correct predictions, and is the number of incorrect predictions. The F-score is a measure of the quality of a classifier. It is a weighted average of the precision and recall. The higher the F-score, the better the classifier is. The F-score is calculated as follows: where is the number of correct predictions, is the number of correct predictions, and is the number of incorrect predictions. The F-score is a measure of the quality of a classifier. It is a weighted average of the precision and",
    "answer_b": "For Named Entity, F-Score Driven I model had 49.40 F1 score,  and F-Score Driven II model had 50.60  F1 score. In case of Nominal Mention, the scores were 58.16 and 59.32. Explanation: Table TABREF23 shows results for NER on test sets. In the Table TABREF23 , we also show micro F1-score (Overall) and out-of-vocabulary entities (OOV) recall. Peng and Dredze peng-dredze:2016:P16-2 is the state-of-the-art NER system in Chinese Social media. By comparing the results of B-LSTM model and B-LSTM + MTNN model, we can know transition probability is significant for NER. Compared with B-LSTM + MMNN model, F-Score Driven Model I improves the result of named entity with a loss in nominal mention. The integrated training model (F-Score Driven Model II) benefits from both label accuracy and F-Score, which achieves a new state-of-the-art NER system in Chinese social media. Our integrated model has better performance on named entity and nominal mention.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is F-score obtained?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it calculated?\n< Explanation: The F-score is calculated as follows: where is the number of correct predictions, is the number of correct predictions, and is the number of incorrect predictions. The F-score is a measure of the quality of a classifier. It is a weighted average of the precision and recall. The higher the F-score, the better the classifier is. The F-score is calculated as follows: where is the number of correct predictions, is the number of correct predictions, and is the number of incorrect predictions. The F-score is a measure of the quality of a classifier. It is a weighted average of the precision and\n\nAnswer B:\nFor Named Entity, F-Score Driven I model had 49.40 F1 score,  and F-Score Driven II model had 50.60  F1 score. In case of Nominal Mention, the scores were 58.16 and 59.32. Explanation: Table TABREF23 shows results for NER on test sets. In the Table TABREF23 , we also show micro F1-score (Overall) and out-of-vocabulary entities (OOV) recall. Peng and Dredze peng-dredze:2016:P16-2 is the state-of-the-art NER system in Chinese Social media. By comparing the results of B-LSTM model and B-LSTM + MTNN model, we can know transition probability is significant for NER. Compared with B-LSTM + MMNN model, F-Score Driven Model I improves the result of named entity with a loss in nominal mention. The integrated training model (F-Score Driven Model II) benefits from both label accuracy and F-Score, which achieves a new state-of-the-art NER system in Chinese social media. Our integrated model has better performance on named entity and nominal mention.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.04234",
    "question": "What is the state-of-the-art?",
    "gold_answer": "",
    "answer_a": "How does the proposed model compare to previous state-of-the-art?\n< Explanation: The state-of-the-art results are shown in Table 1. Our model achieves 7.44\\% improvement over previous state-of-the-art result. Compared to CRF, our model achieves 1.12\\% improvement. Compared to CRF+CRF, our model achieves 1.32\\% improvement. Compared to CRF+CRF+CRF, our model achieves 1.52\\% improvement. Compared to CRF+CRF+CRF+CRF, our model achieves 1.",
    "answer_b": "Peng and Dredze peng-dredze:2016:P16-2. Explanation: Peng and Dredze peng-dredze:2016:P16-2 is the state-of-the-art NER system in Chinese Social media.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the state-of-the-art?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow does the proposed model compare to previous state-of-the-art?\n< Explanation: The state-of-the-art results are shown in Table 1. Our model achieves 7.44\\% improvement over previous state-of-the-art result. Compared to CRF, our model achieves 1.12\\% improvement. Compared to CRF+CRF, our model achieves 1.32\\% improvement. Compared to CRF+CRF+CRF, our model achieves 1.52\\% improvement. Compared to CRF+CRF+CRF+CRF, our model achieves 1.\n\nAnswer B:\nPeng and Dredze peng-dredze:2016:P16-2. Explanation: Peng and Dredze peng-dredze:2016:P16-2 is the state-of-the-art NER system in Chinese Social media.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.04234",
    "question": "Which Chinese social media platform does the data come from?",
    "gold_answer": "",
    "answer_a": "Sina Weibo service. Explanation: We use a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media.",
    "answer_b": "Explanation: We use the social media platform Weibo to collect the data. We collected 1000000 tweets from Weibo in 2016. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich Chinese social media platform does the data come from?\n\nGround Truth Answer:\n\n\nAnswer A:\nSina Weibo service. Explanation: We use a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media.\n\nAnswer B:\nExplanation: We use the social media platform Weibo to collect the data. We collected 1000000 tweets from Weibo in 2016. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use the Twitter API to get the tweets. We use\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.04234",
    "question": "What dataset did they use?",
    "gold_answer": "",
    "answer_a": "Peng and Dredze peng-dredze:2016:P16-2. Explanation: We use a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media.",
    "answer_b": "Explanation: We use the dataset of 1000000 tweets from 2016-01-01 to 2016-01-31. The dataset is collected from Twitter, and contains 1000000 tweets. Each tweet is a text with 140-160 characters. We use the first 1000000 tweets as training set and the remaining 1000000 tweets as test set. The training set is used to train the model, and the test set is used to evaluate the model",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat dataset did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nPeng and Dredze peng-dredze:2016:P16-2. Explanation: We use a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media.\n\nAnswer B:\nExplanation: We use the dataset of 1000000 tweets from 2016-01-01 to 2016-01-31. The dataset is collected from Twitter, and contains 1000000 tweets. Each tweet is a text with 140-160 characters. We use the first 1000000 tweets as training set and the remaining 1000000 tweets as test set. The training set is used to train the model, and the test set is used to evaluate the model\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.00437",
    "question": "What are the five downstream tasks?",
    "gold_answer": "",
    "answer_a": "These include 3 classification tasks: NLI (XNLI dataset), document classification (MLDoc dataset) and intent classification, and 2 sequence tagging tasks: POS tagging and NER.. Explanation: As stated earlier, we use MMTE to perform downstream cross-lingual transfer on 5 NLP tasks. These include 3 classification tasks: NLI (XNLI dataset), document classification (MLDoc dataset) and intent classification, and 2 sequence tagging tasks: POS tagging and NER.",
    "answer_b": "How are they evaluated?\n< Explanation: We evaluate the cross-lingual effectiveness of representations from the encoder of a massively multilingual NMT model on 5 downstream classification and sequence labeling tasks covering a diverse set of over 50 languages. We compare against a strong baseline, multilingual BERT (mBERT), in different cross-lingual transfer learning scenarios and show gains in zero-shot transfer in 4 out of these 5 tasks. We also evaluate the performance of the NMT model on a 1000 language dataset and show that it outperforms mBERT on 4 of the 5 tasks. We",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the five downstream tasks?\n\nGround Truth Answer:\n\n\nAnswer A:\nThese include 3 classification tasks: NLI (XNLI dataset), document classification (MLDoc dataset) and intent classification, and 2 sequence tagging tasks: POS tagging and NER.. Explanation: As stated earlier, we use MMTE to perform downstream cross-lingual transfer on 5 NLP tasks. These include 3 classification tasks: NLI (XNLI dataset), document classification (MLDoc dataset) and intent classification, and 2 sequence tagging tasks: POS tagging and NER.\n\nAnswer B:\nHow are they evaluated?\n< Explanation: We evaluate the cross-lingual effectiveness of representations from the encoder of a massively multilingual NMT model on 5 downstream classification and sequence labeling tasks covering a diverse set of over 50 languages. We compare against a strong baseline, multilingual BERT (mBERT), in different cross-lingual transfer learning scenarios and show gains in zero-shot transfer in 4 out of these 5 tasks. We also evaluate the performance of the NMT model on a 1000 language dataset and show that it outperforms mBERT on 4 of the 5 tasks. We\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.00437",
    "question": "How did they select the 50 languages they test?",
    "gold_answer": "",
    "answer_a": "These languages are chosen based on intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model. Explanation: We use representations from a Massively Multilingual Translation Encoder (MMTE) that can handle 103 languages to achieve cross-lingual transfer on 5 classification and sequence tagging tasks spanning more than 50 languages.",
    "answer_b": "What is the size of the dataset?\n< Explanation: We selected 50 languages from the 100 most frequently used languages in the 2018 IWSLT 2018 dataset. We chose 10 languages from each of the 50 languages, and we chose 10 languages from each of the 100 most frequently used languages. We chose 10 languages from each of the 100 most frequently used languages. We chose 10 languages from each of the 100 most frequently used languages. We chose 10 languages from each of the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow did they select the 50 languages they test?\n\nGround Truth Answer:\n\n\nAnswer A:\nThese languages are chosen based on intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by our mNMT model. Explanation: We use representations from a Massively Multilingual Translation Encoder (MMTE) that can handle 103 languages to achieve cross-lingual transfer on 5 classification and sequence tagging tasks spanning more than 50 languages.\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: We selected 50 languages from the 100 most frequently used languages in the 2018 IWSLT 2018 dataset. We chose 10 languages from each of the 50 languages, and we chose 10 languages from each of the 100 most frequently used languages. We chose 10 languages from each of the 100 most frequently used languages. We chose 10 languages from each of the 100 most frequently used languages. We chose 10 languages from each of the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.07568",
    "question": "What kind of evaluations do use to evaluate dialogue?",
    "gold_answer": "",
    "answer_a": "What are the results?\n< Explanation: We evaluate the performance of the models on the following metrics: accuracy, precision, recall, F1 score, and F1 score of the best model. We also evaluate the performance of the models on the following metrics: accuracy, precision, recall, F1 score, and F1 score of the best model. We also evaluate the performance of the models on the following metrics: accuracy, precision, recall, F1 score, and F1 score of the best model. We also evaluate the performance of the models on the following metrics: accuracy, precision, recall, F1 score, and F",
    "answer_b": "They use automatic evaluation using perplexity and BLEU scores with reference to the human-annotated responses and human evaluation on interestingness, engagingness, and humanness.. Explanation: Hence, we evaluate our models using both automatic and human evaluation.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat kind of evaluations do use to evaluate dialogue?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n< Explanation: We evaluate the performance of the models on the following metrics: accuracy, precision, recall, F1 score, and F1 score of the best model. We also evaluate the performance of the models on the following metrics: accuracy, precision, recall, F1 score, and F1 score of the best model. We also evaluate the performance of the models on the following metrics: accuracy, precision, recall, F1 score, and F1 score of the best model. We also evaluate the performance of the models on the following metrics: accuracy, precision, recall, F1 score, and F\n\nAnswer B:\nThey use automatic evaluation using perplexity and BLEU scores with reference to the human-annotated responses and human evaluation on interestingness, engagingness, and humanness.. Explanation: Hence, we evaluate our models using both automatic and human evaluation.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.07568",
    "question": "By how much do their cross-lingual models lag behind other models?",
    "gold_answer": "",
    "answer_a": "significant gap between the cross-lingual model and other models. Explanation: Table TABREF20 compares monolingual, multilingual, and cross-lingual models in terms of BLEU and perplexity in the human-translated test set.",
    "answer_b": "What are the advantages of their model over other models?\n<sep><br/ >Answer: We compare our models with the following models: (1) a baseline model trained on monolingual data, (2) a baseline model trained on translation-pipeline data, (3) a baseline model trained on both monolingual and translation-pipeline data, (4) a baseline model trained on both monolingual and multilingual data, (5) a baseline model trained on both monolingual and multilingual data with a single language model, (6) a baseline model trained on both monolingual and multilingual data with",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much do their cross-lingual models lag behind other models?\n\nGround Truth Answer:\n\n\nAnswer A:\nsignificant gap between the cross-lingual model and other models. Explanation: Table TABREF20 compares monolingual, multilingual, and cross-lingual models in terms of BLEU and perplexity in the human-translated test set.\n\nAnswer B:\nWhat are the advantages of their model over other models?\n<sep><br/ >Answer: We compare our models with the following models: (1) a baseline model trained on monolingual data, (2) a baseline model trained on translation-pipeline data, (3) a baseline model trained on both monolingual and translation-pipeline data, (4) a baseline model trained on both monolingual and multilingual data, (5) a baseline model trained on both monolingual and multilingual data with a single language model, (6) a baseline model trained on both monolingual and multilingual data with\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.07568",
    "question": "Which translation pipelines do they use to compare against?",
    "gold_answer": "",
    "answer_a": "Translate source sentence to English with Google Translate API and then translate the result to the target language with Poly-encoder.. Explanation: Another strong baseline we compare with is Poly-encoder BIBREF75, a large-scale pre-trained retrieval model that has shown state-of-the-art performance in the English Persona-chat dataset BIBREF6. We adapt this model to the other languages by using the Google Translate API to translate target languages (e.g., Chinese) query to English as the input to the model, then translate the English response back to the target language.",
    "answer_b": "What are their results?\n< Explanation: We use the following translation pipelines: (1) BIBREF10, (2) BIBREF11, (3) BIBREF12, (4) BIBREF13, (5) BIBREF14, (6) BIBREF15, (7) BIBREF16, (8) BIBREF17, (9) BIBREF18, (10) BIBREF19, (11) BIBREF20, (12) BIBREF21, (1",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich translation pipelines do they use to compare against?\n\nGround Truth Answer:\n\n\nAnswer A:\nTranslate source sentence to English with Google Translate API and then translate the result to the target language with Poly-encoder.. Explanation: Another strong baseline we compare with is Poly-encoder BIBREF75, a large-scale pre-trained retrieval model that has shown state-of-the-art performance in the English Persona-chat dataset BIBREF6. We adapt this model to the other languages by using the Google Translate API to translate target languages (e.g., Chinese) query to English as the input to the model, then translate the English response back to the target language.\n\nAnswer B:\nWhat are their results?\n< Explanation: We use the following translation pipelines: (1) BIBREF10, (2) BIBREF11, (3) BIBREF12, (4) BIBREF13, (5) BIBREF14, (6) BIBREF15, (7) BIBREF16, (8) BIBREF17, (9) BIBREF18, (10) BIBREF19, (11) BIBREF20, (12) BIBREF21, (1\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.07568",
    "question": "Which languages does their newly created dataset contain?",
    "gold_answer": "",
    "answer_a": "What are the languages in the dataset?\n< Explanation: The dataset contains 1000 persona conversations in six languages: English, French, German, Spanish, Portuguese, and Chinese. The dataset is divided into 1000 persona conversations, each of which is 1000 words long. The dataset is divided into 1000 persona conversations, each of which is 1000 words long. The dataset is divided into 1000 persona conversations, each of which is 1000 words long. The dataset is divided into 1000 persona conversations, each of",
    "answer_b": "Chinese. Explanation: To evaluate the aforementioned systems, we propose a dataset called Multilingual Persona-Chat, or XPersona, by extending the Persona-Chat corpora BIBREF1 to six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich languages does their newly created dataset contain?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the languages in the dataset?\n< Explanation: The dataset contains 1000 persona conversations in six languages: English, French, German, Spanish, Portuguese, and Chinese. The dataset is divided into 1000 persona conversations, each of which is 1000 words long. The dataset is divided into 1000 persona conversations, each of which is 1000 words long. The dataset is divided into 1000 persona conversations, each of which is 1000 words long. The dataset is divided into 1000 persona conversations, each of\n\nAnswer B:\nChinese. Explanation: To evaluate the aforementioned systems, we propose a dataset called Multilingual Persona-Chat, or XPersona, by extending the Persona-Chat corpora BIBREF1 to six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1810.02268",
    "question": "what are the baselines?",
    "gold_answer": "",
    "answer_a": "bidirectional RNN model with attention. Explanation: We consider the following recurrent baselines:\n\nbaseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus.",
    "answer_b": "How do they compare to the models?\n< Explanation: We compare our models to the following baselines: (1) a baseline that uses only the first sentence of the input, (2) a baseline that uses only the last sentence of the input, (3) a baseline that uses only the first sentence of the output, (4) a baseline that uses only the last sentence of the output, (5) a baseline that uses only the first sentence of the input and output, (6) a baseline that uses only the last sentence of the input and output, (7) a baseline that uses only the first sentence of the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the baselines?\n\nGround Truth Answer:\n\n\nAnswer A:\nbidirectional RNN model with attention. Explanation: We consider the following recurrent baselines:\n\nbaseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus.\n\nAnswer B:\nHow do they compare to the models?\n< Explanation: We compare our models to the following baselines: (1) a baseline that uses only the first sentence of the input, (2) a baseline that uses only the last sentence of the input, (3) a baseline that uses only the first sentence of the output, (4) a baseline that uses only the last sentence of the output, (5) a baseline that uses only the first sentence of the input and output, (6) a baseline that uses only the last sentence of the input and output, (7) a baseline that uses only the first sentence of the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1810.02268",
    "question": "what context aware models were experimented?",
    "gold_answer": "",
    "answer_a": "standard bidirectional RNN model with attention. Explanation: We consider the following recurrent baselines:\n\nbaseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus.",
    "answer_b": "Is there any difference between the models?\n< Explanation: We compare the performance of three different models: a simple encoder model, a multi-encoder model, and a model with parameter tying. We compare the performance of the models on the contrastive test set, which consists of 1000 sentences, and the original test set, which consists of 10000 sentences. We also compare the performance of the models on the original test set, which consists of 10000 sentences. We compare the performance of the models on the original test set, which consists of 10000 sentences,",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat context aware models were experimented?\n\nGround Truth Answer:\n\n\nAnswer A:\nstandard bidirectional RNN model with attention. Explanation: We consider the following recurrent baselines:\n\nbaseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus.\n\nAnswer B:\nIs there any difference between the models?\n< Explanation: We compare the performance of three different models: a simple encoder model, a multi-encoder model, and a model with parameter tying. We compare the performance of the models on the contrastive test set, which consists of 1000 sentences, and the original test set, which consists of 10000 sentences. We also compare the performance of the models on the original test set, which consists of 10000 sentences. We compare the performance of the models on the original test set, which consists of 10000 sentences,\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1810.02268",
    "question": "what languages did they experiment on?",
    "gold_answer": "",
    "answer_a": "Explanation: We tested our models on 10 languages: English, French, German, Spanish, Italian, Portuguese, Arabic, Chinese, and Japanese. We also tested our models on 10000 sentences from the Stanford Sentiment Treebank (SST) 1. We used the Stanford Sentiment Treebank 2.0 as our evaluation set. We also used the Stanford Sentiment Treebank 3.0 as our training set. We used the Stanford Sentiment Treebank 4.0 as our validation set. We used the Stanford Sentiment Treebank 5.0 as our test set. We",
    "answer_b": "English. Explanation: We train all models on the data from the WMT 2017 English INLINEFORM0 German news translation shared task ( INLINEFORM1 5.8 million sentence pairs).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat languages did they experiment on?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We tested our models on 10 languages: English, French, German, Spanish, Italian, Portuguese, Arabic, Chinese, and Japanese. We also tested our models on 10000 sentences from the Stanford Sentiment Treebank (SST) 1. We used the Stanford Sentiment Treebank 2.0 as our evaluation set. We also used the Stanford Sentiment Treebank 3.0 as our training set. We used the Stanford Sentiment Treebank 4.0 as our validation set. We used the Stanford Sentiment Treebank 5.0 as our test set. We\n\nAnswer B:\nEnglish. Explanation: We train all models on the data from the WMT 2017 English INLINEFORM0 German news translation shared task ( INLINEFORM1 5.8 million sentence pairs).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.12079",
    "question": "How do they obtain the entity linking results in their model?",
    "gold_answer": "",
    "answer_a": "They use an EL algorithm that links the mention to the entity with the help of the greatest commonness score.. Explanation: Given a piece of text and the span of an entity mention in this text, fine-grained entity typing (FET) is the task of assigning fine-grained type labels to the mention BIBREF0.",
    "answer_b": "What is the size of the entity linking dataset?\n< Explanation: We use the entity linking dataset from the Stanford NLP Corpus (SNC) BIBREF10 . The dataset contains 10,000 entity mentions and 10,000 entity types. We use the entity linking results to train our model. The entity linking results are obtained from the Stanford NLP Corpus BIBREF10 . The entity linking results are obtained from the Stanford NLP Corpus BIBREF10 . The entity linking results are obtained from the Stanford NLP Corpus BIBREF10 . The entity linking results are",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they obtain the entity linking results in their model?\n\nGround Truth Answer:\n\n\nAnswer A:\nThey use an EL algorithm that links the mention to the entity with the help of the greatest commonness score.. Explanation: Given a piece of text and the span of an entity mention in this text, fine-grained entity typing (FET) is the task of assigning fine-grained type labels to the mention BIBREF0.\n\nAnswer B:\nWhat is the size of the entity linking dataset?\n< Explanation: We use the entity linking dataset from the Stanford NLP Corpus (SNC) BIBREF10 . The dataset contains 10,000 entity mentions and 10,000 entity types. We use the entity linking results to train our model. The entity linking results are obtained from the Stanford NLP Corpus BIBREF10 . The entity linking results are obtained from the Stanford NLP Corpus BIBREF10 . The entity linking results are obtained from the Stanford NLP Corpus BIBREF10 . The entity linking results are\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.12079",
    "question": "Which model architecture do they use?",
    "gold_answer": "",
    "answer_a": "BiLSTMs. Explanation: Our FET approach is illustrated in Figure FIGREF4. It first constructs three representations: context representation, mention string representation, and KB type representation. Note that the KB type representation is obtained from a knowledge base through entity linking and is independent of the context of the mention.",
    "answer_b": "What is the output size?\n< Explanation: We use a deep neural network with a multi-layer perceptron (MLP) as the core of our model. The MLP is composed of a hidden layer with 128 units and a final layer with 10 units. The MLP is trained using the Adam optimizer with a learning rate of 0.001. The MLP is trained for 100 epochs. The MLP is trained on the training set and the training loss is averaged over 10 runs. The MLP is trained on the training set and the training loss is averaged over 10 runs. The",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich model architecture do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nBiLSTMs. Explanation: Our FET approach is illustrated in Figure FIGREF4. It first constructs three representations: context representation, mention string representation, and KB type representation. Note that the KB type representation is obtained from a knowledge base through entity linking and is independent of the context of the mention.\n\nAnswer B:\nWhat is the output size?\n< Explanation: We use a deep neural network with a multi-layer perceptron (MLP) as the core of our model. The MLP is composed of a hidden layer with 128 units and a final layer with 10 units. The MLP is trained using the Adam optimizer with a learning rate of 0.001. The MLP is trained for 100 epochs. The MLP is trained on the training set and the training loss is averaged over 10 runs. The MLP is trained on the training set and the training loss is averaged over 10 runs. The\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.12079",
    "question": "Which datasets do they evaluate on?",
    "gold_answer": "",
    "answer_a": "FIGER (GOLD) BIBREF0. Explanation: We use two datasets: FIGER (GOLD) BIBREF0 and BBN BIBREF5. The sizes of their tag sets are 113 and 47, respectively. FIGER (GOLD) allows mentions to have multiple type paths, but BBN does not. Another commonly used dataset, OntoNotes BIBREF1, is not used since it contains many pronoun and common noun phrase mentions such as \u201cit,\u201d \u201che,\u201d \u201ca thrift institution,\u201d which are not suitable to directly apply entity linking on.",
    "answer_b": "How do they evaluate the accuracy?\n< Explanation: We evaluate our approach on two datasets: the 20 Newsgroups dataset and the 20 Newsgroups with entity linking dataset. The 20 Newsgroups dataset is a collection of 20,000 news articles from the Newsgroups mailing list. The 20 Newsgroups with entity linking dataset is a collection of 20,000 news articles from the Newsgroups with entity linking mailing list. Both datasets are publicly available. We use the 20 Newsgroups dataset for our fine-grained entity type classification task",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich datasets do they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\nFIGER (GOLD) BIBREF0. Explanation: We use two datasets: FIGER (GOLD) BIBREF0 and BBN BIBREF5. The sizes of their tag sets are 113 and 47, respectively. FIGER (GOLD) allows mentions to have multiple type paths, but BBN does not. Another commonly used dataset, OntoNotes BIBREF1, is not used since it contains many pronoun and common noun phrase mentions such as \u201cit,\u201d \u201che,\u201d \u201ca thrift institution,\u201d which are not suitable to directly apply entity linking on.\n\nAnswer B:\nHow do they evaluate the accuracy?\n< Explanation: We evaluate our approach on two datasets: the 20 Newsgroups dataset and the 20 Newsgroups with entity linking dataset. The 20 Newsgroups dataset is a collection of 20,000 news articles from the Newsgroups mailing list. The 20 Newsgroups with entity linking dataset is a collection of 20,000 news articles from the Newsgroups with entity linking mailing list. Both datasets are publicly available. We use the 20 Newsgroups dataset for our fine-grained entity type classification task\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.11687",
    "question": "How many domain experts were involved into creation of dataset?",
    "gold_answer": "",
    "answer_a": "What is the size of dataset?\n< Explanation: We collected 1000 concepts from the Systems Engineering domain. We used the following domain experts to create the dataset: 1) 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
    "answer_b": "1. Explanation: Using domain keywords and a domain expert, we annotate roughly 3700 sentences at the word-token level.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many domain experts were involved into creation of dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of dataset?\n< Explanation: We collected 1000 concepts from the Systems Engineering domain. We used the following domain experts to create the dataset: 1) 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n\nAnswer B:\n1. Explanation: Using domain keywords and a domain expert, we annotate roughly 3700 sentences at the word-token level.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.11687",
    "question": "What metrics are used for evaluation?",
    "gold_answer": "",
    "answer_a": "F1-score. Explanation: The model gives an F1-score of $0.89$ for the concept recognition task.",
    "answer_b": "What is the performance of the model?\n<sep><EXPERIMENT RESULT PAGE FACEBOOK LINK>\nFigure 1: Experimental results. The left column shows the performance of the model on the test set. The right column shows the performance of the model on the test set with the addition of the hyponym relations. The blue line shows the performance of the model without the addition of the hyponym relations. The red line shows the performance of the model with the addition of the hyponym relations. The green line shows the performance of the model with the addition of the hyponym relations and the blue line shows the performance of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat metrics are used for evaluation?\n\nGround Truth Answer:\n\n\nAnswer A:\nF1-score. Explanation: The model gives an F1-score of $0.89$ for the concept recognition task.\n\nAnswer B:\nWhat is the performance of the model?\n<sep><EXPERIMENT RESULT PAGE FACEBOOK LINK>\nFigure 1: Experimental results. The left column shows the performance of the model on the test set. The right column shows the performance of the model on the test set with the addition of the hyponym relations. The blue line shows the performance of the model without the addition of the hyponym relations. The red line shows the performance of the model with the addition of the hyponym relations. The green line shows the performance of the model with the addition of the hyponym relations and the blue line shows the performance of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.11687",
    "question": "What is the performance of fine tuned model on this dataset?",
    "gold_answer": "",
    "answer_a": "What is the performance of the pre-trained model?\n<sep>Answer: We evaluate the performance of the fine-tuned model on the dataset of 10000 concepts. The results show that the model achieves 93.5% accuracy on the test set. The performance of the pre-trained model is 92.5% on the test set. We also evaluate the performance of the pre-trained model on the dataset of 10000 concepts. The results show that the model achieves 92.5% accuracy on the test set. The pre-trained model achieves 92.5% accuracy",
    "answer_b": "F1-score of $0.89$. Explanation: The model gives an F1-score of $0.89$ for the concept recognition task.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the performance of fine tuned model on this dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of the pre-trained model?\n<sep>Answer: We evaluate the performance of the fine-tuned model on the dataset of 10000 concepts. The results show that the model achieves 93.5% accuracy on the test set. The performance of the pre-trained model is 92.5% on the test set. We also evaluate the performance of the pre-trained model on the dataset of 10000 concepts. The results show that the model achieves 92.5% accuracy on the test set. The pre-trained model achieves 92.5% accuracy\n\nAnswer B:\nF1-score of $0.89$. Explanation: The model gives an F1-score of $0.89$ for the concept recognition task.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.11687",
    "question": "How does labeling scheme look like?",
    "gold_answer": "",
    "answer_a": "What are the abbreviations and definitions?\n<sep>Figure 1: The proposed labelling scheme. The first column shows the concept type, the second column shows the concept name, and the third column shows the abbreviation. The fourth column shows the definition. The fifth column shows the definition in English. The sixth column shows the definition in Chinese. The seventh column shows the definition in Japanese. The eighth column shows the definition in Korean. The ninth column shows the definition in Arabic. The tenth column shows the definition in Russian. The eleventh column shows the definition in Turkish. The twelfth column shows the definition in Vietnamese. The th",
    "answer_b": "Based on operation and system concepts, the labels are abb, grp, syscon, opcon, seterm, event, org, art, cardinal, loc and mea.. Explanation: Based on the operational and system concepts described in BIBREF0, we carefully construct a set of concept-labels for the SE handbook which is shown in the next section.\n\nCONCEPT RECOGNITION ::: BIO Labelling Scheme\nabb: represents abbreviations such as TRL representing Technology Readiness Level.\n\ngrp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager.\n\nsyscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts.\n\nopcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc.\n\nseterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc.\n\nevent: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc.\n\norg: represents an organization such as `NASA', `aerospace industry', etc.\n\nart: represents names of artifacts or instruments such as `AS1300'\n\ncardinal: represents numerical values such as `1', `100', 'one' etc.\n\nloc: represents location-like entities such as component facilities or centralized facility.\n\nmea: represents measures, features, or behaviors such as cost, risk, or feasibility.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does labeling scheme look like?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the abbreviations and definitions?\n<sep>Figure 1: The proposed labelling scheme. The first column shows the concept type, the second column shows the concept name, and the third column shows the abbreviation. The fourth column shows the definition. The fifth column shows the definition in English. The sixth column shows the definition in Chinese. The seventh column shows the definition in Japanese. The eighth column shows the definition in Korean. The ninth column shows the definition in Arabic. The tenth column shows the definition in Russian. The eleventh column shows the definition in Turkish. The twelfth column shows the definition in Vietnamese. The th\n\nAnswer B:\nBased on operation and system concepts, the labels are abb, grp, syscon, opcon, seterm, event, org, art, cardinal, loc and mea.. Explanation: Based on the operational and system concepts described in BIBREF0, we carefully construct a set of concept-labels for the SE handbook which is shown in the next section.\n\nCONCEPT RECOGNITION ::: BIO Labelling Scheme\nabb: represents abbreviations such as TRL representing Technology Readiness Level.\n\ngrp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager.\n\nsyscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts.\n\nopcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc.\n\nseterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc.\n\nevent: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc.\n\norg: represents an organization such as `NASA', `aerospace industry', etc.\n\nart: represents names of artifacts or instruments such as `AS1300'\n\ncardinal: represents numerical values such as `1', `100', 'one' etc.\n\nloc: represents location-like entities such as component facilities or centralized facility.\n\nmea: represents measures, features, or behaviors such as cost, risk, or feasibility.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.11687",
    "question": "What pretrained language model is used?",
    "gold_answer": "",
    "answer_a": "BERT. Explanation: Any language model can be used for the purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings.",
    "answer_b": "What is the size of the dataset?\n<sep>Answer: We use a BERT model with a pre-trained weights from the HuggingFace library. The model is trained on the BERT dataset. The model is fine-tuned on the dataset of concepts. The dataset contains 10000 concepts and 100000 tokens. The model is trained on the dataset of concepts for 10 epochs with a batch size of 16. The model is fine-tuned on the dataset of concepts for 10 epochs with a batch size of 16. The model is fine-tuned on",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat pretrained language model is used?\n\nGround Truth Answer:\n\n\nAnswer A:\nBERT. Explanation: Any language model can be used for the purpose of customizing an NER problem to CR. We choose to go with BERT BIBREF16 because of its general-purpose nature and usage of contextualized word embeddings.\n\nAnswer B:\nWhat is the size of the dataset?\n<sep>Answer: We use a BERT model with a pre-trained weights from the HuggingFace library. The model is trained on the BERT dataset. The model is fine-tuned on the dataset of concepts. The dataset contains 10000 concepts and 100000 tokens. The model is trained on the dataset of concepts for 10 epochs with a batch size of 16. The model is fine-tuned on the dataset of concepts for 10 epochs with a batch size of 16. The model is fine-tuned on\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.11687",
    "question": "How big is constructed dataset?",
    "gold_answer": "",
    "answer_a": "What are the abbreviations and definitions?\n<sep>Answer: We collected 1000000 tokens from the SEVA dataset. We also collected 1000000 tokens from the Wikipedia dataset. We also collected 1000000 tokens from the Wikipedia dataset. We also collected 1000000 tokens from the Wikipedia dataset. We also collected 1000000 tokens from the Wikipedia dataset. We also collected 1000000 tokens from the Wikipedia dataset. We also collected 1000000 tokens from",
    "answer_b": "3700 sentences. Explanation: Using domain keywords and a domain expert, we annotate roughly 3700 sentences at the word-token level.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big is constructed dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the abbreviations and definitions?\n<sep>Answer: We collected 1000000 tokens from the SEVA dataset. We also collected 1000000 tokens from the Wikipedia dataset. We also collected 1000000 tokens from the Wikipedia dataset. We also collected 1000000 tokens from the Wikipedia dataset. We also collected 1000000 tokens from the Wikipedia dataset. We also collected 1000000 tokens from the Wikipedia dataset. We also collected 1000000 tokens from\n\nAnswer B:\n3700 sentences. Explanation: Using domain keywords and a domain expert, we annotate roughly 3700 sentences at the word-token level.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1703.10152",
    "question": "What metric is considered?",
    "gold_answer": "",
    "answer_a": "How is it calculated?\n< Explanation: The performance of the proposed method was evaluated using the 10-fold cross-validation scheme. The performance of the proposed method was evaluated using the 10-fold cross-validation scheme. The performance of the proposed method was evaluated using the 10-fold cross-validation scheme. The performance of the proposed method was evaluated using the 10-fold cross-validation scheme. The performance of the proposed method was evaluated using the 10-fold cross-validation scheme. The performance of the proposed method was evaluated using the 10-fold cross-validation scheme. The performance of the proposed method was evaluated using the 1",
    "answer_b": "Precision, recall and F-measure.. Explanation: FLOAT SELECTED: Table 4. Performance of sentence classification per category I (precision/recall/Fmeasure)",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat metric is considered?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it calculated?\n< Explanation: The performance of the proposed method was evaluated using the 10-fold cross-validation scheme. The performance of the proposed method was evaluated using the 10-fold cross-validation scheme. The performance of the proposed method was evaluated using the 10-fold cross-validation scheme. The performance of the proposed method was evaluated using the 10-fold cross-validation scheme. The performance of the proposed method was evaluated using the 10-fold cross-validation scheme. The performance of the proposed method was evaluated using the 10-fold cross-validation scheme. The performance of the proposed method was evaluated using the 1\n\nAnswer B:\nPrecision, recall and F-measure.. Explanation: FLOAT SELECTED: Table 4. Performance of sentence classification per category I (precision/recall/Fmeasure)\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1703.10152",
    "question": "What hand-crafted features are used?",
    "gold_answer": "",
    "answer_a": "position of sentence. Explanation: Teufel et. al. BIBREF2 have built feature pool of sixteen types of features to classify sentences, such as the position of sentence, sentence length and tense. Widyantoro et. al. used content features, qualifying adjectives and meta-discourse features BIBREF14 to explore AZ task.",
    "answer_b": "What are the strengths of these features?\n< Explanation: The hand-crafted features are the following: (1) the number of words in the sentence, (2) the number of words in the sentence and the number of words in the cuewords, (3) the number of words in the sentence and the number of words in the cuewords, (4) the number of words in the sentence and the number of words in the cuewords, (5) the number of words in the sentence and the number of words in the cuewords, (6) the number of words in the sentence and the number of words in the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat hand-crafted features are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nposition of sentence. Explanation: Teufel et. al. BIBREF2 have built feature pool of sixteen types of features to classify sentences, such as the position of sentence, sentence length and tense. Widyantoro et. al. used content features, qualifying adjectives and meta-discourse features BIBREF14 to explore AZ task.\n\nAnswer B:\nWhat are the strengths of these features?\n< Explanation: The hand-crafted features are the following: (1) the number of words in the sentence, (2) the number of words in the sentence and the number of words in the cuewords, (3) the number of words in the sentence and the number of words in the cuewords, (4) the number of words in the sentence and the number of words in the cuewords, (5) the number of words in the sentence and the number of words in the cuewords, (6) the number of words in the sentence and the number of words in the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1703.10152",
    "question": "What word embeddings are used?",
    "gold_answer": "",
    "answer_a": "INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 ). Explanation: In this study, INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 ) inspired by Tang et al. BIBREF21 's model: Sentiment-Specific Word Embedding (unified model: INLINEFORM3 ).",
    "answer_b": "How are they learned?\n< Explanation: The word embeddings used in this study are the word2vec model. The word2vec model is a probabilistic model that learns word embeddings from a large corpus of text. The word2vec model is a probabilistic model that learns word embeddings from a large corpus of text. The word2vec model is a probabilistic model that learns word embeddings from a large corpus of text. The word2vec model is a probabilistic model that learns word embeddings from a large corpus of text. The word2vec model is a probabilistic model that learns word embeddings from a large corpus of text. The",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat word embeddings are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nINLINEFORM1 specific word embeddings were trained ( INLINEFORM2 ). Explanation: In this study, INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 ) inspired by Tang et al. BIBREF21 's model: Sentiment-Specific Word Embedding (unified model: INLINEFORM3 ).\n\nAnswer B:\nHow are they learned?\n< Explanation: The word embeddings used in this study are the word2vec model. The word2vec model is a probabilistic model that learns word embeddings from a large corpus of text. The word2vec model is a probabilistic model that learns word embeddings from a large corpus of text. The word2vec model is a probabilistic model that learns word embeddings from a large corpus of text. The word2vec model is a probabilistic model that learns word embeddings from a large corpus of text. The word2vec model is a probabilistic model that learns word embeddings from a large corpus of text. The\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1703.10152",
    "question": "How are the sentence embeddings generated?",
    "gold_answer": "",
    "answer_a": "What are the features used?\n< Explanation: The sentence embeddings are generated by the word2vec method. The word2vec method is a popular method for learning word embeddings. It is a probabilistic model that learns a word embedding from a large corpus of text. The word2vec method is a probabilistic model that learns a word embedding from a large corpus of text. The word2vec method is a probabilistic model that learns a word embedding from a large corpus of text. The word2vec method is a probabilistic model that learns a word embedding from a large corpus of text. The word2vec method is a probabilistic",
    "answer_b": "sentence embeddings were learned from large text corpus as features to classify sentences into seven categories in the task of AZ. Three models were explored to obtain the sentence vectors: averaging the vectors of the words in one sentence, paragraph vectors and specific word vectors. Explanation: In this study, sentence embeddings were learned from large text corpus as features to classify sentences into seven categories in the task of AZ. Three models were explored to obtain the sentence vectors: averaging the vectors of the words in one sentence, paragraph vectors and specific word vectors.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are the sentence embeddings generated?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the features used?\n< Explanation: The sentence embeddings are generated by the word2vec method. The word2vec method is a popular method for learning word embeddings. It is a probabilistic model that learns a word embedding from a large corpus of text. The word2vec method is a probabilistic model that learns a word embedding from a large corpus of text. The word2vec method is a probabilistic model that learns a word embedding from a large corpus of text. The word2vec method is a probabilistic model that learns a word embedding from a large corpus of text. The word2vec method is a probabilistic\n\nAnswer B:\nsentence embeddings were learned from large text corpus as features to classify sentences into seven categories in the task of AZ. Three models were explored to obtain the sentence vectors: averaging the vectors of the words in one sentence, paragraph vectors and specific word vectors. Explanation: In this study, sentence embeddings were learned from large text corpus as features to classify sentences into seven categories in the task of AZ. Three models were explored to obtain the sentence vectors: averaging the vectors of the words in one sentence, paragraph vectors and specific word vectors.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1703.10152",
    "question": "What is argumentative zoning?",
    "gold_answer": "",
    "answer_a": "Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences. Explanation: Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences.",
    "answer_b": "How is it different from other classification tasks?\n< Explanation: Argumentative-Zoning (AZ) is a task of classifying the argumentative nature of a sentence. It is a challenging task because it requires the classification of the argumentative nature of a sentence, which is not a simple binary classification task. The task is also challenging because it requires the classification of the argumentative nature of a sentence, which is not a simple binary classification task. The task is also challenging because it requires the classification of the argumentative nature of a sentence, which is not a simple binary classification task. The task is also challenging because it requires the classification of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is argumentative zoning?\n\nGround Truth Answer:\n\n\nAnswer A:\nRhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences. Explanation: Rhetorical sentence classification, also known as argumentative zoning (AZ) BIBREF3 , is a process of assigning rhetorical status to the extracted sentences.\n\nAnswer B:\nHow is it different from other classification tasks?\n< Explanation: Argumentative-Zoning (AZ) is a task of classifying the argumentative nature of a sentence. It is a challenging task because it requires the classification of the argumentative nature of a sentence, which is not a simple binary classification task. The task is also challenging because it requires the classification of the argumentative nature of a sentence, which is not a simple binary classification task. The task is also challenging because it requires the classification of the argumentative nature of a sentence, which is not a simple binary classification task. The task is also challenging because it requires the classification of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.04072",
    "question": "How did they obtain the tweets?",
    "gold_answer": "",
    "answer_a": "What is the dataset used?\n< Explanation: We use the Twitter dataset from the Stanford NLP Corpus (SNC) BIBREF11 . The dataset contains 100,000 tweets from 2013 to 2016. We use the tweets that have been posted on the Twitter platform to train our model. We use the tweets that have been posted on the Twitter platform to train our model. We use the tweets that have been posted on the Twitter platform to train our model. We use the tweets that have been posted on the Twitter platform to train our model. We use the tweets that",
    "answer_b": "crawled two blackmarket sites. Explanation: We crawled two blackmarket sites \u2013 YouLikeHits and Like4Like, between the period of February and April 2019. We created dummy accounts (after careful IRB approval) on these sites to participate in the platform and recorded Tweet IDs of the tweets that were posted for gaining retweets. We used Twitter's REST API to collect the tweet objects of these tweets.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow did they obtain the tweets?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the dataset used?\n< Explanation: We use the Twitter dataset from the Stanford NLP Corpus (SNC) BIBREF11 . The dataset contains 100,000 tweets from 2013 to 2016. We use the tweets that have been posted on the Twitter platform to train our model. We use the tweets that have been posted on the Twitter platform to train our model. We use the tweets that have been posted on the Twitter platform to train our model. We use the tweets that have been posted on the Twitter platform to train our model. We use the tweets that\n\nAnswer B:\ncrawled two blackmarket sites. Explanation: We crawled two blackmarket sites \u2013 YouLikeHits and Like4Like, between the period of February and April 2019. We created dummy accounts (after careful IRB approval) on these sites to participate in the platform and recorded Tweet IDs of the tweets that were posted for gaining retweets. We used Twitter's REST API to collect the tweet objects of these tweets.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.04072",
    "question": "What baseline do they compare to?",
    "gold_answer": "",
    "answer_a": "spam detection method proposed by Wu et al. BIBREF4 ,  spam detection method  proposed by Rajdev et. al. BIBREF11,   feature vector by concatenating the tweet content features with Random Forest,  feature vector by concatenating the tweet content features with Multi-layer Perception and  feature vector by concatenating the tweet content features with Support Vector Machine.. Explanation: Since there is no prior work on blackmarket tweet detection, we chose state-of-the-art Twitter spam detection methods as baselines, along with training some state-of-the-art classifiers on the features we generated for our dataset.\n\nSpam Detection 1: We use the Twitter spam detection method proposed by Wu et al. BIBREF4 . It uses the Word2Vec and Doc2Vec models to encode the tweets into a vector representation, which is fed to a MLP classifier in order to classify the tweets as spam or not-spam. We use the same methodology to classify tweets in our dataset as blackmarket or genuine.\n\nSpam Detection 2: For baseline 2, we consider the approach proposed by Rajdev et. al. BIBREF11 . They proposed flat and hierarchical classifications approaches with few of the standard set of features which can classify spam, fake and legitimate tweets. We use their experimental setup with Random Forest classifier on our dataset.\n\nWe generate a combined feature vector by concatenating the tweet content features and the encoding generated by Tweet2Vec. This feature vector is then fed to state-of-the-art machine learning classifiers - Random Forest (RF), Multi-layer Perceptron (MLP), and Support Vector Machine (SVM).",
    "answer_b": "How do they do it?\n< Explanation: We compare our model to the baseline of the original work of BIBREF10. The baseline model is a simple linear regression model that predicts the number of retweets for a tweet based on the number of retweets for a given tweet. The baseline model is trained on a training set of 1000 tweets and the number of retweets for each tweet is known. The baseline model is trained using the same training set and the number of retweets for each tweet is known. The baseline model is trained using the same training set and the number of retweets for each tweet is",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat baseline do they compare to?\n\nGround Truth Answer:\n\n\nAnswer A:\nspam detection method proposed by Wu et al. BIBREF4 ,  spam detection method  proposed by Rajdev et. al. BIBREF11,   feature vector by concatenating the tweet content features with Random Forest,  feature vector by concatenating the tweet content features with Multi-layer Perception and  feature vector by concatenating the tweet content features with Support Vector Machine.. Explanation: Since there is no prior work on blackmarket tweet detection, we chose state-of-the-art Twitter spam detection methods as baselines, along with training some state-of-the-art classifiers on the features we generated for our dataset.\n\nSpam Detection 1: We use the Twitter spam detection method proposed by Wu et al. BIBREF4 . It uses the Word2Vec and Doc2Vec models to encode the tweets into a vector representation, which is fed to a MLP classifier in order to classify the tweets as spam or not-spam. We use the same methodology to classify tweets in our dataset as blackmarket or genuine.\n\nSpam Detection 2: For baseline 2, we consider the approach proposed by Rajdev et. al. BIBREF11 . They proposed flat and hierarchical classifications approaches with few of the standard set of features which can classify spam, fake and legitimate tweets. We use their experimental setup with Random Forest classifier on our dataset.\n\nWe generate a combined feature vector by concatenating the tweet content features and the encoding generated by Tweet2Vec. This feature vector is then fed to state-of-the-art machine learning classifiers - Random Forest (RF), Multi-layer Perceptron (MLP), and Support Vector Machine (SVM).\n\nAnswer B:\nHow do they do it?\n< Explanation: We compare our model to the baseline of the original work of BIBREF10. The baseline model is a simple linear regression model that predicts the number of retweets for a tweet based on the number of retweets for a given tweet. The baseline model is trained on a training set of 1000 tweets and the number of retweets for each tweet is known. The baseline model is trained using the same training set and the number of retweets for each tweet is known. The baseline model is trained using the same training set and the number of retweets for each tweet is\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.04072",
    "question": "What language is explored in this paper?",
    "gold_answer": "",
    "answer_a": "English. Explanation: Out of these, we removed non-English tweets and tweets with a length of less than two characters.",
    "answer_b": "How is the language explored?\n< Explanation: We use English as the language for this paper. We use the Twitter API to retrieve tweets from the last 30 days. We use the Twitter API to retrieve tweets from the last 30 days. We use the Twitter API to retrieve tweets from the last 30 days. We use the Twitter API to retrieve tweets from the last 30 days. We use the Twitter API to retrieve tweets from the last 30 days. We use the Twitter API to retrieve tweets from the last 30 days. We use the Twitter API to retrieve tweets from the last 3",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat language is explored in this paper?\n\nGround Truth Answer:\n\n\nAnswer A:\nEnglish. Explanation: Out of these, we removed non-English tweets and tweets with a length of less than two characters.\n\nAnswer B:\nHow is the language explored?\n< Explanation: We use English as the language for this paper. We use the Twitter API to retrieve tweets from the last 30 days. We use the Twitter API to retrieve tweets from the last 30 days. We use the Twitter API to retrieve tweets from the last 30 days. We use the Twitter API to retrieve tweets from the last 30 days. We use the Twitter API to retrieve tweets from the last 30 days. We use the Twitter API to retrieve tweets from the last 30 days. We use the Twitter API to retrieve tweets from the last 3\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.04072",
    "question": "What blackmarket services do they look at?",
    "gold_answer": "",
    "answer_a": "How do they classify tweets?\n< Explanation: We use the Twitter API to retrieve tweets from the last 30 days. We then use the Twitter API to retrieve the number of retweets for each tweet. We then use the Twitter API to retrieve the number of retweets for each user. We then use the Twitter API to retrieve the number of retweets for each user's followers. We then use the Twitter API to retrieve the number of retweets for each user's followers' followers. We then use the Twitter API to retrieve the number of retweets for each user's followers' followers' followers. We then use the Twitter API",
    "answer_b": "Credit-based Freemium services. Explanation: We collected data from Credit-based Freemium services because their service model is easy to understand.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat blackmarket services do they look at?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they classify tweets?\n< Explanation: We use the Twitter API to retrieve tweets from the last 30 days. We then use the Twitter API to retrieve the number of retweets for each tweet. We then use the Twitter API to retrieve the number of retweets for each user. We then use the Twitter API to retrieve the number of retweets for each user's followers. We then use the Twitter API to retrieve the number of retweets for each user's followers' followers. We then use the Twitter API to retrieve the number of retweets for each user's followers' followers' followers. We then use the Twitter API\n\nAnswer B:\nCredit-based Freemium services. Explanation: We collected data from Credit-based Freemium services because their service model is easy to understand.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.10481",
    "question": "What languages do they use during pretraining?",
    "gold_answer": "",
    "answer_a": "How do they pretrain the model?\n<sep>We pretrain the model on the following languages: English, French, German, Spanish, Italian, Portuguese, and Chinese. We use the same pre-trained model for all languages. The pre-trained model is a BERT model trained on the English Wikipedia corpus. The pre-trained model is pre-trained on the English Wikipedia corpus and then fine-tuned on the following languages: English, French, German, Spanish, Italian, Portuguese, and Chinese. The pre-trained model is fine-tuned on the following languages: English, French, German, Spanish, Italian, Portuguese, and Chinese.",
    "answer_b": "English. Explanation: We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat languages do they use during pretraining?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they pretrain the model?\n<sep>We pretrain the model on the following languages: English, French, German, Spanish, Italian, Portuguese, and Chinese. We use the same pre-trained model for all languages. The pre-trained model is a BERT model trained on the English Wikipedia corpus. The pre-trained model is pre-trained on the English Wikipedia corpus and then fine-tuned on the following languages: English, French, German, Spanish, Italian, Portuguese, and Chinese. The pre-trained model is fine-tuned on the following languages: English, French, German, Spanish, Italian, Portuguese, and Chinese.\n\nAnswer B:\nEnglish. Explanation: We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.10481",
    "question": "What is the architecture of the decoder?",
    "gold_answer": "",
    "answer_a": "What is the architecture of the encoder?\n<sep>Answer: The encoder is a bidirectional LSTM with a hidden size of 512. The decoder is a bidirectional LSTM with a hidden size of 512. The encoder and decoder share the same hidden size. The encoder and decoder are connected by a linear layer. The linear layer is a fully connected layer with a hidden size of 512. The linear layer is connected to the output layer of the encoder and decoder. The output layer of the encoder and decoder is a linear layer with a hidden size of 512. The output layer of the",
    "answer_b": "pre-trained Xnlg. Explanation: We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the architecture of the decoder?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the architecture of the encoder?\n<sep>Answer: The encoder is a bidirectional LSTM with a hidden size of 512. The decoder is a bidirectional LSTM with a hidden size of 512. The encoder and decoder share the same hidden size. The encoder and decoder are connected by a linear layer. The linear layer is a fully connected layer with a hidden size of 512. The linear layer is connected to the output layer of the encoder and decoder. The output layer of the encoder and decoder is a linear layer with a hidden size of 512. The output layer of the\n\nAnswer B:\npre-trained Xnlg. Explanation: We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.10481",
    "question": "What is the architecture of the encoder?",
    "gold_answer": "",
    "answer_a": "pre-trained Xnlg with a 10-layer encoder. Explanation: We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder.",
    "answer_b": "What is the architecture of the decoder?\n<sep>Answer: The encoder consists of a bidirectional LSTM encoder and a bidirectional LSTM decoder. The encoder is trained to learn the shared representation of the input sequence in the shared space. The decoder is trained to generate the output sequence in the shared space. The decoder is trained to learn the output sequence in the shared space. The decoder is trained to learn the output sequence in the shared space. The decoder is trained to learn the output sequence in the shared space. The decoder is trained to learn the output sequence in the shared space. The decoder is trained to learn the output sequence in the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the architecture of the encoder?\n\nGround Truth Answer:\n\n\nAnswer A:\npre-trained Xnlg with a 10-layer encoder. Explanation: We use a pre-trained Xnlg with a 10-layer encoder and a 6-layer decoder.\n\nAnswer B:\nWhat is the architecture of the decoder?\n<sep>Answer: The encoder consists of a bidirectional LSTM encoder and a bidirectional LSTM decoder. The encoder is trained to learn the shared representation of the input sequence in the shared space. The decoder is trained to generate the output sequence in the shared space. The decoder is trained to learn the output sequence in the shared space. The decoder is trained to learn the output sequence in the shared space. The decoder is trained to learn the output sequence in the shared space. The decoder is trained to learn the output sequence in the shared space. The decoder is trained to learn the output sequence in the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.10481",
    "question": "What is their baseline?",
    "gold_answer": "",
    "answer_a": "CorefNqg BIBREF33. Explanation: We first conduct experiments on the supervised English-English QG setting. We compare our model to the following baselines:\n\nCorefNqg BIBREF33 A sequence-to-sequence model with attention mechanism and a feature-rich encoder.\n\nMp-Gsn BIBREF31 A sequence-to-sequence model with gated self-attention and maxout pointer mechanism.\n\nXlm BIBREF5 The current state-of-the-art cross-lingual pre-training model. We initialize the Transformer-based sequence-to-sequence model with pre-trained XLM.",
    "answer_b": "What is their baseline?\n< Explanation: We compare our model with the following baselines: (1) a baseline that uses the same encoder and decoder as ours, but with the same language model architecture (i.e., a single encoder and decoder); (2) a baseline that uses the same encoder and decoder as ours, but with the same language model architecture (i.e., a single encoder and decoder) but with a different language model architecture; (3) a baseline that uses the same encoder and decoder as ours, but with a different language model architecture (i.e., a single encoder and decoder) but with a different language model",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is their baseline?\n\nGround Truth Answer:\n\n\nAnswer A:\nCorefNqg BIBREF33. Explanation: We first conduct experiments on the supervised English-English QG setting. We compare our model to the following baselines:\n\nCorefNqg BIBREF33 A sequence-to-sequence model with attention mechanism and a feature-rich encoder.\n\nMp-Gsn BIBREF31 A sequence-to-sequence model with gated self-attention and maxout pointer mechanism.\n\nXlm BIBREF5 The current state-of-the-art cross-lingual pre-training model. We initialize the Transformer-based sequence-to-sequence model with pre-trained XLM.\n\nAnswer B:\nWhat is their baseline?\n< Explanation: We compare our model with the following baselines: (1) a baseline that uses the same encoder and decoder as ours, but with the same language model architecture (i.e., a single encoder and decoder); (2) a baseline that uses the same encoder and decoder as ours, but with the same language model architecture (i.e., a single encoder and decoder) but with a different language model architecture; (3) a baseline that uses the same encoder and decoder as ours, but with a different language model architecture (i.e., a single encoder and decoder) but with a different language model\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.04833",
    "question": "What human evaluation metrics do they look at?",
    "gold_answer": "",
    "answer_a": "How do they evaluate the models?\n< Explanation: We evaluate the models using the following metrics: BLEU, METEOR, and F1. BLEU is a metric that measures the quality of the generated text relative to the reference text. METEOR is a metric that measures the quality of the generated text relative to the prompt. F1 is a metric that measures the quality of the generated text relative to the reference text. We use the following thresholds: BLEU >= 0.0, METEOR >= 0.0, and F1 >= 0.5. We also evaluate the models using the following metrics",
    "answer_b": "human preference. Explanation: Lastly, we conduct human evaluation to evaluate the importance of hierarchical generation for story writing. We use Amazon Mechanical Turk to compare the stories from hierarchical generation from a prompt with generation without a prompt. 400 pairs of stories were evaluated by 5 judges each in a blind test.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat human evaluation metrics do they look at?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate the models?\n< Explanation: We evaluate the models using the following metrics: BLEU, METEOR, and F1. BLEU is a metric that measures the quality of the generated text relative to the reference text. METEOR is a metric that measures the quality of the generated text relative to the prompt. F1 is a metric that measures the quality of the generated text relative to the reference text. We use the following thresholds: BLEU >= 0.0, METEOR >= 0.0, and F1 >= 0.5. We also evaluate the models using the following metrics\n\nAnswer B:\nhuman preference. Explanation: Lastly, we conduct human evaluation to evaluate the importance of hierarchical generation for story writing. We use Amazon Mechanical Turk to compare the stories from hierarchical generation from a prompt with generation without a prompt. 400 pairs of stories were evaluated by 5 judges each in a blind test.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.04833",
    "question": "Which automated evaluation metrics are used?",
    "gold_answer": "",
    "answer_a": "What are the human evaluations?\n< Explanation: We use the following metrics to evaluate the generated stories: BLEU, METEOR, ROUGE, and F1. We also use the following metrics to evaluate the human evaluations: F1, Recall, Precision, and Accuracy. We use the following metrics to evaluate the human evaluations: F1, Recall, Precision, and Accuracy. We use the following metrics to evaluate the human evaluations: F1, Recall, Precision, and Accuracy. We use the following metrics to evaluate the human evaluations: F1, Recall, Precision, and Accuracy. We use the following metrics to evaluate the human",
    "answer_b": "perplexity. Explanation: For automatic evaluation, we measure model perplexity on the test set and prompt ranking accuracy.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich automated evaluation metrics are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the human evaluations?\n< Explanation: We use the following metrics to evaluate the generated stories: BLEU, METEOR, ROUGE, and F1. We also use the following metrics to evaluate the human evaluations: F1, Recall, Precision, and Accuracy. We use the following metrics to evaluate the human evaluations: F1, Recall, Precision, and Accuracy. We use the following metrics to evaluate the human evaluations: F1, Recall, Precision, and Accuracy. We use the following metrics to evaluate the human evaluations: F1, Recall, Precision, and Accuracy. We use the following metrics to evaluate the human\n\nAnswer B:\nperplexity. Explanation: For automatic evaluation, we measure model perplexity on the test set and prompt ranking accuracy.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.04833",
    "question": "What baselines do they compare against?",
    "gold_answer": "",
    "answer_a": "gated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism. Explanation: We evaluate a number of baselines:\n\n(1) Language Models: Non-hierarchical models for story generation, which do not condition on the prompt. We use both the gated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism.\n\n(2) seq2seq: using LSTMs and convolutional seq2seq architectures, and Conv seq2seq with decoder self-attention.\n\n(3) Ensemble: an ensemble of two Conv seq2seq with self-attention models.\n\n(4) KNN: we also compare with a KNN model to find the closest prompt in the training set for each prompt in the test set. A TF-IDF vector for each prompt was created using fasttext BIBREF24 and faiss BIBREF25 was used for KNN search.",
    "answer_b": "What is the human evaluation?\n< Explanation: We compare our model to the following baselines: (1) a baseline that uses a simple neural model with no attention mechanism, (2) a baseline that uses a simple neural model with no self-attention mechanism, (3) a baseline that uses a simple neural model with no self-attention mechanism and no gating, (4) a baseline that uses a simple neural model with no self-attention mechanism and no gating, (5) a baseline that uses a simple neural model with no self-attention mechanism and no gating, (6) a baseline that uses a simple neural model with",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat baselines do they compare against?\n\nGround Truth Answer:\n\n\nAnswer A:\ngated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism. Explanation: We evaluate a number of baselines:\n\n(1) Language Models: Non-hierarchical models for story generation, which do not condition on the prompt. We use both the gated convolutional language (GCNN) model of BIBREF4 and our additional self-attention mechanism.\n\n(2) seq2seq: using LSTMs and convolutional seq2seq architectures, and Conv seq2seq with decoder self-attention.\n\n(3) Ensemble: an ensemble of two Conv seq2seq with self-attention models.\n\n(4) KNN: we also compare with a KNN model to find the closest prompt in the training set for each prompt in the test set. A TF-IDF vector for each prompt was created using fasttext BIBREF24 and faiss BIBREF25 was used for KNN search.\n\nAnswer B:\nWhat is the human evaluation?\n< Explanation: We compare our model to the following baselines: (1) a baseline that uses a simple neural model with no attention mechanism, (2) a baseline that uses a simple neural model with no self-attention mechanism, (3) a baseline that uses a simple neural model with no self-attention mechanism and no gating, (4) a baseline that uses a simple neural model with no self-attention mechanism and no gating, (5) a baseline that uses a simple neural model with no self-attention mechanism and no gating, (6) a baseline that uses a simple neural model with\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.04833",
    "question": "What model is used to generate the premise?",
    "gold_answer": "",
    "answer_a": "convolutional language model from BIBREF4. Explanation: First, we generate the premise or prompt of the story using the convolutional language model from BIBREF4 .",
    "answer_b": "< Explanation: We use a Transformer-based model to generate the premise. We train a Transformer model with a pre-trained BERT-base model BIBREF10 and fine-tune it with a large amount of data. We use the BERT model as a base model to train the Transformer model. We train the Transformer model with a pre-trained BERT-base model BIBREF10 and fine-tune it with a large amount of data. We use the BERT model as a base model to train the Transformer model. We train the Transformer model with a pre-trained BERT-base model BIBREF10 and fine-t",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat model is used to generate the premise?\n\nGround Truth Answer:\n\n\nAnswer A:\nconvolutional language model from BIBREF4. Explanation: First, we generate the premise or prompt of the story using the convolutional language model from BIBREF4 .\n\nAnswer B:\n< Explanation: We use a Transformer-based model to generate the premise. We train a Transformer model with a pre-trained BERT-base model BIBREF10 and fine-tune it with a large amount of data. We use the BERT model as a base model to train the Transformer model. We train the Transformer model with a pre-trained BERT-base model BIBREF10 and fine-tune it with a large amount of data. We use the BERT model as a base model to train the Transformer model. We train the Transformer model with a pre-trained BERT-base model BIBREF10 and fine-t\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.04833",
    "question": "Where are the stories collected from?",
    "gold_answer": "",
    "answer_a": "Is the dataset publicly available?\n< Explanation: We collected 300K human-written stories from the online forum Storytellers. We also collected 100K stories from the online forum Storytellers. We also collected 100K stories from the online forum Storytellers. We also collected 100K stories from the online forum Storytellers. We also collected 100K stories from the online forum Storytellers. We also collected 100K stories from the online forum Storytellers. We also collected 100K stories from the online forum Storytellers",
    "answer_b": "online forum. Explanation: To train our models, we gathered a large dataset of 303,358 human generated stories paired with writing prompts from an online forum.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhere are the stories collected from?\n\nGround Truth Answer:\n\n\nAnswer A:\nIs the dataset publicly available?\n< Explanation: We collected 300K human-written stories from the online forum Storytellers. We also collected 100K stories from the online forum Storytellers. We also collected 100K stories from the online forum Storytellers. We also collected 100K stories from the online forum Storytellers. We also collected 100K stories from the online forum Storytellers. We also collected 100K stories from the online forum Storytellers. We also collected 100K stories from the online forum Storytellers\n\nAnswer B:\nonline forum. Explanation: To train our models, we gathered a large dataset of 303,358 human generated stories paired with writing prompts from an online forum.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.07882",
    "question": "which pretrained embeddings were experimented with?",
    "gold_answer": "",
    "answer_a": "What are the dimensions of the embeddings?\n< Explanation: We experimented with the following pretrained word embeddings: GloVe, FastText, Word2Vec, BERT, and BERT-base. The dimension of the pretrained embeddings is 128, 300, 300, 128, and 768 respectively. The pretrained embeddings were downloaded from the following websites: https://nlp.stanford.edu/data/glove. The pretrained embeddings were trained using the following methods: Word2Vec, FastText, BERT, and BERT-base. The pretrained embeddings were trained using the following methods:",
    "answer_b": "word2vec. Explanation: We study five pre-trained word embeddings for our model:\n\nword2vec is trained on Google News dataset (100 billion tokens). The model contains 300-dimensional vectors for 3 million words and phrases.\n\nfastText is learned via skip-gram with subword information on Wikipedia text. The embedding representations in fastText are 300-dimensional vectors.\n\nGloVe is a 300-dimensional word embedding model learned on aggregated global word-word co-occurrence statistics from Common Crawl (840 billion tokens).\n\nBaroni uses a context-predict approach to learn a 400-dimensional semantic embedding model. It is trained on 2.8 billion tokens constructed from ukWaC, the English Wikipedia and the British National Corpus.\n\nSL999 is trained under the skip-gram objective with negative sampling on word pairs from the paraphrase database PPDB. This 300-dimensional embedding model is tuned on SimLex-999 dataset BIBREF27 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhich pretrained embeddings were experimented with?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the dimensions of the embeddings?\n< Explanation: We experimented with the following pretrained word embeddings: GloVe, FastText, Word2Vec, BERT, and BERT-base. The dimension of the pretrained embeddings is 128, 300, 300, 128, and 768 respectively. The pretrained embeddings were downloaded from the following websites: https://nlp.stanford.edu/data/glove. The pretrained embeddings were trained using the following methods: Word2Vec, FastText, BERT, and BERT-base. The pretrained embeddings were trained using the following methods:\n\nAnswer B:\nword2vec. Explanation: We study five pre-trained word embeddings for our model:\n\nword2vec is trained on Google News dataset (100 billion tokens). The model contains 300-dimensional vectors for 3 million words and phrases.\n\nfastText is learned via skip-gram with subword information on Wikipedia text. The embedding representations in fastText are 300-dimensional vectors.\n\nGloVe is a 300-dimensional word embedding model learned on aggregated global word-word co-occurrence statistics from Common Crawl (840 billion tokens).\n\nBaroni uses a context-predict approach to learn a 400-dimensional semantic embedding model. It is trained on 2.8 billion tokens constructed from ukWaC, the English Wikipedia and the British National Corpus.\n\nSL999 is trained under the skip-gram objective with negative sampling on word pairs from the paraphrase database PPDB. This 300-dimensional embedding model is tuned on SimLex-999 dataset BIBREF27 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.07882",
    "question": "what datasets where used?",
    "gold_answer": "",
    "answer_a": "< Explanation: We use the STS Benchmark dataset (STSBenchmark) from SemEval 2016. The dataset consists of 1000 sentences, each sentence is a pair of a sentence and its negation. The sentences are labeled as either \u201ctrue\u201d or \u201cfalse\u201d based on the similarity between the sentence and its negation. The dataset contains 1000 sentences, each with 1000 words. The sentences are divided into 1000 pairs of sentences and their negations. The sentences are labeled as either \u201ctrue\u201d or \u201cfalse\u201d based on the similarity",
    "answer_b": "STSB. Explanation: We report the results of these methods in Table TABREF49 . Overall, our M-MaxLSTM-CNN shows competitive performances in these tasks. Especially in the STS task, M-MaxLSTM-CNN outperforms the state-of-the-art methods on the two datasets. Because STSB includes complicated samples compared to SICK, the performances of methods on STSB are quite lower.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat datasets where used?\n\nGround Truth Answer:\n\n\nAnswer A:\n< Explanation: We use the STS Benchmark dataset (STSBenchmark) from SemEval 2016. The dataset consists of 1000 sentences, each sentence is a pair of a sentence and its negation. The sentences are labeled as either \u201ctrue\u201d or \u201cfalse\u201d based on the similarity between the sentence and its negation. The dataset contains 1000 sentences, each with 1000 words. The sentences are divided into 1000 pairs of sentences and their negations. The sentences are labeled as either \u201ctrue\u201d or \u201cfalse\u201d based on the similarity\n\nAnswer B:\nSTSB. Explanation: We report the results of these methods in Table TABREF49 . Overall, our M-MaxLSTM-CNN shows competitive performances in these tasks. Especially in the STS task, M-MaxLSTM-CNN outperforms the state-of-the-art methods on the two datasets. Because STSB includes complicated samples compared to SICK, the performances of methods on STSB are quite lower.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.07882",
    "question": "what are the state of the art methods they compare with?",
    "gold_answer": "",
    "answer_a": "What are the results?\n< Explanation: Table 1 shows the results of our model on STS Benchmark dataset and SICK dataset. Our model outperforms the state-of-the-art methods for textual similarity tasks. Our model also outperforms the state-of-the-art methods for textual entailment tasks. Our model also outperforms the state-of-the-art methods for paraphrase tasks. Our model also outperforms the state-of-the-art methods for sentence classification tasks. Our model also outperforms the state-of-the-art methods for sentence classification tasks. Our model also outperforms the state-of-the-art methods for sentence classification tasks",
    "answer_b": "ECNU. Explanation: HCTI BIBREF5 , which is the current state-of-the-art in the group of neural representation models on STSB, also used the Glove embedding. However, the performance of HTCI in STSB ( INLINEFORM1 ) is lower than our model using the Glove embedding.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the state of the art methods they compare with?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n< Explanation: Table 1 shows the results of our model on STS Benchmark dataset and SICK dataset. Our model outperforms the state-of-the-art methods for textual similarity tasks. Our model also outperforms the state-of-the-art methods for textual entailment tasks. Our model also outperforms the state-of-the-art methods for paraphrase tasks. Our model also outperforms the state-of-the-art methods for sentence classification tasks. Our model also outperforms the state-of-the-art methods for sentence classification tasks. Our model also outperforms the state-of-the-art methods for sentence classification tasks\n\nAnswer B:\nECNU. Explanation: HCTI BIBREF5 , which is the current state-of-the-art in the group of neural representation models on STSB, also used the Glove embedding. However, the performance of HTCI in STSB ( INLINEFORM1 ) is lower than our model using the Glove embedding.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.01820",
    "question": "What agreement measure is used?",
    "gold_answer": "",
    "answer_a": "Fleiss's Kappa. Explanation: For each thread, we collected annotations from three different workers, and from this data we computed our reliability metrics using Fleiss's Kappa for inter-annotator agreement as shown in Table TABREF17.",
    "answer_b": "How is it calculated?\n< Explanation: We use the Krippendorff\u2019s alpha coefficient to measure agreement between annotators. The Krippendorff\u2019s alpha coefficient is a measure of agreement between two or more raters. It is defined as the ratio of the number of times the raters agree to the total number of times the raters disagree. The Krippendorff\u2019s alpha coefficient is a measure of agreement between raters. It is defined as the ratio of the number of times the raters agree to the total number of times the raters disagree. The Krippendorff\u2019s alpha coefficient is a measure of agreement between",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat agreement measure is used?\n\nGround Truth Answer:\n\n\nAnswer A:\nFleiss's Kappa. Explanation: For each thread, we collected annotations from three different workers, and from this data we computed our reliability metrics using Fleiss's Kappa for inter-annotator agreement as shown in Table TABREF17.\n\nAnswer B:\nHow is it calculated?\n< Explanation: We use the Krippendorff\u2019s alpha coefficient to measure agreement between annotators. The Krippendorff\u2019s alpha coefficient is a measure of agreement between two or more raters. It is defined as the ratio of the number of times the raters agree to the total number of times the raters disagree. The Krippendorff\u2019s alpha coefficient is a measure of agreement between raters. It is defined as the ratio of the number of times the raters agree to the total number of times the raters disagree. The Krippendorff\u2019s alpha coefficient is a measure of agreement between\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.01820",
    "question": "How many annotators participated?",
    "gold_answer": "",
    "answer_a": "What was their average accuracy?\n< Explanation: We conducted a pilot study to evaluate the accuracy of the proposed annotation framework. We used 1000 randomly selected cases from the Twitter dataset to train a classifier. We then used the same 1000 cases to evaluate the accuracy of the classifier. We found that the accuracy of the classifier was 70.5% on average. The results of the pilot study are shown in Table TABREF11. The results of the pilot study suggest that the proposed annotation framework is effective in representing and annotating cyberbullying cases.\n\nWe then conducted a second study to evaluate the",
    "answer_b": "170. Explanation: We successfully recruited 170 workers to label all 6,897 available threads in our dataset.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many annotators participated?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat was their average accuracy?\n< Explanation: We conducted a pilot study to evaluate the accuracy of the proposed annotation framework. We used 1000 randomly selected cases from the Twitter dataset to train a classifier. We then used the same 1000 cases to evaluate the accuracy of the classifier. We found that the accuracy of the classifier was 70.5% on average. The results of the pilot study are shown in Table TABREF11. The results of the pilot study suggest that the proposed annotation framework is effective in representing and annotating cyberbullying cases.\n\nWe then conducted a second study to evaluate the\n\nAnswer B:\n170. Explanation: We successfully recruited 170 workers to label all 6,897 available threads in our dataset.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.01820",
    "question": "What social-network features are used?",
    "gold_answer": "",
    "answer_a": "Relative positions of the author and target accounts in the directed following network by\ncomputing modified versions of Jaccard\u2019s similarity index, friends count, followers count, verified status, number of tweets posted within 6 months.. Explanation: Specifically, we measure the relative positions of the author and target accounts in the directed following network by computing modified versions of Jaccard's similarity index as we now explain.",
    "answer_b": "Explanation: We use five explicit features to represent the social and linguistic aspects of cyberbullying: (1) the number of friends a user has, (2) the number of times a user has posted a comment, (3) the number of times a user has been flagged, (4) the number of times a user has been reported, and (5) the number of times a user has been deleted. We use these features to represent the social and linguistic aspects of cyberbullying. We also use the number of comments and flagged posts as features to represent the linguistic aspect of cyberbullying. We use the number of reported",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat social-network features are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nRelative positions of the author and target accounts in the directed following network by\ncomputing modified versions of Jaccard\u2019s similarity index, friends count, followers count, verified status, number of tweets posted within 6 months.. Explanation: Specifically, we measure the relative positions of the author and target accounts in the directed following network by computing modified versions of Jaccard's similarity index as we now explain.\n\nAnswer B:\nExplanation: We use five explicit features to represent the social and linguistic aspects of cyberbullying: (1) the number of friends a user has, (2) the number of times a user has posted a comment, (3) the number of times a user has been flagged, (4) the number of times a user has been reported, and (5) the number of times a user has been deleted. We use these features to represent the social and linguistic aspects of cyberbullying. We also use the number of comments and flagged posts as features to represent the linguistic aspect of cyberbullying. We use the number of reported\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.01820",
    "question": "What are the five factors considered?",
    "gold_answer": "",
    "answer_a": "How are they represented?\n< Explanation: We use five factors to represent cyberbullying: (1) the nature of the cyberbullying, (2) the nature of the victim, (3) the nature of the bully, (4) the nature of the victim\u2019s social network, and (5) the nature of the cyberbullying\u2019s content. We use these factors to represent cyberbullying as a social phenomenon. We represent the factors using a set of five features, each of which is a combination of a social and a linguistic feature. We use these features to train a classifier for cyberbullying detection. We use",
    "answer_b": "Aggressive language. Explanation: With this framework established, we gave the definitions for our five cyberbullying criteria as follows.\n\nAggressive language: (aggr) Regardless of the author's intent, the language of the tweet could be seen as aggressive.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the five factors considered?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they represented?\n< Explanation: We use five factors to represent cyberbullying: (1) the nature of the cyberbullying, (2) the nature of the victim, (3) the nature of the bully, (4) the nature of the victim\u2019s social network, and (5) the nature of the cyberbullying\u2019s content. We use these factors to represent cyberbullying as a social phenomenon. We represent the factors using a set of five features, each of which is a combination of a social and a linguistic feature. We use these features to train a classifier for cyberbullying detection. We use\n\nAnswer B:\nAggressive language. Explanation: With this framework established, we gave the definitions for our five cyberbullying criteria as follows.\n\nAggressive language: (aggr) Regardless of the author's intent, the language of the tweet could be seen as aggressive.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.01820",
    "question": "How is cyberbullying defined?",
    "gold_answer": "",
    "answer_a": "They define cyberbullying as aggressive language, repetition, harmful intent, visibility among peers, and power imbalance. Explanation: With this framework established, we gave the definitions for our five cyberbullying criteria as follows.\n\nAggressive language: (aggr) Regardless of the author's intent, the language of the tweet could be seen as aggressive. The user either addresses a group or individual, and the message contains at least one phrase that could be described as confrontational, derogatory, insulting, threatening, hostile, violent, hateful, or sexually abusive.\n\nRepetition: (rep) The target user has received at least two aggressive messages in total (either from the author or from another user in the visible thread).\n\nHarmful intent: (harm) The tweet was designed to tear down or disadvantage the target user by causing them distress or by harming their public image. The target does not respond agreeably as to a joke or an otherwise lighthearted comment.\n\nVisibility among peers: (peer) At least one other user besides the target has liked, retweeted, or responded to at least one of the author's messages.\n\nPower imbalance: (power) Power is derived from authority and perceived social advantage. Celebrities and public figures are more powerful than common users. Minorities and disadvantaged groups have less power. Bullies can also derive power from peer support.",
    "answer_b": "What are the five explicit factors used to represent its social and linguistic aspects?\n<blockquote>Cyberbullying is defined as the use of technology to harass, intimidate, or threaten another person online, or in person, in a way that is harmful to the person's reputation or well-being. Cyberbullying can be perpetrated by individuals or groups, and can be directed at individuals, groups, or organizations. Cyberbullying can be anonymous, and can be repeated, or it can be targeted at a specific person or group. Cyberbullying can be perpetrated by individuals or groups, and can be directed at individuals, groups, or organizations",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is cyberbullying defined?\n\nGround Truth Answer:\n\n\nAnswer A:\nThey define cyberbullying as aggressive language, repetition, harmful intent, visibility among peers, and power imbalance. Explanation: With this framework established, we gave the definitions for our five cyberbullying criteria as follows.\n\nAggressive language: (aggr) Regardless of the author's intent, the language of the tweet could be seen as aggressive. The user either addresses a group or individual, and the message contains at least one phrase that could be described as confrontational, derogatory, insulting, threatening, hostile, violent, hateful, or sexually abusive.\n\nRepetition: (rep) The target user has received at least two aggressive messages in total (either from the author or from another user in the visible thread).\n\nHarmful intent: (harm) The tweet was designed to tear down or disadvantage the target user by causing them distress or by harming their public image. The target does not respond agreeably as to a joke or an otherwise lighthearted comment.\n\nVisibility among peers: (peer) At least one other user besides the target has liked, retweeted, or responded to at least one of the author's messages.\n\nPower imbalance: (power) Power is derived from authority and perceived social advantage. Celebrities and public figures are more powerful than common users. Minorities and disadvantaged groups have less power. Bullies can also derive power from peer support.\n\nAnswer B:\nWhat are the five explicit factors used to represent its social and linguistic aspects?\n<blockquote>Cyberbullying is defined as the use of technology to harass, intimidate, or threaten another person online, or in person, in a way that is harmful to the person's reputation or well-being. Cyberbullying can be perpetrated by individuals or groups, and can be directed at individuals, groups, or organizations. Cyberbullying can be anonymous, and can be repeated, or it can be targeted at a specific person or group. Cyberbullying can be perpetrated by individuals or groups, and can be directed at individuals, groups, or organizations\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.04387",
    "question": "What evaluation was performed on the output?",
    "gold_answer": "",
    "answer_a": "similarity of the generated texts with training data objectively. Explanation: We evaluated the similarity of the generated texts with training data objectively and the humor content subjectively. We also checked the syntactic correctness of the generated sentences.",
    "answer_b": "How was it evaluated?\n< Explanation: We evaluate the performance of the model on the generated jokes and quotes using the following metrics: Recall, Precision, F1 score, and accuracy. Recall is the proportion of correctly predicted jokes and quotes out of the total number of jokes and quotes generated. Precision is the proportion of correctly predicted jokes and quotes out of the total number of jokes and quotes generated. F1 score is the harmonic mean of precision and recall. Accuracy is the proportion of correctly predicted jokes and quotes out of the total number of jokes and quotes generated. We use the following thresholds: Recall = 0.7, Precision =",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat evaluation was performed on the output?\n\nGround Truth Answer:\n\n\nAnswer A:\nsimilarity of the generated texts with training data objectively. Explanation: We evaluated the similarity of the generated texts with training data objectively and the humor content subjectively. We also checked the syntactic correctness of the generated sentences.\n\nAnswer B:\nHow was it evaluated?\n< Explanation: We evaluate the performance of the model on the generated jokes and quotes using the following metrics: Recall, Precision, F1 score, and accuracy. Recall is the proportion of correctly predicted jokes and quotes out of the total number of jokes and quotes generated. Precision is the proportion of correctly predicted jokes and quotes out of the total number of jokes and quotes generated. F1 score is the harmonic mean of precision and recall. Accuracy is the proportion of correctly predicted jokes and quotes out of the total number of jokes and quotes generated. We use the following thresholds: Recall = 0.7, Precision =\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.04387",
    "question": "Where did the joke data come from?",
    "gold_answer": "",
    "answer_a": "CrowdTruth and Subreddits. Explanation: The two sources for jokes are CrowdTruth and Subreddits.",
    "answer_b": "What is the size of the dataset?\n< Explanation: The jokes dataset is taken from the website jokes.com. The jokes are categorized into 1000 categories. The dataset is split into training and testing sets. The training set is 80% and the testing set is 20%. The jokes are in the form of a string of words. The jokes are in the form of a string of words. The jokes are in the form of a string of words. The jokes are in the form of a string of words. The jokes are in the form of a string of words. The jokes are in the form of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhere did the joke data come from?\n\nGround Truth Answer:\n\n\nAnswer A:\nCrowdTruth and Subreddits. Explanation: The two sources for jokes are CrowdTruth and Subreddits.\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: The jokes dataset is taken from the website jokes.com. The jokes are categorized into 1000 categories. The dataset is split into training and testing sets. The training set is 80% and the testing set is 20%. The jokes are in the form of a string of words. The jokes are in the form of a string of words. The jokes are in the form of a string of words. The jokes are in the form of a string of words. The jokes are in the form of a string of words. The jokes are in the form of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.04387",
    "question": "What type of quotes is this system trying to generate?",
    "gold_answer": "",
    "answer_a": "inspirational. Explanation: A controlled LSTM can be used to train a network in a supervised way on multiple categorical data like jokes, quotes, and tweets by augmenting the category tag to the input word at every time-step",
    "answer_b": "(Hint: they are not jokes)\n<=> Explanation: The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of quotes is this system trying to generate?\n\nGround Truth Answer:\n\n\nAnswer A:\ninspirational. Explanation: A controlled LSTM can be used to train a network in a supervised way on multiple categorical data like jokes, quotes, and tweets by augmenting the category tag to the input word at every time-step\n\nAnswer B:\n(Hint: they are not jokes)\n<=> Explanation: The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes. They are quotes. The jokes are not jokes.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.04122",
    "question": "What size filters do they use in the convolution layer?",
    "gold_answer": "",
    "answer_a": "How many filters are used?\n< Explanation: The convolution layer is composed of 128 filters with kernel size 7x7 and stride 1. The number of filters is 128. The number of output channels is 128. The number of input channels is 3. The number of output channels is 3. The number of input channels is 3. The number of output channels is 3. The number of input channels is 3. The number of output channels is 3. The number of input channels is 3. The number of output channels is 3. The number of",
    "answer_b": "1x3 filter size is used in convolutional layers.. Explanation: The embedding triple [ INLINEFORM7 , INLINEFORM8 , INLINEFORM9 ] of (s, r, o) is fed to the convolution layer where multiple filters of the same INLINEFORM10 shape are repeatedly operated over every row of the matrix to produce INLINEFORM11 -dimensional feature maps.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat size filters do they use in the convolution layer?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow many filters are used?\n< Explanation: The convolution layer is composed of 128 filters with kernel size 7x7 and stride 1. The number of filters is 128. The number of output channels is 128. The number of input channels is 3. The number of output channels is 3. The number of input channels is 3. The number of output channels is 3. The number of input channels is 3. The number of output channels is 3. The number of input channels is 3. The number of output channels is 3. The number of\n\nAnswer B:\n1x3 filter size is used in convolutional layers.. Explanation: The embedding triple [ INLINEFORM7 , INLINEFORM8 , INLINEFORM9 ] of (s, r, o) is fed to the convolution layer where multiple filters of the same INLINEFORM10 shape are repeatedly operated over every row of the matrix to produce INLINEFORM11 -dimensional feature maps.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.04122",
    "question": "By how much do they outperform state-of-the-art models on knowledge graph completion?",
    "gold_answer": "",
    "answer_a": "improvements of INLINEFORM0 in MRR (which is about 25.1% relative improvement). Explanation: Our CapsE performs better than its closely related CNN-based model ConvKB on both experimental datasets (except Hits@10 on WN18RR and MR on FB15k-237), especially on FB15k-237 where our CapsE gains significant improvements of INLINEFORM0 in MRR (which is about 25.1% relative improvement), and INLINEFORM1 % absolute improvement in Hits@10. Table TABREF10 also shows that our CapsE obtains the best MR score on WN18RR and the highest MRR and Hits@10 scores on FB15k-237.",
    "answer_b": "What is the performance on SEARCH17?\n< Explanation: We compare our model with the following state-of-the-art models: (1) CapsNet (2) CapsNet+ (3) CapsNet+ (4) CapsNet+ (5) CapsNet+ (6) CapsNet+ (7) CapsNet+ (8) CapsNet+ (9) CapsNet+ (10) CapsNet+ (11) CapsNet+ (12) CapsNet+ (13) CapsNet+ (14) CapsNet+ (15) CapsNet+ (16) CapsNet+ (",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much do they outperform state-of-the-art models on knowledge graph completion?\n\nGround Truth Answer:\n\n\nAnswer A:\nimprovements of INLINEFORM0 in MRR (which is about 25.1% relative improvement). Explanation: Our CapsE performs better than its closely related CNN-based model ConvKB on both experimental datasets (except Hits@10 on WN18RR and MR on FB15k-237), especially on FB15k-237 where our CapsE gains significant improvements of INLINEFORM0 in MRR (which is about 25.1% relative improvement), and INLINEFORM1 % absolute improvement in Hits@10. Table TABREF10 also shows that our CapsE obtains the best MR score on WN18RR and the highest MRR and Hits@10 scores on FB15k-237.\n\nAnswer B:\nWhat is the performance on SEARCH17?\n< Explanation: We compare our model with the following state-of-the-art models: (1) CapsNet (2) CapsNet+ (3) CapsNet+ (4) CapsNet+ (5) CapsNet+ (6) CapsNet+ (7) CapsNet+ (8) CapsNet+ (9) CapsNet+ (10) CapsNet+ (11) CapsNet+ (12) CapsNet+ (13) CapsNet+ (14) CapsNet+ (15) CapsNet+ (16) CapsNet+ (\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.05338",
    "question": "what models did they compare with?",
    "gold_answer": "",
    "answer_a": "Explanation: We compare our model with the following models: BERT, BERT-XX, BERT-XX-XX, BERT-XX-XX-XX, BERT-XX-XX-XX-XX, BERT-XX-XX-XX-XX-XX, BERT-XX-XX-XX-XX-XX-XX, BERT-XX-XX-XX-XX-XX-XX-XX, BERT-XX-XX-XX-XX-XX-XX-XX-XX, BERT-XX-XX-XX-XX-XX-XX-XX-XX",
    "answer_b": "BERT. Explanation: For strategy finetune-only and strategy stack-and-finetune, we implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top. Eval measure is accuracy and F1 score.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat models did they compare with?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We compare our model with the following models: BERT, BERT-XX, BERT-XX-XX, BERT-XX-XX-XX, BERT-XX-XX-XX-XX, BERT-XX-XX-XX-XX-XX, BERT-XX-XX-XX-XX-XX-XX, BERT-XX-XX-XX-XX-XX-XX-XX, BERT-XX-XX-XX-XX-XX-XX-XX-XX, BERT-XX-XX-XX-XX-XX-XX-XX-XX\n\nAnswer B:\nBERT. Explanation: For strategy finetune-only and strategy stack-and-finetune, we implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top. Eval measure is accuracy and F1 score.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.05338",
    "question": "what datasets were used for testing?",
    "gold_answer": "",
    "answer_a": "Explanation: We use the following datasets for our experiments: (1) the Penn Treebank (PTB) dataset, (2) the Stanford Sentiment Treebank (SST) dataset, (3) the Stanford CoreNLP (SCN) dataset, (4) the Stanford CoreNLP (SCN) dataset, (5) the Stanford CoreNLP (SCN) dataset, (6) the Stanford CoreNLP (SCN) dataset, (7) the Stanford CoreNLP (SCN) dataset, (8) the Stanford CoreNLP (SCN) dataset, (9) the Stanford",
    "answer_b": "CoNLL03. Explanation: In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset BIBREF5 , which is a public available used in many studies to test the accuracy of their proposed methods BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF1 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat datasets were used for testing?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We use the following datasets for our experiments: (1) the Penn Treebank (PTB) dataset, (2) the Stanford Sentiment Treebank (SST) dataset, (3) the Stanford CoreNLP (SCN) dataset, (4) the Stanford CoreNLP (SCN) dataset, (5) the Stanford CoreNLP (SCN) dataset, (6) the Stanford CoreNLP (SCN) dataset, (7) the Stanford CoreNLP (SCN) dataset, (8) the Stanford CoreNLP (SCN) dataset, (9) the Stanford\n\nAnswer B:\nCoNLL03. Explanation: In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset BIBREF5 , which is a public available used in many studies to test the accuracy of their proposed methods BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF1 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.08437",
    "question": "What inter-annotator agreement did they obtain?",
    "gold_answer": "",
    "answer_a": "How did they measure it?\n< Explanation: We conducted a pilot study to assess inter-annotator agreement. We used the same 1000-word sample corpus as in the main study, but we replaced the 1000-word sample with a 100-word sample. We then asked 100 annotators to rate the agreement of the 1000-word sample with the 100-word sample. We found that the inter-annotator agreement was 95.5%. We then conducted a second pilot study to assess inter-annotator agreement on the 100-word",
    "answer_b": "two inter-annotator agreement. Explanation: The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat inter-annotator agreement did they obtain?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow did they measure it?\n< Explanation: We conducted a pilot study to assess inter-annotator agreement. We used the same 1000-word sample corpus as in the main study, but we replaced the 1000-word sample with a 100-word sample. We then asked 100 annotators to rate the agreement of the 1000-word sample with the 100-word sample. We found that the inter-annotator agreement was 95.5%. We then conducted a second pilot study to assess inter-annotator agreement on the 100-word\n\nAnswer B:\ntwo inter-annotator agreement. Explanation: The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.08437",
    "question": "How did they annotate the corpus?",
    "gold_answer": "",
    "answer_a": "The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.. Explanation: The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.",
    "answer_b": "What were the categories of supersenses?\nFor each adposition, we annotated the corresponding supersenses, which were defined as follows: (1) a preposition, (2) a prepositional phrase, (3) a prepositional phrase with a preposition, (4) a prepositional phrase with a preposition and a noun, (5) a prepositional phrase with a preposition and a noun, (6) a prepositional phrase with a preposition and a noun, (7) a prepositional phrase with a preposition and a noun, (8) a prepositional phrase with",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow did they annotate the corpus?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.. Explanation: The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese.\n\nAnswer B:\nWhat were the categories of supersenses?\nFor each adposition, we annotated the corresponding supersenses, which were defined as follows: (1) a preposition, (2) a prepositional phrase, (3) a prepositional phrase with a preposition, (4) a prepositional phrase with a preposition and a noun, (5) a prepositional phrase with a preposition and a noun, (6) a prepositional phrase with a preposition and a noun, (7) a prepositional phrase with a preposition and a noun, (8) a prepositional phrase with\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.08437",
    "question": "What is the size of the corpus?",
    "gold_answer": "",
    "answer_a": "How many tokens are there?\n<blockquote>Table 1: Number of adpositions in the corpus and the number of tokens per adposition. The number of tokens per adposition is the number of tokens in the corpus divided by the number of adpositions. The number of adpositions is the number of adpositions in the corpus divided by the number of tokens per adposition. The number of tokens per adposition is the number of tokens in the corpus divided by the number of adpositions. The number of adpositions is the number of adpositions in the corpus divided by the number of tokens per adposition. The number of tokens per",
    "answer_b": "933 manually identified adpositions. Explanation: Our corpus contains 933 manually identified adpositions.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the size of the corpus?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow many tokens are there?\n<blockquote>Table 1: Number of adpositions in the corpus and the number of tokens per adposition. The number of tokens per adposition is the number of tokens in the corpus divided by the number of adpositions. The number of adpositions is the number of adpositions in the corpus divided by the number of tokens per adposition. The number of tokens per adposition is the number of tokens in the corpus divided by the number of adpositions. The number of adpositions is the number of adpositions in the corpus divided by the number of tokens per adposition. The number of tokens per\n\nAnswer B:\n933 manually identified adpositions. Explanation: Our corpus contains 933 manually identified adpositions.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.04978",
    "question": "Which datasets do they use?",
    "gold_answer": "",
    "answer_a": "What are the features used?\n< Explanation: We used the following datasets for our experiments: (1) The dataset of fake news from the 2016 US presidential election. (2) The dataset of fake news from the 2018 US presidential election. (3) The dataset of fake news from the 2019 US presidential election. (4) The dataset of fake news from the 2020 US presidential election. (5) The dataset of fake news from the 2021 US presidential election. (6) The dataset of fake news from the 2022",
    "answer_b": "https://github.com/Sairamvinay/Fake-News-Dataset. Explanation: Github Repo\nhttps://github.com/Sairamvinay/Fake-News-Dataset",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich datasets do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the features used?\n< Explanation: We used the following datasets for our experiments: (1) The dataset of fake news from the 2016 US presidential election. (2) The dataset of fake news from the 2018 US presidential election. (3) The dataset of fake news from the 2019 US presidential election. (4) The dataset of fake news from the 2020 US presidential election. (5) The dataset of fake news from the 2021 US presidential election. (6) The dataset of fake news from the 2022\n\nAnswer B:\nhttps://github.com/Sairamvinay/Fake-News-Dataset. Explanation: Github Repo\nhttps://github.com/Sairamvinay/Fake-News-Dataset\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.04978",
    "question": "What models are explored in this paper?",
    "gold_answer": "",
    "answer_a": "SVM, Logistic Regression, ANN, LSTM, and Random Forest. Explanation: In our project, we emphasize the content by working with articles whose labels only relate to the text, nothing outside that scope, and have used SVM, Logistic Regression, ANN, LSTM, and Random Forest.",
    "answer_b": "(Please provide the correct answer and show the reasoning)\nThe models explored in this paper are: (1) Naive Bayes, (2) Logistic Regression, (3) Support Vector Machine, (4) K-Nearest Neighbors, (5) Random Forest, (6) Gradient Boosting, (7) Neural Networks, (8) Convolutional Neural Networks, (9) Recurrent Neural Networks, (10) Long Short-Term Memory Networks, (11) Recurrent Neural Networks with Attention, (12) Recurrent Neural Networks with Attention and Memory, (13) Recurrent Neural",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat models are explored in this paper?\n\nGround Truth Answer:\n\n\nAnswer A:\nSVM, Logistic Regression, ANN, LSTM, and Random Forest. Explanation: In our project, we emphasize the content by working with articles whose labels only relate to the text, nothing outside that scope, and have used SVM, Logistic Regression, ANN, LSTM, and Random Forest.\n\nAnswer B:\n(Please provide the correct answer and show the reasoning)\nThe models explored in this paper are: (1) Naive Bayes, (2) Logistic Regression, (3) Support Vector Machine, (4) K-Nearest Neighbors, (5) Random Forest, (6) Gradient Boosting, (7) Neural Networks, (8) Convolutional Neural Networks, (9) Recurrent Neural Networks, (10) Long Short-Term Memory Networks, (11) Recurrent Neural Networks with Attention, (12) Recurrent Neural Networks with Attention and Memory, (13) Recurrent Neural\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.08935",
    "question": "what features of the essays are extracted?",
    "gold_answer": "",
    "answer_a": "Following groups of features are extracted:\n- Numerical Features\n- Language Models\n- Clusters\n- Latent Dirichlet Allocation\n- Part-Of-Speech\n- Bag-of-words. Explanation: FLOAT SELECTED: Table 3: Stratified 3-fold cross-validation scores for the official measure of the challenge.",
    "answer_b": "Explanation: The system uses a combination of word embeddings, word embeddings and word embeddings. The word embeddings are extracted from the GloVe model and the word embeddings are used to create a word embedding matrix. The word embeddings are then used to create a word embedding matrix. The word embedding matrix is then used to create a feature vector. The feature vector is then used to create a feature matrix. The feature matrix is then used to create a feature vector. The feature vector is then used to create a feature matrix. The feature matrix is then used to create a feature vector. The feature vector is then used to create a feature matrix. The feature",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat features of the essays are extracted?\n\nGround Truth Answer:\n\n\nAnswer A:\nFollowing groups of features are extracted:\n- Numerical Features\n- Language Models\n- Clusters\n- Latent Dirichlet Allocation\n- Part-Of-Speech\n- Bag-of-words. Explanation: FLOAT SELECTED: Table 3: Stratified 3-fold cross-validation scores for the official measure of the challenge.\n\nAnswer B:\nExplanation: The system uses a combination of word embeddings, word embeddings and word embeddings. The word embeddings are extracted from the GloVe model and the word embeddings are used to create a word embedding matrix. The word embeddings are then used to create a word embedding matrix. The word embedding matrix is then used to create a feature vector. The feature vector is then used to create a feature matrix. The feature matrix is then used to create a feature vector. The feature vector is then used to create a feature matrix. The feature matrix is then used to create a feature vector. The feature vector is then used to create a feature matrix. The feature\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.08935",
    "question": "what were the evaluation metrics?",
    "gold_answer": "",
    "answer_a": "How were they evaluated?\n< Explanation: The evaluation metrics are the following: F1 score, precision, recall, and accuracy. The F1 score is the harmonic mean of precision and recall. The precision is the ratio of the number of correct predictions to the total number of predictions. The recall is the ratio of the number of correct predictions to the number of actual observations. The accuracy is the ratio of the number of correct predictions to the number of observations. The precision, recall, and accuracy are calculated for each of the 14 systems. The F1 score is calculated for each of the 14 systems. The precision,",
    "answer_b": "Accuracy metric. Explanation: FLOAT SELECTED: Table 4: Ablation study to explore the importance of different feature families.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat were the evaluation metrics?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow were they evaluated?\n< Explanation: The evaluation metrics are the following: F1 score, precision, recall, and accuracy. The F1 score is the harmonic mean of precision and recall. The precision is the ratio of the number of correct predictions to the total number of predictions. The recall is the ratio of the number of correct predictions to the number of actual observations. The accuracy is the ratio of the number of correct predictions to the number of observations. The precision, recall, and accuracy are calculated for each of the 14 systems. The F1 score is calculated for each of the 14 systems. The precision,\n\nAnswer B:\nAccuracy metric. Explanation: FLOAT SELECTED: Table 4: Ablation study to explore the importance of different feature families.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.08935",
    "question": "what model is used?",
    "gold_answer": "",
    "answer_a": "Explanation: The model is a simple feed-forward neural network with a single hidden layer. The network is trained using the cross-entropy loss function. The network is trained using the Adam optimizer with a learning rate of 0.001. The network is trained for 100 epochs. The network is trained using the batch size of 100. The network is trained using the stochastic gradient descent algorithm. The network is trained using the weight decay of 0.0001. The network is trained using the learning rate of 0.001. The network is trained using the momentum of 0",
    "answer_b": "gradient boosted trees. Explanation: As a classification1 algorithm, I have used gradient boosted trees trained with gradient-based one-side sampling as implemented in the Light Gradient Boosting Machine toolkit released by Microsoft..",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat model is used?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The model is a simple feed-forward neural network with a single hidden layer. The network is trained using the cross-entropy loss function. The network is trained using the Adam optimizer with a learning rate of 0.001. The network is trained for 100 epochs. The network is trained using the batch size of 100. The network is trained using the stochastic gradient descent algorithm. The network is trained using the weight decay of 0.0001. The network is trained using the learning rate of 0.001. The network is trained using the momentum of 0\n\nAnswer B:\ngradient boosted trees. Explanation: As a classification1 algorithm, I have used gradient boosted trees trained with gradient-based one-side sampling as implemented in the Light Gradient Boosting Machine toolkit released by Microsoft..\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.08935",
    "question": "what future work is described?",
    "gold_answer": "",
    "answer_a": "the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. Explanation: In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.",
    "answer_b": "Explanation: We propose a new feature extraction method that is based on the use of a combination of word embeddings and a neural network. We also propose a new feature engineering method that is based on the use of a combination of word embeddings and a neural network. We also propose a new model selection method that is based on the use of a combination of word embeddings and a neural network. We also propose a new feature selection method that is based on the use of a combination of word embeddings and a neural network. We also propose a new feature selection method that is based on the use of a combination of word embeddings and a neural network. We also",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat future work is described?\n\nGround Truth Answer:\n\n\nAnswer A:\nthe hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. Explanation: In the latter case, given that the candidates are asked to write an essay given a subject BIBREF0 that depends on their level, the hypothesis that needs be studied is whether LDA was just a clever way to model this information leak in the given data or not. I believe that further analysis and validation can answer this question if the topics of the essays are released so that validation splits can be done on the basis of these topics.\n\nAnswer B:\nExplanation: We propose a new feature extraction method that is based on the use of a combination of word embeddings and a neural network. We also propose a new feature engineering method that is based on the use of a combination of word embeddings and a neural network. We also propose a new model selection method that is based on the use of a combination of word embeddings and a neural network. We also propose a new feature selection method that is based on the use of a combination of word embeddings and a neural network. We also propose a new feature selection method that is based on the use of a combination of word embeddings and a neural network. We also\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.07924",
    "question": "How is the sentence alignment quality evaluated?",
    "gold_answer": "",
    "answer_a": "What is the cutoff used?\n<sep>The automatic alignment score is evaluated by a human evaluator. The human evaluator is asked to align the sentences and to give a score between 0 and 100, where 100 is the highest score. The human evaluator is asked to align the sentences in the order they are given and to give a score between 0 and 100, where 100 is the highest score. The human evaluator is asked to align the sentences in the order they are given and to give a score between 0 and 100, where 100 is the",
    "answer_b": "Through human evaluation on a 5-point scale for text alignment and 3-point scale for audio-text. Explanation: The evaluation of the text alignment quality was conducted according to the 5-point scale used in KocabiyikogluETAL:18:\n\nWrong alignment\n\nPartial alignment with slightly compositional translational equivalence\n\nPartial alignment with compositional translation and additional or missing information\n\nCorrect alignment with compositional translation and few additional or missing information\n\nCorrect alignment and fully compositional translation\n\nThe evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:\n\nWrong alignment\n\nPartial alignment, some words or sentences may be missing\n\nCorrect alignment, allowing non-spoken syllables at start or end.\n\nThe evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is the sentence alignment quality evaluated?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the cutoff used?\n<sep>The automatic alignment score is evaluated by a human evaluator. The human evaluator is asked to align the sentences and to give a score between 0 and 100, where 100 is the highest score. The human evaluator is asked to align the sentences in the order they are given and to give a score between 0 and 100, where 100 is the highest score. The human evaluator is asked to align the sentences in the order they are given and to give a score between 0 and 100, where 100 is the\n\nAnswer B:\nThrough human evaluation on a 5-point scale for text alignment and 3-point scale for audio-text. Explanation: The evaluation of the text alignment quality was conducted according to the 5-point scale used in KocabiyikogluETAL:18:\n\nWrong alignment\n\nPartial alignment with slightly compositional translational equivalence\n\nPartial alignment with compositional translation and additional or missing information\n\nCorrect alignment with compositional translation and few additional or missing information\n\nCorrect alignment and fully compositional translation\n\nThe evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:\n\nWrong alignment\n\nPartial alignment, some words or sentences may be missing\n\nCorrect alignment, allowing non-spoken syllables at start or end.\n\nThe evaluation experiment was performed by two annotators who each rated 30 items from each bin, where 10 items were the same for both annotators in order to calculate inter-annotator reliability.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.07924",
    "question": "How is the speech alignment quality evaluated?",
    "gold_answer": "",
    "answer_a": "What is the cutoff used?\n<sep>The automatic alignment score is computed by comparing the alignment of the audio and the text. The score is computed as the fraction of aligned utterances. The score is normalized to the range [0, 1] and the alignment is considered good if the score is greater than 0.5. The automatic alignment score is computed by comparing the alignment of the audio and the text. The score is computed as the fraction of aligned utterances. The score is normalized to the range [0, 1] and the alignment is considered good if the score is greater than 0.5. The",
    "answer_b": "Through a 3-point scale by annotators.. Explanation: The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:\n\nWrong alignment\n\nPartial alignment, some words or sentences may be missing\n\nCorrect alignment, allowing non-spoken syllables at start or end.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is the speech alignment quality evaluated?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the cutoff used?\n<sep>The automatic alignment score is computed by comparing the alignment of the audio and the text. The score is computed as the fraction of aligned utterances. The score is normalized to the range [0, 1] and the alignment is considered good if the score is greater than 0.5. The automatic alignment score is computed by comparing the alignment of the audio and the text. The score is computed as the fraction of aligned utterances. The score is normalized to the range [0, 1] and the alignment is considered good if the score is greater than 0.5. The\n\nAnswer B:\nThrough a 3-point scale by annotators.. Explanation: The evaluation of the audio-text alignment quality was conducted according to the following 3-point scale:\n\nWrong alignment\n\nPartial alignment, some words or sentences may be missing\n\nCorrect alignment, allowing non-spoken syllables at start or end.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.11899",
    "question": "By how much do they outperform previous state-of-the-art in terms of top-n precision?",
    "gold_answer": "",
    "answer_a": "Outperforms PCNN+HATT by 10.3% and PCNN+BAG-ATT by 5.3%. Explanation: We first compare our proposed SeG with aforementioned approaches in Table TABREF19 for top-N precision (i.e., P@N).",
    "answer_b": "What is the performance on NYT dataset?\n< Explanation: Table 1 shows the results of our proposed method on NYT dataset. Our proposed method achieves the best performance on top-n precision and AUC metrics. The results are also comparable with the state-of-the-art methods. The best results are achieved by the proposed method on top-n precision and AUC metrics. The proposed method outperforms the state-of-the-art methods by 0.01% and 0.02% on top-n precision and AUC metrics, respectively. The proposed method also outperforms the state-of-the-art methods by 0.01",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much do they outperform previous state-of-the-art in terms of top-n precision?\n\nGround Truth Answer:\n\n\nAnswer A:\nOutperforms PCNN+HATT by 10.3% and PCNN+BAG-ATT by 5.3%. Explanation: We first compare our proposed SeG with aforementioned approaches in Table TABREF19 for top-N precision (i.e., P@N).\n\nAnswer B:\nWhat is the performance on NYT dataset?\n< Explanation: Table 1 shows the results of our proposed method on NYT dataset. Our proposed method achieves the best performance on top-n precision and AUC metrics. The results are also comparable with the state-of-the-art methods. The best results are achieved by the proposed method on top-n precision and AUC metrics. The proposed method outperforms the state-of-the-art methods by 0.01% and 0.02% on top-n precision and AUC metrics, respectively. The proposed method also outperforms the state-of-the-art methods by 0.01\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.09405",
    "question": "By how much do they outperform existing methods?",
    "gold_answer": "",
    "answer_a": "In Semantic Relatedness task their model outperforms existing methods by more than 0.023 Pearson Correlation. In Textual Entailment task their model scores 0.004 accuracy lesser than MaxEnt. Explanation: FLOAT SELECTED: Table 1: Semantic Relatedness Task Comparison.",
    "answer_b": "What are the baseline methods?\n< Explanation: We compare our model with the following baseline methods: (1) BIBREF10, (2) BIBREF11, (3) BIBREF12, (4) BIBREF13, (5) BIBREF14, (6) BIBREF15, (7) BIBREF16, (8) BIBREF17, (9) BIBREF18, (10) BIBREF19, (11) BIBREF20, (12) BIBREF2",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much do they outperform existing methods?\n\nGround Truth Answer:\n\n\nAnswer A:\nIn Semantic Relatedness task their model outperforms existing methods by more than 0.023 Pearson Correlation. In Textual Entailment task their model scores 0.004 accuracy lesser than MaxEnt. Explanation: FLOAT SELECTED: Table 1: Semantic Relatedness Task Comparison.\n\nAnswer B:\nWhat are the baseline methods?\n< Explanation: We compare our model with the following baseline methods: (1) BIBREF10, (2) BIBREF11, (3) BIBREF12, (4) BIBREF13, (5) BIBREF14, (6) BIBREF15, (7) BIBREF16, (8) BIBREF17, (9) BIBREF18, (10) BIBREF19, (11) BIBREF20, (12) BIBREF2\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.09405",
    "question": "Which datasets do they evaluate on?",
    "gold_answer": "",
    "answer_a": "How do they evaluate?\n< Explanation: We evaluate our approach on the following datasets: (1) Penn Treebank (PTB) BIBREF10, (2) Penn Treebank (PTB) CTCBIBREF11, (3) Penn Treebank (PTB) CTCBIBREF12, (4) Penn Treebank (PTB) CTCBIBREF13, (5) Penn Treebank (PTB) CTCBIBREF14, (6) Penn Treebank (PTB) CTCBIBREF15, (7) Penn Tree",
    "answer_b": "SICK (Sentences Involving Compositional Knowledge) dataset. Explanation: We use standard SICK (Sentences Involving Compositional Knowledge) dataset for evaluation.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich datasets do they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate?\n< Explanation: We evaluate our approach on the following datasets: (1) Penn Treebank (PTB) BIBREF10, (2) Penn Treebank (PTB) CTCBIBREF11, (3) Penn Treebank (PTB) CTCBIBREF12, (4) Penn Treebank (PTB) CTCBIBREF13, (5) Penn Treebank (PTB) CTCBIBREF14, (6) Penn Treebank (PTB) CTCBIBREF15, (7) Penn Tree\n\nAnswer B:\nSICK (Sentences Involving Compositional Knowledge) dataset. Explanation: We use standard SICK (Sentences Involving Compositional Knowledge) dataset for evaluation.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.11585",
    "question": "What dataset was used in this challenge?",
    "gold_answer": "",
    "answer_a": "Explanation: The dataset used in this challenge is the 2019 speaker recognition evaluation CTS dataset. The dataset contains 10000 samples, each of which is a 10000 word sentence. The dataset is divided into 10000 samples, each of which is a 10000 word sentence. The dataset is divided into 10000 samples, each of which is a 10000 word sentence. The dataset is divided into 10000 samples, each of which is a 10000 word sentence.",
    "answer_b": "SRE18 development and SRE18 evaluation datasets. Explanation: Finally, we report experimental results obtained by each subsystem and fusion system on the SRE18 development and SRE18 evaluation datasets.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat dataset was used in this challenge?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The dataset used in this challenge is the 2019 speaker recognition evaluation CTS dataset. The dataset contains 10000 samples, each of which is a 10000 word sentence. The dataset is divided into 10000 samples, each of which is a 10000 word sentence. The dataset is divided into 10000 samples, each of which is a 10000 word sentence. The dataset is divided into 10000 samples, each of which is a 10000 word sentence.\n\nAnswer B:\nSRE18 development and SRE18 evaluation datasets. Explanation: Finally, we report experimental results obtained by each subsystem and fusion system on the SRE18 development and SRE18 evaluation datasets.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.11585",
    "question": "Which subsystem outperformed the others?",
    "gold_answer": "",
    "answer_a": "primary system is the linear fusion of all the above six subsystems. Explanation: Our primary system is the linear fusion of all the above six subsystems by BOSARIS Toolkit on SRE19 dev and eval BIBREF9.",
    "answer_b": "What was the performance of the best subsystem?\n< Explanation: The etdnn/ams system achieved the best performance, with 99.99% accuracy. The ftdnn/as system achieved 99.98% accuracy, and the eftdnn/ams system achieved 99.97% accuracy. The resnet system achieved 99.96% accuracy, and the multitask system achieved 99.95% accuracy. The c-vector system achieved 99.94% accuracy. The best performance of the etdnn/ams system is 99.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich subsystem outperformed the others?\n\nGround Truth Answer:\n\n\nAnswer A:\nprimary system is the linear fusion of all the above six subsystems. Explanation: Our primary system is the linear fusion of all the above six subsystems by BOSARIS Toolkit on SRE19 dev and eval BIBREF9.\n\nAnswer B:\nWhat was the performance of the best subsystem?\n< Explanation: The etdnn/ams system achieved the best performance, with 99.99% accuracy. The ftdnn/as system achieved 99.98% accuracy, and the eftdnn/ams system achieved 99.97% accuracy. The resnet system achieved 99.96% accuracy, and the multitask system achieved 99.95% accuracy. The c-vector system achieved 99.94% accuracy. The best performance of the etdnn/ams system is 99.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.09816",
    "question": "Do they reduce language variation of text by enhancing frequencies?",
    "gold_answer": "",
    "answer_a": "How?\n<sep><font size=\"4\">Answer: Yes, we found that the thesaurus-based approach can be used to enhance the topic models of different languages. We have conducted experiments with 10 languages, and the results show that the thesaurus-based approach can be used to enhance the topic models of different languages. The results show that the thesaurus-based approach can be used to enhance the topic models of different languages. The results show that the thesaurus-based approach can be used to enhance the topic models of different languages. The results show that the thesaurus-based approach can be used to enhance the",
    "answer_b": "Variation decreases when frequencies of synonyms is enhanced; variation increases when frequencies of synonyms, hyponyms, hypernyms are enhanced. Explanation: At the first step, we include WordNet synonyms (including multiword expressions) into the proposed similarity sets (LDA-Sim+WNsyn). At this step, frequencies of synonyms found in the same document are summed up in process LDA topic learning as described in Algorithm SECREF3 . We can see that the kernel uniqueness becomes very low, topics are very close to each other in content (Table 2: LDA-Sim+WNsyn). At the second step, we add word direct relatives (hyponyms, hypernyms, etc.) to similarity sets. Now the frequencies of semantically related words are added up enhancing the contribution into all topics of the current document.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nDo they reduce language variation of text by enhancing frequencies?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow?\n<sep><font size=\"4\">Answer: Yes, we found that the thesaurus-based approach can be used to enhance the topic models of different languages. We have conducted experiments with 10 languages, and the results show that the thesaurus-based approach can be used to enhance the topic models of different languages. The results show that the thesaurus-based approach can be used to enhance the topic models of different languages. The results show that the thesaurus-based approach can be used to enhance the topic models of different languages. The results show that the thesaurus-based approach can be used to enhance the\n\nAnswer B:\nVariation decreases when frequencies of synonyms is enhanced; variation increases when frequencies of synonyms, hyponyms, hypernyms are enhanced. Explanation: At the first step, we include WordNet synonyms (including multiword expressions) into the proposed similarity sets (LDA-Sim+WNsyn). At this step, frequencies of synonyms found in the same document are summed up in process LDA topic learning as described in Algorithm SECREF3 . We can see that the kernel uniqueness becomes very low, topics are very close to each other in content (Table 2: LDA-Sim+WNsyn). At the second step, we add word direct relatives (hyponyms, hypernyms, etc.) to similarity sets. Now the frequencies of semantically related words are added up enhancing the contribution into all topics of the current document.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.09816",
    "question": "Which domains do they explore?",
    "gold_answer": "",
    "answer_a": "economic. Explanation: At the second series of the experiments, we applied EuroVoc information retrieval thesaurus to two European Union collections: Europarl and JRC. In content, the EuroVoc thesaurus is much smaller than WordNet, it contains terms from economic and political domains and does not include general abstract words.",
    "answer_b": "How do they evaluate their results?\n< Explanation: We have evaluated the proposed approach on the following domains: biology, chemistry, medicine, and physics. We have used the following thesauri: thesaurus of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich domains do they explore?\n\nGround Truth Answer:\n\n\nAnswer A:\neconomic. Explanation: At the second series of the experiments, we applied EuroVoc information retrieval thesaurus to two European Union collections: Europarl and JRC. In content, the EuroVoc thesaurus is much smaller than WordNet, it contains terms from economic and political domains and does not include general abstract words.\n\nAnswer B:\nHow do they evaluate their results?\n< Explanation: We have evaluated the proposed approach on the following domains: biology, chemistry, medicine, and physics. We have used the following thesauri: thesaurus of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (STS) of thesaurus (\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.09816",
    "question": "Which thesauri did they use?",
    "gold_answer": "",
    "answer_a": "WordNet. Explanation: We experiment with three thesauri: WordNet (155 thousand entries), information-retrieval thesaurus of the European Union EuroVoc (15161 terms), and Russian thesaurus RuThes (115 thousand entries) BIBREF19 .",
    "answer_b": "What are their strengths and weaknesses?\n<sep><br/ >They used the following thesauri: WordNet, WordNet3.0, WordNet4.0, WordNet4.1, WordNet4.2, WordNet4.3, WordNet4.4, WordNet4.5, WordNet4.6, WordNet4.7, WordNet4.8, WordNet4.9, WordNet5.0, WordNet5.1, WordNet5.2, WordNet5.3, WordNet5.4, WordNet5.5, Word",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich thesauri did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nWordNet. Explanation: We experiment with three thesauri: WordNet (155 thousand entries), information-retrieval thesaurus of the European Union EuroVoc (15161 terms), and Russian thesaurus RuThes (115 thousand entries) BIBREF19 .\n\nAnswer B:\nWhat are their strengths and weaknesses?\n<sep><br/ >They used the following thesauri: WordNet, WordNet3.0, WordNet4.0, WordNet4.1, WordNet4.2, WordNet4.3, WordNet4.4, WordNet4.5, WordNet4.6, WordNet4.7, WordNet4.8, WordNet4.9, WordNet5.0, WordNet5.1, WordNet5.2, WordNet5.3, WordNet5.4, WordNet5.5, Word\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1703.04009",
    "question": "What is their definition of hate speech?",
    "gold_answer": "",
    "answer_a": "How do they define it?\n< Explanation: Hate speech is defined as offensive language that is intended to cause offense or distress to another person or group. Hate speech is not limited to words, but includes actions, attitudes, and behaviors that are offensive or distressing. Hate speech is not limited to words, but includes actions, attitudes, and behaviors that are offensive or distressing. Hate speech is not limited to words, but includes actions, attitudes, and behaviors that are offensive or distressing. Hate speech is not limited to words, but includes actions, attitudes, and behaviors that are offensive or distressing. Hate speech is not limited to",
    "answer_b": "language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group. Explanation: Drawing upon these definitions, we define hate speech as language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is their definition of hate speech?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they define it?\n< Explanation: Hate speech is defined as offensive language that is intended to cause offense or distress to another person or group. Hate speech is not limited to words, but includes actions, attitudes, and behaviors that are offensive or distressing. Hate speech is not limited to words, but includes actions, attitudes, and behaviors that are offensive or distressing. Hate speech is not limited to words, but includes actions, attitudes, and behaviors that are offensive or distressing. Hate speech is not limited to words, but includes actions, attitudes, and behaviors that are offensive or distressing. Hate speech is not limited to\n\nAnswer B:\nlanguage that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group. Explanation: Drawing upon these definitions, we define hate speech as language that is used to expresses hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1703.04009",
    "question": "What type of model do they train?",
    "gold_answer": "",
    "answer_a": "How do they evaluate it?\n< Explanation: We train a multi-class classifier using the Hate Speech Lexicon (HSL) to distinguish between hate speech and other offensive language. We use the HSL to label a sample of tweets containing hate speech keywords and a sample of tweets containing offensive language keywords. We train a classifier using the HSL to classify the remaining tweets. We use the accuracy of the classifier to evaluate the performance of the HSL. We use the F1 score to evaluate the performance of the classifier. We use the F1 score to evaluate the performance of the classifier. We use the F1 score to evaluate the performance",
    "answer_b": "logistic regression. Explanation: We then test a variety of models that have been used in prior work: logistic regression, na\u00efve Bayes, decision trees, random forests, and linear SVMs.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of model do they train?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate it?\n< Explanation: We train a multi-class classifier using the Hate Speech Lexicon (HSL) to distinguish between hate speech and other offensive language. We use the HSL to label a sample of tweets containing hate speech keywords and a sample of tweets containing offensive language keywords. We train a classifier using the HSL to classify the remaining tweets. We use the accuracy of the classifier to evaluate the performance of the HSL. We use the F1 score to evaluate the performance of the classifier. We use the F1 score to evaluate the performance of the classifier. We use the F1 score to evaluate the performance\n\nAnswer B:\nlogistic regression. Explanation: We then test a variety of models that have been used in prior work: logistic regression, na\u00efve Bayes, decision trees, random forests, and linear SVMs.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1703.04009",
    "question": "How many users does their dataset have?",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n< Explanation: The dataset contains 1,000 tweets from 1000 users. The tweets are randomly selected from the Twitter API. The tweets are then labeled with hate speech, offensive language, or neither. The dataset contains 1000 tweets from 1000 users. The tweets are randomly selected from the Twitter API. The tweets are then labeled with hate speech, offensive language, or neither. The dataset contains 1000 tweets from 1000 users. The tweets are randomly selected from the Twitter API. The tweets are then labeled",
    "answer_b": "33,458. Explanation: Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many users does their dataset have?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n< Explanation: The dataset contains 1,000 tweets from 1000 users. The tweets are randomly selected from the Twitter API. The tweets are then labeled with hate speech, offensive language, or neither. The dataset contains 1000 tweets from 1000 users. The tweets are randomly selected from the Twitter API. The tweets are then labeled with hate speech, offensive language, or neither. The dataset contains 1000 tweets from 1000 users. The tweets are randomly selected from the Twitter API. The tweets are then labeled\n\nAnswer B:\n33,458. Explanation: Using the Twitter API we searched for tweets containing terms from the lexicon, resulting in a sample of tweets from 33,458 Twitter users.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1703.04009",
    "question": "How long is their dataset?",
    "gold_answer": "",
    "answer_a": "85400000. Explanation: We extracted the time-line for each user, resulting in a set of 85.4 million tweets.",
    "answer_b": "What is the size of the dataset?\n< Explanation: We collected tweets from Twitter using the Twitter API. We used the Twitter API to collect tweets containing hate speech keywords from the following sources: (1) the Twitter API search endpoint for tweets containing hate speech keywords (2) the Twitter API search endpoint for tweets containing offensive language keywords (3) the Twitter API search endpoint for tweets containing both offensive language and hate speech keywords. We used the Twitter API to collect tweets containing hate speech keywords from the following sources: (1) the Twitter API search endpoint for tweets containing hate speech keywords (2) the Twitter API search endpoint for tweets containing offensive",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long is their dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\n85400000. Explanation: We extracted the time-line for each user, resulting in a set of 85.4 million tweets.\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: We collected tweets from Twitter using the Twitter API. We used the Twitter API to collect tweets containing hate speech keywords from the following sources: (1) the Twitter API search endpoint for tweets containing hate speech keywords (2) the Twitter API search endpoint for tweets containing offensive language keywords (3) the Twitter API search endpoint for tweets containing both offensive language and hate speech keywords. We used the Twitter API to collect tweets containing hate speech keywords from the following sources: (1) the Twitter API search endpoint for tweets containing hate speech keywords (2) the Twitter API search endpoint for tweets containing offensive\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.03090",
    "question": "In what tasks does fine-tuning all layers hurt performance?",
    "gold_answer": "",
    "answer_a": "How much does it hurt?\n< Explanation: We first examine the effect of fine-tuning all layers on the performance of the BERT model on the standard tasks. We find that fine-tuning all layers does not always help. We show that fine-tuning all layers on the BERT model on the standard tasks does not improve the performance by more than 1% on the standard tasks. We also show that fine-tuning all layers on the BERT model on the standard tasks does not improve the performance by more than 1% on the standard tasks. We show that fine-tuning all layers on the BERT model on the",
    "answer_b": "SST-2. Explanation: Finally, for the large variants of BERT and RoBERTa on SST-2 (second subfigure from both the top and the left), we observe a surprisingly consistent increase in quality when freezing 12\u201316 layers.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nIn what tasks does fine-tuning all layers hurt performance?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow much does it hurt?\n< Explanation: We first examine the effect of fine-tuning all layers on the performance of the BERT model on the standard tasks. We find that fine-tuning all layers does not always help. We show that fine-tuning all layers on the BERT model on the standard tasks does not improve the performance by more than 1% on the standard tasks. We also show that fine-tuning all layers on the BERT model on the standard tasks does not improve the performance by more than 1% on the standard tasks. We show that fine-tuning all layers on the BERT model on the\n\nAnswer B:\nSST-2. Explanation: Finally, for the large variants of BERT and RoBERTa on SST-2 (second subfigure from both the top and the left), we observe a surprisingly consistent increase in quality when freezing 12\u201316 layers.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.03090",
    "question": "Do they test against the large version of RoBERTa?",
    "gold_answer": "",
    "answer_a": "For GLUE bencmark no, for dataset MRPC, SST-B, SST-2 and COLA yes.. Explanation: Due to computational limitations, we set half as the cutoff point. Additionally, we restrict our comprehensive all-datasets exploration to the base variant of BERT, since the large model variants and RoBERTa are much more computationally intensive.",
    "answer_b": "If so, how?\n< Explanation: We test our results against the large version of RoBERTa, which has 12 layers. We fine-tune the model on the same tasks as the small version, and we compare the results. We find that the fine-tuned model is still 90% of the original quality. We also find that the fine-tuned model is still 90% of the original quality when the number of layers is increased to 16. We also find that the fine-tuned model is still 90% of the original quality when the number of layers is increased to 2",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nDo they test against the large version of RoBERTa?\n\nGround Truth Answer:\n\n\nAnswer A:\nFor GLUE bencmark no, for dataset MRPC, SST-B, SST-2 and COLA yes.. Explanation: Due to computational limitations, we set half as the cutoff point. Additionally, we restrict our comprehensive all-datasets exploration to the base variant of BERT, since the large model variants and RoBERTa are much more computationally intensive.\n\nAnswer B:\nIf so, how?\n< Explanation: We test our results against the large version of RoBERTa, which has 12 layers. We fine-tune the model on the same tasks as the small version, and we compare the results. We find that the fine-tuned model is still 90% of the original quality. We also find that the fine-tuned model is still 90% of the original quality when the number of layers is increased to 16. We also find that the fine-tuned model is still 90% of the original quality when the number of layers is increased to 2\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.04242",
    "question": "What is the performance improvement of their method over state-of-the-art models on the used datasets? ",
    "gold_answer": "",
    "answer_a": "What is the performance improvement of their method over state-of-the-art models on the used datasets?\n< Explanation: We compare our method with the state-of-the-art models on the two datasets. The results are shown in Table TABREF11 . Our method outperforms the state-of-the-art models on both datasets. The results show that our method can mitigate the impact of annotation artifacts and improve the generalization ability of models. The results also show that our method can be applied to other NLI datasets.\n\nOur method outperforms the state-of-the-art models on both datasets. The results show that our method can mitigate the impact of",
    "answer_b": "Average improvement in accuracy is 2.26 points. Explanation: FLOAT SELECTED: Table 1: Evaluation Results of Hyp and Norm. Baseline refers to the model trained and validated without using weights. Hard, Easy refers to the Hard-Easy Testing generated from the testing set corresponding to the Trainset column. Results of Hyp are the average numbers of five runs with different random initialization. We report AUC for Hyp and ACC for Norm. \u201c*\u201d indicates where normal-model are better than the baseline.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the performance improvement of their method over state-of-the-art models on the used datasets? \n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance improvement of their method over state-of-the-art models on the used datasets?\n< Explanation: We compare our method with the state-of-the-art models on the two datasets. The results are shown in Table TABREF11 . Our method outperforms the state-of-the-art models on both datasets. The results show that our method can mitigate the impact of annotation artifacts and improve the generalization ability of models. The results also show that our method can be applied to other NLI datasets.\n\nOur method outperforms the state-of-the-art models on both datasets. The results show that our method can mitigate the impact of\n\nAnswer B:\nAverage improvement in accuracy is 2.26 points. Explanation: FLOAT SELECTED: Table 1: Evaluation Results of Hyp and Norm. Baseline refers to the model trained and validated without using weights. Hard, Easy refers to the Hard-Easy Testing generated from the testing set corresponding to the Trainset column. Results of Hyp are the average numbers of five runs with different random initialization. We report AUC for Hyp and ACC for Norm. \u201c*\u201d indicates where normal-model are better than the baseline.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.04242",
    "question": "How does the proposed training framework mitigate the bias pattern?",
    "gold_answer": "",
    "answer_a": "Centering on the results of JOCI, in which the bias pattern of MultiNLI is misleading. Explanation: Centering on the results of JOCI, in which the bias pattern of MultiNLI is misleading, we find that Norm trained with MultiNLI outperforms baseline after debiasing with all smooth values tested.\n\nFurthermore, debiasing can reduce models' dependence on the bias pattern during training, thus force models to better learn semantic information to make predictions.",
    "answer_b": "What are the metrics used to evaluate the generalization ability?\n< Explanation: We propose a training framework to mitigate the impacts of the annotation artifacts in cross-dataset testing. The framework consists of two parts: (1) a pre-training step to learn a pre-trained model, and (2) a post-training step to learn a downstream model. The pre-training step is to learn a pre-trained model that is robust to the annotation artifacts. The downstream model is to learn a downstream model that is robust to the annotation artifacts. The pre-training step is to learn a pre-trained model that is robust to the annotation artifacts. The downstream model is",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does the proposed training framework mitigate the bias pattern?\n\nGround Truth Answer:\n\n\nAnswer A:\nCentering on the results of JOCI, in which the bias pattern of MultiNLI is misleading. Explanation: Centering on the results of JOCI, in which the bias pattern of MultiNLI is misleading, we find that Norm trained with MultiNLI outperforms baseline after debiasing with all smooth values tested.\n\nFurthermore, debiasing can reduce models' dependence on the bias pattern during training, thus force models to better learn semantic information to make predictions.\n\nAnswer B:\nWhat are the metrics used to evaluate the generalization ability?\n< Explanation: We propose a training framework to mitigate the impacts of the annotation artifacts in cross-dataset testing. The framework consists of two parts: (1) a pre-training step to learn a pre-trained model, and (2) a post-training step to learn a downstream model. The pre-training step is to learn a pre-trained model that is robust to the annotation artifacts. The downstream model is to learn a downstream model that is robust to the annotation artifacts. The pre-training step is to learn a pre-trained model that is robust to the annotation artifacts. The downstream model is\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.04242",
    "question": "Which datasets do they use in the cross-dataset evaluation?",
    "gold_answer": "",
    "answer_a": "SNLI. Explanation: We utilize SNLI BIBREF0, MultiNLI BIBREF1, JOCI BIBREF13 and SICK BIBREF14 for cross-dataset testing.",
    "answer_b": "What are the results?\n< Explanation: We use the following datasets: (1) the Stanford NLI dataset (SNTL) [11] , (2) the Stanford Sentiment Treebank (SST) [12] , (3) the Stanford Sentiment Treebank with Annotations (SST-A) [13] , (4) the Stanford Sentiment Treebank with Annotations (SST-A) with annotations from the Stanford NLI dataset (SNTL) [14] , (5) the Stanford Sentiment Treebank with Annotations (SST-A) with annotations from the Stanford NLI",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich datasets do they use in the cross-dataset evaluation?\n\nGround Truth Answer:\n\n\nAnswer A:\nSNLI. Explanation: We utilize SNLI BIBREF0, MultiNLI BIBREF1, JOCI BIBREF13 and SICK BIBREF14 for cross-dataset testing.\n\nAnswer B:\nWhat are the results?\n< Explanation: We use the following datasets: (1) the Stanford NLI dataset (SNTL) [11] , (2) the Stanford Sentiment Treebank (SST) [12] , (3) the Stanford Sentiment Treebank with Annotations (SST-A) [13] , (4) the Stanford Sentiment Treebank with Annotations (SST-A) with annotations from the Stanford NLI dataset (SNTL) [14] , (5) the Stanford Sentiment Treebank with Annotations (SST-A) with annotations from the Stanford NLI\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.12139",
    "question": "Which was the most helpful strategy?",
    "gold_answer": "",
    "answer_a": "Vote entropy and KL divergence. Explanation: Vote entropy and KL divergence, the query-by-committee active learning methods are helpful for the training of machine learning ensemble classifiers. However, all the active learning strategies we tested do not work well with deep learning model (i.e., CNN) or deep learning-based ensemble classifier.",
    "answer_b": "How many tweets were needed to reach a desired performance?\n< Explanation: The most helpful strategy was entropy. It was used to reduce the number of tweets needed to reach a desired performance of automated classification. The number of tweets needed to reach a desired performance was 1000 tweets. The entropy strategy was used to reduce the number of tweets needed to reach a desired performance of automated classification. The number of tweets needed to reach a desired performance was 1000 tweets. The entropy strategy was used to reduce the number of tweets needed to reach a desired performance of automated classification. The number of tweets needed to reach a desired performance",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich was the most helpful strategy?\n\nGround Truth Answer:\n\n\nAnswer A:\nVote entropy and KL divergence. Explanation: Vote entropy and KL divergence, the query-by-committee active learning methods are helpful for the training of machine learning ensemble classifiers. However, all the active learning strategies we tested do not work well with deep learning model (i.e., CNN) or deep learning-based ensemble classifier.\n\nAnswer B:\nHow many tweets were needed to reach a desired performance?\n< Explanation: The most helpful strategy was entropy. It was used to reduce the number of tweets needed to reach a desired performance of automated classification. The number of tweets needed to reach a desired performance was 1000 tweets. The entropy strategy was used to reduce the number of tweets needed to reach a desired performance of automated classification. The number of tweets needed to reach a desired performance was 1000 tweets. The entropy strategy was used to reduce the number of tweets needed to reach a desired performance of automated classification. The number of tweets needed to reach a desired performance\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.12139",
    "question": "How large is their tweets dataset?",
    "gold_answer": "",
    "answer_a": "3,685,984 unique tweets. Explanation: After integrating the tweets from the two data sources, there were 3,685,984 unique tweets.",
    "answer_b": "What is the size of the dataset?\n< Explanation: The Twitter dataset contains 1,000,000 tweets. The tweets are divided into 1000 groups of 1000 tweets each. Each group is labeled with a label indicating whether the event occurred or not. The labels are 0 and 1. The 0 labels indicate that the event did not occur, while the 1 labels indicate that the event occurred. The dataset is divided into 1000 groups of 1000 tweets each. Each group is labeled with a label indicating whether the event occurred or not",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow large is their tweets dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\n3,685,984 unique tweets. Explanation: After integrating the tweets from the two data sources, there were 3,685,984 unique tweets.\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: The Twitter dataset contains 1,000,000 tweets. The tweets are divided into 1000 groups of 1000 tweets each. Each group is labeled with a label indicating whether the event occurred or not. The labels are 0 and 1. The 0 labels indicate that the event did not occur, while the 1 labels indicate that the event occurred. The dataset is divided into 1000 groups of 1000 tweets each. Each group is labeled with a label indicating whether the event occurred or not\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.03391",
    "question": "what is the size of the idn tagged corpus?",
    "gold_answer": "",
    "answer_a": "How was it split?\n< Explanation: The IDN Tagged Corpus consists of 10,000 sentences, each of which is tagged with 10 parts of speech. The corpus is split into 1000 sentences, each of which is tagged with 10 parts of speech. The corpus is further split into 1000 sentences, each of which is tagged with 10 parts of speech. The corpus is further split into 1000 sentences, each of which is tagged with 10 parts of speech. The corpus is further split into 1000 sentences,",
    "answer_b": "10K. Explanation: Dinakaramani et al. BIBREF11 proposed IDN Tagged Corpus, a new manually annotated POS tagging corpus for Indonesian. The corpus consists of 10K sentences and 250K tokens, and its tagset is different than that of the PANL10N dataset.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the size of the idn tagged corpus?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow was it split?\n< Explanation: The IDN Tagged Corpus consists of 10,000 sentences, each of which is tagged with 10 parts of speech. The corpus is split into 1000 sentences, each of which is tagged with 10 parts of speech. The corpus is further split into 1000 sentences, each of which is tagged with 10 parts of speech. The corpus is further split into 1000 sentences, each of which is tagged with 10 parts of speech. The corpus is further split into 1000 sentences,\n\nAnswer B:\n10K. Explanation: Dinakaramani et al. BIBREF11 proposed IDN Tagged Corpus, a new manually annotated POS tagging corpus for Indonesian. The corpus consists of 10K sentences and 250K tokens, and its tagset is different than that of the PANL10N dataset.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.03391",
    "question": "what neural network models were explored?",
    "gold_answer": "",
    "answer_a": "Feedforward, biLSTM. Explanation: FLOAT SELECTED: Table I DEV F1 SCORE OF EACH NEURAL TAGGER ARCHITECTURE",
    "answer_b": "Explanation: We experimented with three different neural network models: (1) a recurrent neural network (RNN) with a single hidden layer, (2) a recurrent neural network with a two hidden layers, and (3) a recurrent neural network with a three hidden layers. We also experimented with a convolutional neural network (CNN) with a single hidden layer. We also experimented with a convolutional neural network (CNN) with a two hidden layers and a three hidden layers. We also experimented with a convolutional neural network (CNN) with a three hidden layers. We also experimented with a convolutional neural network (CNN) with a",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat neural network models were explored?\n\nGround Truth Answer:\n\n\nAnswer A:\nFeedforward, biLSTM. Explanation: FLOAT SELECTED: Table I DEV F1 SCORE OF EACH NEURAL TAGGER ARCHITECTURE\n\nAnswer B:\nExplanation: We experimented with three different neural network models: (1) a recurrent neural network (RNN) with a single hidden layer, (2) a recurrent neural network with a two hidden layers, and (3) a recurrent neural network with a three hidden layers. We also experimented with a convolutional neural network (CNN) with a single hidden layer. We also experimented with a convolutional neural network (CNN) with a two hidden layers and a three hidden layers. We also experimented with a convolutional neural network (CNN) with a three hidden layers. We also experimented with a convolutional neural network (CNN) with a\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.03391",
    "question": "what rule based models were evaluated?",
    "gold_answer": "",
    "answer_a": "Explanation: We evaluated the following rule-based models: (1) simple rule-based model, (2) rule-based model with a simple rule, (3) rule-based model with a simple rule and a simple rule-based model. We also evaluated the following CRF models: (4) CRF model with a simple CRF, (5) CRF model with a simple CRF and a simple CRF, (6) CRF model with a simple CRF and a simple CRF and a simple CRF, (7) CRF model with a simple CRF and a simple CRF and a simple CRF and",
    "answer_b": "Rashel et al. BIBREF14. Explanation: We adopted a rule-based tagger designed by Rashel et al. BIBREF14 as one of our comparisons.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat rule based models were evaluated?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We evaluated the following rule-based models: (1) simple rule-based model, (2) rule-based model with a simple rule, (3) rule-based model with a simple rule and a simple rule-based model. We also evaluated the following CRF models: (4) CRF model with a simple CRF, (5) CRF model with a simple CRF and a simple CRF, (6) CRF model with a simple CRF and a simple CRF and a simple CRF, (7) CRF model with a simple CRF and a simple CRF and a simple CRF and\n\nAnswer B:\nRashel et al. BIBREF14. Explanation: We adopted a rule-based tagger designed by Rashel et al. BIBREF14 as one of our comparisons.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.03391",
    "question": "what datasets have been used for this task?",
    "gold_answer": "",
    "answer_a": "Explanation: The IDN Tagged Corpus is a dataset of 10,000 sentences from Indonesian news articles. The corpus is divided into 1000 sentences for training, 1000 sentences for validation, and 1000 sentences for testing. The corpus is available at http://www.ist.ac.id/ist/ist/ist-2015-16/IDNTaggedCorpus.zip. The corpus is divided into 1000 sentences for training, 1000 sentences for validation, and 1000 sentences for testing. The corpus",
    "answer_b": "IDN Tagged Corpus. Explanation: We evaluated our experiments on the IDN Tagged Corpus BIBREF11 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat datasets have been used for this task?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The IDN Tagged Corpus is a dataset of 10,000 sentences from Indonesian news articles. The corpus is divided into 1000 sentences for training, 1000 sentences for validation, and 1000 sentences for testing. The corpus is available at http://www.ist.ac.id/ist/ist/ist-2015-16/IDNTaggedCorpus.zip. The corpus is divided into 1000 sentences for training, 1000 sentences for validation, and 1000 sentences for testing. The corpus\n\nAnswer B:\nIDN Tagged Corpus. Explanation: We evaluated our experiments on the IDN Tagged Corpus BIBREF11 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.04287",
    "question": "How much data do they use to train the embeddings?",
    "gold_answer": "",
    "answer_a": "11,529,432 segmented words and 20,402 characters. Explanation: Finally, we get 11,529,432 segmented words.",
    "answer_b": "What are the evaluation metrics?\n< Explanation: We use 100000 words from the Chinese WordNet corpus as training data. We use the word embeddings obtained from the Word2Vec model trained on the training data. The word embeddings are trained using the word2vec algorithm with the default parameters. The word embeddings are then used to train the DWE model. The DWE model is trained using the word embeddings obtained from the Word2Vec model trained on the training data. The word embeddings are trained using the word2vec algorithm with the default parameters. The word embeddings are then used to train the DWE model.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much data do they use to train the embeddings?\n\nGround Truth Answer:\n\n\nAnswer A:\n11,529,432 segmented words and 20,402 characters. Explanation: Finally, we get 11,529,432 segmented words.\n\nAnswer B:\nWhat are the evaluation metrics?\n< Explanation: We use 100000 words from the Chinese WordNet corpus as training data. We use the word embeddings obtained from the Word2Vec model trained on the training data. The word embeddings are trained using the word2vec algorithm with the default parameters. The word embeddings are then used to train the DWE model. The DWE model is trained using the word embeddings obtained from the Word2Vec model trained on the training data. The word embeddings are trained using the word2vec algorithm with the default parameters. The word embeddings are then used to train the DWE model.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.10162",
    "question": "What are the issues identified for out-of-vocabulary words?",
    "gold_answer": "",
    "answer_a": "Explanation: The proposed model is trained on a large corpus of 100,000 words. The model is trained on a corpus of 100,000 words, and the model is trained on a corpus of 100,000 words. The model is trained on a corpus of 100,000 words, and the model is trained on a corpus of 100,000 words. The model is trained on a corpus of 100,000 words, and the model is trained on a corpus of 100,",
    "answer_b": "model did not have a flexibility in OOV words. Explanation: Also, the model with the dataset vectors did not have the flexibility to classify unknown words.\n\nAs a next step, the test set of the dataset was altered by replacing words with syntactical mistakes to test the tolerance of the model in OOV words. Suffixes of verbs were altered and vowels were replaced with others, affecting 20% of the tokens of the dataset. Using again the more complex tagset for training, the results can be found in Table 3.\n\nWhat can be concluded is that the model did not have a flexibility in OOV words. Of course, this can also be an advantage, meaning that the model recognized the mismatch of a wrong word with its class.\n\nOne disadvantage that the previous model had is that for unknown words the model assigned a zero vector, affecting the testing results. In order to minimize this problem, the unknown words were first passed through a FastText model to get a vector from their subwords. The resulting vectors were imported in the vocabulary with the CC vectors before training. The model was also trained using as a vocabulary the unknown words and the tokens from the Common Crawl vectors, both buffered in the same FastText model. Results are listed in Table 4.\n\nIt was noticed that the model performed better when using the vectors from different FastText models.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the issues identified for out-of-vocabulary words?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The proposed model is trained on a large corpus of 100,000 words. The model is trained on a corpus of 100,000 words, and the model is trained on a corpus of 100,000 words. The model is trained on a corpus of 100,000 words, and the model is trained on a corpus of 100,000 words. The model is trained on a corpus of 100,000 words, and the model is trained on a corpus of 100,\n\nAnswer B:\nmodel did not have a flexibility in OOV words. Explanation: Also, the model with the dataset vectors did not have the flexibility to classify unknown words.\n\nAs a next step, the test set of the dataset was altered by replacing words with syntactical mistakes to test the tolerance of the model in OOV words. Suffixes of verbs were altered and vowels were replaced with others, affecting 20% of the tokens of the dataset. Using again the more complex tagset for training, the results can be found in Table 3.\n\nWhat can be concluded is that the model did not have a flexibility in OOV words. Of course, this can also be an advantage, meaning that the model recognized the mismatch of a wrong word with its class.\n\nOne disadvantage that the previous model had is that for unknown words the model assigned a zero vector, affecting the testing results. In order to minimize this problem, the unknown words were first passed through a FastText model to get a vector from their subwords. The resulting vectors were imported in the vocabulary with the CC vectors before training. The model was also trained using as a vocabulary the unknown words and the tokens from the Common Crawl vectors, both buffered in the same FastText model. Results are listed in Table 4.\n\nIt was noticed that the model performed better when using the vectors from different FastText models.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.10162",
    "question": "How does the model proposed extend ENAMEX?",
    "gold_answer": "",
    "answer_a": "Extended with facility (FAC) type.. Explanation: In order to gain more information about the context of the Greek entities, a percentage of Greek Wikipedia was used. After applying sentence and token segmentation on Wikipedia text and using a pretrained model from polyglot, the keyword list increased. The keyword list had at this point about 350,000 records and consisted of 4 classes: location (LOC), organization (ORG), person (PERSON) and facility (FAC).",
    "answer_b": "What are the out-of-vocabulary words?\n< Explanation: The proposed model extends the ENAMEX type (organization, location, person) by adding a new type for the organization type. The model is trained on a large corpus of Greek texts and is trained on a large corpus of English texts. The model is trained on a large corpus of English texts and is trained on a large corpus of Greek texts. The model is trained on a large corpus of English texts and is trained on a large corpus of Greek texts. The model is trained on a large corpus of English texts and is trained on a large corpus of Greek texts. The model",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does the model proposed extend ENAMEX?\n\nGround Truth Answer:\n\n\nAnswer A:\nExtended with facility (FAC) type.. Explanation: In order to gain more information about the context of the Greek entities, a percentage of Greek Wikipedia was used. After applying sentence and token segmentation on Wikipedia text and using a pretrained model from polyglot, the keyword list increased. The keyword list had at this point about 350,000 records and consisted of 4 classes: location (LOC), organization (ORG), person (PERSON) and facility (FAC).\n\nAnswer B:\nWhat are the out-of-vocabulary words?\n< Explanation: The proposed model extends the ENAMEX type (organization, location, person) by adding a new type for the organization type. The model is trained on a large corpus of Greek texts and is trained on a large corpus of English texts. The model is trained on a large corpus of English texts and is trained on a large corpus of Greek texts. The model is trained on a large corpus of English texts and is trained on a large corpus of Greek texts. The model is trained on a large corpus of English texts and is trained on a large corpus of Greek texts. The model\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.10162",
    "question": "Which morphological features are extracted?",
    "gold_answer": "",
    "answer_a": "like the gender, the number, and the case. Explanation: Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the gender, the number, and the case BIBREF6.",
    "answer_b": "What are the classes of the named entities?\n< Explanation: The proposed model consists of a word embedding layer, a word classifier, and a morphological feature extractor. The word embedding layer is a word embedding layer that is trained on the word embeddings of the Greek language. The word classifier is a multi-class classifier that is trained on the training data. The morphological feature extractor is a feature extractor that is trained on the training data. The feature extractor consists of a word embedding layer, a word classifier, and a morphological feature extractor. The word embedding layer is a word embedding layer that is trained on the word embeddings of the Greek language",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich morphological features are extracted?\n\nGround Truth Answer:\n\n\nAnswer A:\nlike the gender, the number, and the case. Explanation: Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the gender, the number, and the case BIBREF6.\n\nAnswer B:\nWhat are the classes of the named entities?\n< Explanation: The proposed model consists of a word embedding layer, a word classifier, and a morphological feature extractor. The word embedding layer is a word embedding layer that is trained on the word embeddings of the Greek language. The word classifier is a multi-class classifier that is trained on the training data. The morphological feature extractor is a feature extractor that is trained on the training data. The feature extractor consists of a word embedding layer, a word classifier, and a morphological feature extractor. The word embedding layer is a word embedding layer that is trained on the word embeddings of the Greek language\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.13184",
    "question": "What are the characteristics of the dataset of Twitter users?",
    "gold_answer": "",
    "answer_a": "413 $(4\\%)$ users were annotated as \"bot,\" 7849 $(75.35\\%)$ as \"non-bot,\" and $20.69$ $(19.9\\%)$ as \"unavailable\". Explanation: Upon resolving the disagreements, 413 $(4\\%)$ users were annotated as \"bot,\" 7849 $(75.35\\%)$ as \"non-bot,\" and $20.69$ $(19.9\\%)$ as \"unavailable\".",
    "answer_b": "How is it used?\n< Explanation: The dataset of Twitter users is a subset of the Twitter dataset from the Twitter Bot Detection project (TBD) BIBREF1 . The dataset contains 10,000 tweets from 10,000 users. The dataset is used to train a system for detecting bots in Twitter posts. The dataset is split into training and testing sets. The training set contains 9,000 tweets and the testing set contains 1,000 tweets. The tweets are labeled as either \"bot\" or \"non-bot\" and the labels are used to train the system",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the characteristics of the dataset of Twitter users?\n\nGround Truth Answer:\n\n\nAnswer A:\n413 $(4\\%)$ users were annotated as \"bot,\" 7849 $(75.35\\%)$ as \"non-bot,\" and $20.69$ $(19.9\\%)$ as \"unavailable\". Explanation: Upon resolving the disagreements, 413 $(4\\%)$ users were annotated as \"bot,\" 7849 $(75.35\\%)$ as \"non-bot,\" and $20.69$ $(19.9\\%)$ as \"unavailable\".\n\nAnswer B:\nHow is it used?\n< Explanation: The dataset of Twitter users is a subset of the Twitter dataset from the Twitter Bot Detection project (TBD) BIBREF1 . The dataset contains 10,000 tweets from 10,000 users. The dataset is used to train a system for detecting bots in Twitter posts. The dataset is split into training and testing sets. The training set contains 9,000 tweets and the testing set contains 1,000 tweets. The tweets are labeled as either \"bot\" or \"non-bot\" and the labels are used to train the system\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.13184",
    "question": "How can an existing bot detection system by customized for health-related research?",
    "gold_answer": "",
    "answer_a": "What are the additional features and statistical machine learning classifier?\n< Explanation: We first show that the existing system, which was designed for political bot detection, underperforms when applied to health-related Twitter users. We then incorporate additional features and a statistical machine learning classifier to significantly improve bot detection performance. Our approach obtains F_1 scores of 0.7 for the \"bot\" class, representing improvements of 0.339. Our approach is customizable and generalizable for bot detection in other health-related social media cohorts.\n\nOur approach is based on a simple rule-based approach that uses the following features: (1) the number of",
    "answer_b": "An existing bot detection score for each user  can be used as a feature in training. Explanation: First, we evaluated Botometer on our held-out test set. Botometer is a publicly available bot detection system designed for political dot detection. It outputs a score between 0 and 1 for a user, representing the likelihood that a user is a bot. Second, we used the Botometer score for each user as a feature in training a gradient boosting classifier which is a decision tree-based ensemble machine learning algorithm with gradient boosting BIBREF23 and can be used to address class imbalance.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow can an existing bot detection system by customized for health-related research?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the additional features and statistical machine learning classifier?\n< Explanation: We first show that the existing system, which was designed for political bot detection, underperforms when applied to health-related Twitter users. We then incorporate additional features and a statistical machine learning classifier to significantly improve bot detection performance. Our approach obtains F_1 scores of 0.7 for the \"bot\" class, representing improvements of 0.339. Our approach is customizable and generalizable for bot detection in other health-related social media cohorts.\n\nOur approach is based on a simple rule-based approach that uses the following features: (1) the number of\n\nAnswer B:\nAn existing bot detection score for each user  can be used as a feature in training. Explanation: First, we evaluated Botometer on our held-out test set. Botometer is a publicly available bot detection system designed for political dot detection. It outputs a score between 0 and 1 for a user, representing the likelihood that a user is a bot. Second, we used the Botometer score for each user as a feature in training a gradient boosting classifier which is a decision tree-based ensemble machine learning algorithm with gradient boosting BIBREF23 and can be used to address class imbalance.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.13184",
    "question": "What type of health-related research takes place in social media?",
    "gold_answer": "",
    "answer_a": "Social networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth.. Explanation: In recent years, social media has evolved into an important source of information for various types of health-related research. Social networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth. Twitter, for example, has 330 million monthly active users worldwide that generate almost 500 million micro-blogs (tweets) per day. For some years, the use of the platform to share personal health information has been growing, particularly amongst people living with one or more chronic conditions and those living with disability. Twenty percent of social network site users living with chronic conditions gather and share health information on the sites, compared with 12% of social network site users who report no chronic conditions. Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject recruitment for cancer trials BIBREF2, and characterizing drug abuse BIBREF3, to name a few. Twitter is particularly popular in research due to the availability of the public streaming API, which releases a sample of publicly posted data in real time. While early health-related research from social media focused almost exclusively on population-level studies, some very recent research tasks have focused on performing longitudinal data analysis at the user level, such as mining health-related information from cohorts of pregnant women BIBREF4.",
    "answer_b": "How is the data collected?\n< Explanation: The data collected from Twitter is used to answer questions about health-related topics. The data is collected from users who are posting about health-related topics. The data is collected from users who are posting about health-related topics. The data is collected from users who are posting about health-related topics. The data is collected from users who are posting about health-related topics. The data is collected from users who are posting about health-related topics. The data is collected from users who are posting about health-related topics. The data is collected from users who are posting about health-related topics. The data is collected from users",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of health-related research takes place in social media?\n\nGround Truth Answer:\n\n\nAnswer A:\nSocial networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth.. Explanation: In recent years, social media has evolved into an important source of information for various types of health-related research. Social networks encapsulate large volumes of data associated with diverse health topics, generated by active user bases in continuous growth. Twitter, for example, has 330 million monthly active users worldwide that generate almost 500 million micro-blogs (tweets) per day. For some years, the use of the platform to share personal health information has been growing, particularly amongst people living with one or more chronic conditions and those living with disability. Twenty percent of social network site users living with chronic conditions gather and share health information on the sites, compared with 12% of social network site users who report no chronic conditions. Social media data is thus being widely used for health-related research, for tasks such as adverse drug reaction detection BIBREF0, syndromic surveillance BIBREF1, subject recruitment for cancer trials BIBREF2, and characterizing drug abuse BIBREF3, to name a few. Twitter is particularly popular in research due to the availability of the public streaming API, which releases a sample of publicly posted data in real time. While early health-related research from social media focused almost exclusively on population-level studies, some very recent research tasks have focused on performing longitudinal data analysis at the user level, such as mining health-related information from cohorts of pregnant women BIBREF4.\n\nAnswer B:\nHow is the data collected?\n< Explanation: The data collected from Twitter is used to answer questions about health-related topics. The data is collected from users who are posting about health-related topics. The data is collected from users who are posting about health-related topics. The data is collected from users who are posting about health-related topics. The data is collected from users who are posting about health-related topics. The data is collected from users who are posting about health-related topics. The data is collected from users who are posting about health-related topics. The data is collected from users who are posting about health-related topics. The data is collected from users\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.13793",
    "question": "Do the QA tuples fall under a specific domain?",
    "gold_answer": "",
    "answer_a": "conversations, which consist of at least one question and one free-form answer. Explanation: For our models, we'll use a customer support dataset with a relatively high usage of emoji. The dataset contains 2000 tuples collected by BIBREF24 that are sourced from Twitter. They provide conversations, which consist of at least one question and one free-form answer. Some conversations are longer, in this case we ignored the previous context and only looked at the last tuple. This dataset illustrates that even when contacting companies, Twitter users keep using emoji relatively often, 8.75% of all utterances.",
    "answer_b": "How many?\n< Explanation: We use the QA dataset from the BIBREF11, which contains 10,000 question-answer pairs, with 1,000 pairs per domain. We use the domain-specific question types: 1) \"What is the capital of France?\", 2) \"What is the capital of the United States?\", 3) \"What is the capital of Canada?\", 4) \"What is the capital of Australia?\", 5) \"What is the capital of Japan?\", 6) \"What is the capital of Germany?\", 7) \"What is the capital of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nDo the QA tuples fall under a specific domain?\n\nGround Truth Answer:\n\n\nAnswer A:\nconversations, which consist of at least one question and one free-form answer. Explanation: For our models, we'll use a customer support dataset with a relatively high usage of emoji. The dataset contains 2000 tuples collected by BIBREF24 that are sourced from Twitter. They provide conversations, which consist of at least one question and one free-form answer. Some conversations are longer, in this case we ignored the previous context and only looked at the last tuple. This dataset illustrates that even when contacting companies, Twitter users keep using emoji relatively often, 8.75% of all utterances.\n\nAnswer B:\nHow many?\n< Explanation: We use the QA dataset from the BIBREF11, which contains 10,000 question-answer pairs, with 1,000 pairs per domain. We use the domain-specific question types: 1) \"What is the capital of France?\", 2) \"What is the capital of the United States?\", 3) \"What is the capital of Canada?\", 4) \"What is the capital of Australia?\", 5) \"What is the capital of Japan?\", 6) \"What is the capital of Germany?\", 7) \"What is the capital of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.13793",
    "question": "What is the baseline model?",
    "gold_answer": "",
    "answer_a": "What is the baseline accuracy?\n< Explanation: We use the BERT model from the original paper BIBREF10. We train the model on the QA corpus with the following hyperparameters: batch size = 16, learning rate = 1e-5, and number of epochs = 10. We use the same hyperparameters for the baseline model. We train the model for 10 epochs and report the final accuracy. We use the same hyperparameters for the baseline model. We train the model for 10 epochs and report the final accuracy. We use the same hyperparameters for the baseline model. We",
    "answer_b": "pre-trained version of BERT without special emoji tokens. Explanation: Finally, our model is compared against the pre-trained version of BERT without special emoji tokens.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the baseline model?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the baseline accuracy?\n< Explanation: We use the BERT model from the original paper BIBREF10. We train the model on the QA corpus with the following hyperparameters: batch size = 16, learning rate = 1e-5, and number of epochs = 10. We use the same hyperparameters for the baseline model. We train the model for 10 epochs and report the final accuracy. We use the same hyperparameters for the baseline model. We train the model for 10 epochs and report the final accuracy. We use the same hyperparameters for the baseline model. We\n\nAnswer B:\npre-trained version of BERT without special emoji tokens. Explanation: Finally, our model is compared against the pre-trained version of BERT without special emoji tokens.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.13793",
    "question": "How large is the corpus of QA tuples?",
    "gold_answer": "",
    "answer_a": "How many emoji are there?\n< Explanation: We use the QA corpus from the BIBREF11, which contains 10,000 QA tuples, each with a question and an answer. We use the question-answer format, where the question is a string and the answer is a string. We also use the question-answer format, where the question is a string and the answer is a string. We use the question-answer format, where the question is a string and the answer is a string. We use the question-answer format, where the question is a string and the answer is a string. We use the question-answer",
    "answer_b": "2000 tuples. Explanation: For our models, we'll use a customer support dataset with a relatively high usage of emoji. The dataset contains 2000 tuples collected by BIBREF24 that are sourced from Twitter. They provide conversations, which consist of at least one question and one free-form answer. Some conversations are longer, in this case we ignored the previous context and only looked at the last tuple. This dataset illustrates that even when contacting companies, Twitter users keep using emoji relatively often, 8.75% of all utterances.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow large is the corpus of QA tuples?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow many emoji are there?\n< Explanation: We use the QA corpus from the BIBREF11, which contains 10,000 QA tuples, each with a question and an answer. We use the question-answer format, where the question is a string and the answer is a string. We also use the question-answer format, where the question is a string and the answer is a string. We use the question-answer format, where the question is a string and the answer is a string. We use the question-answer format, where the question is a string and the answer is a string. We use the question-answer\n\nAnswer B:\n2000 tuples. Explanation: For our models, we'll use a customer support dataset with a relatively high usage of emoji. The dataset contains 2000 tuples collected by BIBREF24 that are sourced from Twitter. They provide conversations, which consist of at least one question and one free-form answer. Some conversations are longer, in this case we ignored the previous context and only looked at the last tuple. This dataset illustrates that even when contacting companies, Twitter users keep using emoji relatively often, 8.75% of all utterances.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.13793",
    "question": "What corpus did they use?",
    "gold_answer": "",
    "answer_a": "a customer support dataset. Explanation: For our models, we'll use a customer support dataset with a relatively high usage of emoji. The dataset contains 2000 tuples collected by BIBREF24 that are sourced from Twitter.",
    "answer_b": "Explanation: We use the QA corpus from the BIBREF11 . The corpus contains 10,000 QA tuples, each with a question and an answer. The question is a question about a given entity, and the answer is a short answer. The corpus contains 10,000 tuples, and we use 1,000 of them for training and 9,000 for testing. The corpus is available at https://github.com/zhengyuanliu/qa-corpus. We use the first 1,000 tuples for training, and the remaining",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat corpus did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\na customer support dataset. Explanation: For our models, we'll use a customer support dataset with a relatively high usage of emoji. The dataset contains 2000 tuples collected by BIBREF24 that are sourced from Twitter.\n\nAnswer B:\nExplanation: We use the QA corpus from the BIBREF11 . The corpus contains 10,000 QA tuples, each with a question and an answer. The question is a question about a given entity, and the answer is a short answer. The corpus contains 10,000 tuples, and we use 1,000 of them for training and 9,000 for testing. The corpus is available at https://github.com/zhengyuanliu/qa-corpus. We use the first 1,000 tuples for training, and the remaining\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.11085",
    "question": "what boosting techniques were used?",
    "gold_answer": "",
    "answer_a": "Light Gradient Boosting Machine (LGBM). Explanation: We investigate an important family of ensemble methods known as boosting, and more specifically a Light Gradient Boosting Machine (LGBM) algorithm, which consists of an implementation of fast gradient boosting on decision trees.",
    "answer_b": "Explanation: We use the following boosting techniques: (1) Bagging: We use bagging to train the model. We use 10-fold cross validation to train the model. We use 10-fold cross validation to evaluate the model. (2) Stacking: We use stacking to train the model. We use 10-fold cross validation to train the model. We use 10-fold cross validation to evaluate the model. (3) Boosting: We use boosting to train the model. We use 10-fold cross validation to train the model. We use 10-fold cross validation to evaluate the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat boosting techniques were used?\n\nGround Truth Answer:\n\n\nAnswer A:\nLight Gradient Boosting Machine (LGBM). Explanation: We investigate an important family of ensemble methods known as boosting, and more specifically a Light Gradient Boosting Machine (LGBM) algorithm, which consists of an implementation of fast gradient boosting on decision trees.\n\nAnswer B:\nExplanation: We use the following boosting techniques: (1) Bagging: We use bagging to train the model. We use 10-fold cross validation to train the model. We use 10-fold cross validation to evaluate the model. (2) Stacking: We use stacking to train the model. We use 10-fold cross validation to train the model. We use 10-fold cross validation to evaluate the model. (3) Boosting: We use boosting to train the model. We use 10-fold cross validation to train the model. We use 10-fold cross validation to evaluate the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.11085",
    "question": "what is the size of this improved dataset?",
    "gold_answer": "",
    "answer_a": "363,078 structured abstracts. Explanation: We performed a search on April 11, 2019 on PubMed for 363,078 structured abstracts with the following filters: Article Types (Clinical Trial), Species (Humans), and Languages (English).",
    "answer_b": "How is it different from the previous dataset?\n< Explanation: The dataset we use is a subset of the dataset released by the EBM2018. The dataset contains 10000 sentences and 10000 labels. The labels are divided into 10 categories, each with 1000 labels. The dataset is released in a CSV file. The dataset contains 10000 sentences and 10000 labels. The labels are divided into 10 categories, each with 1000 labels. The dataset is released in a CSV file. The dataset",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the size of this improved dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\n363,078 structured abstracts. Explanation: We performed a search on April 11, 2019 on PubMed for 363,078 structured abstracts with the following filters: Article Types (Clinical Trial), Species (Humans), and Languages (English).\n\nAnswer B:\nHow is it different from the previous dataset?\n< Explanation: The dataset we use is a subset of the dataset released by the EBM2018. The dataset contains 10000 sentences and 10000 labels. The labels are divided into 10 categories, each with 1000 labels. The dataset is released in a CSV file. The dataset contains 10000 sentences and 10000 labels. The labels are divided into 10 categories, each with 1000 labels. The dataset is released in a CSV file. The dataset\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.11085",
    "question": "how was the new dataset collected?",
    "gold_answer": "",
    "answer_a": "What were the shortcomings of the previous datasets?\n< Explanation: We collected the dataset from the EMBASE database. The dataset contains 10,000 records. The records are divided into 1000 records per category. The records are sorted by the number of records per category. The records are then sorted by the number of records per category. The records are then sorted by the number of records per category. The records are then sorted by the number of records per category. The records are then sorted by the number of records per category. The records are then sorted by the number of records per category. The records",
    "answer_b": "The new dataset was collected from structured abstracts from PubMed and filtering abstract headings representative of the desired categories.. Explanation: This dataset was created by collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the desired categories.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow was the new dataset collected?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat were the shortcomings of the previous datasets?\n< Explanation: We collected the dataset from the EMBASE database. The dataset contains 10,000 records. The records are divided into 1000 records per category. The records are sorted by the number of records per category. The records are then sorted by the number of records per category. The records are then sorted by the number of records per category. The records are then sorted by the number of records per category. The records are then sorted by the number of records per category. The records are then sorted by the number of records per category. The records\n\nAnswer B:\nThe new dataset was collected from structured abstracts from PubMed and filtering abstract headings representative of the desired categories.. Explanation: This dataset was created by collecting structured abstracts from PubMed and carefully choosing abstract headings representative of the desired categories.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.11085",
    "question": "who annotated the new dataset?",
    "gold_answer": "",
    "answer_a": "The P, I, and O labels were automatically assigned after clustering lemmatized labels from the structured abstract sections.. Explanation: Structured abstract sections from PubMed have labels such as introduction, goals, study design, findings, or discussion; however, the majority of these labels are not useful for P, I, and O extraction since most are general (e.g. methods) and do not isolate a specific P, I, O sequence. Therefore, in order to narrow down abstract sections that correspond to the P label, for example, we needed to find a subset of labels such as, but not limited to population, patients, and subjects. We performed a lemmatization of the abstract section labels in order to cluster similar categories such as subject and subjects. Using this approach, we carefully chose candidate labels for each P, I, and O, and manually looked at a small number of samples for each label to determine if text was representative.",
    "answer_b": "What is the size of the dataset?\n< Explanation: The dataset was created by the authors by annotating the 1000 most common PIO elements from the EBM-2019 dataset. The dataset was then further annotated by 1000 randomly selected PIO elements. The dataset was further annotated by 1000 randomly selected PIO elements. The dataset was further annotated by 1000 randomly selected PIO elements. The dataset was further annotated by 1000 randomly selected PIO elements. The dataset was further annotated by 1000 randomly selected PIO elements. The dataset was further annotated",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwho annotated the new dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe P, I, and O labels were automatically assigned after clustering lemmatized labels from the structured abstract sections.. Explanation: Structured abstract sections from PubMed have labels such as introduction, goals, study design, findings, or discussion; however, the majority of these labels are not useful for P, I, and O extraction since most are general (e.g. methods) and do not isolate a specific P, I, O sequence. Therefore, in order to narrow down abstract sections that correspond to the P label, for example, we needed to find a subset of labels such as, but not limited to population, patients, and subjects. We performed a lemmatization of the abstract section labels in order to cluster similar categories such as subject and subjects. Using this approach, we carefully chose candidate labels for each P, I, and O, and manually looked at a small number of samples for each label to determine if text was representative.\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: The dataset was created by the authors by annotating the 1000 most common PIO elements from the EBM-2019 dataset. The dataset was then further annotated by 1000 randomly selected PIO elements. The dataset was further annotated by 1000 randomly selected PIO elements. The dataset was further annotated by 1000 randomly selected PIO elements. The dataset was further annotated by 1000 randomly selected PIO elements. The dataset was further annotated by 1000 randomly selected PIO elements. The dataset was further annotated\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.11085",
    "question": "what shortcomings of previous datasets are mentioned?",
    "gold_answer": "",
    "answer_a": "using a keyword-based approach would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study design making them poor candidates for a P label.. Explanation: Since our goal was to collect sequences that are uniquely representative of a description of Population, Intervention, and Outcome, we avoided a keyword-based approach such as in BIBREF12 . For example, using a keyword-based approach would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study design making them poor candidates for a P label.",
    "answer_b": "Explanation: The first dataset we used is the dataset of the 2016 EBM Task 1 dataset. This dataset is a subset of the 2016 EBM Task 1 dataset. The dataset contains 10000 sentences and 10000 labels. The labels are the following: 1) the presence of a specific intervention, 2) the presence of a specific outcome, 3) the presence of a specific intervention and outcome, 4) the presence of a specific intervention, outcome, and a specific outcome, 5) the presence of a specific intervention, outcome",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat shortcomings of previous datasets are mentioned?\n\nGround Truth Answer:\n\n\nAnswer A:\nusing a keyword-based approach would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study design making them poor candidates for a P label.. Explanation: Since our goal was to collect sequences that are uniquely representative of a description of Population, Intervention, and Outcome, we avoided a keyword-based approach such as in BIBREF12 . For example, using a keyword-based approach would yield a sequence labeled population and methods with the label P, but such abstract sections were not purely about the population and contained information about the interventions and study design making them poor candidates for a P label.\n\nAnswer B:\nExplanation: The first dataset we used is the dataset of the 2016 EBM Task 1 dataset. This dataset is a subset of the 2016 EBM Task 1 dataset. The dataset contains 10000 sentences and 10000 labels. The labels are the following: 1) the presence of a specific intervention, 2) the presence of a specific outcome, 3) the presence of a specific intervention and outcome, 4) the presence of a specific intervention, outcome, and a specific outcome, 5) the presence of a specific intervention, outcome\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.09892",
    "question": "Do single-language BERT outperforms multilingual BERT?",
    "gold_answer": "",
    "answer_a": "How much?\n< Explanation: We compare the performance of BERT on the dataset with the following metrics: (1) accuracy on the single-language dataset, (2) accuracy on the multilingual dataset, and (3) accuracy on the multilingual dataset with the addition of the English language. We also compare the performance of BERT on the single-language dataset with the addition of the English language. We find that BERT outperforms the single-language BERT model on all metrics. The multilingual BERT model performs similarly to the single-language BERT model on the single-language dataset, but outperforms the multilingual BERT",
    "answer_b": "For some language yes, but not for another.. Explanation: Figure FIGREF13 shows that BERT performed above 90% accuracy in each type. Performance is best on determiner and attributive agreement relations, while worst on subject-verb and predicate adjective.\n\nIn figure FIGREF14, we see BERT's performance for each language. BERT performs well for the majority of languages, although some fare much worse than others.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nDo single-language BERT outperforms multilingual BERT?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow much?\n< Explanation: We compare the performance of BERT on the dataset with the following metrics: (1) accuracy on the single-language dataset, (2) accuracy on the multilingual dataset, and (3) accuracy on the multilingual dataset with the addition of the English language. We also compare the performance of BERT on the single-language dataset with the addition of the English language. We find that BERT outperforms the single-language BERT model on all metrics. The multilingual BERT model performs similarly to the single-language BERT model on the single-language dataset, but outperforms the multilingual BERT\n\nAnswer B:\nFor some language yes, but not for another.. Explanation: Figure FIGREF13 shows that BERT performed above 90% accuracy in each type. Performance is best on determiner and attributive agreement relations, while worst on subject-verb and predicate adjective.\n\nIn figure FIGREF14, we see BERT's performance for each language. BERT performs well for the majority of languages, although some fare much worse than others.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.09892",
    "question": "What types of agreement relations do they explore?",
    "gold_answer": "",
    "answer_a": "subject-verb. Explanation: Specifically, we identified all instances of subject-verb, noun-determiner, noun-attributive adjective and subject-predicate adjective word pairs.",
    "answer_b": "How do they evaluate them?\n< Explanation: We evaluate the performance of BERT on the dataset by comparing its performance on the single-language and multilingual versions of the dataset. We use the following agreement relations: (1) subject-verb agreement, (2) subject-object agreement, (3) subject-object-object agreement, and (4) subject-object-object-object agreement. We also evaluate the performance of BERT on the dataset by comparing its performance on the single-language and multilingual versions of the dataset. We use the following agreement relations: (1) subject-verb agreement, (2) subject-object agreement, (3) subject",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat types of agreement relations do they explore?\n\nGround Truth Answer:\n\n\nAnswer A:\nsubject-verb. Explanation: Specifically, we identified all instances of subject-verb, noun-determiner, noun-attributive adjective and subject-predicate adjective word pairs.\n\nAnswer B:\nHow do they evaluate them?\n< Explanation: We evaluate the performance of BERT on the dataset by comparing its performance on the single-language and multilingual versions of the dataset. We use the following agreement relations: (1) subject-verb agreement, (2) subject-object agreement, (3) subject-object-object agreement, and (4) subject-object-object-object agreement. We also evaluate the performance of BERT on the dataset by comparing its performance on the single-language and multilingual versions of the dataset. We use the following agreement relations: (1) subject-verb agreement, (2) subject-object agreement, (3) subject\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.01970",
    "question": "what text classification datasets do they evaluate on?",
    "gold_answer": "",
    "answer_a": "Explanation: We evaluate our attack on the following text classification datasets: (1) the Stanford Sentiment Treebank (SST) BIBREF10, (2) the Stanford Question Answering Dataset (SQuAD) BIBREF11, (3) the Stanford CoreNLP Sentiment Treebank (SCNT) BIBREF12, (4) the Stanford CoreNLP Question Answering Dataset (SCQA) BIBREF13, (5) the Stanford CoreNLP Question Answering Corpus (SCQA) BIBREF14, (6) the Stanford CoreNLP Question Answer",
    "answer_b": "Amazon. Explanation: Datasets and Models We evaluate our adversarial attacks on different text classification datasets from tasks such as sentiment classification, subjectivity detection and question type classification. Amazon, Yelp, IMDB are sentence-level sentiment classification datasets which have been used in recent work BIBREF15 while MR BIBREF16 contains movie reviews based on sentiment polarity. MPQA BIBREF17 is a dataset for opinion polarity detection, Subj BIBREF18 for classifying a sentence as subjective or objective and TREC BIBREF19 is a dataset for question type classification.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat text classification datasets do they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We evaluate our attack on the following text classification datasets: (1) the Stanford Sentiment Treebank (SST) BIBREF10, (2) the Stanford Question Answering Dataset (SQuAD) BIBREF11, (3) the Stanford CoreNLP Sentiment Treebank (SCNT) BIBREF12, (4) the Stanford CoreNLP Question Answering Dataset (SCQA) BIBREF13, (5) the Stanford CoreNLP Question Answering Corpus (SCQA) BIBREF14, (6) the Stanford CoreNLP Question Answer\n\nAnswer B:\nAmazon. Explanation: Datasets and Models We evaluate our adversarial attacks on different text classification datasets from tasks such as sentiment classification, subjectivity detection and question type classification. Amazon, Yelp, IMDB are sentence-level sentiment classification datasets which have been used in recent work BIBREF15 while MR BIBREF16 contains movie reviews based on sentiment polarity. MPQA BIBREF17 is a dataset for opinion polarity detection, Subj BIBREF18 for classifying a sentence as subjective or objective and TREC BIBREF19 is a dataset for question type classification.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.01970",
    "question": "which models is their approach compared to?",
    "gold_answer": "",
    "answer_a": "TextFooler. Explanation: As a baseline, we consider TextFooler BIBREF11 which performs synonym replacement using a fixed word embedding space BIBREF22.",
    "answer_b": "(see figure for reference)\n<statement> BAE is compared to the following models: BERT, BERT+LM, BERT+LM+LM, BERT+LM+LM+LM, BERT+LM+LM+LM+LM, BERT+LM+LM+LM+LM+LM, BERT+LM+LM+LM+LM+LM+LM, BERT+LM+LM+LM+LM+LM+LM+LM, BERT+LM+LM+LM+LM+LM+LM+LM+LM, BERT+LM+LM+LM+LM+",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhich models is their approach compared to?\n\nGround Truth Answer:\n\n\nAnswer A:\nTextFooler. Explanation: As a baseline, we consider TextFooler BIBREF11 which performs synonym replacement using a fixed word embedding space BIBREF22.\n\nAnswer B:\n(see figure for reference)\n<statement> BAE is compared to the following models: BERT, BERT+LM, BERT+LM+LM, BERT+LM+LM+LM, BERT+LM+LM+LM+LM, BERT+LM+LM+LM+LM+LM, BERT+LM+LM+LM+LM+LM+LM, BERT+LM+LM+LM+LM+LM+LM+LM, BERT+LM+LM+LM+LM+LM+LM+LM+LM, BERT+LM+LM+LM+LM+\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.03369",
    "question": "by how much did their approach outperform previous work?",
    "gold_answer": "",
    "answer_a": "By 0,008 F1,  0, 02 Recall and 0,02 Precision.. Explanation: In comparison, training only on the Amazon reviews produced an F1 of 0.713 (training and testing only on Amazon reviews with our features but with the same classifier and cross-validation settings as BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 led to an F1 of 0.752, outperforming prior best results on that dataset).",
    "answer_b": "What is the best-performing scenario?\n< Explanation: We compare our approach to the best-performing results from previous work on the same target domain. The best-performing results are from the work of BIBREF10, which uses a combination of a bag-of-words model and a neural network. The best-performing results from the work of BIBREF11, which uses a bag-of-words model and a recurrent neural network. The best-performing results from the work of BIBREF12, which uses a bag-of-words model and a recurrent neural network. The best-performing results from the work",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nby how much did their approach outperform previous work?\n\nGround Truth Answer:\n\n\nAnswer A:\nBy 0,008 F1,  0, 02 Recall and 0,02 Precision.. Explanation: In comparison, training only on the Amazon reviews produced an F1 of 0.713 (training and testing only on Amazon reviews with our features but with the same classifier and cross-validation settings as BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 led to an F1 of 0.752, outperforming prior best results on that dataset).\n\nAnswer B:\nWhat is the best-performing scenario?\n< Explanation: We compare our approach to the best-performing results from previous work on the same target domain. The best-performing results are from the work of BIBREF10, which uses a combination of a bag-of-words model and a neural network. The best-performing results from the work of BIBREF11, which uses a bag-of-words model and a recurrent neural network. The best-performing results from the work of BIBREF12, which uses a bag-of-words model and a recurrent neural network. The best-performing results from the work\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.03369",
    "question": "what was the previous best results model?",
    "gold_answer": "",
    "answer_a": "F1 (0.744). Explanation: The results, including each of the training scenarios noted earlier, are presented in Table TABREF18 . Precision, recall, and F1 on the positive (sarcastic) class were recorded.",
    "answer_b": "< Explanation: We compare our approach to the following baseline models: (1) a baseline model trained on the training data, (2) a baseline model trained on the training data and the out-of-domain data, (3) a baseline model trained on the training data and the in-domain data, (4) a baseline model trained on the training data and the in-domain data and the out-of-domain data, (5) a baseline model trained on the training data and the in-domain data and the out-of-domain data and the out-of-domain data, (6) a baseline model trained on the training data and the in-domain",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat was the previous best results model?\n\nGround Truth Answer:\n\n\nAnswer A:\nF1 (0.744). Explanation: The results, including each of the training scenarios noted earlier, are presented in Table TABREF18 . Precision, recall, and F1 on the positive (sarcastic) class were recorded.\n\nAnswer B:\n< Explanation: We compare our approach to the following baseline models: (1) a baseline model trained on the training data, (2) a baseline model trained on the training data and the out-of-domain data, (3) a baseline model trained on the training data and the in-domain data, (4) a baseline model trained on the training data and the in-domain data and the out-of-domain data, (5) a baseline model trained on the training data and the in-domain data and the out-of-domain data and the out-of-domain data, (6) a baseline model trained on the training data and the in-domain\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.03369",
    "question": "what are the baseline models?",
    "gold_answer": "",
    "answer_a": "the All Sarcasm case. Explanation: We also include two baselines: the All Sarcasm case assumes that every instance is sarcastic, and the Random case randomly assigns each instance as sarcastic or non-sarcastic.",
    "answer_b": "What are the best-performing models?\n< Explanation: Table 1 shows the results of the baseline models. The best-performing model is the one that uses the same features as the best model in Table 2. The best model is the one that uses the same features as the best model in Table 3. The best model is the one that uses the same features as the best model in Table 4. The best model is the one that uses the same features as the best model in Table 5. The best model is the one that uses the same features as the best model in Table 6. The best model is",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the baseline models?\n\nGround Truth Answer:\n\n\nAnswer A:\nthe All Sarcasm case. Explanation: We also include two baselines: the All Sarcasm case assumes that every instance is sarcastic, and the Random case randomly assigns each instance as sarcastic or non-sarcastic.\n\nAnswer B:\nWhat are the best-performing models?\n< Explanation: Table 1 shows the results of the baseline models. The best-performing model is the one that uses the same features as the best model in Table 2. The best model is the one that uses the same features as the best model in Table 3. The best model is the one that uses the same features as the best model in Table 4. The best model is the one that uses the same features as the best model in Table 5. The best model is the one that uses the same features as the best model in Table 6. The best model is\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.03369",
    "question": "what domains are explored?",
    "gold_answer": "",
    "answer_a": "Explanation: We evaluate our approach on the following domains: English, German, French, Spanish, Italian, Portuguese, and Chinese. We also evaluate our approach on the following domains: English, German, French, Spanish, Italian, Portuguese, and Chinese. We also evaluate our approach on the following domains: English, German, French, Spanish, Italian, Portuguese, and Chinese. We also evaluate our approach on the following domains: English, German, French, Spanish, Italian, Portuguese, and Chinese. We also evaluate our approach on the following domains: English, German, French, Spanish, Italian, Portuguese, and Chinese. We also",
    "answer_b": "Twitter, and Amazon product reviews. Explanation: Data was taken from two domains: Twitter, and Amazon product reviews.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat domains are explored?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We evaluate our approach on the following domains: English, German, French, Spanish, Italian, Portuguese, and Chinese. We also evaluate our approach on the following domains: English, German, French, Spanish, Italian, Portuguese, and Chinese. We also evaluate our approach on the following domains: English, German, French, Spanish, Italian, Portuguese, and Chinese. We also evaluate our approach on the following domains: English, German, French, Spanish, Italian, Portuguese, and Chinese. We also evaluate our approach on the following domains: English, German, French, Spanish, Italian, Portuguese, and Chinese. We also\n\nAnswer B:\nTwitter, and Amazon product reviews. Explanation: Data was taken from two domains: Twitter, and Amazon product reviews.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.03369",
    "question": "what training data was used?",
    "gold_answer": "",
    "answer_a": "Twitter dataset. Explanation: Data was taken from two domains: Twitter, and Amazon product reviews. The Amazon reviews were from the publicly available sarcasm corpus developed by Filatova FILATOVA12.661. To build our Twitter dataset, tweets containing exactly one of the trailing hashtags \u201c#sarcasm,\u201d \u201c#happiness,\u201d \u201c#sadness,\u201d \u201c#anger,\u201d \u201c#surprise,\u201d \u201c#fear,\u201d and \u201c#disgust\u201d were downloaded regularly during February and March 2016. Tweets containing the latter six hashtags, corresponding to Ekman's six basic emotions BIBREF14 , were labeled as non-sarcastic. Those hashtags were chosen because their associated tweets were expected to still express opinions, similarly to sarcastic tweets, but in a non-sarcastic way. Tweets containing #sarcasm were labeled as sarcastic; annotating tweets with the #sarcasm hashtag as such is consistent with the vast majority of prior work in the Twitter domain BIBREF6 , BIBREF2 , BIBREF15 , BIBREF3 , BIBREF5 , BIBREF8 , BIBREF10 .",
    "answer_b": "< Explanation: We use the same training data as in the previous work, which consists of 1000 sentences from the English Wikipedia corpus. We use the same training and testing data for the two domains. The training data is split into 80% for training and 20% for testing. The testing data is used to evaluate the performance of the model. The testing data is split into 80% for training and 20% for testing. The testing data is used to evaluate the performance of the model. The training and testing data are used to train and test the model. The training and testing",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat training data was used?\n\nGround Truth Answer:\n\n\nAnswer A:\nTwitter dataset. Explanation: Data was taken from two domains: Twitter, and Amazon product reviews. The Amazon reviews were from the publicly available sarcasm corpus developed by Filatova FILATOVA12.661. To build our Twitter dataset, tweets containing exactly one of the trailing hashtags \u201c#sarcasm,\u201d \u201c#happiness,\u201d \u201c#sadness,\u201d \u201c#anger,\u201d \u201c#surprise,\u201d \u201c#fear,\u201d and \u201c#disgust\u201d were downloaded regularly during February and March 2016. Tweets containing the latter six hashtags, corresponding to Ekman's six basic emotions BIBREF14 , were labeled as non-sarcastic. Those hashtags were chosen because their associated tweets were expected to still express opinions, similarly to sarcastic tweets, but in a non-sarcastic way. Tweets containing #sarcasm were labeled as sarcastic; annotating tweets with the #sarcasm hashtag as such is consistent with the vast majority of prior work in the Twitter domain BIBREF6 , BIBREF2 , BIBREF15 , BIBREF3 , BIBREF5 , BIBREF8 , BIBREF10 .\n\nAnswer B:\n< Explanation: We use the same training data as in the previous work, which consists of 1000 sentences from the English Wikipedia corpus. We use the same training and testing data for the two domains. The training data is split into 80% for training and 20% for testing. The testing data is used to evaluate the performance of the model. The testing data is split into 80% for training and 20% for testing. The testing data is used to evaluate the performance of the model. The training and testing data are used to train and test the model. The training and testing\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.07459",
    "question": "What is the performance of the best model?",
    "gold_answer": "",
    "answer_a": "F1 Macro of 0.89. Explanation: FLOAT SELECTED: Table 7: Results for offensive language detection for Deep Learning models with Greek word embeddings. For each model, Precision (P), Recall (R), and F1 are reported on all classes, and weighted averages. Macro-F1 is also listed (best in bold).",
    "answer_b": "What is the performance of the worst model?\n<blockquote>Table 1: Results of the best model. The best model is the one with the highest accuracy (0.91) and the lowest F1 score (0.00). The best model is the one with the highest accuracy (0.91) and the lowest F1 score (0.00). The best model is the one with the highest accuracy (0.91) and the lowest F1 score (0.00). The best model is the one with the highest accuracy (0.91) and the lowest F1 score",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the performance of the best model?\n\nGround Truth Answer:\n\n\nAnswer A:\nF1 Macro of 0.89. Explanation: FLOAT SELECTED: Table 7: Results for offensive language detection for Deep Learning models with Greek word embeddings. For each model, Precision (P), Recall (R), and F1 are reported on all classes, and weighted averages. Macro-F1 is also listed (best in bold).\n\nAnswer B:\nWhat is the performance of the worst model?\n<blockquote>Table 1: Results of the best model. The best model is the one with the highest accuracy (0.91) and the lowest F1 score (0.00). The best model is the one with the highest accuracy (0.91) and the lowest F1 score (0.00). The best model is the one with the highest accuracy (0.91) and the lowest F1 score (0.00). The best model is the one with the highest accuracy (0.91) and the lowest F1 score\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.07459",
    "question": "What are the models tested on the dataset?",
    "gold_answer": "",
    "answer_a": "linear SVM, RBF SVM, linear classifier with SGDC, multinomial naive bayes, bernoulli naive bayes, pooled GRU, stacked LSTM with attention, LSTM and GRU with attention, 2d convolution with pooling, GRU with Capsule, LSTM with Capsule and attention, and BERT. Explanation: Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient of the loss is estimated each sample at a time and the SGDC is updated along the way with a decreasing learning rate. The parameters for maximum epochs and the stopping criterion were defined using the default values in scikit-learn. The final classifier was two models based on the Bayes theorem: Multinomial Na\u00efve Bayes, which works with occurrence counts, and Bernoulli Na\u00efve Bayes, which is designed for binary features.",
    "answer_b": "What are the results?\n<blockquote>\n<font size=\"2\">The results of the experiments are shown in Table 1. The results show that the proposed models outperform the baseline models. The best model achieved a F1 score of 0.73, which is significantly higher than the baseline models. The results of the experiments are shown in Table 1. The results show that the proposed models outperform the baseline models. The best model achieved a F1 score of 0.73, which is significantly higher than the baseline models. </font>\n<font size=\"2\">The results of the experiments are shown",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the models tested on the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nlinear SVM, RBF SVM, linear classifier with SGDC, multinomial naive bayes, bernoulli naive bayes, pooled GRU, stacked LSTM with attention, LSTM and GRU with attention, 2d convolution with pooling, GRU with Capsule, LSTM with Capsule and attention, and BERT. Explanation: Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term. The gamma value of the RBF SVM which indicates how much influence a single training example has, was set to 2. The third classifier trained was another linear classifier with Stochastic Gradient Descent (SGDC) learning. The gradient of the loss is estimated each sample at a time and the SGDC is updated along the way with a decreasing learning rate. The parameters for maximum epochs and the stopping criterion were defined using the default values in scikit-learn. The final classifier was two models based on the Bayes theorem: Multinomial Na\u00efve Bayes, which works with occurrence counts, and Bernoulli Na\u00efve Bayes, which is designed for binary features.\n\nAnswer B:\nWhat are the results?\n<blockquote>\n<font size=\"2\">The results of the experiments are shown in Table 1. The results show that the proposed models outperform the baseline models. The best model achieved a F1 score of 0.73, which is significantly higher than the baseline models. The results of the experiments are shown in Table 1. The results show that the proposed models outperform the baseline models. The best model achieved a F1 score of 0.73, which is significantly higher than the baseline models. </font>\n<font size=\"2\">The results of the experiments are shown\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.07459",
    "question": "Which method best performs on the offensive language identification task?",
    "gold_answer": "",
    "answer_a": "What is the performance of the model?\n<sep><table cellspacing=\"0\" cellpadding=\"0\" width=\"600\" border=\"0\" align=\"center\">\n<tr>\n<td align=\"center\">\nFigure 1: Performance of the different models on the Offensive Greek Tweet Dataset (OGTD). The left column shows the performance of the baseline model, the right column shows the performance of the baseline model with the addition of a word embedding. The models are trained on the training set and tested on the test set. The models are trained on the training set and tested on the test set. The models are trained on the training set and",
    "answer_b": "LSTM and GRU with Attention can be considered as the best model trained for OGTD. Explanation: As we can see LSTM and GRU with Attention outperformed all the other models in-terms of macro-f1. Notably it outperformed all other classifical models and deep learning models in precision, recall and f1 for Offensive class as well as the Not Offensive class. However, fine tuning BERT-Base Multilingual Cased model did not achieve good results. For this task monolingual Greek word embeddings perform significantly better than the multilingual bert embeddings. LSTM and GRU with Attention can be considered as the best model trained for OGTD.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich method best performs on the offensive language identification task?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of the model?\n<sep><table cellspacing=\"0\" cellpadding=\"0\" width=\"600\" border=\"0\" align=\"center\">\n<tr>\n<td align=\"center\">\nFigure 1: Performance of the different models on the Offensive Greek Tweet Dataset (OGTD). The left column shows the performance of the baseline model, the right column shows the performance of the baseline model with the addition of a word embedding. The models are trained on the training set and tested on the test set. The models are trained on the training set and tested on the test set. The models are trained on the training set and\n\nAnswer B:\nLSTM and GRU with Attention can be considered as the best model trained for OGTD. Explanation: As we can see LSTM and GRU with Attention outperformed all the other models in-terms of macro-f1. Notably it outperformed all other classifical models and deep learning models in precision, recall and f1 for Offensive class as well as the Not Offensive class. However, fine tuning BERT-Base Multilingual Cased model did not achieve good results. For this task monolingual Greek word embeddings perform significantly better than the multilingual bert embeddings. LSTM and GRU with Attention can be considered as the best model trained for OGTD.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.07459",
    "question": "How many annotators did they have?",
    "gold_answer": "",
    "answer_a": "What was their role?\nNumber of annotators: 10\nRole of annotators: 10\nExplanation: The dataset was created by 10 annotators, who were trained to identify offensive tweets. The annotators were trained to identify offensive tweets by using a set of pre-trained models. The models were trained on a dataset of 1000 tweets annotated as offensive and 1000 tweets annotated as not offensive. The models were trained using a supervised learning approach. The models were trained using a supervised learning approach. The models were trained using a supervised learning approach. The models were trained using a supervised",
    "answer_b": "Three, plus 2 in case of disagreement below 66%.. Explanation: Based on explicit annotation guidelines written in Greek and our proposal of the definition of offensive language, a team of three volunteers were asked to classify each tweet found in the dataset with one of the following tags: Offensive, Not Offensive and Spam, which was introduced to filter out spam from the dataset.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many annotators did they have?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat was their role?\nNumber of annotators: 10\nRole of annotators: 10\nExplanation: The dataset was created by 10 annotators, who were trained to identify offensive tweets. The annotators were trained to identify offensive tweets by using a set of pre-trained models. The models were trained on a dataset of 1000 tweets annotated as offensive and 1000 tweets annotated as not offensive. The models were trained using a supervised learning approach. The models were trained using a supervised learning approach. The models were trained using a supervised learning approach. The models were trained using a supervised\n\nAnswer B:\nThree, plus 2 in case of disagreement below 66%.. Explanation: Based on explicit annotation guidelines written in Greek and our proposal of the definition of offensive language, a team of three volunteers were asked to classify each tweet found in the dataset with one of the following tags: Offensive, Not Offensive and Spam, which was introduced to filter out spam from the dataset.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.07459",
    "question": "What models do they experiment on?",
    "gold_answer": "",
    "answer_a": "What are their results?\n<blockquote>Table 1: Results of the baseline models. The first column shows the accuracy of the baseline model, the second column shows the accuracy of the baseline model with the addition of the word \u201coffensive\u201d to the training set, the third column shows the accuracy of the baseline model with the addition of the word \u201chate\u201d to the training set, the fourth column shows the accuracy of the baseline model with the addition of the word \u201caggression\u201d to the training set, the fifth column shows the accuracy of the baseline model with the addition of the word \u201chate speech\u201d to the training",
    "answer_b": "Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF). Explanation: Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat models do they experiment on?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are their results?\n<blockquote>Table 1: Results of the baseline models. The first column shows the accuracy of the baseline model, the second column shows the accuracy of the baseline model with the addition of the word \u201coffensive\u201d to the training set, the third column shows the accuracy of the baseline model with the addition of the word \u201chate\u201d to the training set, the fourth column shows the accuracy of the baseline model with the addition of the word \u201caggression\u201d to the training set, the fifth column shows the accuracy of the baseline model with the addition of the word \u201chate speech\u201d to the training\n\nAnswer B:\nTwo SVMs, one with linear kernel and the other with a radial basis function kernel (RBF). Explanation: Five models were trained: Two SVMs, one with linear kernel and the other with a radial basis function kernel (RBF), both with a value of 1 in the penalty parameter C of the error term.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.08614",
    "question": "What is the size of their published dataset?",
    "gold_answer": "",
    "answer_a": "911. Explanation: This finally gave us a total of 568 reviews in Catalan and 343 reviews in Basque, collected from November 2015 to January 2016.",
    "answer_b": "How many people contributed to it?\n< Explanation: We provide a dataset of 10000 reviews in Basque and 10000 reviews in Catalan. We also provide a dataset of 10000 reviews in Basque and 10000 reviews in Catalan. The dataset is available at https://github.com/abdulrahman1991/aspect-level-sentiment-analysis. We provide high-quality annotations and benchmarks with the hope that they will be useful to the growing community of researchers working on these languages. We provide the following information about the datasets:\n\nBasque",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the size of their published dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\n911. Explanation: This finally gave us a total of 568 reviews in Catalan and 343 reviews in Basque, collected from November 2015 to January 2016.\n\nAnswer B:\nHow many people contributed to it?\n< Explanation: We provide a dataset of 10000 reviews in Basque and 10000 reviews in Catalan. We also provide a dataset of 10000 reviews in Basque and 10000 reviews in Catalan. The dataset is available at https://github.com/abdulrahman1991/aspect-level-sentiment-analysis. We provide high-quality annotations and benchmarks with the hope that they will be useful to the growing community of researchers working on these languages. We provide the following information about the datasets:\n\nBasque\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.08250",
    "question": "How does sentence construction component works?",
    "gold_answer": "",
    "answer_a": "What is the input and output of the system?\n<sep><br>Input: A sentence in the application domain. Output: A GF for the sentence. Explanation: The sentence construction component is responsible for constructing a GF for a given sentence. It is implemented using the Stanford parser and an answer set program. The parser is used to parse the sentence and the answer set program is used to construct the GF. The GF is constructed by the answer set program and is used to answer the questions about the sentence. The answer set program is implemented using the Stanford answer set program. The answer set program is responsible for answering the questions about the sentence",
    "answer_b": "Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation. Explanation: Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation. The system consists of two components, understanding sentences and generating GF grammar. The first component is divided into two sub-components, one for recognizing the sentence structure and one for recognizing the sentence components. The second component consists of a GF grammar encoder and a GF grammar exporter. The encoder is responsible for generating a GF grammar for each sentence, while the exporter aggregates the grammars generated from the encoder, and produces a comprehensive grammar for the whole paragraph.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does sentence construction component works?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the input and output of the system?\n<sep><br>Input: A sentence in the application domain. Output: A GF for the sentence. Explanation: The sentence construction component is responsible for constructing a GF for a given sentence. It is implemented using the Stanford parser and an answer set program. The parser is used to parse the sentence and the answer set program is used to construct the GF. The GF is constructed by the answer set program and is used to answer the questions about the sentence. The answer set program is implemented using the Stanford answer set program. The answer set program is responsible for answering the questions about the sentence\n\nAnswer B:\nGiven a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation. Explanation: Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation. The system consists of two components, understanding sentences and generating GF grammar. The first component is divided into two sub-components, one for recognizing the sentence structure and one for recognizing the sentence components. The second component consists of a GF grammar encoder and a GF grammar exporter. The encoder is responsible for generating a GF grammar for each sentence, while the exporter aggregates the grammars generated from the encoder, and produces a comprehensive grammar for the whole paragraph.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.08250",
    "question": "What are two use cases that demonstrate capability of created system?",
    "gold_answer": "",
    "answer_a": "The first application is to generate a natural language description for workflow created by the system built in the Phylotastic project. Explanation: The first application is to generate a natural language description for workflow created by the system built in the Phylotastic project described in BIBREF2.",
    "answer_b": "Please include a link to the paper.\n<blockquote>1. A user wants to create an abstract Wikipedia. The system is able to generate an abstract Wikipedia with the same content as the original one. The system is able to generate an abstract Wikipedia with the same content as the original one. The system is able to generate an abstract Wikipedia with the same content as the original one. The system is able to generate an abstract Wikipedia with the same content as the original one. The system is able to generate an abstract Wikipedia with the same content as the original one. The system is able to generate an abstract Wikipedia with the same content as the original",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are two use cases that demonstrate capability of created system?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe first application is to generate a natural language description for workflow created by the system built in the Phylotastic project. Explanation: The first application is to generate a natural language description for workflow created by the system built in the Phylotastic project described in BIBREF2.\n\nAnswer B:\nPlease include a link to the paper.\n<blockquote>1. A user wants to create an abstract Wikipedia. The system is able to generate an abstract Wikipedia with the same content as the original one. The system is able to generate an abstract Wikipedia with the same content as the original one. The system is able to generate an abstract Wikipedia with the same content as the original one. The system is able to generate an abstract Wikipedia with the same content as the original one. The system is able to generate an abstract Wikipedia with the same content as the original one. The system is able to generate an abstract Wikipedia with the same content as the original\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1612.07486",
    "question": "Which neural language model architecture do they use?",
    "gold_answer": "",
    "answer_a": "How do they learn it?\n< Explanation: We use a character-based neural language model (NLM) to learn the language vectors. The model consists of a character-level encoder, a decoder, and a language model. The encoder is a simple convolutional neural network (CNN) with a character-level attention mechanism. The decoder is a recurrent neural network (RNN) with a character-level attention mechanism. The language model is a simple language model (LM) with a character-level attention mechanism. The model is trained using a stochastic gradient descent (SGD) algorithm. The model is trained on a corpus of 1303 Bible",
    "answer_b": "character-level RNN. Explanation: Our model is based on a standard stacked character-based LSTM BIBREF4 with two layers, followed by a hidden layer and a final output layer with softmax activations",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich neural language model architecture do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they learn it?\n< Explanation: We use a character-based neural language model (NLM) to learn the language vectors. The model consists of a character-level encoder, a decoder, and a language model. The encoder is a simple convolutional neural network (CNN) with a character-level attention mechanism. The decoder is a recurrent neural network (RNN) with a character-level attention mechanism. The language model is a simple language model (LM) with a character-level attention mechanism. The model is trained using a stochastic gradient descent (SGD) algorithm. The model is trained on a corpus of 1303 Bible\n\nAnswer B:\ncharacter-level RNN. Explanation: Our model is based on a standard stacked character-based LSTM BIBREF4 with two layers, followed by a hidden layer and a final output layer with softmax activations\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1612.07486",
    "question": "How do they show genetic relationships between languages?",
    "gold_answer": "",
    "answer_a": "What is the definition of genetic relationships?\n< Explanation: We show that the language vectors capture genetic relationships between languages. We define a genetic relationship between two languages as a set of languages that are related by a common ancestor. We show that the language vectors capture this relationship, and that the relationship is preserved when the vectors are used to predict the language of a new language. We also show that the relationship is preserved when the vectors are used to predict the language of a new language, and that the relationship is preserved when the vectors are used to predict the language of a new language, and that the relationship is preserved when the vectors are used to",
    "answer_b": "hierarchical clustering. Explanation: fig:germanic shows a hierarchical clustering of the subset of Germanic languages, which closely matches the established genetic relationships in this language family.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they show genetic relationships between languages?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the definition of genetic relationships?\n< Explanation: We show that the language vectors capture genetic relationships between languages. We define a genetic relationship between two languages as a set of languages that are related by a common ancestor. We show that the language vectors capture this relationship, and that the relationship is preserved when the vectors are used to predict the language of a new language. We also show that the relationship is preserved when the vectors are used to predict the language of a new language, and that the relationship is preserved when the vectors are used to predict the language of a new language, and that the relationship is preserved when the vectors are used to\n\nAnswer B:\nhierarchical clustering. Explanation: fig:germanic shows a hierarchical clustering of the subset of Germanic languages, which closely matches the established genetic relationships in this language family.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1612.03762",
    "question": "What are the steps in the MagiCoder algorithm?",
    "gold_answer": "",
    "answer_a": "What are the parameters of the algorithm?\n< Explanation: The MagiCoder algorithm consists of three main steps: (1) the description is automatically annotated with MedDRA terms; (2) the MedDRA terms are automatically encoded into a vocabulary of 70K terms; (3) the MedDRA terms are automatically classified into 10 classes. The algorithm is based on the MedDRA standard terminology, which is a set of 10 classes, each corresponding to a MedDRA term. The MedDRA terms are automatically encoded into a vocabulary of 70K terms, which is the size of",
    "answer_b": "Preprocessing of the original text. Explanation: MagiCoder: overview\nThe main idea of INLINEFORM0 is that a single linear scan of the free-text is sufficient, in order to recognize INLINEFORM1 terms.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the steps in the MagiCoder algorithm?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the parameters of the algorithm?\n< Explanation: The MagiCoder algorithm consists of three main steps: (1) the description is automatically annotated with MedDRA terms; (2) the MedDRA terms are automatically encoded into a vocabulary of 70K terms; (3) the MedDRA terms are automatically classified into 10 classes. The algorithm is based on the MedDRA standard terminology, which is a set of 10 classes, each corresponding to a MedDRA term. The MedDRA terms are automatically encoded into a vocabulary of 70K terms, which is the size of\n\nAnswer B:\nPreprocessing of the original text. Explanation: MagiCoder: overview\nThe main idea of INLINEFORM0 is that a single linear scan of the free-text is sufficient, in order to recognize INLINEFORM1 terms.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1612.03762",
    "question": "How is the system constructed to be linear in the size of the narrative input and the terminology?",
    "gold_answer": "",
    "answer_a": "Is it linear in the size of the term dictionary?\n<sep><br>Answer: The system is linear in the size of the narrative input and the terminology. The size of the term dictionary is 70K. The size of the narrative input is 1000000 characters. The size of the term dictionary is 1000000 characters. The size of the narrative input is 1000000 characters. The size of the term dictionary is 1000000 characters. The size of the narrative input is 1000000",
    "answer_b": "The system scans the text word-by-word once and performs a voting task for each word. It also keeps track of the position of the previous words.. Explanation: NLINEFORM0 scans the text word-by-word (remember that each word corresponds to a token) once and performs a \u201cvoting task\u201d: at the INLINEFORM1 -th step, it marks (i.e., \u201cvotes\u201d) with index INLINEFORM2 each LLT INLINEFORM3 containing the current ( INLINEFORM4 -th) word of the ADR description. Moreover, it keeps track of the position where the INLINEFORM5 -th word occurs in INLINEFORM6 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is the system constructed to be linear in the size of the narrative input and the terminology?\n\nGround Truth Answer:\n\n\nAnswer A:\nIs it linear in the size of the term dictionary?\n<sep><br>Answer: The system is linear in the size of the narrative input and the terminology. The size of the term dictionary is 70K. The size of the narrative input is 1000000 characters. The size of the term dictionary is 1000000 characters. The size of the narrative input is 1000000 characters. The size of the term dictionary is 1000000 characters. The size of the narrative input is 1000000\n\nAnswer B:\nThe system scans the text word-by-word once and performs a voting task for each word. It also keeps track of the position of the previous words.. Explanation: NLINEFORM0 scans the text word-by-word (remember that each word corresponds to a token) once and performs a \u201cvoting task\u201d: at the INLINEFORM1 -th step, it marks (i.e., \u201cvotes\u201d) with index INLINEFORM2 each LLT INLINEFORM3 containing the current ( INLINEFORM4 -th) word of the ADR description. Moreover, it keeps track of the position where the INLINEFORM5 -th word occurs in INLINEFORM6 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01010",
    "question": "What conclusions do the authors draw about the aspects and mechanisms of personal recovery in bipolar disorder?",
    "gold_answer": "",
    "answer_a": "Explanation: The authors found that the recovery process is not linear, but rather involves a complex interplay of factors, such as the individual's personal and social circumstances, the severity of the illness, the patient's self-efficacy, the patient's self-concept, the patient's self-esteem, the patient's self-care, the patient's self-disclosure, the patient's self-acceptance, the patient's self-acceptance, the patient's self-acceptance, the patient's self-acceptance, the patient's self-acceptance, the patient's self-acceptance, the patient's self-acceptance",
    "answer_b": "Occurrences of the identified keywords or concepts can be quantified in the full corpus to identify the importance of the related personal recovery aspects. Explanation: Occurrences of the identified keywords or concepts can be quantified in the full corpus to identify the importance of the related personal recovery aspects.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat conclusions do the authors draw about the aspects and mechanisms of personal recovery in bipolar disorder?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The authors found that the recovery process is not linear, but rather involves a complex interplay of factors, such as the individual's personal and social circumstances, the severity of the illness, the patient's self-efficacy, the patient's self-concept, the patient's self-esteem, the patient's self-care, the patient's self-disclosure, the patient's self-acceptance, the patient's self-acceptance, the patient's self-acceptance, the patient's self-acceptance, the patient's self-acceptance, the patient's self-acceptance, the patient's self-acceptance\n\nAnswer B:\nOccurrences of the identified keywords or concepts can be quantified in the full corpus to identify the importance of the related personal recovery aspects. Explanation: Occurrences of the identified keywords or concepts can be quantified in the full corpus to identify the importance of the related personal recovery aspects.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01010",
    "question": "What computational linguistic methods were used for the analysis?",
    "gold_answer": "",
    "answer_a": "language identification. Explanation: For language identification, Twitter employs an automatic tool BIBREF48 , which can be used to filter tweets according to 60 language codes, and there are free, fairly accurate tools such as the Google Compact Language Detector, which can be applied to Reddit and blog posts. T",
    "answer_b": "Explanation: The data were analysed using a combination of statistical and computational methods. The statistical methods include descriptive statistics, t-tests, chi-squared tests, and ANOVA. The computational methods include the use of natural language processing (NLP) techniques, such as part-of-speech tagging, named entity recognition, and sentiment analysis. The NLP techniques were used to extract and analyse the social media posts of the participants. The sentiment analysis was used to analyse the emotional tone of the posts. The sentiment analysis was performed using the Stanford Sentiment Treebank (SSTB) 2.0. The sentiment analysis was performed using the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat computational linguistic methods were used for the analysis?\n\nGround Truth Answer:\n\n\nAnswer A:\nlanguage identification. Explanation: For language identification, Twitter employs an automatic tool BIBREF48 , which can be used to filter tweets according to 60 language codes, and there are free, fairly accurate tools such as the Google Compact Language Detector, which can be applied to Reddit and blog posts. T\n\nAnswer B:\nExplanation: The data were analysed using a combination of statistical and computational methods. The statistical methods include descriptive statistics, t-tests, chi-squared tests, and ANOVA. The computational methods include the use of natural language processing (NLP) techniques, such as part-of-speech tagging, named entity recognition, and sentiment analysis. The NLP techniques were used to extract and analyse the social media posts of the participants. The sentiment analysis was used to analyse the emotional tone of the posts. The sentiment analysis was performed using the Stanford Sentiment Treebank (SSTB) 2.0. The sentiment analysis was performed using the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01010",
    "question": "Was permission sought from the bipolar patients to use this data?",
    "gold_answer": "",
    "answer_a": "For Twitter and Reddit users , implicit consent is assumed to use their public tweets. Blog users are  contacted to obtain consent for using their texts.. Explanation: Twitter and Reddit users with a BD diagnosis will be identified automatically via self-reported diagnosis statements, such as `I was diagnosed with BD-I last week'. To do so, we will extend on the diagnosis patterns and terms for BD provided by BIBREF47 . Implicit consent is assumed from users on these platforms to use their public tweets and posts. SECREF3 Relevant blogs will be manually identified, and their authors will be contacted to obtain informed consent for using their texts.",
    "answer_b": "Yes, the patients were given permission to use the data for research purposes. The data were anonymised and the patients were informed about the purpose of the data and the rights of the patients to withdraw from the study. The data were collected and analysed by the researchers in accordance with the principles of the Declaration of Helsinki. The data were used for the purpose of the research and the data were not used for any other purpose. The data were not shared with any other parties. The data were not used for any other purpose. The data were not used for any other purpose. The data were not used for any other purpose. The data were not",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWas permission sought from the bipolar patients to use this data?\n\nGround Truth Answer:\n\n\nAnswer A:\nFor Twitter and Reddit users , implicit consent is assumed to use their public tweets. Blog users are  contacted to obtain consent for using their texts.. Explanation: Twitter and Reddit users with a BD diagnosis will be identified automatically via self-reported diagnosis statements, such as `I was diagnosed with BD-I last week'. To do so, we will extend on the diagnosis patterns and terms for BD provided by BIBREF47 . Implicit consent is assumed from users on these platforms to use their public tweets and posts. SECREF3 Relevant blogs will be manually identified, and their authors will be contacted to obtain informed consent for using their texts.\n\nAnswer B:\nYes, the patients were given permission to use the data for research purposes. The data were anonymised and the patients were informed about the purpose of the data and the rights of the patients to withdraw from the study. The data were collected and analysed by the researchers in accordance with the principles of the Declaration of Helsinki. The data were used for the purpose of the research and the data were not used for any other purpose. The data were not shared with any other parties. The data were not used for any other purpose. The data were not used for any other purpose. The data were not used for any other purpose. The data were not\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01010",
    "question": "How are the individuals with bipolar disorder identified?",
    "gold_answer": "",
    "answer_a": "What is the sample size?\n<blockquote>Participants were recruited from the Internet using the following criteria: (1) the participant had a diagnosis of bipolar disorder according to the DSM-IV-TR; (2) the participant had a diagnosis of bipolar disorder according to the ICD-10; (3) the participant had a diagnosis of bipolar disorder according to the ICD-10; (4) the participant had a diagnosis of bipolar disorder according to the ICD-10; (5) the participant had a diagnosis of bipolar disorder according to the ICD-10; (6) the participant had a diagnosis",
    "answer_b": "characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12. Explanation: Here, we focus on BD, which is characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are the individuals with bipolar disorder identified?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the sample size?\n<blockquote>Participants were recruited from the Internet using the following criteria: (1) the participant had a diagnosis of bipolar disorder according to the DSM-IV-TR; (2) the participant had a diagnosis of bipolar disorder according to the ICD-10; (3) the participant had a diagnosis of bipolar disorder according to the ICD-10; (4) the participant had a diagnosis of bipolar disorder according to the ICD-10; (5) the participant had a diagnosis of bipolar disorder according to the ICD-10; (6) the participant had a diagnosis\n\nAnswer B:\ncharacterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12. Explanation: Here, we focus on BD, which is characterised by recurring episodes of depressed and elated (hypomanic or manic) mood BIBREF25 , BIBREF12 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.11528",
    "question": "What is the source of the training/testing data?",
    "gold_answer": "",
    "answer_a": "How is it divided?\n< Explanation: The training data is collected from the Chinese Poetry Repository (CPR) (Wang et al., 2019). The data is divided into three categories: 1) Classical Chinese poetry, 2) Modern Chinese poetry, and 3) Other. The training data is collected from the CCRB (Chu et al., 2019). The training data is divided into three categories: 1) Classical Chinese poetry, 2) Modern Chinese poetry, and 3) Other. The training data is collected from the CCRB (Chu et al",
    "answer_b": "CCPC1.0. Explanation: Chinese Classical poetry can be classified into two primary categories, SHI and CI. According to the statistical data from CCPC1.0, a Chinese Classical Poetry Corpus consisting of 834,902 poems in total (We believe it is almost a full collection of Chinese Classical poems). 92.87% poems in CCPC1.0 fall into the category of SHI and 7.13% fall into the category of CI. SHI and CI can be further divided into many different types in terms of their forms. We briefly introduce the related background knowledge as follows.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the source of the training/testing data?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it divided?\n< Explanation: The training data is collected from the Chinese Poetry Repository (CPR) (Wang et al., 2019). The data is divided into three categories: 1) Classical Chinese poetry, 2) Modern Chinese poetry, and 3) Other. The training data is collected from the CCRB (Chu et al., 2019). The training data is divided into three categories: 1) Classical Chinese poetry, 2) Modern Chinese poetry, and 3) Other. The training data is collected from the CCRB (Chu et al\n\nAnswer B:\nCCPC1.0. Explanation: Chinese Classical poetry can be classified into two primary categories, SHI and CI. According to the statistical data from CCPC1.0, a Chinese Classical Poetry Corpus consisting of 834,902 poems in total (We believe it is almost a full collection of Chinese Classical poems). 92.87% poems in CCPC1.0 fall into the category of SHI and 7.13% fall into the category of CI. SHI and CI can be further divided into many different types in terms of their forms. We briefly introduce the related background knowledge as follows.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.11528",
    "question": "What are the types of chinese poetry that are generated?",
    "gold_answer": "",
    "answer_a": "SHI. Explanation: Chinese Classical poetry can be classified into two primary categories, SHI and CI. According to the statistical data from CCPC1.0, a Chinese Classical Poetry Corpus consisting of 834,902 poems in total (We believe it is almost a full collection of Chinese Classical poems). 92.87% poems in CCPC1.0 fall into the category of SHI and 7.13% fall into the category of CI",
    "answer_b": "Please provide a table.\n<table> <thead> <tr> <th> Type </th> <th> Sample </th> <th> Result </th> </tr> </thead> <tbody> <tr> <td> Song </td> <td> \u300a\u6625\u6c5f\u82b1\u6708\u591c\u300b </td> <td> \u300a\u6625\u6c5f\u82b1\u6708\u591c\u300b </td> </tr> <tr> <td> \u300a\u4e5d\u6b4c\u300b </td> <td> \u300a\u4e5d\u6b4c\u300b </td> <td> \u300a\u4e5d\u6b4c\u300b </td> </tr> <tr> <",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the types of chinese poetry that are generated?\n\nGround Truth Answer:\n\n\nAnswer A:\nSHI. Explanation: Chinese Classical poetry can be classified into two primary categories, SHI and CI. According to the statistical data from CCPC1.0, a Chinese Classical Poetry Corpus consisting of 834,902 poems in total (We believe it is almost a full collection of Chinese Classical poems). 92.87% poems in CCPC1.0 fall into the category of SHI and 7.13% fall into the category of CI\n\nAnswer B:\nPlease provide a table.\n<table> <thead> <tr> <th> Type </th> <th> Sample </th> <th> Result </th> </tr> </thead> <tbody> <tr> <td> Song </td> <td> \u300a\u6625\u6c5f\u82b1\u6708\u591c\u300b </td> <td> \u300a\u6625\u6c5f\u82b1\u6708\u591c\u300b </td> </tr> <tr> <td> \u300a\u4e5d\u6b4c\u300b </td> <td> \u300a\u4e5d\u6b4c\u300b </td> <td> \u300a\u4e5d\u6b4c\u300b </td> </tr> <tr> <\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1801.03615",
    "question": "what is the previous work they are comparing to?",
    "gold_answer": "",
    "answer_a": "Answer: previous work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to",
    "answer_b": "RNN and Transformer baseline systems utilize BPE BIBREF3. Explanation: Our RNN and Transformer baseline systems utilize BPE BIBREF3 to transfer the original word sequence to subword sequence on both the source and the target sides, since the subword method had a stable improvement compared with word based system, especially on morphologically rich languages.\n\nBesides, we compared our system with a fully character-based baseline system, which is an implementation of BIBREF4 ( BIBREF4 )'s work, and is available on github.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the previous work they are comparing to?\n\nGround Truth Answer:\n\n\nAnswer A:\nAnswer: previous work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to Russian translation\nPrevious work on English to\n\nAnswer B:\nRNN and Transformer baseline systems utilize BPE BIBREF3. Explanation: Our RNN and Transformer baseline systems utilize BPE BIBREF3 to transfer the original word sequence to subword sequence on both the source and the target sides, since the subword method had a stable improvement compared with word based system, especially on morphologically rich languages.\n\nBesides, we compared our system with a fully character-based baseline system, which is an implementation of BIBREF4 ( BIBREF4 )'s work, and is available on github.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.09362",
    "question": "How is quality of the word vectors measured?",
    "gold_answer": "",
    "answer_a": "What is the evaluation task?\n< Explanation: We evaluate the quality of the word vectors by comparing the mean word vector distance between the learned word vectors and the ground truth word vectors. We also evaluate the quality of the word vectors by comparing the mean word vector distance between the learned word vectors and the ground truth word vectors. We also evaluate the quality of the word vectors by comparing the mean word vector distance between the learned word vectors and the ground truth word vectors. We also evaluate the quality of the word vectors by comparing the mean word vector distance between the learned word vectors and the ground truth word vectors. We also evaluate the quality of the word",
    "answer_b": "correlation between the word similarity scores by human judgment and the word distances in vector space. Explanation: Experiments ::: Task 1: Word Similarity Task ::: Task Description\nThe task computes the correlation between the word similarity scores by human judgment and the word distances in vector space. We use Pearson correlation coefficient $\\rho _p$ as the metric, the higher of which the better the word vectors are.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is quality of the word vectors measured?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the evaluation task?\n< Explanation: We evaluate the quality of the word vectors by comparing the mean word vector distance between the learned word vectors and the ground truth word vectors. We also evaluate the quality of the word vectors by comparing the mean word vector distance between the learned word vectors and the ground truth word vectors. We also evaluate the quality of the word vectors by comparing the mean word vector distance between the learned word vectors and the ground truth word vectors. We also evaluate the quality of the word vectors by comparing the mean word vector distance between the learned word vectors and the ground truth word vectors. We also evaluate the quality of the word\n\nAnswer B:\ncorrelation between the word similarity scores by human judgment and the word distances in vector space. Explanation: Experiments ::: Task 1: Word Similarity Task ::: Task Description\nThe task computes the correlation between the word similarity scores by human judgment and the word distances in vector space. We use Pearson correlation coefficient $\\rho _p$ as the metric, the higher of which the better the word vectors are.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12559",
    "question": "Where do the news texts come from?",
    "gold_answer": "",
    "answer_a": "What are the lengths of the texts?\n< Explanation: The news texts are from the 2018-2019 academic year. The dataset contains 1,000 news articles, each of which is 1000 words long. The dataset is split into training and testing sets. The training set contains 500 news articles, and the testing set contains 500 news articles. The training set is used to train the models, and the testing set is used to evaluate the models. The training set is split into 100 training and 100 testing sets. The",
    "answer_b": "online news websites. Explanation: To create the KPTimes dataset, we collected over half a million newswire articles by crawling selected online news websites.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhere do the news texts come from?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the lengths of the texts?\n< Explanation: The news texts are from the 2018-2019 academic year. The dataset contains 1,000 news articles, each of which is 1000 words long. The dataset is split into training and testing sets. The training set contains 500 news articles, and the testing set contains 500 news articles. The training set is used to train the models, and the testing set is used to evaluate the models. The training set is split into 100 training and 100 testing sets. The\n\nAnswer B:\nonline news websites. Explanation: To create the KPTimes dataset, we collected over half a million newswire articles by crawling selected online news websites.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12559",
    "question": "What baseline is used for this task?",
    "gold_answer": "",
    "answer_a": "FirstPhrases baseline. Explanation: Similar to the lead baseline, we compute the FirstPhrases baseline that extracts the first $N$ keyphrase candidates from a document.",
    "answer_b": "What is the performance of the baseline?\n< Explanation: We use the following baseline: a simple bag of words model, a simple bag of words model with a word embedding, a simple bag of words model with a word embedding and a simple bag of words model with a word embedding and a word embedding. We also use a simple bag of words model with a word embedding and a word embedding. We use a simple bag of words model with a word embedding and a word embedding. We use a simple bag of words model with a word embedding and a word embedding. We use a simple bag of words model with a word embedding and a word",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat baseline is used for this task?\n\nGround Truth Answer:\n\n\nAnswer A:\nFirstPhrases baseline. Explanation: Similar to the lead baseline, we compute the FirstPhrases baseline that extracts the first $N$ keyphrase candidates from a document.\n\nAnswer B:\nWhat is the performance of the baseline?\n< Explanation: We use the following baseline: a simple bag of words model, a simple bag of words model with a word embedding, a simple bag of words model with a word embedding and a simple bag of words model with a word embedding and a word embedding. We also use a simple bag of words model with a word embedding and a word embedding. We use a simple bag of words model with a word embedding and a word embedding. We use a simple bag of words model with a word embedding and a word embedding. We use a simple bag of words model with a word embedding and a word\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12559",
    "question": "What type of nerual keyphrase generation models are trained?",
    "gold_answer": "",
    "answer_a": "What are the results?\n< Explanation: We train a set of 10 different models on KPTimes, and evaluate them on the test set. The models are trained with a combination of a bidirectional LSTM and a transformer. The models are trained with a combination of a bidirectional LSTM and a transformer with a combination of a bidirectional LSTM and a transformer with a combination of a bidirectional LSTM and a transformer with a combination of a bidirectional LSTM and a transformer with a combination of a bidirectional LSTM and a transformer with a combination of a bidirectional LSTM and a transformer with a combination of a bidirectional LSTM and a transformer",
    "answer_b": "CopySci was trained using scientific abstracts (KP20k) and CopyNews using newspaper articles (KPTimes). Explanation: The generative neural model we include in this study is CopyRNN BIBREF2, an encoder-decoder model that incorporates a copying mechanism BIBREF19 in order to be able to generate phrases that rarely occur.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of nerual keyphrase generation models are trained?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n< Explanation: We train a set of 10 different models on KPTimes, and evaluate them on the test set. The models are trained with a combination of a bidirectional LSTM and a transformer. The models are trained with a combination of a bidirectional LSTM and a transformer with a combination of a bidirectional LSTM and a transformer with a combination of a bidirectional LSTM and a transformer with a combination of a bidirectional LSTM and a transformer with a combination of a bidirectional LSTM and a transformer with a combination of a bidirectional LSTM and a transformer with a combination of a bidirectional LSTM and a transformer\n\nAnswer B:\nCopySci was trained using scientific abstracts (KP20k) and CopyNews using newspaper articles (KPTimes). Explanation: The generative neural model we include in this study is CopyRNN BIBREF2, an encoder-decoder model that incorporates a copying mechanism BIBREF19 in order to be able to generate phrases that rarely occur.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12559",
    "question": "How do the editors' annotations differ from those in existing datasets?",
    "gold_answer": "",
    "answer_a": "Existing datasets are annotated by non-experts who use a larger, less controlled indexed vocabulary lacking the domain expertise shown by the editors. Explanation: Frequently used datasets for keyphrase generation have a common characteristic that they are, by and large, made from scholarly documents (abstracts or full texts) paired with non-expert (mostly from authors) annotations.",
    "answer_b": "What are the differences?\n< Explanation: The dataset contains 10,000 news articles, each annotated with 100 keyphrases. The annotations are made by 100 editors, and the annotations are not publicly available. The editors' annotations are not aligned with the keyphrases in the dataset. The dataset contains 100 keyphrases, and the annotations are aligned with the keyphrases in the dataset. The annotations are aligned to the keyphrases in the dataset by the following rules: (1) the first keyphrase is always the most important one, (2) the second",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do the editors' annotations differ from those in existing datasets?\n\nGround Truth Answer:\n\n\nAnswer A:\nExisting datasets are annotated by non-experts who use a larger, less controlled indexed vocabulary lacking the domain expertise shown by the editors. Explanation: Frequently used datasets for keyphrase generation have a common characteristic that they are, by and large, made from scholarly documents (abstracts or full texts) paired with non-expert (mostly from authors) annotations.\n\nAnswer B:\nWhat are the differences?\n< Explanation: The dataset contains 10,000 news articles, each annotated with 100 keyphrases. The annotations are made by 100 editors, and the annotations are not publicly available. The editors' annotations are not aligned with the keyphrases in the dataset. The dataset contains 100 keyphrases, and the annotations are aligned with the keyphrases in the dataset. The annotations are aligned to the keyphrases in the dataset by the following rules: (1) the first keyphrase is always the most important one, (2) the second\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.09059",
    "question": "How many layers does their model have?",
    "gold_answer": "",
    "answer_a": "6. Explanation: The architecture of our model, depicted in Fig. FIGREF4 , consist of 6 layers which are a sigmoid layer (at the top), a fully-connected layer, a concatenation layer, a BLSTM layer, a cosine layer, and a sense and word embeddings layer (on the bottom).",
    "answer_b": "What are the number of hidden units in each layer?\n< Explanation: The model consists of two layers, each with 128 hidden units. The first layer is a bidirectional LSTM layer with 128 hidden units and the second layer is a bidirectional LSTM layer with 128 hidden units. The number of hidden units in each layer is 128. The model is trained with Adam optimizer with a learning rate of 0.0001 and a batch size of 128. The model is trained for 100 epochs. The model is evaluated on SensEval-3 benchmark",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many layers does their model have?\n\nGround Truth Answer:\n\n\nAnswer A:\n6. Explanation: The architecture of our model, depicted in Fig. FIGREF4 , consist of 6 layers which are a sigmoid layer (at the top), a fully-connected layer, a concatenation layer, a BLSTM layer, a cosine layer, and a sense and word embeddings layer (on the bottom).\n\nAnswer B:\nWhat are the number of hidden units in each layer?\n< Explanation: The model consists of two layers, each with 128 hidden units. The first layer is a bidirectional LSTM layer with 128 hidden units and the second layer is a bidirectional LSTM layer with 128 hidden units. The number of hidden units in each layer is 128. The model is trained with Adam optimizer with a learning rate of 0.0001 and a batch size of 128. The model is trained for 100 epochs. The model is evaluated on SensEval-3 benchmark\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.09059",
    "question": "What metrics do they use?",
    "gold_answer": "",
    "answer_a": "How do they evaluate the performance of their model?\n< Explanation: We use BLEU as our evaluation metric. BLEU is a widely used metric for evaluating the quality of translations. It is defined as follows: where $w$ is the word, $s$ is the sentence, $t$ is the translation, and $p$ is the gold translation. The higher the BLEU score, the better the translation. We use the following formula to calculate BLEU score: where $w$ is the word, $s$ is the sentence, $t$ is the translation, and $p$ is the gold translation. The",
    "answer_b": "F-measure. Explanation: By evaluating our one-model-fits-all WSD network over the public gold standard dataset of SensEval-3 BIBREF3 , we demonstrate that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat metrics do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate the performance of their model?\n< Explanation: We use BLEU as our evaluation metric. BLEU is a widely used metric for evaluating the quality of translations. It is defined as follows: where $w$ is the word, $s$ is the sentence, $t$ is the translation, and $p$ is the gold translation. The higher the BLEU score, the better the translation. We use the following formula to calculate BLEU score: where $w$ is the word, $s$ is the sentence, $t$ is the translation, and $p$ is the gold translation. The\n\nAnswer B:\nF-measure. Explanation: By evaluating our one-model-fits-all WSD network over the public gold standard dataset of SensEval-3 BIBREF3 , we demonstrate that the accuracy of our model in terms of F-measure is comparable with the state-of-the-art WSD algorithms'.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.09000",
    "question": "what dataset did they use?",
    "gold_answer": "",
    "answer_a": "DUC-2001 dataset BIBREF6. Explanation: The DUC-2001 dataset BIBREF6 , which is a collection of 308 news articles, is annotated by BIBREF7 .",
    "answer_b": "Explanation: We use the 20 Newsgroups dataset BIBREF11 . The dataset consists of 20000 documents, each containing 10000 words. The documents are labeled as either spam or non-spam. We use the first 10000 documents for training and the remaining 10000 documents for testing. The training set is used to train the model and the test set is used to evaluate the model. We use the same training and test set for all the models. We use the same training and test set for all the models. We use the same",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat dataset did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nDUC-2001 dataset BIBREF6. Explanation: The DUC-2001 dataset BIBREF6 , which is a collection of 308 news articles, is annotated by BIBREF7 .\n\nAnswer B:\nExplanation: We use the 20 Newsgroups dataset BIBREF11 . The dataset consists of 20000 documents, each containing 10000 words. The documents are labeled as either spam or non-spam. We use the first 10000 documents for training and the remaining 10000 documents for testing. The training set is used to train the model and the test set is used to evaluate the model. We use the same training and test set for all the models. We use the same training and test set for all the models. We use the same\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.09000",
    "question": "what was their model's f1 score?",
    "gold_answer": "",
    "answer_a": "On DUC 27.53, on Inspec 27.01, on ICSI 4.30, and on Nus 9.10. Explanation: FLOAT SELECTED: Table 2: The Result of our System as well as the Reimplementation of SingleRank and Topical PageRank on four Corpora",
    "answer_b": "Explanation: The results of the experiments are shown in Table 1. The results show that our method outperforms the state-of-the-art models by more than 2% in F1-score. The best results are obtained by the model with 100000000000000000000000000000000000000000000000000000000000000000000000000000",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat was their model's f1 score?\n\nGround Truth Answer:\n\n\nAnswer A:\nOn DUC 27.53, on Inspec 27.01, on ICSI 4.30, and on Nus 9.10. Explanation: FLOAT SELECTED: Table 2: The Result of our System as well as the Reimplementation of SingleRank and Topical PageRank on four Corpora\n\nAnswer B:\nExplanation: The results of the experiments are shown in Table 1. The results show that our method outperforms the state-of-the-art models by more than 2% in F1-score. The best results are obtained by the model with 100000000000000000000000000000000000000000000000000000000000000000000000000000\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.09000",
    "question": "what are the state of the art models?",
    "gold_answer": "",
    "answer_a": "What are the improvements?\n< Explanation: We compare our method with the following state-of-art models: (1) the baseline model: the model without background knowledge, (2) the baseline model with background knowledge: the model that uses the background knowledge to extract the keyphrases, (3) the baseline model with the background knowledge and the baseline model without the background knowledge: the model that combines the two models. We also compare our method with the following baselines: (1) the baseline model without background knowledge: the model that uses the background knowledge to extract the keyphrases, (2) the baseline model with background knowledge",
    "answer_b": "SingleRank and Topical PageRank. Explanation: For comparing with our system, we reimplemented SingleRank and Topical PageRank.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the state of the art models?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the improvements?\n< Explanation: We compare our method with the following state-of-art models: (1) the baseline model: the model without background knowledge, (2) the baseline model with background knowledge: the model that uses the background knowledge to extract the keyphrases, (3) the baseline model with the background knowledge and the baseline model without the background knowledge: the model that combines the two models. We also compare our method with the following baselines: (1) the baseline model without background knowledge: the model that uses the background knowledge to extract the keyphrases, (2) the baseline model with background knowledge\n\nAnswer B:\nSingleRank and Topical PageRank. Explanation: For comparing with our system, we reimplemented SingleRank and Topical PageRank.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.06708",
    "question": "How do you know the word alignments are correct?",
    "gold_answer": "",
    "answer_a": "What is the difference between the word alignments and the hidden states?\n< Explanation: The word alignments are computed by the attention mechanism. The attention mechanism is a special case of the attention mechanism in which the attention weights are computed by the attention mechanism. The attention mechanism is a special case of the attention mechanism in which the attention weights are computed by the attention mechanism. The attention mechanism is a special case of the attention mechanism in which the attention weights are computed by the attention mechanism. The attention mechanism is a special case of the attention mechanism in which the attention weights are computed by the attention mechanism. The attention mechanism is a special case of the attention",
    "answer_b": "we use the word alignment information from the ART model. Explanation: Therefore, we use the word alignment information from the ART model to help the training of the NART model.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do you know the word alignments are correct?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the word alignments and the hidden states?\n< Explanation: The word alignments are computed by the attention mechanism. The attention mechanism is a special case of the attention mechanism in which the attention weights are computed by the attention mechanism. The attention mechanism is a special case of the attention mechanism in which the attention weights are computed by the attention mechanism. The attention mechanism is a special case of the attention mechanism in which the attention weights are computed by the attention mechanism. The attention mechanism is a special case of the attention mechanism in which the attention weights are computed by the attention mechanism. The attention mechanism is a special case of the attention\n\nAnswer B:\nwe use the word alignment information from the ART model. Explanation: Therefore, we use the word alignment information from the ART model to help the training of the NART model.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.06708",
    "question": "How slow is the unparallelizable ART model in the first place?  ",
    "gold_answer": "",
    "answer_a": "What is the inference latency of the model?\n< Explanation: The unparallelizable ART model is trained with a fixed number of hidden states and word alignments. The number of hidden states is fixed at 1000 and the number of word alignments is fixed at 100. The inference latency of the model is measured by the time it takes to generate a single token. The inference latency of the model is measured by the time it takes to generate a single token. The inference latency of the model is measured by the time it takes to generate a single token. The inference latency of the model is measured by the time it takes",
    "answer_b": "784 miliseconds. Explanation: FLOAT SELECTED: Table 1: Performance on WMT14 En-De, De-En and IWSLT14 De-En tasks. \u201c/\u201d means non-reportable.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow slow is the unparallelizable ART model in the first place?  \n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the inference latency of the model?\n< Explanation: The unparallelizable ART model is trained with a fixed number of hidden states and word alignments. The number of hidden states is fixed at 1000 and the number of word alignments is fixed at 100. The inference latency of the model is measured by the time it takes to generate a single token. The inference latency of the model is measured by the time it takes to generate a single token. The inference latency of the model is measured by the time it takes to generate a single token. The inference latency of the model is measured by the time it takes\n\nAnswer B:\n784 miliseconds. Explanation: FLOAT SELECTED: Table 1: Performance on WMT14 En-De, De-En and IWSLT14 De-En tasks. \u201c/\u201d means non-reportable.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.06708",
    "question": "What metric is used to measure translation accuracy?",
    "gold_answer": "",
    "answer_a": "BLEU. Explanation: The evaluation is on two widely used public machine translation datasets: IWSLT14 German-to-English (De-En) BIBREF9, BIBREF1 and WMT14 English-to-German (En-De) dataset BIBREF4, BIBREF10. To compare with previous works, we also reverse WMT14 English-to-German dataset and obtain WMT14 German-to-English dataset.",
    "answer_b": "How is it computed?\n< Explanation: We use BLEU as the metric to measure translation accuracy. BLEU is a widely used metric for evaluating the quality of translations. It is defined as follows: where $w$ is the word, $s$ is the sentence, $t$ is the target sentence, and $a$ is the alignment between $w$ and $s$. The higher the BLEU score, the better the translation. The BLEU score is computed by averaging the scores of all possible alignments. The alignment is computed by the following formula: where $a$ is the alignment between $w$ and $s",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat metric is used to measure translation accuracy?\n\nGround Truth Answer:\n\n\nAnswer A:\nBLEU. Explanation: The evaluation is on two widely used public machine translation datasets: IWSLT14 German-to-English (De-En) BIBREF9, BIBREF1 and WMT14 English-to-German (En-De) dataset BIBREF4, BIBREF10. To compare with previous works, we also reverse WMT14 English-to-German dataset and obtain WMT14 German-to-English dataset.\n\nAnswer B:\nHow is it computed?\n< Explanation: We use BLEU as the metric to measure translation accuracy. BLEU is a widely used metric for evaluating the quality of translations. It is defined as follows: where $w$ is the word, $s$ is the sentence, $t$ is the target sentence, and $a$ is the alignment between $w$ and $s$. The higher the BLEU score, the better the translation. The BLEU score is computed by averaging the scores of all possible alignments. The alignment is computed by the following formula: where $a$ is the alignment between $w$ and $s\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1705.10754",
    "question": "What dicrimating features are discovered?",
    "gold_answer": "",
    "answer_a": "Highest gain is obtained by average, maximum, minimum, and standard deviation. Probability and proportionality features have low information gain. Explanation: FLOAT SELECTED: Table 8. Features sorted by information gain.",
    "answer_b": "How are they selected?\n< Explanation: We use the following features: 1) the number of words in the text, 2) the number of unique words, 3) the number of unique words in the text, 4) the number of unique words in the text, 5) the number of unique words in the text, 6) the number of unique words in the text, 7) the number of unique words in the text, 8) the number of unique words in the text, 9) the number of unique words in the text, 10) the number of unique words in",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat dicrimating features are discovered?\n\nGround Truth Answer:\n\n\nAnswer A:\nHighest gain is obtained by average, maximum, minimum, and standard deviation. Probability and proportionality features have low information gain. Explanation: FLOAT SELECTED: Table 8. Features sorted by information gain.\n\nAnswer B:\nHow are they selected?\n< Explanation: We use the following features: 1) the number of words in the text, 2) the number of unique words, 3) the number of unique words in the text, 4) the number of unique words in the text, 5) the number of unique words in the text, 6) the number of unique words in the text, 7) the number of unique words in the text, 8) the number of unique words in the text, 9) the number of unique words in the text, 10) the number of unique words in\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1705.10754",
    "question": "What results are obtained on the alternate datasets?",
    "gold_answer": "",
    "answer_a": "How do they compare to the results on the original dataset?\n< Explanation: We compare our LDR model with the following models: (i) a standard LDA model (LDA) (ii) a standard LDA model with a Gaussian kernel (Gaussian LDA) (iii) a standard LDA model with a linear kernel (Linear LDA) (iv) a standard LDA model with a polynomial kernel (Polynomial LDA) (v) a standard LDA model with a radial basis function (RBF) kernel (RBF LDA) (vi) a standard LDA model with a sigmoid kernel (",
    "answer_b": "Accuracy results range from 74.4 to 100. Explanation: FLOAT SELECTED: Table 9. Accuracy results in the development set of the DSLCC. The significance is marked in bold when some representation obtains significantly better results than the next best performing representation (e.g. results for SenVec in Portugal Portuguese are significantly higher than LDR, which at the same time are significantly higher than Skip-gram).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat results are obtained on the alternate datasets?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they compare to the results on the original dataset?\n< Explanation: We compare our LDR model with the following models: (i) a standard LDA model (LDA) (ii) a standard LDA model with a Gaussian kernel (Gaussian LDA) (iii) a standard LDA model with a linear kernel (Linear LDA) (iv) a standard LDA model with a polynomial kernel (Polynomial LDA) (v) a standard LDA model with a radial basis function (RBF) kernel (RBF LDA) (vi) a standard LDA model with a sigmoid kernel (\n\nAnswer B:\nAccuracy results range from 74.4 to 100. Explanation: FLOAT SELECTED: Table 9. Accuracy results in the development set of the DSLCC. The significance is marked in bold when some representation obtains significantly better results than the next best performing representation (e.g. results for SenVec in Portugal Portuguese are significantly higher than LDR, which at the same time are significantly higher than Skip-gram).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.05190",
    "question": "What is the machine learning method used to make the predictions?",
    "gold_answer": "",
    "answer_a": "SGNN. Explanation: As SGNN proposed by BIBREF21 (BIBREF21) achieved state-of-the-art performances for this task, we use the framework of SGNN, and only replace their input event embeddings with our intent and sentiment-enhanced event embeddings.",
    "answer_b": "What is the dataset used?\n< Explanation: We use the following three datasets to train our models: (1) the event similarity dataset, which consists of 1000 pairs of event pairs, each pair is a pair of event names and their corresponding event descriptions. The event names are extracted from the Wikipedia page of the event, and the event descriptions are extracted from the Wikipedia page of the event description. The event names and descriptions are used to create event pairs. The event pairs are then used to train our model. (2) the stock market dataset, which consists of 1000 pairs of stock market pairs,",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the machine learning method used to make the predictions?\n\nGround Truth Answer:\n\n\nAnswer A:\nSGNN. Explanation: As SGNN proposed by BIBREF21 (BIBREF21) achieved state-of-the-art performances for this task, we use the framework of SGNN, and only replace their input event embeddings with our intent and sentiment-enhanced event embeddings.\n\nAnswer B:\nWhat is the dataset used?\n< Explanation: We use the following three datasets to train our models: (1) the event similarity dataset, which consists of 1000 pairs of event pairs, each pair is a pair of event names and their corresponding event descriptions. The event names are extracted from the Wikipedia page of the event, and the event descriptions are extracted from the Wikipedia page of the event description. The event names and descriptions are used to create event pairs. The event pairs are then used to train our model. (2) the stock market dataset, which consists of 1000 pairs of stock market pairs,\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.05190",
    "question": "How is the event prediction task evaluated?",
    "gold_answer": "",
    "answer_a": "accuracy. Explanation: BIBREF22 (BIBREF22) and BIBREF21 (BIBREF21) showed that script event prediction is a challenging problem, and even 1% of accuracy improvement is very difficult.",
    "answer_b": "What is the evaluation metric?\n< Explanation: We evaluate the performance of our model on the event similarity task and stock market prediction task. The evaluation metrics are the F1 score and the accuracy, respectively. The evaluation metrics are calculated by the following formula:\n\nwhere $p$ is the predicted event and $q$ is the true event. The accuracy is calculated by the following formula:\n\nwhere $p$ is the predicted event and $q$ is the true event. The F1 score is calculated by the following formula:\n\nwhere $p$ is the predicted event and $q$ is the true event. The accuracy and F1 score",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is the event prediction task evaluated?\n\nGround Truth Answer:\n\n\nAnswer A:\naccuracy. Explanation: BIBREF22 (BIBREF22) and BIBREF21 (BIBREF21) showed that script event prediction is a challenging problem, and even 1% of accuracy improvement is very difficult.\n\nAnswer B:\nWhat is the evaluation metric?\n< Explanation: We evaluate the performance of our model on the event similarity task and stock market prediction task. The evaluation metrics are the F1 score and the accuracy, respectively. The evaluation metrics are calculated by the following formula:\n\nwhere $p$ is the predicted event and $q$ is the true event. The accuracy is calculated by the following formula:\n\nwhere $p$ is the predicted event and $q$ is the true event. The F1 score is calculated by the following formula:\n\nwhere $p$ is the predicted event and $q$ is the true event. The accuracy and F1 score\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.05190",
    "question": "What are the datasets used in the paper?",
    "gold_answer": "",
    "answer_a": "How are they used?\n< Explanation: We use the following datasets: (1) the event corpus, which is the first event corpus with 100k events and 10k pairs of events, and the event pairs are generated by the event generation algorithm. The event pairs are used to train the event embeddings. The event corpus is available at https://github.com/zhengyuanliu/EventCorpus. (2) the commonsense corpus, which is the second event corpus with 100k events and 10k pairs of events. The event pairs are generated by the commonsense generation",
    "answer_b": "ATOMIC. Explanation: We also use ATOMIC BIBREF7 as the event sentiment labeled dataset.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the datasets used in the paper?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they used?\n< Explanation: We use the following datasets: (1) the event corpus, which is the first event corpus with 100k events and 10k pairs of events, and the event pairs are generated by the event generation algorithm. The event pairs are used to train the event embeddings. The event corpus is available at https://github.com/zhengyuanliu/EventCorpus. (2) the commonsense corpus, which is the second event corpus with 100k events and 10k pairs of events. The event pairs are generated by the commonsense generation\n\nAnswer B:\nATOMIC. Explanation: We also use ATOMIC BIBREF7 as the event sentiment labeled dataset.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1606.02601",
    "question": "Is there a difference between the model's performance for morphologically impoverished and morphologically complex languages?",
    "gold_answer": "",
    "answer_a": "How does this affect the performance of the model?\n<blockquote>\n<font size=\"2\">Yes, we find that the performance of the model is better for morphologically complex languages. This is because morphologically complex languages have more morphemes, and the model is able to learn more context-sensitive word embeddings. </font>\n<font size=\"2\">Yes, we find that the performance of the model is better for morphologically impoverished languages. This is because morphologically impoverished languages have fewer morphemes, and the model is able to learn more context-sensitive word embeddings. </font>\n<font size=\"2\">Yes, we find that",
    "answer_b": "They did not report results for English but expect that morphologically complex languages will perform better.. Explanation: We only report results for English.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nIs there a difference between the model's performance for morphologically impoverished and morphologically complex languages?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow does this affect the performance of the model?\n<blockquote>\n<font size=\"2\">Yes, we find that the performance of the model is better for morphologically complex languages. This is because morphologically complex languages have more morphemes, and the model is able to learn more context-sensitive word embeddings. </font>\n<font size=\"2\">Yes, we find that the performance of the model is better for morphologically impoverished languages. This is because morphologically impoverished languages have fewer morphemes, and the model is able to learn more context-sensitive word embeddings. </font>\n<font size=\"2\">Yes, we find that\n\nAnswer B:\nThey did not report results for English but expect that morphologically complex languages will perform better.. Explanation: We only report results for English.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1606.02601",
    "question": "What languages do they apply the model to?",
    "gold_answer": "",
    "answer_a": "English. Explanation: We only report results for English.",
    "answer_b": "How do they evaluate the model?\n< Explanation: We apply our model to English, Chinese, and Japanese. We use the same character-level model as in our previous work [10] , which consists of a character-level embedding layer followed by a character-level word embedding layer. We use the same character-level word embedding layer as in our previous work [10] , which consists of a character-level word embedding layer followed by a character-level word embedding layer. We use the same character-level word embedding layer as in our previous work [10] , which consists of a character-level word embedding layer followed by a character-level word embedding",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat languages do they apply the model to?\n\nGround Truth Answer:\n\n\nAnswer A:\nEnglish. Explanation: We only report results for English.\n\nAnswer B:\nHow do they evaluate the model?\n< Explanation: We apply our model to English, Chinese, and Japanese. We use the same character-level model as in our previous work [10] , which consists of a character-level embedding layer followed by a character-level word embedding layer. We use the same character-level word embedding layer as in our previous work [10] , which consists of a character-level word embedding layer followed by a character-level word embedding layer. We use the same character-level word embedding layer as in our previous work [10] , which consists of a character-level word embedding layer followed by a character-level word embedding\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1606.02601",
    "question": "How are the embeddings evaluated in the human judgement comparison?",
    "gold_answer": "",
    "answer_a": "human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments. Explanation: For these datasets, human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments BIBREF20 .",
    "answer_b": "What is the human judgement metric?\n< Explanation: We use the Word2Vec model to generate embeddings for the words in the test set. We use the same test set for both the word embedding and morphological analysis. We use the same test set for both the word embedding and morphological analysis. We use the same test set for both the word embedding and morphological analysis. We use the same test set for both the word embedding and morphological analysis. We use the same test set for both the word embedding and morphological analysis. We use the same test set for both the word embedding and morphological analysis. We use the same",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are the embeddings evaluated in the human judgement comparison?\n\nGround Truth Answer:\n\n\nAnswer A:\nhuman annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments. Explanation: For these datasets, human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments BIBREF20 .\n\nAnswer B:\nWhat is the human judgement metric?\n< Explanation: We use the Word2Vec model to generate embeddings for the words in the test set. We use the same test set for both the word embedding and morphological analysis. We use the same test set for both the word embedding and morphological analysis. We use the same test set for both the word embedding and morphological analysis. We use the same test set for both the word embedding and morphological analysis. We use the same test set for both the word embedding and morphological analysis. We use the same test set for both the word embedding and morphological analysis. We use the same\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1602.04341",
    "question": "what was the margin their system outperformed previous ones?",
    "gold_answer": "",
    "answer_a": "15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500. Explanation: Concretely, we propose HABCNN, a hierarchical attention-based convolutional neural network, to address this task in two roadmaps.",
    "answer_b": "Explanation: We compare our model with the following baselines: (1) BERT (2) BERT+ (3) BERT+ (4) BERT+ (5) BERT+ (6) BERT+ (7) BERT+ (8) BERT+ (9) BERT+ (10) BERT+ (11) BERT+ (12) BERT+ (13) BERT+ (14) BERT+ (15) BERT+ (16) BERT+ (17) BERT+ (18) BERT+",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat was the margin their system outperformed previous ones?\n\nGround Truth Answer:\n\n\nAnswer A:\n15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500. Explanation: Concretely, we propose HABCNN, a hierarchical attention-based convolutional neural network, to address this task in two roadmaps.\n\nAnswer B:\nExplanation: We compare our model with the following baselines: (1) BERT (2) BERT+ (3) BERT+ (4) BERT+ (5) BERT+ (6) BERT+ (7) BERT+ (8) BERT+ (9) BERT+ (10) BERT+ (11) BERT+ (12) BERT+ (13) BERT+ (14) BERT+ (15) BERT+ (16) BERT+ (17) BERT+ (18) BERT+\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1602.04341",
    "question": "what prior approaches did they compare to?",
    "gold_answer": "",
    "answer_a": "What is the baseline?\n<sep>baseline: BERT. Explanation: We compare our model with BERT [1] and the baseline model, which is a simple attention-based model. BERT is a state-of-the-art transformer-based model that has been widely used in NLP tasks. BERT is trained on a large corpus of text and is trained to learn a representation of the text. BERT is trained on a large corpus of text and is trained to learn a representation of the text. BERT is trained on a large corpus of text and is trained to learn a representation of the text. BERT is trained on a",
    "answer_b": "Addition. Explanation: This work focuses on the comparison with systems about distributed representation learning and deep learning:\n\nAddition. Directly compare question and answers without considering the D.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat prior approaches did they compare to?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the baseline?\n<sep>baseline: BERT. Explanation: We compare our model with BERT [1] and the baseline model, which is a simple attention-based model. BERT is a state-of-the-art transformer-based model that has been widely used in NLP tasks. BERT is trained on a large corpus of text and is trained to learn a representation of the text. BERT is trained on a large corpus of text and is trained to learn a representation of the text. BERT is trained on a large corpus of text and is trained to learn a representation of the text. BERT is trained on a\n\nAnswer B:\nAddition. Explanation: This work focuses on the comparison with systems about distributed representation learning and deep learning:\n\nAddition. Directly compare question and answers without considering the D.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.02284",
    "question": "what are the baselines?",
    "gold_answer": "",
    "answer_a": "one-stage RNN system containing 2-layer BLSTM. Explanation: The baseline we use for comparison is a one-stage RNN system, the RNN structure is the same as the last stage containing 2-layer BLSTM and directly trained to recognize dialect category.",
    "answer_b": "What are the results?\nThe baseline system is a three-stage system, which consists of two stages: acoustic model (AM) and RNN. The AM is trained by using CTCS to recognize the given phonetic sequence annotation and then trained by using the intermediate features as inputs from the AM. The RNN is trained by using the intermediate features as inputs from the AM. The baseline system is evaluated by using the following metrics: accuracy, F1-score, and precision. The results are shown in Table 1. The results show that the two-stage system can achieve high accuracy for Chinese dialects recognition under both short utterance and long",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the baselines?\n\nGround Truth Answer:\n\n\nAnswer A:\none-stage RNN system containing 2-layer BLSTM. Explanation: The baseline we use for comparison is a one-stage RNN system, the RNN structure is the same as the last stage containing 2-layer BLSTM and directly trained to recognize dialect category.\n\nAnswer B:\nWhat are the results?\nThe baseline system is a three-stage system, which consists of two stages: acoustic model (AM) and RNN. The AM is trained by using CTCS to recognize the given phonetic sequence annotation and then trained by using the intermediate features as inputs from the AM. The RNN is trained by using the intermediate features as inputs from the AM. The baseline system is evaluated by using the following metrics: accuracy, F1-score, and precision. The results are shown in Table 1. The results show that the two-stage system can achieve high accuracy for Chinese dialects recognition under both short utterance and long\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.02284",
    "question": "what results do they achieve?",
    "gold_answer": "",
    "answer_a": "relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task relative to the baseline. Explanation: From the results in the Table TABREF20 , we can see that the relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task relative to the baseline and the two-stage system performs best. We also observe that both two multi-stage systems perform excellently in long duration ( INLINEFORM0 3s) task and the two-stage system illustrates its advantageous and robustness in short duration ( INLINEFORM1 3s) task.",
    "answer_b": "How do they compare to the previous system?\n< Explanation: The two-stage system achieves 99.6% accuracy on the short utterance test set and 99.7% accuracy on the long utterance test set. The previous system achieves 99.6% accuracy on the short utterance test set and 99.8% accuracy on the long utterance test set. The previous system also achieves 99.6% accuracy on the short utterance test set and 99.7% accuracy on the long utterance test set. The previous system also achieves 99.6% accuracy",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat results do they achieve?\n\nGround Truth Answer:\n\n\nAnswer A:\nrelative accuracy (ACC) of the two multi-stage systems increases by 10% on every task relative to the baseline. Explanation: From the results in the Table TABREF20 , we can see that the relative accuracy (ACC) of the two multi-stage systems increases by 10% on every task relative to the baseline and the two-stage system performs best. We also observe that both two multi-stage systems perform excellently in long duration ( INLINEFORM0 3s) task and the two-stage system illustrates its advantageous and robustness in short duration ( INLINEFORM1 3s) task.\n\nAnswer B:\nHow do they compare to the previous system?\n< Explanation: The two-stage system achieves 99.6% accuracy on the short utterance test set and 99.7% accuracy on the long utterance test set. The previous system achieves 99.6% accuracy on the short utterance test set and 99.8% accuracy on the long utterance test set. The previous system also achieves 99.6% accuracy on the short utterance test set and 99.7% accuracy on the long utterance test set. The previous system also achieves 99.6% accuracy\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.02284",
    "question": "what chinese dialects are explored?",
    "gold_answer": "",
    "answer_a": "Explanation: The two-stage system is applied to the Xunfei (iFlyTek) Chinese Dialect Recognition Challenge 2018. The dataset consists of 1000 utterances from 10 different dialects. The dataset is divided into training set (800 utterances) and test set (200 utterances). The training set is used to train the AM and the test set is used to evaluate the performance of the system. The results of the two-stage system are compared with the three-stage system. The results show that the two-stage system can achieve high accuracy for Chinese dialects recognition",
    "answer_b": "Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian. Explanation: We use a database covering 10 most widespread Chinese dialects, the dialects are Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat chinese dialects are explored?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The two-stage system is applied to the Xunfei (iFlyTek) Chinese Dialect Recognition Challenge 2018. The dataset consists of 1000 utterances from 10 different dialects. The dataset is divided into training set (800 utterances) and test set (200 utterances). The training set is used to train the AM and the test set is used to evaluate the performance of the system. The results of the two-stage system are compared with the three-stage system. The results show that the two-stage system can achieve high accuracy for Chinese dialects recognition\n\nAnswer B:\nNingxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian. Explanation: We use a database covering 10 most widespread Chinese dialects, the dialects are Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka and Fujian.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.00168",
    "question": "Which neural machine translation model was used?",
    "gold_answer": "",
    "answer_a": "What is the size of the model?\n< Explanation: The neural machine translation model used is a standard Transformer model with a single layer of attention. The size of the model is 128 hidden units. The model is trained with a combination of back-translation and checkpoint averaging. The checkpoint averaging is done by averaging the weights of the model over the last 100 epochs. The training is done with a batch size of 128 and a learning rate of 0.001. The model is trained for 100 epochs. The model is trained with a batch size of 128 and",
    "answer_b": "SGNMT tool BIBREF13 , BIBREF14 with OpenFST backend BIBREF8. Explanation: For decoding, we use our SGNMT tool BIBREF13 , BIBREF14 with OpenFST backend BIBREF8 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich neural machine translation model was used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the model?\n< Explanation: The neural machine translation model used is a standard Transformer model with a single layer of attention. The size of the model is 128 hidden units. The model is trained with a combination of back-translation and checkpoint averaging. The checkpoint averaging is done by averaging the weights of the model over the last 100 epochs. The training is done with a batch size of 128 and a learning rate of 0.001. The model is trained for 100 epochs. The model is trained with a batch size of 128 and\n\nAnswer B:\nSGNMT tool BIBREF13 , BIBREF14 with OpenFST backend BIBREF8. Explanation: For decoding, we use our SGNMT tool BIBREF13 , BIBREF14 with OpenFST backend BIBREF8 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.00168",
    "question": "What are the restrictions of the restricted track?",
    "gold_answer": "",
    "answer_a": "explore the potential of purely neural models for grammatical error correction. Explanation: Our goal on the restricted track was to explore the potential of purely neural models for grammatical error correction.",
    "answer_b": "How is the system trained?\n<sep><br></br>The restricted track is restricted to a set of 1000 sentences, each of which is 100 words long. The sentences are randomly selected from the corpus of the Cambridge University Corpus. The sentences are then used to train a neural machine translation model. The neural machine translation model is trained using the following training procedure: (1) the neural machine translation model is trained using the following training procedure: (1) the neural machine translation model is trained using the following training procedure: (1) the neural machine translation model is trained using the following training procedure:",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the restrictions of the restricted track?\n\nGround Truth Answer:\n\n\nAnswer A:\nexplore the potential of purely neural models for grammatical error correction. Explanation: Our goal on the restricted track was to explore the potential of purely neural models for grammatical error correction.\n\nAnswer B:\nHow is the system trained?\n<sep><br></br>The restricted track is restricted to a set of 1000 sentences, each of which is 100 words long. The sentences are randomly selected from the corpus of the Cambridge University Corpus. The sentences are then used to train a neural machine translation model. The neural machine translation model is trained using the following training procedure: (1) the neural machine translation model is trained using the following training procedure: (1) the neural machine translation model is trained using the following training procedure: (1) the neural machine translation model is trained using the following training procedure:\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.09709",
    "question": "Which works better according to human evaluation, the concurrent or the modular system?",
    "gold_answer": "",
    "answer_a": "Why?\nThe concurrent system is better. The concurrent system is better because it is more transparent and interpretable. The concurrent system is more transparent and interpretable because it uses a BERT-based classifier to identify problematic words and a novel join embedding through which the classifier can edit the hidden states of the encoder. The concurrent system is more transparent and interpretable because it uses a BERT-based classifier to identify problematic words and a novel join embedding through which the classifier can edit the hidden states of the encoder. The concurrent system is more transparent and interpretable because it uses a BERT-based classifier to identify problematic words and a novel join embedding through which",
    "answer_b": "Modular. Explanation: FLOAT SELECTED: Table 8: Performance of various bias detectors. Rows with asterisks are statistically different than the preceding row.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich works better according to human evaluation, the concurrent or the modular system?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhy?\nThe concurrent system is better. The concurrent system is better because it is more transparent and interpretable. The concurrent system is more transparent and interpretable because it uses a BERT-based classifier to identify problematic words and a novel join embedding through which the classifier can edit the hidden states of the encoder. The concurrent system is more transparent and interpretable because it uses a BERT-based classifier to identify problematic words and a novel join embedding through which the classifier can edit the hidden states of the encoder. The concurrent system is more transparent and interpretable because it uses a BERT-based classifier to identify problematic words and a novel join embedding through which\n\nAnswer B:\nModular. Explanation: FLOAT SELECTED: Table 8: Performance of various bias detectors. Rows with asterisks are statistically different than the preceding row.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.09709",
    "question": "Were the Wikipedia edits that removed framings, presuppositions and attitudes from biased sentences a Wiki community effort, or were annotators trained to do it?",
    "gold_answer": "",
    "answer_a": "Wiki community effort. Explanation: The Wiki Neutrality Corpus consists of aligned sentences pre and post-neutralization by English Wikipedia editors (Table TABREF3).",
    "answer_b": "How did they do it?\n< Explanation: We first collected 180,000 sentences from Wikipedia edits that removed various framings, presuppositions, and attitudes from biased sentences. We then manually annotated these sentences with the following criteria: (1) whether the sentence was a neutralization, (2) whether the sentence was a neutralization of a specific framing, (3) whether the sentence was a neutralization of a specific presupposition, and (4) whether the sentence was a neutralization of a specific attitude. We then used these annotations to create a corpus of 180,000 sentences that",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWere the Wikipedia edits that removed framings, presuppositions and attitudes from biased sentences a Wiki community effort, or were annotators trained to do it?\n\nGround Truth Answer:\n\n\nAnswer A:\nWiki community effort. Explanation: The Wiki Neutrality Corpus consists of aligned sentences pre and post-neutralization by English Wikipedia editors (Table TABREF3).\n\nAnswer B:\nHow did they do it?\n< Explanation: We first collected 180,000 sentences from Wikipedia edits that removed various framings, presuppositions, and attitudes from biased sentences. We then manually annotated these sentences with the following criteria: (1) whether the sentence was a neutralization, (2) whether the sentence was a neutralization of a specific framing, (3) whether the sentence was a neutralization of a specific presupposition, and (4) whether the sentence was a neutralization of a specific attitude. We then used these annotations to create a corpus of 180,000 sentences that\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.09709",
    "question": "How is subjective text automatically neutralized?",
    "gold_answer": "",
    "answer_a": "What is the baseline?\n< Explanation: We first introduce a novel testbed for natural language generation: automatically bringing inappropriately subjective text into a neutral point of view (\u201cneutralizing\u201d biased text). We also offer the first parallel corpus of biased language. The corpus contains 180,000 sentence pairs and originates from Wikipedia edits that removed various framings, presuppositions, and attitudes from biased sentences. We then introduce two strong encoder-decoder baselines for the task. A straightforward yet opaque CONCURRENT system uses a BERT encoder to identify subjective words as part of the generation process. An interpretable and controllable MOD",
    "answer_b": "Identify subjective bias via the method of BIBREF2: using Wikipedia's neutral point of view (NPOV) policy and suggest edits that would make it more neutral.. Explanation: This work presents data and algorithms for automatically reducing bias in text. We focus on a particular kind of bias: inappropriate subjectivity (\u201csubjective bias\u201d). Subjective bias occurs when language that should be neutral and fair is skewed by feeling, opinion, or taste (whether consciously or unconsciously). In practice, we identify subjective bias via the method of BIBREF2: using Wikipedia's neutral point of view (NPOV) policy. This policy is a set of principles which includes \u201cavoiding stating opinions as facts\u201d and \u201cpreferring nonjudgemental language\u201d.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is subjective text automatically neutralized?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the baseline?\n< Explanation: We first introduce a novel testbed for natural language generation: automatically bringing inappropriately subjective text into a neutral point of view (\u201cneutralizing\u201d biased text). We also offer the first parallel corpus of biased language. The corpus contains 180,000 sentence pairs and originates from Wikipedia edits that removed various framings, presuppositions, and attitudes from biased sentences. We then introduce two strong encoder-decoder baselines for the task. A straightforward yet opaque CONCURRENT system uses a BERT encoder to identify subjective words as part of the generation process. An interpretable and controllable MOD\n\nAnswer B:\nIdentify subjective bias via the method of BIBREF2: using Wikipedia's neutral point of view (NPOV) policy and suggest edits that would make it more neutral.. Explanation: This work presents data and algorithms for automatically reducing bias in text. We focus on a particular kind of bias: inappropriate subjectivity (\u201csubjective bias\u201d). Subjective bias occurs when language that should be neutral and fair is skewed by feeling, opinion, or taste (whether consciously or unconsciously). In practice, we identify subjective bias via the method of BIBREF2: using Wikipedia's neutral point of view (NPOV) policy. This policy is a set of principles which includes \u201cavoiding stating opinions as facts\u201d and \u201cpreferring nonjudgemental language\u201d.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.11232",
    "question": "What is the sign language recognition task investigated?",
    "gold_answer": "",
    "answer_a": "How is it evaluated?\n< Explanation: We used the ASL dataset of 12 users and 13107 samples across 51 signs. The dataset was collected over 6 months and it will be publicly released in the hope of spurring further machine learning research towards providing improved accessibility for digital assistants. The dataset was collected from the ASL Research Center at the University of California, Los Angeles (UCLA) and the University of Washington. The dataset was collected from the ASL Research Center at the University of California, Los Angeles (UCLA) and the University of Washington. The dataset was collected from the AS",
    "answer_b": "We present a deep learning based approach for ASL recognition that leverages skeletal and video data. The proposed model captures the underlying temporal dynamics associated with sign language and also identifies specific hand shape patterns from video data to improve recognition performance.. Explanation: An automatic sign language recognizer enables an ASL user to translate the sign language to written text or speech, allowing them to communicate with people who are not familiar with ASL.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the sign language recognition task investigated?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it evaluated?\n< Explanation: We used the ASL dataset of 12 users and 13107 samples across 51 signs. The dataset was collected over 6 months and it will be publicly released in the hope of spurring further machine learning research towards providing improved accessibility for digital assistants. The dataset was collected from the ASL Research Center at the University of California, Los Angeles (UCLA) and the University of Washington. The dataset was collected from the ASL Research Center at the University of California, Los Angeles (UCLA) and the University of Washington. The dataset was collected from the AS\n\nAnswer B:\nWe present a deep learning based approach for ASL recognition that leverages skeletal and video data. The proposed model captures the underlying temporal dynamics associated with sign language and also identifies specific hand shape patterns from video data to improve recognition performance.. Explanation: An automatic sign language recognizer enables an ASL user to translate the sign language to written text or speech, allowing them to communicate with people who are not familiar with ASL.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.11232",
    "question": "What is the performance of the best model in the sign language recognition task?",
    "gold_answer": "",
    "answer_a": "Spatial AI-LSTM. Explanation: FLOAT SELECTED: TABLE I AVERAGE CROSS SUBJECT (CS) ACCURACY ACROSS ALL TEST SUBJECTS FOR DIFFERENT PROPOSED ARCHITECTURES AND BASELINES. STANDARD DEVIATION ACROSS TEST SUBJECTS\u2019 ACCURACY IS ALSO SHOWN.",
    "answer_b": "What is the performance of the best model in the sign language recognition task?\n< Explanation: The best model achieved 98.6% accuracy on the test set. The best model achieved 98.6% accuracy on the test set. The best model achieved 98.6% accuracy on the test set. The best model achieved 98.6% accuracy on the test set. The best model achieved 98.6% accuracy on the test set. The best model achieved 98.6% accuracy on the test set. The best model achieved 98.6% accuracy on the test",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the performance of the best model in the sign language recognition task?\n\nGround Truth Answer:\n\n\nAnswer A:\nSpatial AI-LSTM. Explanation: FLOAT SELECTED: TABLE I AVERAGE CROSS SUBJECT (CS) ACCURACY ACROSS ALL TEST SUBJECTS FOR DIFFERENT PROPOSED ARCHITECTURES AND BASELINES. STANDARD DEVIATION ACROSS TEST SUBJECTS\u2019 ACCURACY IS ALSO SHOWN.\n\nAnswer B:\nWhat is the performance of the best model in the sign language recognition task?\n< Explanation: The best model achieved 98.6% accuracy on the test set. The best model achieved 98.6% accuracy on the test set. The best model achieved 98.6% accuracy on the test set. The best model achieved 98.6% accuracy on the test set. The best model achieved 98.6% accuracy on the test set. The best model achieved 98.6% accuracy on the test set. The best model achieved 98.6% accuracy on the test\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.11232",
    "question": "What are the deep learning architectures used?",
    "gold_answer": "",
    "answer_a": "How are they evaluated?\n< Explanation: We use the following deep learning architectures: (1) Convolutional Neural Networks (CNN) (2) Recurrent Neural Networks (RNN) (3) Long Short-Term Memory (LSTM) (4) Bidirectional Long Short-Term Memory (BiLSTM) (5) Transformer (6) Transformer-X (7) Transformer-XL (8) Transformer-XL-CL (9) Transformer-XL-CL-CL (10) Transformer-XL-CL-CL-CL (11) Transformer-XL-CL-CL-CL-CL (1",
    "answer_b": "Axis Independent Architecture (AI-LSTM). Explanation: Inspired by the success of deep learning approaches in computer vision BIBREF20, we applied different deep learning architectures to model sign languages from both input modes (RGB and skeletal).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the deep learning architectures used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they evaluated?\n< Explanation: We use the following deep learning architectures: (1) Convolutional Neural Networks (CNN) (2) Recurrent Neural Networks (RNN) (3) Long Short-Term Memory (LSTM) (4) Bidirectional Long Short-Term Memory (BiLSTM) (5) Transformer (6) Transformer-X (7) Transformer-XL (8) Transformer-XL-CL (9) Transformer-XL-CL-CL (10) Transformer-XL-CL-CL-CL (11) Transformer-XL-CL-CL-CL-CL (1\n\nAnswer B:\nAxis Independent Architecture (AI-LSTM). Explanation: Inspired by the success of deep learning approaches in computer vision BIBREF20, we applied different deep learning architectures to model sign languages from both input modes (RGB and skeletal).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.09180",
    "question": "Who made the stated claim (that \"this is because character-level models learn morphology\")?",
    "gold_answer": "",
    "answer_a": "What is the empirical evidence for this claim?\nThe claim is that character-level models learn morphology. We show that this is not the case. We show that character-level models are not able to learn morphology, and that they are not able to learn morphology in a way that is consistent with the claim. We show that explicit morphological analysis is not necessary for character-level models to learn morphology. We show that explicit morphological analysis is necessary for character-level models to learn morphology, and that it is necessary for character-level models to learn morphology in a way that is consistent with the claim. We show that explicit morphological analysis is not necessary for",
    "answer_b": "Chung et al. (2016). Explanation: BIBREF0 propose that \u201cprior information regarding morphology ... among others, should be incorporated\u201d into character-level models, while BIBREF6 counter that it is \u201cunnecessary to consider these prior information\u201d when modeling characters.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWho made the stated claim (that \"this is because character-level models learn morphology\")?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the empirical evidence for this claim?\nThe claim is that character-level models learn morphology. We show that this is not the case. We show that character-level models are not able to learn morphology, and that they are not able to learn morphology in a way that is consistent with the claim. We show that explicit morphological analysis is not necessary for character-level models to learn morphology. We show that explicit morphological analysis is necessary for character-level models to learn morphology, and that it is necessary for character-level models to learn morphology in a way that is consistent with the claim. We show that explicit morphological analysis is not necessary for\n\nAnswer B:\nChung et al. (2016). Explanation: BIBREF0 propose that \u201cprior information regarding morphology ... among others, should be incorporated\u201d into character-level models, while BIBREF6 counter that it is \u201cunnecessary to consider these prior information\u201d when modeling characters.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.09180",
    "question": "Which languages do they use?",
    "gold_answer": "",
    "answer_a": "Finnish, Turkish, Czech, English, German, Hindi, Portuguese, Russian, Spanish, Urdu, Arabic, and Hebrew. Explanation: FLOAT SELECTED: Table 1: Training data statistics. Languages are grouped by their dominant morphological processes, from top to bottom: agglutinative, fusional, and root & pattern.",
    "answer_b": "Explanation: We use the following languages: English, French, German, Spanish, Italian, Portuguese, Arabic, Chinese, Japanese, Korean, and Arabic. We use the following morphological typologies: English, French, German, Spanish, Italian, Portuguese, Arabic, Chinese, Japanese, Korean, and Arabic. We use the following languages: English, French, German, Spanish, Italian, Portuguese, Arabic, Chinese, Japanese, Korean, and Arabic. We use the following languages: English, French, German, Spanish, Italian, Portuguese, Arabic, Chinese, Japanese, Korean, and Arabic. We use the following languages: English",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich languages do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nFinnish, Turkish, Czech, English, German, Hindi, Portuguese, Russian, Spanish, Urdu, Arabic, and Hebrew. Explanation: FLOAT SELECTED: Table 1: Training data statistics. Languages are grouped by their dominant morphological processes, from top to bottom: agglutinative, fusional, and root & pattern.\n\nAnswer B:\nExplanation: We use the following languages: English, French, German, Spanish, Italian, Portuguese, Arabic, Chinese, Japanese, Korean, and Arabic. We use the following morphological typologies: English, French, German, Spanish, Italian, Portuguese, Arabic, Chinese, Japanese, Korean, and Arabic. We use the following languages: English, French, German, Spanish, Italian, Portuguese, Arabic, Chinese, Japanese, Korean, and Arabic. We use the following languages: English, French, German, Spanish, Italian, Portuguese, Arabic, Chinese, Japanese, Korean, and Arabic. We use the following languages: English\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.09180",
    "question": "What is case syncretism?",
    "gold_answer": "",
    "answer_a": "How does it affect the performance of the model?\n< Explanation: Case syncretism is the phenomenon where words with different forms of the same morpheme are used in the same sentence. For example, in English, the word \"dog\" can be used to refer to a dog, a dog, or a dog. In the context of morphological analysis, this is often used to distinguish between the different forms of the morpheme \"dog\". In the case of English, the word \"dog\" is often analyzed as a single morpheme, but in other languages, such as Spanish, the word \"dog\" can be analyzed as two",
    "answer_b": "A situation in which a noun's syntactic function is ambiguous without context.. Explanation: Specifically, it struggles with case syncretism, in which noun case\u2014and thus syntactic function\u2014is ambiguous.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is case syncretism?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow does it affect the performance of the model?\n< Explanation: Case syncretism is the phenomenon where words with different forms of the same morpheme are used in the same sentence. For example, in English, the word \"dog\" can be used to refer to a dog, a dog, or a dog. In the context of morphological analysis, this is often used to distinguish between the different forms of the morpheme \"dog\". In the case of English, the word \"dog\" is often analyzed as a single morpheme, but in other languages, such as Spanish, the word \"dog\" can be analyzed as two\n\nAnswer B:\nA situation in which a noun's syntactic function is ambiguous without context.. Explanation: Specifically, it struggles with case syncretism, in which noun case\u2014and thus syntactic function\u2014is ambiguous.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.04251",
    "question": "What models are used to generate responses?",
    "gold_answer": "",
    "answer_a": "How are they evaluated?\n< Explanation: We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses",
    "answer_b": "Seq2Seq. Explanation: or generative hate speech intervention, we evaluated the following three methods.\n\nSeq2Seq BIBREF25, BIBREF24: The encoder consists of 2 bidirectional GRU layers. The decoder consists of 2 GRU layers followed by a 3-layer MLP (Multi-Layer Perceptron).\n\nVariational Auto-Encoder (VAE) BIBREF26: The structure of the VAE model is similar to that of the Seq2Seq model, except that it has two independent linear layers followed by the encoder to calculate the mean and variance of the distribution of the latent variable separately. We assume the latent variable follows a multivariate Gaussian Distribution. KL annealing BIBREF27 is applied during training.\n\nReinforcement Learning (RL): We also implement the Reinforcement Learning method described in Section SECREF5. The backbone of this model is the Seq2Seq model, which follows the same Seq2Seq network structure described above. This network is used to parameterize the probability of a response given the conversation. Besides this backbone Seq2Seq model, another Seq2Seq model is used to generate the backward probability. This network is trained in a similar way as the backbone Seq2Seq model, but with a response as input and the corresponding conversation as the target.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat models are used to generate responses?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they evaluated?\n< Explanation: We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses. We use a simple neural network model to generate responses\n\nAnswer B:\nSeq2Seq. Explanation: or generative hate speech intervention, we evaluated the following three methods.\n\nSeq2Seq BIBREF25, BIBREF24: The encoder consists of 2 bidirectional GRU layers. The decoder consists of 2 GRU layers followed by a 3-layer MLP (Multi-Layer Perceptron).\n\nVariational Auto-Encoder (VAE) BIBREF26: The structure of the VAE model is similar to that of the Seq2Seq model, except that it has two independent linear layers followed by the encoder to calculate the mean and variance of the distribution of the latent variable separately. We assume the latent variable follows a multivariate Gaussian Distribution. KL annealing BIBREF27 is applied during training.\n\nReinforcement Learning (RL): We also implement the Reinforcement Learning method described in Section SECREF5. The backbone of this model is the Seq2Seq model, which follows the same Seq2Seq network structure described above. This network is used to parameterize the probability of a response given the conversation. Besides this backbone Seq2Seq model, another Seq2Seq model is used to generate the backward probability. This network is trained in a similar way as the backbone Seq2Seq model, but with a response as input and the corresponding conversation as the target.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.04251",
    "question": "What types of hate speech are considered?",
    "gold_answer": "",
    "answer_a": "How are they classified?\n< Explanation: The hate speech dataset contains 1000 hate speech instances, each of which is labeled as either hate speech or neutral. The hate speech labels are divided into three categories: hate speech, neutral, and no hate speech. The hate speech labels are obtained from the Hate Speech Dataset (HSD) BIBREF10 . The neutral labels are obtained from the Neutral Dataset (ND) BIBREF11 . The no hate speech labels are obtained from the No Hate Speech Dataset (NHS) BIBREF12 . The hate speech labels are further divided into 10 categories",
    "answer_b": "Potentially hateful comments are identified using hate keywords.. Explanation: To further focus on conversations with hate speech in each submission, we use hate keywords BIBREF6 to identify potentially hateful comments and then reconstructed the conversational context of each comment. This context consists of all comments preceding and following a potentially hateful comment. Thus for each potentially hateful comment, we rebuild the conversation where the comment appears.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat types of hate speech are considered?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they classified?\n< Explanation: The hate speech dataset contains 1000 hate speech instances, each of which is labeled as either hate speech or neutral. The hate speech labels are divided into three categories: hate speech, neutral, and no hate speech. The hate speech labels are obtained from the Hate Speech Dataset (HSD) BIBREF10 . The neutral labels are obtained from the Neutral Dataset (ND) BIBREF11 . The no hate speech labels are obtained from the No Hate Speech Dataset (NHS) BIBREF12 . The hate speech labels are further divided into 10 categories\n\nAnswer B:\nPotentially hateful comments are identified using hate keywords.. Explanation: To further focus on conversations with hate speech in each submission, we use hate keywords BIBREF6 to identify potentially hateful comments and then reconstructed the conversational context of each comment. This context consists of all comments preceding and following a potentially hateful comment. Thus for each potentially hateful comment, we rebuild the conversation where the comment appears.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.11980",
    "question": "Which baselines to they compare to?",
    "gold_answer": "",
    "answer_a": "What are their results?\n< Explanation: We compare our proposed sentence compression method with the following baselines: (1) the Transformer-based encoder with self-attention mechanism (2) the Transformer-based encoder with self-attention mechanism and the Transformer-based decoder with self-attention mechanism (3) the Transformer-based encoder with self-attention mechanism and the Transformer-based decoder with self-attention mechanism and (4) the Transformer-based encoder with self-attention mechanism and the Transformer-based decoder with self-attention mechanism and the Transformer-based decoder with self-attention mechanism. The results are shown in Table 1. We can see that the proposed sentence compression",
    "answer_b": "AllText. Explanation: To evaluate the quality of our sentence compression model, we used the Annotated Gigaword corpus BIBREF21 as the benchmark BIBREF22.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich baselines to they compare to?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are their results?\n< Explanation: We compare our proposed sentence compression method with the following baselines: (1) the Transformer-based encoder with self-attention mechanism (2) the Transformer-based encoder with self-attention mechanism and the Transformer-based decoder with self-attention mechanism (3) the Transformer-based encoder with self-attention mechanism and the Transformer-based decoder with self-attention mechanism and (4) the Transformer-based encoder with self-attention mechanism and the Transformer-based decoder with self-attention mechanism and the Transformer-based decoder with self-attention mechanism. The results are shown in Table 1. We can see that the proposed sentence compression\n\nAnswer B:\nAllText. Explanation: To evaluate the quality of our sentence compression model, we used the Annotated Gigaword corpus BIBREF21 as the benchmark BIBREF22.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.11980",
    "question": "Which sentence compression technique works best?",
    "gold_answer": "",
    "answer_a": "How does it work?\n< Explanation: We compare the three sentence compression techniques, including the backbone source-side fusion, target-side fusion, and both-side fusion, to the standard encoder-decoder framework. The backbone source-side fusion is to use the source sentence representation as the input to the encoder. The target-side fusion is to use the target sentence representation as the input to the encoder. The both-side fusion is to use the source and target sentence representations as the input to the encoder. We use the sentence compression rate as the evaluation metric. The results are shown in Table 1. The results show that the sentence compression rate of the backbone",
    "answer_b": "ESC model. Explanation: To evaluate the quality of our sentence compression model, we conducted a horizontal comparison between the proposed sentence compression model and other sentence compression models in different settings. Table TABREF34 shows the comparison results. We observed that the proposed unsupervised ESC model performed substantially better than Fevry and BIBREF17's unsupervised method. The proposed supervised ESC model also substantially outperformed the RNN-based Seq2seq and BIBREF11's baseline method. That is, our supervised model gave +2.0 improvements on R-1, R-2, and R-L scores over the RNN-based Seq2seq. This means that the proposed Transformer-based approaches can generate compressed sentences of high quality.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich sentence compression technique works best?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow does it work?\n< Explanation: We compare the three sentence compression techniques, including the backbone source-side fusion, target-side fusion, and both-side fusion, to the standard encoder-decoder framework. The backbone source-side fusion is to use the source sentence representation as the input to the encoder. The target-side fusion is to use the target sentence representation as the input to the encoder. The both-side fusion is to use the source and target sentence representations as the input to the encoder. We use the sentence compression rate as the evaluation metric. The results are shown in Table 1. The results show that the sentence compression rate of the backbone\n\nAnswer B:\nESC model. Explanation: To evaluate the quality of our sentence compression model, we conducted a horizontal comparison between the proposed sentence compression model and other sentence compression models in different settings. Table TABREF34 shows the comparison results. We observed that the proposed unsupervised ESC model performed substantially better than Fevry and BIBREF17's unsupervised method. The proposed supervised ESC model also substantially outperformed the RNN-based Seq2seq and BIBREF11's baseline method. That is, our supervised model gave +2.0 improvements on R-1, R-2, and R-L scores over the RNN-based Seq2seq. This means that the proposed Transformer-based approaches can generate compressed sentences of high quality.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.04060",
    "question": "What is the performance of large state-of-the-art models on these datasets?",
    "gold_answer": "",
    "answer_a": "Average  92.87 for CoNLL-01 and Average 8922 for Ontonotes 5. Explanation: FLOAT SELECTED: Table 2: Results on CoNLL-03 and OntoNotes 5.",
    "answer_b": "How does GazSelfAttn compare to them?\n< Explanation: We compare our model to the state-of-the-art models on CoNLL-03 and Ontonotes 5 datasets. The results are shown in Table TABREF11 . We observe that our model achieves the best performance on both datasets, with a F1 score of 92.86 and 92.34 respectively. We also observe that our model outperforms the state-of-the-art models on both datasets, with a F1 score of 92.86 and 92.34 respectively. We also observe that",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the performance of large state-of-the-art models on these datasets?\n\nGround Truth Answer:\n\n\nAnswer A:\nAverage  92.87 for CoNLL-01 and Average 8922 for Ontonotes 5. Explanation: FLOAT SELECTED: Table 2: Results on CoNLL-03 and OntoNotes 5.\n\nAnswer B:\nHow does GazSelfAttn compare to them?\n< Explanation: We compare our model to the state-of-the-art models on CoNLL-03 and Ontonotes 5 datasets. The results are shown in Table TABREF11 . We observe that our model achieves the best performance on both datasets, with a F1 score of 92.86 and 92.34 respectively. We also observe that our model outperforms the state-of-the-art models on both datasets, with a F1 score of 92.86 and 92.34 respectively. We also observe that\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.04060",
    "question": "What is used as a baseline model?",
    "gold_answer": "",
    "answer_a": "Neural CRF model with and without ELMo embeddings. Explanation: We experiment with the Neural CRF model with and without ELMo embeddings.",
    "answer_b": "How is it evaluated?\n< Explanation: We use the baseline model from the paper \u201cGazetteer Embeddings for Named Entity Recognition\u201d (GazSelfAttn) as the baseline model. The baseline model is trained on the CoNLL-03 dataset with the following hyperparameters: batch size = 100, learning rate = 0.001, number of epochs = 100, and number of workers = 4. The baseline model is trained on the CoNLL-03 dataset with the following hyperparameters: batch size = 100, learning rate = 0",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is used as a baseline model?\n\nGround Truth Answer:\n\n\nAnswer A:\nNeural CRF model with and without ELMo embeddings. Explanation: We experiment with the Neural CRF model with and without ELMo embeddings.\n\nAnswer B:\nHow is it evaluated?\n< Explanation: We use the baseline model from the paper \u201cGazetteer Embeddings for Named Entity Recognition\u201d (GazSelfAttn) as the baseline model. The baseline model is trained on the CoNLL-03 dataset with the following hyperparameters: batch size = 100, learning rate = 0.001, number of epochs = 100, and number of workers = 4. The baseline model is trained on the CoNLL-03 dataset with the following hyperparameters: batch size = 100, learning rate = 0\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.04060",
    "question": "How do they build gazetter resources from Wikipedia knowlege base?",
    "gold_answer": "",
    "answer_a": "What are the resources?\nThere are two main steps to building gazetter resources from Wikipedia knowledge base: (1) creating a gazetteer embedding model and (2) creating a gazetteer resource. The gazetteer embedding model is a simple word embedding model that learns a gazetteer embedding for each word in the corpus. The gazetteer resource is a set of gazetteer embeddings for each word in the corpus. The gazetteer embedding model is trained on a gazetteer resource. The gazetteer embedding model is trained on a gazetteer resource and the gazetteer resource is trained on a gazette",
    "answer_b": "process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet. Explanation: To extract gazetteers from Wikidata, we process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet, example resulting tuples are Boston $\\rightarrow $ City and Massachusetts $\\rightarrow $ State. Each entity is associated with a set of aliases, we keep only the aliases that are less than seven tokens long.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they build gazetter resources from Wikipedia knowlege base?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the resources?\nThere are two main steps to building gazetter resources from Wikipedia knowledge base: (1) creating a gazetteer embedding model and (2) creating a gazetteer resource. The gazetteer embedding model is a simple word embedding model that learns a gazetteer embedding for each word in the corpus. The gazetteer resource is a set of gazetteer embeddings for each word in the corpus. The gazetteer embedding model is trained on a gazetteer resource. The gazetteer embedding model is trained on a gazetteer resource and the gazetteer resource is trained on a gazette\n\nAnswer B:\nprocess the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet. Explanation: To extract gazetteers from Wikidata, we process the official dumps into tuples of entity and type based only on the left and right part of the instance_of triplet, example resulting tuples are Boston $\\rightarrow $ City and Massachusetts $\\rightarrow $ State. Each entity is associated with a set of aliases, we keep only the aliases that are less than seven tokens long.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1807.08089",
    "question": "What is the dataset that is used to train the embeddings?",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n< Explanation: We use the dataset of 10000 utterances from the 2016-2017 National Mandarin Chinese Corpus (NMC 2016-2017) BIBREF10. The dataset consists of 10000 utterances, each of which is 10000 words long. The utterances are divided into 10000 segments, each of which is 1000 words long. The segments are used to train the embeddings. The dataset is divided into training and",
    "answer_b": "LibriSpeech BIBREF46. Explanation: We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the \u201cclean\" and \u201cothers\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the dataset that is used to train the embeddings?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n< Explanation: We use the dataset of 10000 utterances from the 2016-2017 National Mandarin Chinese Corpus (NMC 2016-2017) BIBREF10. The dataset consists of 10000 utterances, each of which is 10000 words long. The utterances are divided into 10000 segments, each of which is 1000 words long. The segments are used to train the embeddings. The dataset is divided into training and\n\nAnswer B:\nLibriSpeech BIBREF46. Explanation: We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the \u201cclean\" and \u201cothers\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1807.08089",
    "question": "What speaker characteristics are used?",
    "gold_answer": "",
    "answer_a": "How are they disentangled?\n< Explanation: We use the following three features to disentangle the speaker characteristics: (1) the speaker's age, (2) the speaker's gender, and (3) the speaker's accent. The age is the age of the speaker, the gender is the gender of the speaker, and the accent is the speaker's accent. The age is the age of the speaker, the gender is the gender of the speaker, and the accent is the speaker's accent. The age is the age of the speaker, the gender is the gender of the speaker, and the accent is the speaker's",
    "answer_b": "speaker characteristics. Explanation: A text word with a given phonetic structure corresponds to infinite number of audio signals with varying acoustic factors such as speaker characteristics, microphone characteristics, background noise, etc. All the latter acoustic factors are jointly referred to as speaker characteristics here for simplicity, which obviously disturbs the goal of phonetic-and-semantic embedding.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat speaker characteristics are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they disentangled?\n< Explanation: We use the following three features to disentangle the speaker characteristics: (1) the speaker's age, (2) the speaker's gender, and (3) the speaker's accent. The age is the age of the speaker, the gender is the gender of the speaker, and the accent is the speaker's accent. The age is the age of the speaker, the gender is the gender of the speaker, and the accent is the speaker's accent. The age is the age of the speaker, the gender is the gender of the speaker, and the accent is the speaker's\n\nAnswer B:\nspeaker characteristics. Explanation: A text word with a given phonetic structure corresponds to infinite number of audio signals with varying acoustic factors such as speaker characteristics, microphone characteristics, background noise, etc. All the latter acoustic factors are jointly referred to as speaker characteristics here for simplicity, which obviously disturbs the goal of phonetic-and-semantic embedding.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1807.08089",
    "question": "What language is used for the experiments?",
    "gold_answer": "",
    "answer_a": "How is the dataset generated?\n< Explanation: We use English as the target language. The dataset is generated from the 2016-2017 National English Corpus (NEC) BIBREF1 . The dataset consists of 10,000 utterances, each of which is 100 seconds long. Each utterance is a sequence of words, and each word is a sequence of characters. The dataset is divided into training, validation and test sets. The training set is 80%, the validation set is 10%, and the test set is 10%. The training set",
    "answer_b": "English. Explanation: We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat language is used for the experiments?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is the dataset generated?\n< Explanation: We use English as the target language. The dataset is generated from the 2016-2017 National English Corpus (NEC) BIBREF1 . The dataset consists of 10,000 utterances, each of which is 100 seconds long. Each utterance is a sequence of words, and each word is a sequence of characters. The dataset is divided into training, validation and test sets. The training set is 80%, the validation set is 10%, and the test set is 10%. The training set\n\nAnswer B:\nEnglish. Explanation: We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1703.05320",
    "question": "what is the baseline model",
    "gold_answer": "",
    "answer_a": "two baseline models TF-IDF and LSI which only use Cosine similarity. Explanation: We compared our method to two baseline models TF-IDF and LSI which only use Cosine similarity to retrieve the relevant articles.",
    "answer_b": "?\n<sep><solution>Baseline model: We use the following baseline model for the legal information retrieval task: a simple bag of words model (BoW) with 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the baseline model\n\nGround Truth Answer:\n\n\nAnswer A:\ntwo baseline models TF-IDF and LSI which only use Cosine similarity. Explanation: We compared our method to two baseline models TF-IDF and LSI which only use Cosine similarity to retrieve the relevant articles.\n\nAnswer B:\n?\n<sep><solution>Baseline model: We use the following baseline model for the legal information retrieval task: a simple bag of words model (BoW) with 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1703.05320",
    "question": "What contribute to improve the accuracy on legal question answering task?",
    "gold_answer": "",
    "answer_a": "Explanation: The proposed model is trained on the training set and tested on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance",
    "answer_b": "Adding more features to the traditional sets such as TF-IDF, BM25 and PL2F as well as using voting in a ranking system help to improve accuracy on a legal question answering task. Explanation: In order to build a legal IR, traditional models such as TF-IDF, BM25 or PL2F can be used to generate basic features for matching documents with a query. Nevertheless, to improve not only the accuracy but also the robustness of ranking function, it is essential to take into account a combination of fundamental features and other potential features.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat contribute to improve the accuracy on legal question answering task?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The proposed model is trained on the training set and tested on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance of the model is evaluated on the test set. The performance\n\nAnswer B:\nAdding more features to the traditional sets such as TF-IDF, BM25 and PL2F as well as using voting in a ranking system help to improve accuracy on a legal question answering task. Explanation: In order to build a legal IR, traditional models such as TF-IDF, BM25 or PL2F can be used to generate basic features for matching documents with a query. Nevertheless, to improve not only the accuracy but also the robustness of ranking function, it is essential to take into account a combination of fundamental features and other potential features.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.10762",
    "question": "What sizes were their datasets?",
    "gold_answer": "",
    "answer_a": "ast-20h: 20 hours,\nzh-ai-small: 20 hours,\nzh-ai-large: 150 hours,\nzh-ai-hanzi: 150 hours,\nhr-gp: 12 hours,\nsv-gp: 18 hours,\npl-gp: 19 hours,\npt-gp: 23 hours,\nfr-gp: 25 hours,\nzh-gp: 26 hours,\ncs-gp: 27 hours,\nmultilin6: 124 hours. Explanation: FLOAT SELECTED: Table 1: Dataset statistics (left); dev set results from ASR pretraining and from the final AST system (right). AST results in all rows except the first are from pretraining using the dataset listed in that row, followed by fine-tuning using ast-20h. Numbers in brackets are the improvement over the baseline.",
    "answer_b": "How many samples were in each?\n< Explanation: We use the following datasets: (1) ASR data from the ASR dataset of the University of California, Berkeley (UCB) BIBREF1 , which contains 10,000 utterances of 100 languages, and (2) a new dataset of 10,000 utterances of 10 languages, which we call the ASR+AST dataset. The ASR+AST dataset is a subset of the ASR dataset, and contains 10,000 utterances of 10 languages, but with",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat sizes were their datasets?\n\nGround Truth Answer:\n\n\nAnswer A:\nast-20h: 20 hours,\nzh-ai-small: 20 hours,\nzh-ai-large: 150 hours,\nzh-ai-hanzi: 150 hours,\nhr-gp: 12 hours,\nsv-gp: 18 hours,\npl-gp: 19 hours,\npt-gp: 23 hours,\nfr-gp: 25 hours,\nzh-gp: 26 hours,\ncs-gp: 27 hours,\nmultilin6: 124 hours. Explanation: FLOAT SELECTED: Table 1: Dataset statistics (left); dev set results from ASR pretraining and from the final AST system (right). AST results in all rows except the first are from pretraining using the dataset listed in that row, followed by fine-tuning using ast-20h. Numbers in brackets are the improvement over the baseline.\n\nAnswer B:\nHow many samples were in each?\n< Explanation: We use the following datasets: (1) ASR data from the ASR dataset of the University of California, Berkeley (UCB) BIBREF1 , which contains 10,000 utterances of 100 languages, and (2) a new dataset of 10,000 utterances of 10 languages, which we call the ASR+AST dataset. The ASR+AST dataset is a subset of the ASR dataset, and contains 10,000 utterances of 10 languages, but with\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.10762",
    "question": "How many layers does their model have?",
    "gold_answer": "",
    "answer_a": "What is the size of the ASR data?\n< Explanation: The model consists of a word embedding layer, a convolutional layer, a fully connected layer, and a softmax layer. The word embedding layer is a 100-dimensional vector, the convolutional layer is a 100-dimensional vector, and the fully connected layer is a 100-dimensional vector. The softmax layer is a 100-dimensional vector. The ASR data consists of 1000 words, and the AST data consists of 1000 words. The model has 1000 hidden units in the fully",
    "answer_b": "10. Explanation: Following the architecture and training procedure described in BIBREF4, input speech features are fed into a stack of two CNN layers. In each CNN layer we stride the input with a factor of 2 along time, apply ReLU activation BIBREF19 followed by batch normalization BIBREF20.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many layers does their model have?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the ASR data?\n< Explanation: The model consists of a word embedding layer, a convolutional layer, a fully connected layer, and a softmax layer. The word embedding layer is a 100-dimensional vector, the convolutional layer is a 100-dimensional vector, and the fully connected layer is a 100-dimensional vector. The softmax layer is a 100-dimensional vector. The ASR data consists of 1000 words, and the AST data consists of 1000 words. The model has 1000 hidden units in the fully\n\nAnswer B:\n10. Explanation: Following the architecture and training procedure described in BIBREF4, input speech features are fed into a stack of two CNN layers. In each CNN layer we stride the input with a factor of 2 along time, apply ReLU activation BIBREF19 followed by batch normalization BIBREF20.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.10762",
    "question": "What is their model's architecture?",
    "gold_answer": "",
    "answer_a": "What is the size of the ASR data set?\n< Explanation: We use a simple RNN model with a single hidden layer, where the input is a sequence of words, and the output is a sequence of words. We use a 128 hidden units RNN cell, and a dropout layer with a dropout probability of 0.5. We use a 128 hidden units RNN cell, and a dropout layer with a dropout probability of 0.5. We use a 128 hidden units RNN cell, and a dropout layer with a dropout probability of 0.5. We use",
    "answer_b": "the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2. Explanation: For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure FIGREF1: the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2. Details of the architecture and training parameters are described in Section SECREF9.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is their model's architecture?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the ASR data set?\n< Explanation: We use a simple RNN model with a single hidden layer, where the input is a sequence of words, and the output is a sequence of words. We use a 128 hidden units RNN cell, and a dropout layer with a dropout probability of 0.5. We use a 128 hidden units RNN cell, and a dropout layer with a dropout probability of 0.5. We use a 128 hidden units RNN cell, and a dropout layer with a dropout probability of 0.5. We use\n\nAnswer B:\nthe encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2. Explanation: For both ASR and AST tasks we use the same end-to-end system architecture shown in Figure FIGREF1: the encoder-decoder model from BIBREF4, which itself is adapted from BIBREF1, BIBREF3 and BIBREF2. Details of the architecture and training parameters are described in Section SECREF9.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.10762",
    "question": "What languages did they use?",
    "gold_answer": "",
    "answer_a": "How were they related?\n< Explanation: We use the following languages: English, French, German, Spanish, Italian, Portuguese, and Russian. We use the same ASR data for all languages, and the same AST data for all languages. We use the same pretraining data for all languages, and the same data augmentation data for all languages. We use the same model architecture for all languages, and the same number of layers for all languages. We use the same number of epochs for all languages, and the same learning rate for all languages. We use the same number of training examples for all languages, and the same number of validation",
    "answer_b": "Spanish. Explanation: To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. BIBREF4, but pretrain the encoder using a number of different ASR datasets: the 150-hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat languages did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow were they related?\n< Explanation: We use the following languages: English, French, German, Spanish, Italian, Portuguese, and Russian. We use the same ASR data for all languages, and the same AST data for all languages. We use the same pretraining data for all languages, and the same data augmentation data for all languages. We use the same model architecture for all languages, and the same number of layers for all languages. We use the same number of epochs for all languages, and the same learning rate for all languages. We use the same number of training examples for all languages, and the same number of validation\n\nAnswer B:\nSpanish. Explanation: To answer these questions, we use the same AST architecture and Spanish-English parallel data as Bansal et al. BIBREF4, but pretrain the encoder using a number of different ASR datasets: the 150-hour AISHELL corpus of Chinese as well as seven GlobalPhone languages, each with about 20 hours of data.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1711.01567",
    "question": "Are there experiments with real data?",
    "gold_answer": "",
    "answer_a": "If so, what are they?\n< Explanation: We conducted experiments on real data, using the same dataset as in the previous work (BIBREF10). We used the same model architecture and hyperparameters as in the previous work. We also used the same training data and hyperparameters. We used the same model architecture and hyperparameters as in the previous work. We used the same training data and hyperparameters. We used the same model architecture and hyperparameters as in the previous work. We used the same training data and hyperparameters. We used the same model architecture and hyperparameters as in the previous work. We used the same",
    "answer_b": "Yes. They show results on the Wall Street Journal Corpus, which consists of recordings of real speech.. Explanation: Section \"RESULTS\" shows results on the Wall Street Journal (WSJ) dataset with simulated far-field effects.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nAre there experiments with real data?\n\nGround Truth Answer:\n\n\nAnswer A:\nIf so, what are they?\n< Explanation: We conducted experiments on real data, using the same dataset as in the previous work (BIBREF10). We used the same model architecture and hyperparameters as in the previous work. We also used the same training data and hyperparameters. We used the same model architecture and hyperparameters as in the previous work. We used the same training data and hyperparameters. We used the same model architecture and hyperparameters as in the previous work. We used the same training data and hyperparameters. We used the same model architecture and hyperparameters as in the previous work. We used the same\n\nAnswer B:\nYes. They show results on the Wall Street Journal Corpus, which consists of recordings of real speech.. Explanation: Section \"RESULTS\" shows results on the Wall Street Journal (WSJ) dataset with simulated far-field effects.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1706.04206",
    "question": "What supervised machine learning models do they use?",
    "gold_answer": "",
    "answer_a": "ZeroR, Na\u00efve Bayes, J48, and random forest classifiers. Explanation: ZeroR, Na\u00efve Bayes, J48, and random forest classifiers were applied in our project.",
    "answer_b": "What are their results?\n< Explanation: We use a combination of supervised learning and unsupervised learning to classify sentences as to whether they express conditions and actions. We use a supervised learning model to classify sentences as to whether they express conditions and actions. We use a supervised learning model to classify sentences as to whether they express conditions and actions. We use a supervised learning model to classify sentences as to whether they express conditions and actions. We use a supervised learning model to classify sentences as to whether they express conditions and actions. We use a supervised learning model to classify sentences as to whether they express conditions and actions. We use a supervised learning model",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat supervised machine learning models do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nZeroR, Na\u00efve Bayes, J48, and random forest classifiers. Explanation: ZeroR, Na\u00efve Bayes, J48, and random forest classifiers were applied in our project.\n\nAnswer B:\nWhat are their results?\n< Explanation: We use a combination of supervised learning and unsupervised learning to classify sentences as to whether they express conditions and actions. We use a supervised learning model to classify sentences as to whether they express conditions and actions. We use a supervised learning model to classify sentences as to whether they express conditions and actions. We use a supervised learning model to classify sentences as to whether they express conditions and actions. We use a supervised learning model to classify sentences as to whether they express conditions and actions. We use a supervised learning model to classify sentences as to whether they express conditions and actions. We use a supervised learning model\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1706.04206",
    "question": "How large is the released data set?",
    "gold_answer": "",
    "answer_a": "1470 sentences. Explanation: FLOAT SELECTED: Table 2: Statistical information about annotated guidelines",
    "answer_b": "What is the size of the annotated dataset?\n< Explanation: We release two datasets of clinical guidelines, one containing 10,000 sentences and the other containing 10,000 sentences. The first dataset contains 10,000 sentences from the 2017 National Institute of Health (NIH) Clinical Practice Guidelines for the Diagnosis and Management of 100 Common Diseases and Conditions. The second dataset contains 10,000 sentences from the 2017 National Institute of Health Clinical Practice Guidelines for the Diagnosis and Management of 100 Common Diseases and Conditions",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow large is the released data set?\n\nGround Truth Answer:\n\n\nAnswer A:\n1470 sentences. Explanation: FLOAT SELECTED: Table 2: Statistical information about annotated guidelines\n\nAnswer B:\nWhat is the size of the annotated dataset?\n< Explanation: We release two datasets of clinical guidelines, one containing 10,000 sentences and the other containing 10,000 sentences. The first dataset contains 10,000 sentences from the 2017 National Institute of Health (NIH) Clinical Practice Guidelines for the Diagnosis and Management of 100 Common Diseases and Conditions. The second dataset contains 10,000 sentences from the 2017 National Institute of Health Clinical Practice Guidelines for the Diagnosis and Management of 100 Common Diseases and Conditions\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1706.04206",
    "question": "What is an example of a condition-action pair?",
    "gold_answer": "",
    "answer_a": "Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation. Explanation: \"Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation\u201d",
    "answer_b": "Explanation: The first example is a sentence from the guideline that describes a patient\u2019s condition and action. The second example is a sentence that describes a patient\u2019s condition and action. The third example is a sentence that describes a patient\u2019s condition and action. The fourth example is a sentence that describes a patient\u2019s condition and action. The fifth example is a sentence that describes a patient\u2019s condition and action. The sixth example is a sentence that describes a patient\u2019s condition and action. The seventh example is a sentence that describes a patient\u2019s condition and action. The eighth example is a sentence that describes a patient\u2019s condition and action. The ninth example",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is an example of a condition-action pair?\n\nGround Truth Answer:\n\n\nAnswer A:\nConditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation. Explanation: \"Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation\u201d\n\nAnswer B:\nExplanation: The first example is a sentence from the guideline that describes a patient\u2019s condition and action. The second example is a sentence that describes a patient\u2019s condition and action. The third example is a sentence that describes a patient\u2019s condition and action. The fourth example is a sentence that describes a patient\u2019s condition and action. The fifth example is a sentence that describes a patient\u2019s condition and action. The sixth example is a sentence that describes a patient\u2019s condition and action. The seventh example is a sentence that describes a patient\u2019s condition and action. The eighth example is a sentence that describes a patient\u2019s condition and action. The ninth example\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.06875",
    "question": "Which metrics were considered?",
    "gold_answer": "",
    "answer_a": "ter. Explanation: We consider frequently used metrics, including ter BIBREF18 , bleu BIBREF0 , rouge BIBREF19 , nist BIBREF20 , lepor BIBREF21 , cider BIBREF22 , and meteor BIBREF23 .",
    "answer_b": "What are the results?\n< Explanation: We consider the following metrics: BLEU, BLEU-4, BLEU-5, BLEU-6, BLEU-7, BLEU-8, BLEU-9, BLEU-10, BLEU-11, BLEU-12, BLEU-13, BLEU-14, BLEU-15, BLEU-16, BLEU-17, BLEU-18, BLEU-19, BLEU-20, BLEU-21, BLEU-22, BLEU-",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich metrics were considered?\n\nGround Truth Answer:\n\n\nAnswer A:\nter. Explanation: We consider frequently used metrics, including ter BIBREF18 , bleu BIBREF0 , rouge BIBREF19 , nist BIBREF20 , lepor BIBREF21 , cider BIBREF22 , and meteor BIBREF23 .\n\nAnswer B:\nWhat are the results?\n< Explanation: We consider the following metrics: BLEU, BLEU-4, BLEU-5, BLEU-6, BLEU-7, BLEU-8, BLEU-9, BLEU-10, BLEU-11, BLEU-12, BLEU-13, BLEU-14, BLEU-15, BLEU-16, BLEU-17, BLEU-18, BLEU-19, BLEU-20, BLEU-21, BLEU-22, BLEU-\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.06875",
    "question": "What NLG tasks were considered?",
    "gold_answer": "",
    "answer_a": "What metrics were used?\n< Explanation: We consider three NLG tasks: (1) text-to-text generation, (2) text-to-speech generation, and (3) text-to-speech synthesis. We use the following metrics: (1) BLEU, (2) BLEU-4, (3) BLEU-5, (4) BLEU-6, (5) BLEU-7, (6) BLEU-8, (7) BLEU-9, (8) BLEU-10, (9) BLEU-11, (10) BLEU-1",
    "answer_b": "provide information about hotels and restaurants in San Francisco. Explanation: $\\bullet $ SFHotel & SFRest BIBREF10 provide information about hotels and restaurants in San Francisco. There are 8 system dialogue act types, such as inform, confirm, goodbye etc.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat NLG tasks were considered?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat metrics were used?\n< Explanation: We consider three NLG tasks: (1) text-to-text generation, (2) text-to-speech generation, and (3) text-to-speech synthesis. We use the following metrics: (1) BLEU, (2) BLEU-4, (3) BLEU-5, (4) BLEU-6, (5) BLEU-7, (6) BLEU-8, (7) BLEU-9, (8) BLEU-10, (9) BLEU-11, (10) BLEU-1\n\nAnswer B:\nprovide information about hotels and restaurants in San Francisco. Explanation: $\\bullet $ SFHotel & SFRest BIBREF10 provide information about hotels and restaurants in San Francisco. There are 8 system dialogue act types, such as inform, confirm, goodbye etc.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.07690",
    "question": "what state of the art methods are compared to?",
    "gold_answer": "",
    "answer_a": "Explanation: We compare our method to the following baselines: (1) the original centroid-based model (2) the greedy algorithm (3) the greedy algorithm with a small number of sentences (4) the greedy algorithm with a large number of sentences (5) the greedy algorithm with a small number of sentences and a large number of sentences (6) the greedy algorithm with a small number of sentences and a small number of sentences (7) the greedy algorithm with a large number of sentences and a small number of sentences (8) the greedy algorithm with a large number of sentences and a large number of sentences (9) the greedy algorithm",
    "answer_b": "CLASSY04, ICSI, Submodular, DPP, RegSum. Explanation: BIBREF6 published SumRepo, a repository of summaries for the DUC2004 dataset generated by several baseline and state-of-the-art methods .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat state of the art methods are compared to?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We compare our method to the following baselines: (1) the original centroid-based model (2) the greedy algorithm (3) the greedy algorithm with a small number of sentences (4) the greedy algorithm with a large number of sentences (5) the greedy algorithm with a small number of sentences and a large number of sentences (6) the greedy algorithm with a small number of sentences and a small number of sentences (7) the greedy algorithm with a large number of sentences and a small number of sentences (8) the greedy algorithm with a large number of sentences and a large number of sentences (9) the greedy algorithm\n\nAnswer B:\nCLASSY04, ICSI, Submodular, DPP, RegSum. Explanation: BIBREF6 published SumRepo, a repository of summaries for the DUC2004 dataset generated by several baseline and state-of-the-art methods .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.07690",
    "question": "what are the performance metrics?",
    "gold_answer": "",
    "answer_a": "How are they evaluated?\n< Explanation: We use the following metrics to evaluate the performance of the proposed model: Recall, Precision, F1, and the average precision. Recall is the proportion of documents that are correctly classified as summaries. Precision is the proportion of documents that are correctly classified as summaries that are actually summaries. F1 is the harmonic mean of precision and recall. The average precision is the average of all the precision scores. The recall, precision, F1, and average precision are computed for each document in the test set. The recall is computed by counting the number of summaries that are correctly classified as summaries and dividing by the",
    "answer_b": "Rouge-1, Rouge-2 and Rouge-4 recall. Explanation: In order to compare our results to BIBREF6 we use the same Rouge settings as they do and report results for Rouge-1, Rouge-2 and Rouge-4 recall.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the performance metrics?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they evaluated?\n< Explanation: We use the following metrics to evaluate the performance of the proposed model: Recall, Precision, F1, and the average precision. Recall is the proportion of documents that are correctly classified as summaries. Precision is the proportion of documents that are correctly classified as summaries that are actually summaries. F1 is the harmonic mean of precision and recall. The average precision is the average of all the precision scores. The recall, precision, F1, and average precision are computed for each document in the test set. The recall is computed by counting the number of summaries that are correctly classified as summaries and dividing by the\n\nAnswer B:\nRouge-1, Rouge-2 and Rouge-4 recall. Explanation: In order to compare our results to BIBREF6 we use the same Rouge settings as they do and report results for Rouge-1, Rouge-2 and Rouge-4 recall.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.07690",
    "question": "what is the original model they refer to?",
    "gold_answer": "",
    "answer_a": "Explanation: The original model is a simple and fast baseline that ranks sentences based on their similarity to a centroid vector. In this paper, we apply this ranking to possible summaries instead of sentences and use a simple greedy algorithm to find the best summary. Furthermore, we show possi- bilities to scale up to larger input docu- ment collections by selecting a small num- ber of sentences from each document prior to constructing the summary. Experiments were done on the DUC2004 dataset for multi-document summarization. We observe a higher performance over the original model, on par with more complex state-of-the-art methods.\n\nThe",
    "answer_b": "BIBREF0 , BIBREF6. Explanation: This method is often found as a baseline in evaluations where it usually is outperformed BIBREF0 , BIBREF6 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the original model they refer to?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The original model is a simple and fast baseline that ranks sentences based on their similarity to a centroid vector. In this paper, we apply this ranking to possible summaries instead of sentences and use a simple greedy algorithm to find the best summary. Furthermore, we show possi- bilities to scale up to larger input docu- ment collections by selecting a small num- ber of sentences from each document prior to constructing the summary. Experiments were done on the DUC2004 dataset for multi-document summarization. We observe a higher performance over the original model, on par with more complex state-of-the-art methods.\n\nThe\n\nAnswer B:\nBIBREF0 , BIBREF6. Explanation: This method is often found as a baseline in evaluations where it usually is outperformed BIBREF0 , BIBREF6 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.07690",
    "question": "how are sentences selected prior to making the summary?",
    "gold_answer": "",
    "answer_a": "< Explanation: We first select a small number of sentences from each document, and then rank them based on their similarity to the centroid vector. We use the centroid vector as a reference for the similarity between sentences. We then rank the sentences based on their similarity to the centroid vector. We use the centroid vector as a reference for the similarity between sentences. We then rank the sentences based on their similarity to the centroid vector. We use the centroid vector as a reference for the similarity between sentences. We then rank the sentences based on their similarity to the centroid vector. We use the centroid vector as a reference for the similarity between sentences. We",
    "answer_b": "Using three algorithms: N-first, N-best and New-TF-IDF.. Explanation: We test the following methods of selecting INLINEFORM0 sentences from each document as candidates for the greedy sentence selection algorithm:\n\nThe first INLINEFORM0 sentences of the document are selected. This results in a mixture of a lead- INLINEFORM1 baseline and the centroid-based method.\n\nThe sentences are ranked separately in each document by their cosine similarity to the centroid vector, in decreasing order. The INLINEFORM0 best sentences of each document are selected as candidates.\n\nEach sentence is scored by the sum of the TF-IDF scores of the terms that are mentioned in that sentence for the first time in the document. The intuition is that sentences are preferred if they introduce new important information to a document.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow are sentences selected prior to making the summary?\n\nGround Truth Answer:\n\n\nAnswer A:\n< Explanation: We first select a small number of sentences from each document, and then rank them based on their similarity to the centroid vector. We use the centroid vector as a reference for the similarity between sentences. We then rank the sentences based on their similarity to the centroid vector. We use the centroid vector as a reference for the similarity between sentences. We then rank the sentences based on their similarity to the centroid vector. We use the centroid vector as a reference for the similarity between sentences. We then rank the sentences based on their similarity to the centroid vector. We use the centroid vector as a reference for the similarity between sentences. We\n\nAnswer B:\nUsing three algorithms: N-first, N-best and New-TF-IDF.. Explanation: We test the following methods of selecting INLINEFORM0 sentences from each document as candidates for the greedy sentence selection algorithm:\n\nThe first INLINEFORM0 sentences of the document are selected. This results in a mixture of a lead- INLINEFORM1 baseline and the centroid-based method.\n\nThe sentences are ranked separately in each document by their cosine similarity to the centroid vector, in decreasing order. The INLINEFORM0 best sentences of each document are selected as candidates.\n\nEach sentence is scored by the sum of the TF-IDF scores of the terms that are mentioned in that sentence for the first time in the document. The intuition is that sentences are preferred if they introduce new important information to a document.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.05253",
    "question": "Do they evaluate only on English datasets?",
    "gold_answer": "",
    "answer_a": "The twitter dataset is English-only; no information for the reddit dataset is given. Explanation: As pre-processing, we removed the retweets, spam, duplicates, and tweets written in languages other than English.",
    "answer_b": "How do they evaluate?\n<sep><table> <tr> <td> Explanation: We evaluate the performance of our system on two datasets: English and Chinese. We use the Twitter dataset from the Twitter Corpus of Chinese (TCOC) BIBREF10 and the Reddit dataset from the Reddit Corpus BIBREF11. We use the same evaluation metrics as in BIBREF12. We use the same evaluation criteria as in BIBREF13. We use the same evaluation criteria as in BIBREF14. We use the same evaluation criteria as in BIBREF15. We use the same evaluation",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nDo they evaluate only on English datasets?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe twitter dataset is English-only; no information for the reddit dataset is given. Explanation: As pre-processing, we removed the retweets, spam, duplicates, and tweets written in languages other than English.\n\nAnswer B:\nHow do they evaluate?\n<sep><table> <tr> <td> Explanation: We evaluate the performance of our system on two datasets: English and Chinese. We use the Twitter dataset from the Twitter Corpus of Chinese (TCOC) BIBREF10 and the Reddit dataset from the Reddit Corpus BIBREF11. We use the same evaluation metrics as in BIBREF12. We use the same evaluation criteria as in BIBREF13. We use the same evaluation criteria as in BIBREF14. We use the same evaluation criteria as in BIBREF15. We use the same evaluation\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.05253",
    "question": "What type of frequency analysis was used?",
    "gold_answer": "",
    "answer_a": "mean of occurrence per utterance and the standard deviation (SD) of each marker. Explanation: We report the mean of occurrence per utterance and the standard deviation (SD) of each marker.",
    "answer_b": "What is the accuracy of the model?\n<sep><table cellspacing=10 cellpadding=4>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of frequency analysis was used?\n\nGround Truth Answer:\n\n\nAnswer A:\nmean of occurrence per utterance and the standard deviation (SD) of each marker. Explanation: We report the mean of occurrence per utterance and the standard deviation (SD) of each marker.\n\nAnswer B:\nWhat is the accuracy of the model?\n<sep><table cellspacing=10 cellpadding=4>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=200>\n<col width=\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.05253",
    "question": "What type of classifiers were used?",
    "gold_answer": "",
    "answer_a": "Support Vector Machines (SVM) classifier with linear kernel BIBREF16. Explanation: We use Support Vector Machines (SVM) classifier with linear kernel BIBREF16 .",
    "answer_b": "What are the results?\n<blockquote>Figure 1: Classification results for irony markers in Twitter. The left column shows the results for emoticons and emojis, while the right column shows the results for morphological markers. The results are shown as a percentage of the total number of tweets. The results are shown as a percentage of the total number of tweets. The results are shown as a percentage of the total number of tweets. The results are shown as a percentage of the total number of tweets. The results are shown as a percentage of the total number of tweets. The results are shown as a percentage of the total number of tweets.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of classifiers were used?\n\nGround Truth Answer:\n\n\nAnswer A:\nSupport Vector Machines (SVM) classifier with linear kernel BIBREF16. Explanation: We use Support Vector Machines (SVM) classifier with linear kernel BIBREF16 .\n\nAnswer B:\nWhat are the results?\n<blockquote>Figure 1: Classification results for irony markers in Twitter. The left column shows the results for emoticons and emojis, while the right column shows the results for morphological markers. The results are shown as a percentage of the total number of tweets. The results are shown as a percentage of the total number of tweets. The results are shown as a percentage of the total number of tweets. The results are shown as a percentage of the total number of tweets. The results are shown as a percentage of the total number of tweets. The results are shown as a percentage of the total number of tweets.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.05253",
    "question": "Who annotated the Twitter and Reddit data for irony?",
    "gold_answer": "",
    "answer_a": "collected using hashtags, such as #irony, #sarcasm, and #sarcastic. Explanation: The ironic/sarcastic tweets are collected using hashtags, such as #irony, #sarcasm, and #sarcastic whereas the non-sarcastic tweets do not contain these hashtags, but they might include sentiment hashtags, such as #happy, #love, #sad, #hate (similar to BIBREF8 , BIBREF9 ).",
    "answer_b": "What are the annotations?\n<blockquote cite=\"1\">1. The irony markers are: emoticons, emojis, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks,",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWho annotated the Twitter and Reddit data for irony?\n\nGround Truth Answer:\n\n\nAnswer A:\ncollected using hashtags, such as #irony, #sarcasm, and #sarcastic. Explanation: The ironic/sarcastic tweets are collected using hashtags, such as #irony, #sarcasm, and #sarcastic whereas the non-sarcastic tweets do not contain these hashtags, but they might include sentiment hashtags, such as #happy, #love, #sad, #hate (similar to BIBREF8 , BIBREF9 ).\n\nAnswer B:\nWhat are the annotations?\n<blockquote cite=\"1\">1. The irony markers are: emoticons, emojis, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks, exclamation marks, question marks,\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.11598",
    "question": "what resources are combined to build the labeler?",
    "gold_answer": "",
    "answer_a": "< Explanation: We use the following resources to build the labeler: (1) the CoNLL 2009 shared task data set (2) the Stanford Dependency Parser (3) the Stanford CoreNLP (4) the Stanford CoreNLP (5) the Stanford CoreNLP (6) the Stanford CoreNLP (7) the Stanford CoreNLP (8) the Stanford CoreNLP (9) the Stanford CoreNLP (10) the Stanford CoreNLP (11) the Stanford CoreNLP (12) the Stanford CoreNLP (13) the Stanford Core",
    "answer_b": "multilingual word vectors. Explanation: In this work, we have explored a straightforward method for polyglot training in SRL: use multilingual word vectors and combine training data across languages.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat resources are combined to build the labeler?\n\nGround Truth Answer:\n\n\nAnswer A:\n< Explanation: We use the following resources to build the labeler: (1) the CoNLL 2009 shared task data set (2) the Stanford Dependency Parser (3) the Stanford CoreNLP (4) the Stanford CoreNLP (5) the Stanford CoreNLP (6) the Stanford CoreNLP (7) the Stanford CoreNLP (8) the Stanford CoreNLP (9) the Stanford CoreNLP (10) the Stanford CoreNLP (11) the Stanford CoreNLP (12) the Stanford CoreNLP (13) the Stanford Core\n\nAnswer B:\nmultilingual word vectors. Explanation: In this work, we have explored a straightforward method for polyglot training in SRL: use multilingual word vectors and combine training data across languages.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.11598",
    "question": "what datasets were used?",
    "gold_answer": "",
    "answer_a": "semantic role labeling portion of the CoNLL-2009 shared task BIBREF0. Explanation: We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish.",
    "answer_b": "Explanation: We use the CoNLL 2009 shared task for the evaluation of our approach. The dataset consists of 1000 annotated sentences in English and 1000 annotated sentences in Spanish. The English and Spanish sentences are split into 1000 pairs of sentences, with each pair consisting of one English sentence and one Spanish sentence. The English and Spanish sentences are annotated with the following semantic roles: subject, object, and predicate. The English and Spanish sentences are also annotated with the following syntactic roles: noun, verb, and adjective. The English and Spanish sentences are annotated with the following",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat datasets were used?\n\nGround Truth Answer:\n\n\nAnswer A:\nsemantic role labeling portion of the CoNLL-2009 shared task BIBREF0. Explanation: We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish.\n\nAnswer B:\nExplanation: We use the CoNLL 2009 shared task for the evaluation of our approach. The dataset consists of 1000 annotated sentences in English and 1000 annotated sentences in Spanish. The English and Spanish sentences are split into 1000 pairs of sentences, with each pair consisting of one English sentence and one Spanish sentence. The English and Spanish sentences are annotated with the following semantic roles: subject, object, and predicate. The English and Spanish sentences are also annotated with the following syntactic roles: noun, verb, and adjective. The English and Spanish sentences are annotated with the following\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.11598",
    "question": "what is the monolingual baseline?",
    "gold_answer": "",
    "answer_a": "For each of the shared task languages,  they produced GloVe vectors BIBREF7 from the news, web, and Wikipedia text of the Leipzig Corpora Collection and trained 300-dimensional vectors then reduced them to 100 dimensions with principal component analysis for efficiency.. Explanation: We use pretrained word embeddings as input to the model. For each of the shared task languages, we produced GloVe vectors BIBREF7 from the news, web, and Wikipedia text of the Leipzig Corpora Collection BIBREF8 . We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component analysis for efficiency.",
    "answer_b": "How is it evaluated?\n< Explanation: The baseline is a monolingual model trained on the CoNLL 2009 shared task data set. The baseline is trained on the CoNLL 2009 shared task data set, and trained on the CoNLL 2009 shared task data set. The baseline is trained on the CoNLL 2009 shared task data set, and trained on the CoNLL 2009 shared task data set. The baseline is trained on the CoNLL 2009 shared task data set, and trained on the Co",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the monolingual baseline?\n\nGround Truth Answer:\n\n\nAnswer A:\nFor each of the shared task languages,  they produced GloVe vectors BIBREF7 from the news, web, and Wikipedia text of the Leipzig Corpora Collection and trained 300-dimensional vectors then reduced them to 100 dimensions with principal component analysis for efficiency.. Explanation: We use pretrained word embeddings as input to the model. For each of the shared task languages, we produced GloVe vectors BIBREF7 from the news, web, and Wikipedia text of the Leipzig Corpora Collection BIBREF8 . We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component analysis for efficiency.\n\nAnswer B:\nHow is it evaluated?\n< Explanation: The baseline is a monolingual model trained on the CoNLL 2009 shared task data set. The baseline is trained on the CoNLL 2009 shared task data set, and trained on the CoNLL 2009 shared task data set. The baseline is trained on the CoNLL 2009 shared task data set, and trained on the CoNLL 2009 shared task data set. The baseline is trained on the CoNLL 2009 shared task data set, and trained on the Co\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.11598",
    "question": "what languages are explored in this paper?",
    "gold_answer": "",
    "answer_a": "Catalan. Explanation: We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish.",
    "answer_b": "Explanation: We use the CoNLL 2009 shared task to evaluate our approach. The task consists of two parts: (1) a multilingual dependency parser for English and German, and (2) a polyglot semantic role labeler for English and German. The task is split into two sets of tasks: (1) a dependency parser task and (2) a role labeler task. The dependency parser task is split into two sets: (1) a dependency parser task for English and German, and (2) a dependency parser task for German. The role labeler task is split into two sets:",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat languages are explored in this paper?\n\nGround Truth Answer:\n\n\nAnswer A:\nCatalan. Explanation: We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish.\n\nAnswer B:\nExplanation: We use the CoNLL 2009 shared task to evaluate our approach. The task consists of two parts: (1) a multilingual dependency parser for English and German, and (2) a polyglot semantic role labeler for English and German. The task is split into two sets of tasks: (1) a dependency parser task and (2) a role labeler task. The dependency parser task is split into two sets: (1) a dependency parser task for English and German, and (2) a dependency parser task for German. The role labeler task is split into two sets:\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.03955",
    "question": "What is the problem of session segmentation?",
    "gold_answer": "",
    "answer_a": "ot all sentences in the current conversation session are equally important. Explanation: Second, we observe that not all sentences in the current conversation session are equally important. The sentence \u201cWant to take a walk?\u201d is irrelevant to the current context, and should not be considered when the computer synthesizes the reply. Therefore, it raises the question of session segmentation in conversation systems.",
    "answer_b": "How does the proposed approach solve it?\n< Explanation: The problem of session segmentation is that the user-issued utterance is noisy and the user-issued utterance is not always the most important one. In this paper, we propose an embedding-enhanced TextTiling approach, inspired by the observation that conversation utterances are highly noisy, and that word embeddings provide a robust way of capturing semantics. Experimental results show that our approach achieves better performance than the TextTiling, MMD approaches.\n\nThe problem of session segmentation is that the user-issued utterance is noisy and the user-issued utterance is not always the most important one",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the problem of session segmentation?\n\nGround Truth Answer:\n\n\nAnswer A:\not all sentences in the current conversation session are equally important. Explanation: Second, we observe that not all sentences in the current conversation session are equally important. The sentence \u201cWant to take a walk?\u201d is irrelevant to the current context, and should not be considered when the computer synthesizes the reply. Therefore, it raises the question of session segmentation in conversation systems.\n\nAnswer B:\nHow does the proposed approach solve it?\n< Explanation: The problem of session segmentation is that the user-issued utterance is noisy and the user-issued utterance is not always the most important one. In this paper, we propose an embedding-enhanced TextTiling approach, inspired by the observation that conversation utterances are highly noisy, and that word embeddings provide a robust way of capturing semantics. Experimental results show that our approach achieves better performance than the TextTiling, MMD approaches.\n\nThe problem of session segmentation is that the user-issued utterance is noisy and the user-issued utterance is not always the most important one\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.03955",
    "question": "What dataset do they use?",
    "gold_answer": "",
    "answer_a": "real-world chatting corpus from DuMi. Explanation: o evaluate the session segmentation method, we used a real-world chatting corpus from DuMi, a state-of-the-practice open-domain conversation system in Chinese.",
    "answer_b": "How do they evaluate their approach?\n< Explanation: We use the dataset of the 2016 Stanford Conversation Corpus (SCC) BIBREF1 . The dataset consists of 100 utterances, each of which is a conversation between two users. The utterances are labeled as either a question or an answer. The question-answer pairs are labeled by a human annotator. The question is a natural question, and the answer is a natural answer. The question-answer pairs are labeled as either a question-answer pair or a question-answer pair pair. The question-answer pairs are labeled as either a question-answer pair or a question",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat dataset do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nreal-world chatting corpus from DuMi. Explanation: o evaluate the session segmentation method, we used a real-world chatting corpus from DuMi, a state-of-the-practice open-domain conversation system in Chinese.\n\nAnswer B:\nHow do they evaluate their approach?\n< Explanation: We use the dataset of the 2016 Stanford Conversation Corpus (SCC) BIBREF1 . The dataset consists of 100 utterances, each of which is a conversation between two users. The utterances are labeled as either a question or an answer. The question-answer pairs are labeled by a human annotator. The question is a natural question, and the answer is a natural answer. The question-answer pairs are labeled as either a question-answer pair or a question-answer pair pair. The question-answer pairs are labeled as either a question-answer pair or a question\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.03807",
    "question": "How was domain relevance estimated?",
    "gold_answer": "",
    "answer_a": "Explanation: We use a simple heuristic to estimate the domain relevance of a question. For each question, we first compute the probability of the question being true given the KB. Then, we compute the probability of the question being true given the domain of the KB. The domain relevance of the question is then computed as the ratio of the two probabilities. We use this heuristic to estimate the domain relevance of the generated questions. We also use the domain relevance of the generated questions to evaluate the fluency of the generated questions. We use the domain relevance of the generated questions to evaluate the fluency of the generated questions. We use the domain relevance of",
    "answer_b": "For domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:\n\n$$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$ (Eq. 7)\n\nwhere $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document.. Explanation: Shown in Table 3 , we compare our domain-relevance evaluation method (section \"Experiments\" ) with previous state-of-the-art methods",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow was domain relevance estimated?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We use a simple heuristic to estimate the domain relevance of a question. For each question, we first compute the probability of the question being true given the KB. Then, we compute the probability of the question being true given the domain of the KB. The domain relevance of the question is then computed as the ratio of the two probabilities. We use this heuristic to estimate the domain relevance of the generated questions. We also use the domain relevance of the generated questions to evaluate the fluency of the generated questions. We use the domain relevance of the generated questions to evaluate the fluency of the generated questions. We use the domain relevance of\n\nAnswer B:\nFor domain relevance, we take the seed question set as the in-domain data $D_{in}$ , the domain relevance of expanded question $q$ is defined as:\n\n$$\\textsc {Rel}(q) = \\cos (v(q),v(D_{in}))$$ (Eq. 7)\n\nwhere $v(\\cdot )$ is the document embedding defined as the averaged word embedding within the document.. Explanation: Shown in Table 3 , we compare our domain-relevance evaluation method (section \"Experiments\" ) with previous state-of-the-art methods\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.03807",
    "question": "How many hand-crafted templates did they have to make?",
    "gold_answer": "",
    "answer_a": "269.. Explanation: For the 500 triples, we hand-crafted 106 templates,",
    "answer_b": "What is the size of the seed question set?\n<sep>Answer: We used 1000 templates, each consisting of 100 words. The size of the seed question set was 10000. Explanation: We used 1000 templates, each consisting of 100 words. The size of the seed question set was 10000. The templates were generated by hand, and the size of the seed question set was 10000. The templates were generated by hand, and the size of the seed question set was 10000",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many hand-crafted templates did they have to make?\n\nGround Truth Answer:\n\n\nAnswer A:\n269.. Explanation: For the 500 triples, we hand-crafted 106 templates,\n\nAnswer B:\nWhat is the size of the seed question set?\n<sep>Answer: We used 1000 templates, each consisting of 100 words. The size of the seed question set was 10000. Explanation: We used 1000 templates, each consisting of 100 words. The size of the seed question set was 10000. The templates were generated by hand, and the size of the seed question set was 10000. The templates were generated by hand, and the size of the seed question set was 10000\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.03807",
    "question": "How was the fluency measured?",
    "gold_answer": "",
    "answer_a": "For fluency, we define the averaged language model score as:\n\n$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$ (Eq. 8)\n\nwhere $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count.. Explanation: For fluency, we define the averaged language model score as:\n\n$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$ (Eq. 8)\n\nwhere $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count.",
    "answer_b": "What is the domain relevance metric?\n<sep>Answer: The fluency of a question is measured by the number of words in the question and the number of words in the seed question. The domain relevance metric is measured by the number of words in the question and the number of words in the seed question. The domain relevance metric is calculated by the number of words in the question and the number of words in the seed question. The domain relevance metric is calculated by the number of words in the question and the number of words in the seed question. The domain relevance metric is calculated by the number of words in the question and the number of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow was the fluency measured?\n\nGround Truth Answer:\n\n\nAnswer A:\nFor fluency, we define the averaged language model score as:\n\n$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$ (Eq. 8)\n\nwhere $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count.. Explanation: For fluency, we define the averaged language model score as:\n\n$$\\textsc {AvgLM}(q) = \\frac{\\textsc {Lm}(q)}{\\textsc {Len}(q)}$$ (Eq. 8)\n\nwhere $\\textsc {Lm}(\\cdot )$ is the general-domain language model score (log probability), and $\\textsc {Len}(\\cdot )$ is the word count.\n\nAnswer B:\nWhat is the domain relevance metric?\n<sep>Answer: The fluency of a question is measured by the number of words in the question and the number of words in the seed question. The domain relevance metric is measured by the number of words in the question and the number of words in the seed question. The domain relevance metric is calculated by the number of words in the question and the number of words in the seed question. The domain relevance metric is calculated by the number of words in the question and the number of words in the seed question. The domain relevance metric is calculated by the number of words in the question and the number of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1607.03895",
    "question": "What data is used in this work?",
    "gold_answer": "",
    "answer_a": "What are the limitations of the data?\n<sep><xml>\n<statement>Our dataset consists of 1000 tennis post-match interviews conducted by 100 journalists. The interviews were conducted in 10 different countries, and were recorded by 10 different journalists. The interviews were conducted between 2015 and 2017. </statement>\n<statement>Our dataset is based on the interviews conducted by 100 journalists in 10 different countries, and was recorded by 10 different journalists. </statement>\n<statement>Our dataset consists of 1000 tennis",
    "answer_b": "Post-match interviews for tennis singles matches from ASAP Sport's website with match information from a dataset provided by Tennis-Data. Explanation: To help with further analysis of interview questions and answers, we introduce a dataset of tennis post-match interview transcripts along with corresponding match information.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat data is used in this work?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the limitations of the data?\n<sep><xml>\n<statement>Our dataset consists of 1000 tennis post-match interviews conducted by 100 journalists. The interviews were conducted in 10 different countries, and were recorded by 10 different journalists. The interviews were conducted between 2015 and 2017. </statement>\n<statement>Our dataset is based on the interviews conducted by 100 journalists in 10 different countries, and was recorded by 10 different journalists. </statement>\n<statement>Our dataset consists of 1000 tennis\n\nAnswer B:\nPost-match interviews for tennis singles matches from ASAP Sport's website with match information from a dataset provided by Tennis-Data. Explanation: To help with further analysis of interview questions and answers, we introduce a dataset of tennis post-match interview transcripts along with corresponding match information.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1704.06960",
    "question": "What dataset is used?",
    "gold_answer": "",
    "answer_a": "the XKCD color dataset. Explanation: For examples of human communication strategies for these tasks, we obtain the XKCD color dataset BIBREF17 , BIBREF18 and the Caltech\u2013UCSD Birds dataset BIBREF19 with accompanying natural language descriptions BIBREF20 .",
    "answer_b": "How is the dataset divided?\n< Explanation: We use the 2016-2017 DeepMind OpenAI Gym dataset (OpenAI, 2017). This dataset consists of 1000 episodes of 1000 agents, each with 1000 actions. Each episode consists of 1000 episodes, each with 1000 actions. Each episode consists of 1000 episodes, each with 1000 actions. Each episode consists of 1000 episodes, each with 1000 actions. Each episode consists",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat dataset is used?\n\nGround Truth Answer:\n\n\nAnswer A:\nthe XKCD color dataset. Explanation: For examples of human communication strategies for these tasks, we obtain the XKCD color dataset BIBREF17 , BIBREF18 and the Caltech\u2013UCSD Birds dataset BIBREF19 with accompanying natural language descriptions BIBREF20 .\n\nAnswer B:\nHow is the dataset divided?\n< Explanation: We use the 2016-2017 DeepMind OpenAI Gym dataset (OpenAI, 2017). This dataset consists of 1000 episodes of 1000 agents, each with 1000 actions. Each episode consists of 1000 episodes, each with 1000 actions. Each episode consists of 1000 episodes, each with 1000 actions. Each episode consists of 1000 episodes, each with 1000 actions. Each episode consists\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1904.03670",
    "question": "How was the dataset collected?",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n< Explanation: We collected a dataset of 1000 utterances from Fluent Speech Commands, a speech command dataset created by the University of California, Berkeley. The dataset contains 1000 utterances of 1000 words each, with 1000 words in total. The dataset was collected from the UC Berkeley speech command corpus, which is a collection of 1000 utterances of 1000 words each. The dataset was collected from the UC Berkeley speech command corpus, which is a collection of 1000 utterances of",
    "answer_b": "data was collected using crowdsourcing where speakers were recorded saying random ordered phrases for each intent twice. Explanation: The data was collected using crowdsourcing. Each speaker was recorded saying each wording for each intent twice. The phrases to record were presented in a random order.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow was the dataset collected?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n< Explanation: We collected a dataset of 1000 utterances from Fluent Speech Commands, a speech command dataset created by the University of California, Berkeley. The dataset contains 1000 utterances of 1000 words each, with 1000 words in total. The dataset was collected from the UC Berkeley speech command corpus, which is a collection of 1000 utterances of 1000 words each. The dataset was collected from the UC Berkeley speech command corpus, which is a collection of 1000 utterances of\n\nAnswer B:\ndata was collected using crowdsourcing where speakers were recorded saying random ordered phrases for each intent twice. Explanation: The data was collected using crowdsourcing. Each speaker was recorded saying each wording for each intent twice. The phrases to record were presented in a random order.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02363",
    "question": "what evaluation metrics were used?",
    "gold_answer": "",
    "answer_a": "Accuracy. Explanation: We compare the models on two evaluation metrics: MAE: Mean Absolute Error between the predicted and ground-truth agreed prices along with Accuracy$\\pm k$: the percentage of cases where the predicted price lies within $k$ percent of the ground-truth.",
    "answer_b": "Explanation: We use the following metrics to evaluate the performance of the model: Recall, Precision, F1 score, and Accuracy. Recall is the proportion of correct predictions out of the total number of predictions. Precision is the proportion of correct predictions out of the total number of positive predictions. F1 score is the harmonic mean of precision and recall. Accuracy is the proportion of correct predictions out of the total number of predictions. We use these metrics to evaluate the performance of the model on the test set. We also use the following metrics to evaluate the performance of the model on the training set: Recall, Precision, F1 score, and Accuracy",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat evaluation metrics were used?\n\nGround Truth Answer:\n\n\nAnswer A:\nAccuracy. Explanation: We compare the models on two evaluation metrics: MAE: Mean Absolute Error between the predicted and ground-truth agreed prices along with Accuracy$\\pm k$: the percentage of cases where the predicted price lies within $k$ percent of the ground-truth.\n\nAnswer B:\nExplanation: We use the following metrics to evaluate the performance of the model: Recall, Precision, F1 score, and Accuracy. Recall is the proportion of correct predictions out of the total number of predictions. Precision is the proportion of correct predictions out of the total number of positive predictions. F1 score is the harmonic mean of precision and recall. Accuracy is the proportion of correct predictions out of the total number of predictions. We use these metrics to evaluate the performance of the model on the test set. We also use the following metrics to evaluate the performance of the model on the training set: Recall, Precision, F1 score, and Accuracy\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02363",
    "question": "What datasets are used?",
    "gold_answer": "",
    "answer_a": "Explanation: We use the following datasets: (1) the negotiation dataset, which consists of 10000 negotiations, and (2) the negotiation dataset, which consists of 10000 negotiations. The negotiation dataset is obtained from the negotiation dataset, which is obtained from the negotiation dataset. The negotiation dataset is obtained from the negotiation dataset. The negotiation dataset is obtained from the negotiation dataset. The negotiation dataset is obtained from the negotiation dataset. The negotiation dataset is obtained from the negotiation dataset. The negotiation dataset is obtained from the negotiation dataset. The negotiation dataset is obtained from the negotiation dataset. The negotiation dataset is obtained",
    "answer_b": "Craigslist Bargaining dataset (CB). Explanation: Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by BIBREF4. Instead of focusing on the previously studied game environments BIBREF5, BIBREF6, the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist. The dataset consists of 6682 dialogues between a buyer and a seller who converse in natural language to negotiate the price of a given product (sample in Table TABREF1). In total, 1402 product ad postings were scraped from Craigslist, belonging to six categories: phones, bikes, housing, furniture, car and electronics. Each ad posting contains details such as Product Title, Category Type and a Listing Price. Moreover, a secret target price is also pre-decided for the buyer. The final price after the agreement is called the Agreed Price, which we aim to predict.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We use the following datasets: (1) the negotiation dataset, which consists of 10000 negotiations, and (2) the negotiation dataset, which consists of 10000 negotiations. The negotiation dataset is obtained from the negotiation dataset, which is obtained from the negotiation dataset. The negotiation dataset is obtained from the negotiation dataset. The negotiation dataset is obtained from the negotiation dataset. The negotiation dataset is obtained from the negotiation dataset. The negotiation dataset is obtained from the negotiation dataset. The negotiation dataset is obtained from the negotiation dataset. The negotiation dataset is obtained from the negotiation dataset. The negotiation dataset is obtained\n\nAnswer B:\nCraigslist Bargaining dataset (CB). Explanation: Dataset: For our explorations, we use the Craigslist Bargaining dataset (CB) introduced by BIBREF4. Instead of focusing on the previously studied game environments BIBREF5, BIBREF6, the dataset considers a more realistic setup: negotiating the price of products listed on Craigslist. The dataset consists of 6682 dialogues between a buyer and a seller who converse in natural language to negotiate the price of a given product (sample in Table TABREF1). In total, 1402 product ad postings were scraped from Craigslist, belonging to six categories: phones, bikes, housing, furniture, car and electronics. Each ad posting contains details such as Product Title, Category Type and a Listing Price. Moreover, a secret target price is also pre-decided for the buyer. The final price after the agreement is called the Agreed Price, which we aim to predict.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1601.03313",
    "question": "how did they measure grammatical correctness?",
    "gold_answer": "",
    "answer_a": "What grammatical rules were used?\n<blockquote>We used the Stanford CoreNLP toolkit to evaluate the quality of the generated speeches. We used the following rules to evaluate grammatical correctness: 1) the number of words in the speech; 2) the number of sentences in the speech; 3) the number of words per sentence; 4) the number of sentences per word; 5) the number of words per sentence and the number of sentences per word; 6) the number of words per sentence and the number of sentences per word; 7) the number of words per sentence and the number of sentences",
    "answer_b": "Identify  POS tags for each sentence,  check whether one sentence from the corpus has the same sequence of POS tags. If the same POS sequence has been found, that points in a certain direction, if not found, the evaluation for that sentence is performed manually.. Explanation: For evaluating the grammatical correctness we identify for each sentence of the speech its POS tags. Then we check all sentences of the entire corpus whether one has the same sequence of POS tags. Having a sentence with the same POS tag structure does not necessarily mean that the grammar is correct. Neither does the lack of finding a matching sentence imply the existence of an error. But it points in a certain direction. Furthermore, we let the system output the sentence for which it could not find a matching sentence so that we can evaluate those sentences manually.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow did they measure grammatical correctness?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat grammatical rules were used?\n<blockquote>We used the Stanford CoreNLP toolkit to evaluate the quality of the generated speeches. We used the following rules to evaluate grammatical correctness: 1) the number of words in the speech; 2) the number of sentences in the speech; 3) the number of words per sentence; 4) the number of sentences per word; 5) the number of words per sentence and the number of sentences per word; 6) the number of words per sentence and the number of sentences per word; 7) the number of words per sentence and the number of sentences\n\nAnswer B:\nIdentify  POS tags for each sentence,  check whether one sentence from the corpus has the same sequence of POS tags. If the same POS sequence has been found, that points in a certain direction, if not found, the evaluation for that sentence is performed manually.. Explanation: For evaluating the grammatical correctness we identify for each sentence of the speech its POS tags. Then we check all sentences of the entire corpus whether one has the same sequence of POS tags. Having a sentence with the same POS tag structure does not necessarily mean that the grammar is correct. Neither does the lack of finding a matching sentence imply the existence of an error. But it points in a certain direction. Furthermore, we let the system output the sentence for which it could not find a matching sentence so that we can evaluate those sentences manually.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1601.03313",
    "question": "how was quality of sentence transition measured?",
    "gold_answer": "",
    "answer_a": "Yes, the system was evaluated by a human evaluator. The evaluation was conducted by a group of 10 human evaluators. The evaluation was conducted in two phases. In the first phase, the system was evaluated by a human evaluator. The second phase was conducted by a human evaluator who was not part of the evaluation group. The evaluation group was composed of 10 human evaluators. The evaluation group was composed of 10 human evaluators. The evaluation group was composed of 10 human evaluators. The evaluation group was composed of 10 human evaluators. The evaluation group was composed of 10 human",
    "answer_b": "Manually, using the criterion score between 0 and 3.. Explanation: Table TABREF15 shows the results from the manual evaluation. Note that each criterion scores between 0 and 3 which leads to a maximum total score of 12. The achieved total score range from 5 to 10 with an average of 8.1. In particular, the grammatical correctness and the sentence transitions were very good.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow was quality of sentence transition measured?\n\nGround Truth Answer:\n\n\nAnswer A:\nYes, the system was evaluated by a human evaluator. The evaluation was conducted by a group of 10 human evaluators. The evaluation was conducted in two phases. In the first phase, the system was evaluated by a human evaluator. The second phase was conducted by a human evaluator who was not part of the evaluation group. The evaluation group was composed of 10 human evaluators. The evaluation group was composed of 10 human evaluators. The evaluation group was composed of 10 human evaluators. The evaluation group was composed of 10 human evaluators. The evaluation group was composed of 10 human\n\nAnswer B:\nManually, using the criterion score between 0 and 3.. Explanation: Table TABREF15 shows the results from the manual evaluation. Note that each criterion scores between 0 and 3 which leads to a maximum total score of 12. The achieved total score range from 5 to 10 with an average of 8.1. In particular, the grammatical correctness and the sentence transitions were very good.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1601.03313",
    "question": "what is the size of the dataset?",
    "gold_answer": "",
    "answer_a": "3857 speech segments. Explanation: The main data source for this project is the Convote data set UID41 . It contains a total of 3857 speech segments from 53 US Congressional floor debates from the year 2005.",
    "answer_b": "< Explanation: The Convote dataset contains transcripts from US congressional floor debates. The dataset consists of 10,000 transcripts and 10,000 sentences. The sentences are split into 1000 sentences for training and 1000 sentences for testing. The dataset is available at http://www.cs.cornell.edu/~james/convote/.\n\nThe dataset contains 10,000 transcripts and 10,000 sentences. The sentences are split into 1000 sentences for training and 1000 sentences for testing. The dataset is",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the size of the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\n3857 speech segments. Explanation: The main data source for this project is the Convote data set UID41 . It contains a total of 3857 speech segments from 53 US Congressional floor debates from the year 2005.\n\nAnswer B:\n< Explanation: The Convote dataset contains transcripts from US congressional floor debates. The dataset consists of 10,000 transcripts and 10,000 sentences. The sentences are split into 1000 sentences for training and 1000 sentences for testing. The dataset is available at http://www.cs.cornell.edu/~james/convote/.\n\nThe dataset contains 10,000 transcripts and 10,000 sentences. The sentences are split into 1000 sentences for training and 1000 sentences for testing. The dataset is\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1601.03313",
    "question": "what manual evaluation is presented?",
    "gold_answer": "",
    "answer_a": "Manual evaluation of four evaluation criteria:   grammatical correctness,  sentence transitions, speech structure, and speech content.. Explanation: Table TABREF15 shows the results from the manual evaluation. Note that each criterion scores between 0 and 3 which leads to a maximum total score of 12. The achieved total score range from 5 to 10 with an average of 8.1. In particular, the grammatical correctness and the sentence transitions were very good. Each of them scored on average 2.3 out of 3. The speech content yielded the lowest scores. This indicates that the topic model may need some improvement.",
    "answer_b": "Explanation: The manual evaluation was performed by a human annotator. The evaluation was performed on the test set which contains 10 speeches. The evaluation was performed on the basis of the following criteria: grammatical correctness, sentence transitions, and coherence. The evaluation was performed by a human annotator who was trained on the test set. The evaluation was performed on the basis of the following criteria: grammatical correctness, sentence transitions, and coherence. The evaluation was performed on the basis of the following criteria: grammatical correctness, sentence transitions, and coherence. The evaluation was performed on the basis of the following criteria: grammatical correctness, sentence",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat manual evaluation is presented?\n\nGround Truth Answer:\n\n\nAnswer A:\nManual evaluation of four evaluation criteria:   grammatical correctness,  sentence transitions, speech structure, and speech content.. Explanation: Table TABREF15 shows the results from the manual evaluation. Note that each criterion scores between 0 and 3 which leads to a maximum total score of 12. The achieved total score range from 5 to 10 with an average of 8.1. In particular, the grammatical correctness and the sentence transitions were very good. Each of them scored on average 2.3 out of 3. The speech content yielded the lowest scores. This indicates that the topic model may need some improvement.\n\nAnswer B:\nExplanation: The manual evaluation was performed by a human annotator. The evaluation was performed on the test set which contains 10 speeches. The evaluation was performed on the basis of the following criteria: grammatical correctness, sentence transitions, and coherence. The evaluation was performed by a human annotator who was trained on the test set. The evaluation was performed on the basis of the following criteria: grammatical correctness, sentence transitions, and coherence. The evaluation was performed on the basis of the following criteria: grammatical correctness, sentence transitions, and coherence. The evaluation was performed on the basis of the following criteria: grammatical correctness, sentence\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.01108",
    "question": "What downstream tasks are analyzed?",
    "gold_answer": "",
    "answer_a": "sentiment classification. Explanation: We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - BIBREF13) and a question answering task (SQuAD v1.1 - BIBREF14).",
    "answer_b": "How are the downstream tasks evaluated?\n< Explanation: We evaluate the performance of the proposed model on the following downstream tasks: (1) Sentiment Analysis, (2) Question Answering, (3) Question Generation, (4) Question Classification, (5) Question Classification with a single question, (6) Question Classification with a single question and a single answer, (7) Question Classification with a single question and a single answer and (8) Question Classification with a single question and a single answer and a single answer. We also evaluate the performance of the DistilBERT model on the following tasks: (1) Sentiment Analysis",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat downstream tasks are analyzed?\n\nGround Truth Answer:\n\n\nAnswer A:\nsentiment classification. Explanation: We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - BIBREF13) and a question answering task (SQuAD v1.1 - BIBREF14).\n\nAnswer B:\nHow are the downstream tasks evaluated?\n< Explanation: We evaluate the performance of the proposed model on the following downstream tasks: (1) Sentiment Analysis, (2) Question Answering, (3) Question Generation, (4) Question Classification, (5) Question Classification with a single question, (6) Question Classification with a single question and a single answer, (7) Question Classification with a single question and a single answer and (8) Question Classification with a single question and a single answer and a single answer. We also evaluate the performance of the DistilBERT model on the following tasks: (1) Sentiment Analysis\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.01108",
    "question": "How much time takes the training of DistilBERT?",
    "gold_answer": "",
    "answer_a": "How much time is spent on the pre-training?\n< Explanation: We pre-train DistilBERT on the BERT model with 100 epochs and 100GB of memory. We use the same training and evaluation procedure as the BERT model. We pre-train DistilBERT with 100 epochs and 100GB of memory. We use the same training and evaluation procedure as the BERT model. We pre-train DistilBERT with 100 epochs and 100GB of memory. We use the same training and evaluation procedure as the BERT model. We pre",
    "answer_b": "on 8 16GB V100 GPUs for approximately 90 hours. Explanation: DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much time takes the training of DistilBERT?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow much time is spent on the pre-training?\n< Explanation: We pre-train DistilBERT on the BERT model with 100 epochs and 100GB of memory. We use the same training and evaluation procedure as the BERT model. We pre-train DistilBERT with 100 epochs and 100GB of memory. We use the same training and evaluation procedure as the BERT model. We pre-train DistilBERT with 100 epochs and 100GB of memory. We use the same training and evaluation procedure as the BERT model. We pre\n\nAnswer B:\non 8 16GB V100 GPUs for approximately 90 hours. Explanation: DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.06519",
    "question": "Which datasets do they use?",
    "gold_answer": "",
    "answer_a": "How do they evaluate the performance?\n< Explanation: We use the following datasets: (1) the dataset of the 2016-2017 National Mandarin Chinese Test (NMC) (2) the dataset of the 2016-2017 National Mandarin Chinese Test (NMC) (3) the dataset of the 2016-2017 National Mandarin Chinese Test (NMC) (4) the dataset of the 2016-2017 National Mandarin Chinese Test (NMC) (5) the dataset of the 2016",
    "answer_b": "LibriSpeech corpus BIBREF46. Explanation: Two corpora across five languages were used in the experiment. One of the corpora we used is LibriSpeech corpus BIBREF46 (English).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich datasets do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate the performance?\n< Explanation: We use the following datasets: (1) the dataset of the 2016-2017 National Mandarin Chinese Test (NMC) (2) the dataset of the 2016-2017 National Mandarin Chinese Test (NMC) (3) the dataset of the 2016-2017 National Mandarin Chinese Test (NMC) (4) the dataset of the 2016-2017 National Mandarin Chinese Test (NMC) (5) the dataset of the 2016\n\nAnswer B:\nLibriSpeech corpus BIBREF46. Explanation: Two corpora across five languages were used in the experiment. One of the corpora we used is LibriSpeech corpus BIBREF46 (English).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.06519",
    "question": "How do they compare representations performance obtained from a naive encoder versus ones learned from large amount of source language data?",
    "gold_answer": "",
    "answer_a": "They compare retrieval performance in MAP.. Explanation: FLOAT SELECTED: Table 2: Retrieval Performance in MAP. Dim is the dimension of the vector representation.Small DB is the small database with 20000 examples, Large DB is the large database with 250000 examples",
    "answer_b": "What is the difference between the two?\n< Explanation: We compare the performance of the two methods in terms of the number of words learned and the number of words in the learned representations. We also compare the performance of the two methods in terms of the number of words learned and the number of words in the learned representations. We find that the number of words learned from the SA learned representations is significantly higher than the number of words learned from the naive encoder. This suggests that the SA learned representations are more informative than the naive encoder. We also find that the number of words learned from the SA learned representations is significantly higher than the number of words",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they compare representations performance obtained from a naive encoder versus ones learned from large amount of source language data?\n\nGround Truth Answer:\n\n\nAnswer A:\nThey compare retrieval performance in MAP.. Explanation: FLOAT SELECTED: Table 2: Retrieval Performance in MAP. Dim is the dimension of the vector representation.Small DB is the small database with 20000 examples, Large DB is the large database with 250000 examples\n\nAnswer B:\nWhat is the difference between the two?\n< Explanation: We compare the performance of the two methods in terms of the number of words learned and the number of words in the learned representations. We also compare the performance of the two methods in terms of the number of words learned and the number of words in the learned representations. We find that the number of words learned from the SA learned representations is significantly higher than the number of words learned from the naive encoder. This suggests that the SA learned representations are more informative than the naive encoder. We also find that the number of words learned from the SA learned representations is significantly higher than the number of words\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.06519",
    "question": "Which pairs of languages do they consider similar enough to capture phonetic structure?",
    "gold_answer": "",
    "answer_a": "German and French. Explanation: English, German and French are more similar compared with Czech and Spanish.",
    "answer_b": "What is the size of the source and target language datasets?\n<blockquote>\n<font size=\"2\">The two languages considered are English and Japanese. </font>\n<font size=\"2\">The source language is English and the target language is Japanese. </font>\n<font size=\"2\">The source language is English and the target language is Japanese. </font>\n<font size=\"2\">The source language is English and the target language is Japanese. </font>\n<font size=\"2\">The source language is English and the target language is Japanese. </font>\n<font size=\"2\">The source language is English",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich pairs of languages do they consider similar enough to capture phonetic structure?\n\nGround Truth Answer:\n\n\nAnswer A:\nGerman and French. Explanation: English, German and French are more similar compared with Czech and Spanish.\n\nAnswer B:\nWhat is the size of the source and target language datasets?\n<blockquote>\n<font size=\"2\">The two languages considered are English and Japanese. </font>\n<font size=\"2\">The source language is English and the target language is Japanese. </font>\n<font size=\"2\">The source language is English and the target language is Japanese. </font>\n<font size=\"2\">The source language is English and the target language is Japanese. </font>\n<font size=\"2\">The source language is English and the target language is Japanese. </font>\n<font size=\"2\">The source language is English\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.06905",
    "question": "What are their results on this task?",
    "gold_answer": "",
    "answer_a": "How do they compare to the results of the other models?\n< Explanation: The results of the experiments are presented in Table 1. The results show that the BiLSTM model outperforms the other models in terms of the F1 score. The BiLSTM model also outperforms the other models in terms of the accuracy. The results of the experiments are presented in Table 1. The results show that the BiLSTM model outperforms the other models in terms of the F1 score. The BiLSTM model also outperforms the other models in terms of the accuracy. The results of the experiments are presented",
    "answer_b": "98.11% accuracy with a 0.4% improvement upon the benchmark model. Explanation: Our first model with the BiLSTM based framework and the linear classifier reaches a 97.97% accuracy with a 1.1% improvement upon the benchmark model. Similarly, the second model with the SVM classifier reaches a remarkable 98.11% accuracy with a 0.4% improvement upon the benchmark model.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are their results on this task?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they compare to the results of the other models?\n< Explanation: The results of the experiments are presented in Table 1. The results show that the BiLSTM model outperforms the other models in terms of the F1 score. The BiLSTM model also outperforms the other models in terms of the accuracy. The results of the experiments are presented in Table 1. The results show that the BiLSTM model outperforms the other models in terms of the F1 score. The BiLSTM model also outperforms the other models in terms of the accuracy. The results of the experiments are presented\n\nAnswer B:\n98.11% accuracy with a 0.4% improvement upon the benchmark model. Explanation: Our first model with the BiLSTM based framework and the linear classifier reaches a 97.97% accuracy with a 1.1% improvement upon the benchmark model. Similarly, the second model with the SVM classifier reaches a remarkable 98.11% accuracy with a 0.4% improvement upon the benchmark model.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.06905",
    "question": "How is the text segmented?",
    "gold_answer": "",
    "answer_a": "dividing documents into chunks before processing. Explanation: We create different models with respect to the number of chunks that we divide the initial text into, in order to observe how the different number of chunks affect the efficiency of the final model.",
    "answer_b": "What is the length of the input text?\n<sep>The text is segmented into sentences using the sentence tokenizer. The sentences are then tokenized using the tokenizer provided by the tokenizer library. The tokenized sentences are then split into words using the word tokenizer. The words are then tokenized using the tokenizer provided by the tokenizer library. The tokenized words are then tokenized using the tokenizer provided by the tokenizer library. The tokenized words are then tokenized using the tokenizer provided by the tokenizer library. The tokenized words are then tokenized using the tokenizer provided by the tokenizer library. The tokenized words are then tokenized using",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is the text segmented?\n\nGround Truth Answer:\n\n\nAnswer A:\ndividing documents into chunks before processing. Explanation: We create different models with respect to the number of chunks that we divide the initial text into, in order to observe how the different number of chunks affect the efficiency of the final model.\n\nAnswer B:\nWhat is the length of the input text?\n<sep>The text is segmented into sentences using the sentence tokenizer. The sentences are then tokenized using the tokenizer provided by the tokenizer library. The tokenized sentences are then split into words using the word tokenizer. The words are then tokenized using the tokenizer provided by the tokenizer library. The tokenized words are then tokenized using the tokenizer provided by the tokenizer library. The tokenized words are then tokenized using the tokenizer provided by the tokenizer library. The tokenized words are then tokenized using the tokenizer provided by the tokenizer library. The tokenized words are then tokenized using\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.03312",
    "question": "what are the state of the art models?",
    "gold_answer": "",
    "answer_a": "The character-aware neural language model BIBREF1. Explanation: The character-aware neural language model BIBREF1 : It is an RNN language model that takes character embeddings as the inputs, encodes them with CNNs and then input them to RNNs for prediction. It achieved the state-of-the-art as a language model on alphabetic languages.",
    "answer_b": "What are the results?\n<blockquote>Our model achieves 90% smaller vocabulary and 13% fewer parameters than the state-of-the-art word embedding-based models, and 80% fewer parameters than the state-of-the-art character embedding-based models. </blockquote>\n<blockquote>Our model achieves 90% smaller vocabulary and 13% fewer parameters than the state-of-the-art word embedding-based models, and 80% fewer parameters than the state-of-the-art character embedding-based models. </blockquote>\n<blockquote>Our model achieves 90% smaller vocabulary and 13% fewer parameters than the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the state of the art models?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe character-aware neural language model BIBREF1. Explanation: The character-aware neural language model BIBREF1 : It is an RNN language model that takes character embeddings as the inputs, encodes them with CNNs and then input them to RNNs for prediction. It achieved the state-of-the-art as a language model on alphabetic languages.\n\nAnswer B:\nWhat are the results?\n<blockquote>Our model achieves 90% smaller vocabulary and 13% fewer parameters than the state-of-the-art word embedding-based models, and 80% fewer parameters than the state-of-the-art character embedding-based models. </blockquote>\n<blockquote>Our model achieves 90% smaller vocabulary and 13% fewer parameters than the state-of-the-art word embedding-based models, and 80% fewer parameters than the state-of-the-art character embedding-based models. </blockquote>\n<blockquote>Our model achieves 90% smaller vocabulary and 13% fewer parameters than the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.05731",
    "question": "Which language pairs do they evaluate on?",
    "gold_answer": "",
    "answer_a": "English-German. Explanation: For English-German (En-De) we train on WMT'17 data, validate on news2016 and test on news2017.",
    "answer_b": "How do they evaluate?\n< Explanation: We evaluate our models on four language pairs: German-English, English-French, French-English, and English-German. We use the same model architecture as in our previous work on noisy channel modeling (BIBREF10), but we use a different source and target model. We use the same source model architecture as in our previous work on noisy channel modeling (BIBREF10), but we use a different target model architecture. We use the same target model architecture as in our previous work on noisy channel modeling (BIBREF10), but we use a different source model",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich language pairs do they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\nEnglish-German. Explanation: For English-German (En-De) we train on WMT'17 data, validate on news2016 and test on news2017.\n\nAnswer B:\nHow do they evaluate?\n< Explanation: We evaluate our models on four language pairs: German-English, English-French, French-English, and English-German. We use the same model architecture as in our previous work on noisy channel modeling (BIBREF10), but we use a different source and target model. We use the same source model architecture as in our previous work on noisy channel modeling (BIBREF10), but we use a different target model architecture. We use the same target model architecture as in our previous work on noisy channel modeling (BIBREF10), but we use a different source model\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.10059",
    "question": "How large the improvement margin is?",
    "gold_answer": "",
    "answer_a": "+7.24 for train size of 2000, +11.03 for train size of 1000, and +14.67 for train size of 500. Explanation: FLOAT SELECTED: Table 2: F1-scores obtained by each of the six models for the 10 domains, with the highest score in each row marked as bold. Table (a), (b) and (c) report the results for 2000, 1000 and 500 training instances, respectively. The average improvement is computed over the CRF model, with the ones marked \u2217 being statistically significant with p-value < 0.05.",
    "answer_b": "What is the low data regime?\n< Explanation: We compare our model with the following baselines: (1) a baseline that uses the slot description to transfer the concept from the source domain to the target domain, (2) a baseline that uses the slot description to transfer the concept from the source domain to the target domain, (3) a baseline that uses the slot description to transfer the concept from the source domain to the target domain, (4) a baseline that uses the slot description to transfer the concept from the source domain to the target domain, (5) a baseline that uses the slot description to transfer the concept from the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow large the improvement margin is?\n\nGround Truth Answer:\n\n\nAnswer A:\n+7.24 for train size of 2000, +11.03 for train size of 1000, and +14.67 for train size of 500. Explanation: FLOAT SELECTED: Table 2: F1-scores obtained by each of the six models for the 10 domains, with the highest score in each row marked as bold. Table (a), (b) and (c) report the results for 2000, 1000 and 500 training instances, respectively. The average improvement is computed over the CRF model, with the ones marked \u2217 being statistically significant with p-value < 0.05.\n\nAnswer B:\nWhat is the low data regime?\n< Explanation: We compare our model with the following baselines: (1) a baseline that uses the slot description to transfer the concept from the source domain to the target domain, (2) a baseline that uses the slot description to transfer the concept from the source domain to the target domain, (3) a baseline that uses the slot description to transfer the concept from the source domain to the target domain, (4) a baseline that uses the slot description to transfer the concept from the source domain to the target domain, (5) a baseline that uses the slot description to transfer the concept from the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12848",
    "question": "Which languages do they explore?",
    "gold_answer": "",
    "answer_a": "Irish, Tibetian, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil, Hinglish, Bengali,Arabic, French, German, Odia. Explanation: For e.g. Irish, Tibetan, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil are the indigenous languages.",
    "answer_b": "Explanation: The paper explores the sentiment analysis of the reviews of the Indigenous languages. The languages explored are: Yoruba, Igbo, Yauy, Yorom, Yoruba, Igbo, Yauy, Yorom, Yoruba, Igbo, Yauy, Yorom, Yoruba, Igbo, Yauy, Yorom, Yoruba, Igbo, Yauy, Yorom, Yoruba, Igbo, Yauy, Yorom, Yoruba, Igbo, Yauy, Yorom, Yor",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich languages do they explore?\n\nGround Truth Answer:\n\n\nAnswer A:\nIrish, Tibetian, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil, Hinglish, Bengali,Arabic, French, German, Odia. Explanation: For e.g. Irish, Tibetan, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil are the indigenous languages.\n\nAnswer B:\nExplanation: The paper explores the sentiment analysis of the reviews of the Indigenous languages. The languages explored are: Yoruba, Igbo, Yauy, Yorom, Yoruba, Igbo, Yauy, Yorom, Yoruba, Igbo, Yauy, Yorom, Yoruba, Igbo, Yauy, Yorom, Yoruba, Igbo, Yauy, Yorom, Yoruba, Igbo, Yauy, Yorom, Yoruba, Igbo, Yauy, Yorom, Yor\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.01770",
    "question": "What are two baseline methods?",
    "gold_answer": "",
    "answer_a": "How do they compare?\n< Explanation: The baseline methods are as follows: (1) a simple recipe retrieval method, which simply retrieves recipes from the dataset by using the recipe text as the input. (2) a recipe text retrieval method, which uses the recipe text as the input. (3) a recipe text retrieval method, which uses the recipe text as the input, and then uses the ingredient and instruction information as the input. (4) a recipe text retrieval method, which uses the recipe text as the input, and then uses the ingredient and instruction information as the input, and finally uses the ingredient and instruction information as the",
    "answer_b": "Joint Neural Embedding (JNE)\nAdaMine. Explanation: Further, we estimate the recall percentage at the top K percent over all queries. The recall percentage describes the quantity of queries ranked amid the top K closest results. In Table TABREF11 the results are presented, in comparison to baseline methods.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are two baseline methods?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they compare?\n< Explanation: The baseline methods are as follows: (1) a simple recipe retrieval method, which simply retrieves recipes from the dataset by using the recipe text as the input. (2) a recipe text retrieval method, which uses the recipe text as the input. (3) a recipe text retrieval method, which uses the recipe text as the input, and then uses the ingredient and instruction information as the input. (4) a recipe text retrieval method, which uses the recipe text as the input, and then uses the ingredient and instruction information as the input, and finally uses the ingredient and instruction information as the\n\nAnswer B:\nJoint Neural Embedding (JNE)\nAdaMine. Explanation: Further, we estimate the recall percentage at the top K percent over all queries. The recall percentage describes the quantity of queries ranked amid the top K closest results. In Table TABREF11 the results are presented, in comparison to baseline methods.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.01770",
    "question": "How does model compare to the baselines?",
    "gold_answer": "",
    "answer_a": "The model outperforms the two baseline models,  since it has higher recall values.. Explanation: The recall percentage describes the quantity of queries ranked amid the top K closest results. In Table TABREF11 the results are presented, in comparison to baseline methods.",
    "answer_b": "What are the results?\n< Explanation: Table 1 shows the results of the proposed model and the baseline methods. The proposed model achieves the best performance, with an average accuracy of 93.65%, while the baseline methods achieve an average accuracy of 89.25%. The proposed model also outperforms the baseline methods in terms of intra-class variability, with an average intra-class variability of 0.000000000000000000000000000000000000000000",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does model compare to the baselines?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe model outperforms the two baseline models,  since it has higher recall values.. Explanation: The recall percentage describes the quantity of queries ranked amid the top K closest results. In Table TABREF11 the results are presented, in comparison to baseline methods.\n\nAnswer B:\nWhat are the results?\n< Explanation: Table 1 shows the results of the proposed model and the baseline methods. The proposed model achieves the best performance, with an average accuracy of 93.65%, while the baseline methods achieve an average accuracy of 89.25%. The proposed model also outperforms the baseline methods in terms of intra-class variability, with an average intra-class variability of 0.000000000000000000000000000000000000000000\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.06893",
    "question": "What is their baseline?",
    "gold_answer": "",
    "answer_a": "synthetic emails generated by Dada engine. Explanation: Comparison with a baseline - synthetic emails generated by Dada engine BIBREF6.",
    "answer_b": "How does it compare to the proposed system?\n< Explanation: We compare the performance of the proposed system with the baseline system. The baseline system is a simple text generation system that uses a simple RNN to generate text. The baseline system is trained on a corpus of 10000 emails. The baseline system is trained on a corpus of 10000 emails. The baseline system is trained on a corpus of 10000 emails. The baseline system is trained on a corpus of 10000 emails. The baseline system is trained on a corpus of 10000 emails",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is their baseline?\n\nGround Truth Answer:\n\n\nAnswer A:\nsynthetic emails generated by Dada engine. Explanation: Comparison with a baseline - synthetic emails generated by Dada engine BIBREF6.\n\nAnswer B:\nHow does it compare to the proposed system?\n< Explanation: We compare the performance of the proposed system with the baseline system. The baseline system is a simple text generation system that uses a simple RNN to generate text. The baseline system is trained on a corpus of 10000 emails. The baseline system is trained on a corpus of 10000 emails. The baseline system is trained on a corpus of 10000 emails. The baseline system is trained on a corpus of 10000 emails. The baseline system is trained on a corpus of 10000 emails\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.12699",
    "question": "by how much did their model outperform the other models?",
    "gold_answer": "",
    "answer_a": "What is the difference in performance?\n<blockquote>Our model outperforms the other models by 0.01 F1 and 0.02 accuracy. The best model is 0.81 F1 and 0.87 accuracy. </blockquote>\n<blockquote>Our model outperforms the other models by 0.01 F1 and 0.02 accuracy. The best model is 0.81 F1 and 0.87 accuracy. </blockquote>\nThe best model is 0.81 F1 and 0.87 accuracy. The best model is",
    "answer_b": "In terms of macro F1 score their model has 0.65 compared to 0.58 of best other model.. Explanation: The CNN model has the highest macro average $\\textrm {F}_1$ score with a value of 0.65.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nby how much did their model outperform the other models?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference in performance?\n<blockquote>Our model outperforms the other models by 0.01 F1 and 0.02 accuracy. The best model is 0.81 F1 and 0.87 accuracy. </blockquote>\n<blockquote>Our model outperforms the other models by 0.01 F1 and 0.02 accuracy. The best model is 0.81 F1 and 0.87 accuracy. </blockquote>\nThe best model is 0.81 F1 and 0.87 accuracy. The best model is\n\nAnswer B:\nIn terms of macro F1 score their model has 0.65 compared to 0.58 of best other model.. Explanation: The CNN model has the highest macro average $\\textrm {F}_1$ score with a value of 0.65.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.01590",
    "question": "What is reordering in the context of the paper?",
    "gold_answer": "",
    "answer_a": "How is it handled?\n< Explanation: We use a simple but effective method to handle reordering, which is to use a language model for context-aware search. The language model is trained on a monolingual corpus, and the search is performed on the cross-lingual embedding. The search is performed by using the language model to find the most similar words in the search space. The most similar words are then used to generate the reordering. The reordering is then used to generate the translation. The reordering is performed by using the language model to find the most similar words in the search space. The most similar words are then",
    "answer_b": "changing the order of the word-by-word translation so it matches the target language. Explanation: Also, translations generated word-by-word are not in an order of the target language.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is reordering in the context of the paper?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it handled?\n< Explanation: We use a simple but effective method to handle reordering, which is to use a language model for context-aware search. The language model is trained on a monolingual corpus, and the search is performed on the cross-lingual embedding. The search is performed by using the language model to find the most similar words in the search space. The most similar words are then used to generate the reordering. The reordering is then used to generate the translation. The reordering is performed by using the language model to find the most similar words in the search space. The most similar words are then\n\nAnswer B:\nchanging the order of the word-by-word translation so it matches the target language. Explanation: Also, translations generated word-by-word are not in an order of the target language.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.01590",
    "question": "How does the paper use language model for context aware search?",
    "gold_answer": "",
    "answer_a": "the language model is combined with cross-lingual word embedding to obtain context information in the word-by-word translation. Explanation: In this paper, we integrate context information into word-by-word translation by combining a language model (LM) with cross-lingual word embedding. Let $f$ be a source word in the current position and $e$ a possible target word. Given a history $h$ of target words before $e$ , the score of $e$ to be the translation of $f$ would be:\n\n$ L(e;f,h) = \\lambda _\\text{emb}\\log q(f,e) + \\lambda _\\text{LM}\\log p(e|h) $\n\nHere, $q(f,e)$ is a lexical score defined as:\n\n$ q(f,e) = \\frac{d(f,e) + 1}{2} $\n\nwhere $d(f,e) \\in [-1,1]$ is a cosine similarity between $f$ and $e$ . It is transformed to the range $[0,1]$ to make it similar in scale with the LM probability. In our experiments, we found that this simple linear scaling is better than sigmoid or softmax functions in the final translation performance.",
    "answer_b": "What is the language model used?\n< Explanation: The language model is a simple yet effective model for context-aware search. It is based on the language model for context-aware search (LM-CAS) [1] , which is a language model for context-aware search that is trained on a monolingual corpus. The LM-CAS model is trained on a monolingual corpus and is trained to predict the next word in a sentence given the previous word. The LM-CAS model is trained to predict the next word in a sentence given the previous word. The LM-CAS model is trained to predict the next word in a sentence given",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does the paper use language model for context aware search?\n\nGround Truth Answer:\n\n\nAnswer A:\nthe language model is combined with cross-lingual word embedding to obtain context information in the word-by-word translation. Explanation: In this paper, we integrate context information into word-by-word translation by combining a language model (LM) with cross-lingual word embedding. Let $f$ be a source word in the current position and $e$ a possible target word. Given a history $h$ of target words before $e$ , the score of $e$ to be the translation of $f$ would be:\n\n$ L(e;f,h) = \\lambda _\\text{emb}\\log q(f,e) + \\lambda _\\text{LM}\\log p(e|h) $\n\nHere, $q(f,e)$ is a lexical score defined as:\n\n$ q(f,e) = \\frac{d(f,e) + 1}{2} $\n\nwhere $d(f,e) \\in [-1,1]$ is a cosine similarity between $f$ and $e$ . It is transformed to the range $[0,1]$ to make it similar in scale with the LM probability. In our experiments, we found that this simple linear scaling is better than sigmoid or softmax functions in the final translation performance.\n\nAnswer B:\nWhat is the language model used?\n< Explanation: The language model is a simple yet effective model for context-aware search. It is based on the language model for context-aware search (LM-CAS) [1] , which is a language model for context-aware search that is trained on a monolingual corpus. The LM-CAS model is trained on a monolingual corpus and is trained to predict the next word in a sentence given the previous word. The LM-CAS model is trained to predict the next word in a sentence given the previous word. The LM-CAS model is trained to predict the next word in a sentence given\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.03876",
    "question": "What datasets are used?",
    "gold_answer": "",
    "answer_a": "How are they evaluated?\n< Explanation: We use the following datasets: (1) the BIBREF10 corpus, which consists of 10000 sentences, each of which is annotated with a discourse relation and a pair of arguments. (2) the BIBREF11 corpus, which consists of 10000 sentences, each of which is annotated with a discourse relation and a pair of arguments. (3) the BIBREF12 corpus, which consists of 10000 sentences, each of which is annotated with a discourse relation and a pair of arguments. (4)",
    "answer_b": "PDTB 2.0. Explanation: We used the largest hand-annotated discourse corpus PDTB 2.0 BIBREF12 (PDTB hereafter).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they evaluated?\n< Explanation: We use the following datasets: (1) the BIBREF10 corpus, which consists of 10000 sentences, each of which is annotated with a discourse relation and a pair of arguments. (2) the BIBREF11 corpus, which consists of 10000 sentences, each of which is annotated with a discourse relation and a pair of arguments. (3) the BIBREF12 corpus, which consists of 10000 sentences, each of which is annotated with a discourse relation and a pair of arguments. (4)\n\nAnswer B:\nPDTB 2.0. Explanation: We used the largest hand-annotated discourse corpus PDTB 2.0 BIBREF12 (PDTB hereafter).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.01065",
    "question": "what evaluation metrics were used?",
    "gold_answer": "",
    "answer_a": "How were they evaluated?\n< Explanation: We evaluate the performance of the proposed model using the following metrics: Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP",
    "answer_b": "F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4. Explanation: We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat evaluation metrics were used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow were they evaluated?\n< Explanation: We evaluate the performance of the proposed model using the following metrics: Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP), Mean Average Precision (MAP\n\nAnswer B:\nF-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4. Explanation: We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.01065",
    "question": "what is the source of their dataset?",
    "gold_answer": "",
    "answer_a": "Google News. Explanation: We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .",
    "answer_b": "Is it a survey?\nYes, the dataset is a survey. The dataset is collected from the 2018 ACM Conference on Information Retrieval (ICIR) held in San Francisco, California, USA. The dataset contains 1000 news documents and 1000 reader comments. The dataset is divided into 1000 documents and 1000 comments. The documents are randomly selected from the 2017 news articles published by the New York Times. The comments are randomly selected from the 2017 comments on the same news articles. The comments are annotated by 1",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the source of their dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nGoogle News. Explanation: We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .\n\nAnswer B:\nIs it a survey?\nYes, the dataset is a survey. The dataset is collected from the 2018 ACM Conference on Information Retrieval (ICIR) held in San Francisco, California, USA. The dataset contains 1000 news documents and 1000 reader comments. The dataset is divided into 1000 documents and 1000 comments. The documents are randomly selected from the 2017 news articles published by the New York Times. The comments are randomly selected from the 2017 comments on the same news articles. The comments are annotated by 1\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.01065",
    "question": "by how much did the performance improve?",
    "gold_answer": "",
    "answer_a": "What is the significance of the improvement?\n< Explanation: We compare the performance of the proposed model with the baseline model (baseline) and the baseline model with the reader comments (reader). The baseline model is the VAE-based MDS model. The reader comments are the comments of the reader who has read the news. The baseline model is trained on the news documents and the reader comments. The reader model is trained on the news documents and the reader comments. The reader model is trained on the news documents and the reader comments. The reader model is trained on the news documents and the reader comments. The reader model is trained on the news",
    "answer_b": "The proposed RAVAESum method improves from 0.001 to  0.059  Rouge1.. Explanation: FLOAT SELECTED: Table 1: Summarization performance.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nby how much did the performance improve?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the significance of the improvement?\n< Explanation: We compare the performance of the proposed model with the baseline model (baseline) and the baseline model with the reader comments (reader). The baseline model is the VAE-based MDS model. The reader comments are the comments of the reader who has read the news. The baseline model is trained on the news documents and the reader comments. The reader model is trained on the news documents and the reader comments. The reader model is trained on the news documents and the reader comments. The reader model is trained on the news documents and the reader comments. The reader model is trained on the news\n\nAnswer B:\nThe proposed RAVAESum method improves from 0.001 to  0.059  Rouge1.. Explanation: FLOAT SELECTED: Table 1: Summarization performance.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.01065",
    "question": "how many experts were there?",
    "gold_answer": "",
    "answer_a": "Each topic is assigned to 4 experts. Explanation: Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing.",
    "answer_b": "What is the size of the dataset?\n< Explanation: We collected 1000 readers from 1000 different countries. We randomly selected 1000 news documents from each country. We then randomly selected 1000 news documents from each country. We randomly selected 1000 news documents from each country. We randomly selected 1000 news documents from each country. We randomly selected 1000 news documents from each country. We randomly selected 1000 news documents from each country. We randomly selected 1000 news documents from each country. We",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow many experts were there?\n\nGround Truth Answer:\n\n\nAnswer A:\nEach topic is assigned to 4 experts. Explanation: Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing.\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: We collected 1000 readers from 1000 different countries. We randomly selected 1000 news documents from each country. We then randomly selected 1000 news documents from each country. We randomly selected 1000 news documents from each country. We randomly selected 1000 news documents from each country. We randomly selected 1000 news documents from each country. We randomly selected 1000 news documents from each country. We randomly selected 1000 news documents from each country. We\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.01065",
    "question": "what is the size of the data collected?",
    "gold_answer": "",
    "answer_a": "The dataset contains 19k annotated aspect facets, 45 topics, 6 predefined categories, 450 news document, 180 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. Explanation: The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \u201cMalaysia Airlines Disappearance\u201d, \u201cFlappy Bird\u201d, \u201cBitcoin Mt. Gox\u201d, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.",
    "answer_b": "How many documents are there?\n< Explanation: The dataset contains 1000 documents, each of which is annotated with 1000 comments. The total number of documents is 100000. The dataset is divided into 1000 training documents and 9000 test documents. The training documents are used to train the VAE, and the test documents are used to evaluate the model. The dataset is available online at http://www.cs.cornell.edu/~james/RA-MDS/.\n\nThe dataset contains 1000 documents, each of which is annotated with",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the size of the data collected?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe dataset contains 19k annotated aspect facets, 45 topics, 6 predefined categories, 450 news document, 180 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. Explanation: The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \u201cMalaysia Airlines Disappearance\u201d, \u201cFlappy Bird\u201d, \u201cBitcoin Mt. Gox\u201d, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.\n\nAnswer B:\nHow many documents are there?\n< Explanation: The dataset contains 1000 documents, each of which is annotated with 1000 comments. The total number of documents is 100000. The dataset is divided into 1000 training documents and 9000 test documents. The training documents are used to train the VAE, and the test documents are used to evaluate the model. The dataset is available online at http://www.cs.cornell.edu/~james/RA-MDS/.\n\nThe dataset contains 1000 documents, each of which is annotated with\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.01065",
    "question": "how was annotation conducted?",
    "gold_answer": "",
    "answer_a": "What aspect was annotated?\n< Explanation: We collected news documents and reader comments from the Chinese Wikipedia. We annotated the aspect of the news documents with the aspect of the reader comments. The aspect of the news document is the aspect of the reader comment. The aspect of the reader comment is the aspect of the news document. The aspect of the news document is the aspect of the aspect of the reader comment. The aspect of the aspect of the reader comment is the aspect of the aspect of the news document. The aspect of the aspect of the news document is the aspect of the aspect of the aspect of the aspect of the reader comment. The",
    "answer_b": "Experts identified aspect facets and wrote summaries.. Explanation: The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow was annotation conducted?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat aspect was annotated?\n< Explanation: We collected news documents and reader comments from the Chinese Wikipedia. We annotated the aspect of the news documents with the aspect of the reader comments. The aspect of the news document is the aspect of the reader comment. The aspect of the reader comment is the aspect of the news document. The aspect of the news document is the aspect of the aspect of the reader comment. The aspect of the aspect of the reader comment is the aspect of the aspect of the news document. The aspect of the aspect of the news document is the aspect of the aspect of the aspect of the aspect of the reader comment. The\n\nAnswer B:\nExperts identified aspect facets and wrote summaries.. Explanation: The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.01065",
    "question": "what does their dataset contain?",
    "gold_answer": "",
    "answer_a": "topics. Explanation: The dataset contains 45 topics from those 6 predefined categories.",
    "answer_b": "Is it a new dataset?\nYes, the dataset contains 1000 news documents and 10000 comments. The documents are randomly selected from the 1000 news articles. The comments are randomly selected from the 10000 comments. The comments are annotated by 1000 readers. The readers are randomly selected from the 1000 readers. The readers are asked to write a summary of the news articles. The summary is written in the form of a sentence. The summary is written by the reader and the reader is not aware of the document. The reader is also",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat does their dataset contain?\n\nGround Truth Answer:\n\n\nAnswer A:\ntopics. Explanation: The dataset contains 45 topics from those 6 predefined categories.\n\nAnswer B:\nIs it a new dataset?\nYes, the dataset contains 1000 news documents and 10000 comments. The documents are randomly selected from the 1000 news articles. The comments are randomly selected from the 10000 comments. The comments are annotated by 1000 readers. The readers are randomly selected from the 1000 readers. The readers are asked to write a summary of the news articles. The summary is written in the form of a sentence. The summary is written by the reader and the reader is not aware of the document. The reader is also\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.07562",
    "question": "How do the authors measure the extent to which LGI has learned the task?",
    "gold_answer": "",
    "answer_a": "precision. Explanation: After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed.",
    "answer_b": "What are the metrics used?\n< Explanation: The LGI network is trained on a dataset of 10000 sentences, each of which is a combination of a text and an image. The training data is divided into training and validation sets. The training set is used to train the LGI network, and the validation set is used to evaluate the LGI network. The LGI network is trained using the training set, and the training process is repeated until the LGI network reaches a maximum training accuracy of 0.99. The LGI network is trained using the training set, and the training process is repeated until",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do the authors measure the extent to which LGI has learned the task?\n\nGround Truth Answer:\n\n\nAnswer A:\nprecision. Explanation: After 50000 steps training, LGI could not only reconstruct the input image with high precision but also predict the 'mentally' moved object with specified morphology, correct manipulated direction and position just after the command sentence completed.\n\nAnswer B:\nWhat are the metrics used?\n< Explanation: The LGI network is trained on a dataset of 10000 sentences, each of which is a combination of a text and an image. The training data is divided into training and validation sets. The training set is used to train the LGI network, and the validation set is used to evaluate the LGI network. The LGI network is trained using the training set, and the training process is repeated until the LGI network reaches a maximum training accuracy of 0.99. The LGI network is trained using the training set, and the training process is repeated until\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.07562",
    "question": "Which 8 tasks has LGI learned?",
    "gold_answer": "",
    "answer_a": "How LGI is incrementally learned?\n< Explanation: The LGI network consists of three subsystems: (1) vision system that contains an encoder to disentangle the input or imagined scenarios into abstract population representations, and an imagination decoder to reconstruct imagined scenario from higher level representations; (2) Language system, that contains a binarizer to transfer symbol texts into binary vectors, an IPS (mimicking the human IntraParietal Sulcus, implemented by an LSTM) to extract the quantity information from the input texts, and a textizer to convert binary vectors into text symbols; (3) a PFC (m",
    "answer_b": "move left. Explanation: age and imagined pictures.\n\nExperiment\nThe first syntaxes that LG",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich 8 tasks has LGI learned?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow LGI is incrementally learned?\n< Explanation: The LGI network consists of three subsystems: (1) vision system that contains an encoder to disentangle the input or imagined scenarios into abstract population representations, and an imagination decoder to reconstruct imagined scenario from higher level representations; (2) Language system, that contains a binarizer to transfer symbol texts into binary vectors, an IPS (mimicking the human IntraParietal Sulcus, implemented by an LSTM) to extract the quantity information from the input texts, and a textizer to convert binary vectors into text symbols; (3) a PFC (m\n\nAnswer B:\nmove left. Explanation: age and imagined pictures.\n\nExperiment\nThe first syntaxes that LG\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.07562",
    "question": "In what was does an LSTM mimic the prefrontal cortex?",
    "gold_answer": "",
    "answer_a": "the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output. Explanation: In the language subsystem, we use an LSTM layer to mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output, instead of traditional softmax classifier.",
    "answer_b": "<=> Explanation: The PFC is a part of the prefrontal cortex, which is the most important part of the human brain. It is located in the anterior part of the frontal lobe, and is the most important part of the human brain. The PFC is responsible for the executive functions, such as attention, working memory, planning, and decision making. The PFC is also involved in the control of the body, such as the control of the heart, breathing, and the body temperature. The PFC is also involved in the control of the emotions, such as the control of the facial expressions, the control",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nIn what was does an LSTM mimic the prefrontal cortex?\n\nGround Truth Answer:\n\n\nAnswer A:\nthe human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output. Explanation: In the language subsystem, we use an LSTM layer to mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output, instead of traditional softmax classifier.\n\nAnswer B:\n<=> Explanation: The PFC is a part of the prefrontal cortex, which is the most important part of the human brain. It is located in the anterior part of the frontal lobe, and is the most important part of the human brain. The PFC is responsible for the executive functions, such as attention, working memory, planning, and decision making. The PFC is also involved in the control of the body, such as the control of the heart, breathing, and the body temperature. The PFC is also involved in the control of the emotions, such as the control of the facial expressions, the control\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.07562",
    "question": "In what way does an LSTM mimic the intra parietal sulcus?",
    "gold_answer": "",
    "answer_a": "mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output. Explanation: In the language subsystem, we use an LSTM layer to mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output, instead of traditional softmax classifier.",
    "answer_b": "What is the difference between the two?\n<sep><EXPLANATION> The PFC is a part of the human brain that is responsible for the processing of visual information. It is a part of the prefrontal cortex, which is located in the front of the brain. The PFC is responsible for the processing of visual information, such as the recognition of faces, objects, and other visual stimuli. The PFC is also responsible for the processing of language, such as the recognition of words and sentences. The PFC is a part of the human brain that is responsible for the processing of visual information. It is a part of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nIn what way does an LSTM mimic the intra parietal sulcus?\n\nGround Truth Answer:\n\n\nAnswer A:\nmimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output. Explanation: In the language subsystem, we use an LSTM layer to mimic the human IPS to extract the quantity information from language text and proposed a biologically plausible textizer to produce text symbols output, instead of traditional softmax classifier.\n\nAnswer B:\nWhat is the difference between the two?\n<sep><EXPLANATION> The PFC is a part of the human brain that is responsible for the processing of visual information. It is a part of the prefrontal cortex, which is located in the front of the brain. The PFC is responsible for the processing of visual information, such as the recognition of faces, objects, and other visual stimuli. The PFC is also responsible for the processing of language, such as the recognition of words and sentences. The PFC is a part of the human brain that is responsible for the processing of visual information. It is a part of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.07562",
    "question": "How do the authors define imagination, or imagined scenarios?",
    "gold_answer": "",
    "answer_a": "What is the difference between imagination and fantasy?\n<sep><hr>Answer: The imagination is a process of imagining a scenario that is not real, and the fantasy is a process of imagining a scenario that is real. The imagination is a process of imagining a scenario that is not real, and the fantasy is a process of imagining a scenario that is real. The imagination is a process of imagining a scenario that is not real, and the fantasy is a process of imagining a scenario that is real. The imagination is a process of imagining a scenario that is not real, and the fantasy is a process of imagining a scenario that is real.",
    "answer_b": "Ability to change the answering contents by considering the consequence of the next few output sentences.. Explanation: With imagination, people can change the answering contents (or even tell good-will lies) by considering or imagining the consequence of the next few output sentences.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do the authors define imagination, or imagined scenarios?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between imagination and fantasy?\n<sep><hr>Answer: The imagination is a process of imagining a scenario that is not real, and the fantasy is a process of imagining a scenario that is real. The imagination is a process of imagining a scenario that is not real, and the fantasy is a process of imagining a scenario that is real. The imagination is a process of imagining a scenario that is not real, and the fantasy is a process of imagining a scenario that is real. The imagination is a process of imagining a scenario that is not real, and the fantasy is a process of imagining a scenario that is real.\n\nAnswer B:\nAbility to change the answering contents by considering the consequence of the next few output sentences.. Explanation: With imagination, people can change the answering contents (or even tell good-will lies) by considering or imagining the consequence of the next few output sentences.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12893",
    "question": "Which classifiers did they experiment with?",
    "gold_answer": "",
    "answer_a": "What were their results?\n< Explanation: We experimented with three different classifiers: a simple bag-of-words classifier, a simple neural network classifier, and a simple SVM classifier. We also experimented with a simple neural network classifier that used a bag-of-words model as a feature extractor. We also experimented with a simple neural network classifier that used a bag-of-words model as a feature extractor. We also experimented with a simple neural network classifier that used a bag-of-words model as a feature extractor. We also experimented with a simple neural network classifier that used a bag-of-words model as a feature extractor. We also experimented with a",
    "answer_b": "logistic regression classifier. Explanation: We then built a logistic regression classifier (with no regularization) per language using the annotated edits and their labels.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich classifiers did they experiment with?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat were their results?\n< Explanation: We experimented with three different classifiers: a simple bag-of-words classifier, a simple neural network classifier, and a simple SVM classifier. We also experimented with a simple neural network classifier that used a bag-of-words model as a feature extractor. We also experimented with a simple neural network classifier that used a bag-of-words model as a feature extractor. We also experimented with a simple neural network classifier that used a bag-of-words model as a feature extractor. We also experimented with a simple neural network classifier that used a bag-of-words model as a feature extractor. We also experimented with a\n\nAnswer B:\nlogistic regression classifier. Explanation: We then built a logistic regression classifier (with no regularization) per language using the annotated edits and their labels.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12893",
    "question": "How did they identify what language the text was?",
    "gold_answer": "",
    "answer_a": "What features did they use?\n< Explanation: We first used a simple rule-based approach to identify the language of the text. We used the following rules to identify the language of the text:\n\n1. If the text contains a word that is not in the English dictionary, then the language is English.\n\n2. If the text contains a word that is in the English dictionary, then the language is not English.\n\n3. If the text contains a word that is in the English dictionary and the language is English, then the language is English.\n\n4. If the text contains a word that is not in the English dictionary, then the language is",
    "answer_b": "used NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks) BIBREF17 that supports human languages as well as programming languages. Explanation: Due to its nature, repositories on GitHub contain a large amount of code (in programming language) as well as natural language texts. We used NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks) BIBREF17 that supports human languages as well as programming languages. Specifically, we ran the language detector against both the source and the target and discarded all the edits where either is determined as written in a non-human language.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow did they identify what language the text was?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat features did they use?\n< Explanation: We first used a simple rule-based approach to identify the language of the text. We used the following rules to identify the language of the text:\n\n1. If the text contains a word that is not in the English dictionary, then the language is English.\n\n2. If the text contains a word that is in the English dictionary, then the language is not English.\n\n3. If the text contains a word that is in the English dictionary and the language is English, then the language is English.\n\n4. If the text contains a word that is not in the English dictionary, then the language is\n\nAnswer B:\nused NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks) BIBREF17 that supports human languages as well as programming languages. Explanation: Due to its nature, repositories on GitHub contain a large amount of code (in programming language) as well as natural language texts. We used NanigoNet, a language detector based on GCNNs (Gated Convolutional Neural Networks) BIBREF17 that supports human languages as well as programming languages. Specifically, we ran the language detector against both the source and the target and discarded all the edits where either is determined as written in a non-human language.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12893",
    "question": "Which repositories did they collect from?",
    "gold_answer": "",
    "answer_a": "Has at least one pull request or pull request review comment event between November 2017 and September 2019,\n\nHas 50 or more starts,\n\nHas a size between 1MB and 1GB, and\n\nHas a permissive license.. Explanation: Specifically, we collected every repository from GH Archive that:\n\nHas at least one pull request or pull request review comment event between November 2017 and September 2019,\n\nHas 50 or more starts,\n\nHas a size between 1MB and 1GB, and\n\nHas a permissive license.\n\nNote the \u201cand\u201d in the list above\u2014a repository needs to meet all the conditions mentioned above to be eligible. The first two criteria (pull request events and the number of starts) are a sign of a quality repository. As for the license, we allowed apache-2.0 (Apache License 2.0), mit (MIT License), bsd-3-clause (BSD 3-Clause License), bsd-2-clause (BSD 2-Clause License), cc0-1.0 (Creative Commons Zero v1.0 Universal), unlicense (Unlicense), cc-by-4.0 (Creative Commons Attribution 4.0), and bsl-1.0 (Boost Software License 1.0 (BSL-1.0). A repository's number of stars, size, and license are determined as of the event in the first condition.",
    "answer_b": "How many?\n< Explanation: The dataset is collected from the GitHub repository of the following 15 languages: English, French, German, Spanish, Italian, Portuguese, Russian, Japanese, Chinese, Korean, Arabic, Hebrew, and Turkish. The dataset is also available in English, French, German, Spanish, Italian, Portuguese, Russian, Japanese, Chinese, Korean, Arabic, Hebrew, and Turkish. The dataset is available in English, French, German, Spanish, Italian, Portuguese, Russian, Japanese, Chinese, Korean, Arabic, Hebrew, and Turkish. The dataset is available in English, French, German, Spanish, Italian",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich repositories did they collect from?\n\nGround Truth Answer:\n\n\nAnswer A:\nHas at least one pull request or pull request review comment event between November 2017 and September 2019,\n\nHas 50 or more starts,\n\nHas a size between 1MB and 1GB, and\n\nHas a permissive license.. Explanation: Specifically, we collected every repository from GH Archive that:\n\nHas at least one pull request or pull request review comment event between November 2017 and September 2019,\n\nHas 50 or more starts,\n\nHas a size between 1MB and 1GB, and\n\nHas a permissive license.\n\nNote the \u201cand\u201d in the list above\u2014a repository needs to meet all the conditions mentioned above to be eligible. The first two criteria (pull request events and the number of starts) are a sign of a quality repository. As for the license, we allowed apache-2.0 (Apache License 2.0), mit (MIT License), bsd-3-clause (BSD 3-Clause License), bsd-2-clause (BSD 2-Clause License), cc0-1.0 (Creative Commons Zero v1.0 Universal), unlicense (Unlicense), cc-by-4.0 (Creative Commons Attribution 4.0), and bsl-1.0 (Boost Software License 1.0 (BSL-1.0). A repository's number of stars, size, and license are determined as of the event in the first condition.\n\nAnswer B:\nHow many?\n< Explanation: The dataset is collected from the GitHub repository of the following 15 languages: English, French, German, Spanish, Italian, Portuguese, Russian, Japanese, Chinese, Korean, Arabic, Hebrew, and Turkish. The dataset is also available in English, French, German, Spanish, Italian, Portuguese, Russian, Japanese, Chinese, Korean, Arabic, Hebrew, and Turkish. The dataset is available in English, French, German, Spanish, Italian, Portuguese, Russian, Japanese, Chinese, Korean, Arabic, Hebrew, and Turkish. The dataset is available in English, French, German, Spanish, Italian\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12893",
    "question": "Which three features do they use?",
    "gold_answer": "",
    "answer_a": "mechanical, spell, and grammatical edits. Explanation: We note that the first three (mechanical, spell, and grammatical edits, also called typos) are within the scope of the dataset we build, while the last one (semantic edits) is not.",
    "answer_b": "What is the F1 score?\n< Explanation: We use the following three features to filter out misspellings:\n\n1. The length of the misspelled word. We filter out misspellings with length less than 3 characters. This is because the length of the misspelled word is a strong indicator of the type of misspelling. For example, the misspelling \"the\" is a typo for \"the\", but the misspelling \"the\" is not a typo for \"the\" because the length of the misspelled word is 4 characters. Therefore, we filter out misspellings with length",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich three features do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nmechanical, spell, and grammatical edits. Explanation: We note that the first three (mechanical, spell, and grammatical edits, also called typos) are within the scope of the dataset we build, while the last one (semantic edits) is not.\n\nAnswer B:\nWhat is the F1 score?\n< Explanation: We use the following three features to filter out misspellings:\n\n1. The length of the misspelled word. We filter out misspellings with length less than 3 characters. This is because the length of the misspelled word is a strong indicator of the type of misspelling. For example, the misspelling \"the\" is a typo for \"the\", but the misspelling \"the\" is not a typo for \"the\" because the length of the misspelled word is 4 characters. Therefore, we filter out misspellings with length\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12893",
    "question": "Which languages are covered in the corpus?",
    "gold_answer": "",
    "answer_a": "the top 10 languages are English, simplified Chinese, Japanese, Russian, French, German, Portuguese, Spanish, Korean and Hindi. Explanation: FLOAT SELECTED: Table 3: Statistics of the dataset (top 10 languages)",
    "answer_b": "How many edits are there in total?\n< Explanation: The dataset contains 15 languages, with 11 languages having more than 100,000 edits. The dataset is divided into 1000 edits per language, with 100 edits per language in total. The dataset contains 350,000 edits in total. The dataset is available at https://github.com/alexanderkowalski/typo-corpus. The dataset is divided into 1000 edits per language, with 100 edits per language in total. The dataset contains 3",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich languages are covered in the corpus?\n\nGround Truth Answer:\n\n\nAnswer A:\nthe top 10 languages are English, simplified Chinese, Japanese, Russian, French, German, Portuguese, Spanish, Korean and Hindi. Explanation: FLOAT SELECTED: Table 3: Statistics of the dataset (top 10 languages)\n\nAnswer B:\nHow many edits are there in total?\n< Explanation: The dataset contains 15 languages, with 11 languages having more than 100,000 edits. The dataset is divided into 1000 edits per language, with 100 edits per language in total. The dataset contains 350,000 edits in total. The dataset is available at https://github.com/alexanderkowalski/typo-corpus. The dataset is divided into 1000 edits per language, with 100 edits per language in total. The dataset contains 3\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.07973",
    "question": "Which BERT layers were combined to boost performance?",
    "gold_answer": "",
    "answer_a": "What were the results?\n<sep>Figure 1: BERT embeddings are evaluated against two state-of-the-art models on downstream and probing tasks from SentEval. The results are shown in bold. The left column shows the performance on the factoid QA dataset, while the right column shows the performance on the non-factoid QA dataset. The results are averaged over 10 runs. The bolded results are the best results on the two datasets. The bolded results are the best results on the two datasets. The bolded results are the best results on the two datasets. The bolded results are the best results on the",
    "answer_b": "Top and bottom layers. Explanation: Inspired by the fact that embeddings from different layers excel in different tasks, we also conducted experiments by concatenating embeddings from multiple layers. The results are presented in Table TABREF3, and the raw values are provided in the Appendix.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich BERT layers were combined to boost performance?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat were the results?\n<sep>Figure 1: BERT embeddings are evaluated against two state-of-the-art models on downstream and probing tasks from SentEval. The results are shown in bold. The left column shows the performance on the factoid QA dataset, while the right column shows the performance on the non-factoid QA dataset. The results are averaged over 10 runs. The bolded results are the best results on the two datasets. The bolded results are the best results on the two datasets. The bolded results are the best results on the two datasets. The bolded results are the best results on the\n\nAnswer B:\nTop and bottom layers. Explanation: Inspired by the fact that embeddings from different layers excel in different tasks, we also conducted experiments by concatenating embeddings from multiple layers. The results are presented in Table TABREF3, and the raw values are provided in the Appendix.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.07973",
    "question": "Which NLI data was used to improve the quality of the embeddings?",
    "gold_answer": "",
    "answer_a": "MNLI BIBREF11. Explanation: Two natural language inference datasets, MNLI BIBREF11 and SNLI, were considered in the experiment.",
    "answer_b": "How was the quality of the embeddings improved?\n<sep>We use the NLI dataset from the Stanford NLI Task 2.0 dataset collection. We use the 2017 version of the dataset. The dataset contains 10,000 pairs of question and answer pairs. The dataset is split into training, validation, and test sets. The training set contains 5,000 pairs, the validation set contains 1,000 pairs, and the test set contains 1,000 pairs. The training set is used for training the BERT model, the validation set is used",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich NLI data was used to improve the quality of the embeddings?\n\nGround Truth Answer:\n\n\nAnswer A:\nMNLI BIBREF11. Explanation: Two natural language inference datasets, MNLI BIBREF11 and SNLI, were considered in the experiment.\n\nAnswer B:\nHow was the quality of the embeddings improved?\n<sep>We use the NLI dataset from the Stanford NLI Task 2.0 dataset collection. We use the 2017 version of the dataset. The dataset contains 10,000 pairs of question and answer pairs. The dataset is split into training, validation, and test sets. The training set contains 5,000 pairs, the validation set contains 1,000 pairs, and the test set contains 1,000 pairs. The training set is used for training the BERT model, the validation set is used\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.07973",
    "question": "Which four QA datasets are examined?",
    "gold_answer": "",
    "answer_a": "What are the evaluation metrics?\n<sep><font size=\"4\">Answer: We evaluate the performance of the BERT embeddings on four different QA datasets: FactoidQA, FactoidQA-100, FactoidQA-100-1000, and FactoidQA-100-1000-1000. We use the following evaluation metrics: Recall, Precision, F1, and F1@100. We also report the results of the in-domain fine-tuning BERT on the FactoidQA-100-1000 dataset. </",
    "answer_b": "(1) WikiPassageQA BIBREF13. Explanation: We experimented on four datasets: (1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, and (4) SearchQA BIBREF16.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich four QA datasets are examined?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the evaluation metrics?\n<sep><font size=\"4\">Answer: We evaluate the performance of the BERT embeddings on four different QA datasets: FactoidQA, FactoidQA-100, FactoidQA-100-1000, and FactoidQA-100-1000-1000. We use the following evaluation metrics: Recall, Precision, F1, and F1@100. We also report the results of the in-domain fine-tuning BERT on the FactoidQA-100-1000 dataset. </\n\nAnswer B:\n(1) WikiPassageQA BIBREF13. Explanation: We experimented on four datasets: (1) WikiPassageQA BIBREF13, (2) InsuranceQA (version 1.0) BIBREF14, (3) Quasar-t BIBREF15, and (4) SearchQA BIBREF16.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.07973",
    "question": "Which two tasks from SentEval are the sentence embeddings evaluated against?",
    "gold_answer": "",
    "answer_a": "Which two tasks are the passage embeddings evaluated against?\nFor the first two tasks, we evaluate the embeddings against the following datasets: SentEval-1, SentEval-2, SentEval-3, SentEval-4, SentEval-5, SentEval-6, SentEval-7, SentEval-8, SentEval-9, SentEval-10, SentEval-11, SentEval-12, SentEval-13, SentEval-14, SentEval-15, SentEval-16, SentEval-17, SentEval-18, SentEval-1",
    "answer_b": "Downstream tasks include text classification, natural language inference, paraphrase detection, and semantic similarity. Explanation: The evaluation encompasses a variety of downstream and probing tasks. Downstream tasks include text classification, natural language inference, paraphrase detection, and semantic similarity. Probing tasks use single sentence embedding as input, are designed to probe sentence-level linguistic phenomena, from superficial properties of sentences to syntactic information to semantic acceptability.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich two tasks from SentEval are the sentence embeddings evaluated against?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhich two tasks are the passage embeddings evaluated against?\nFor the first two tasks, we evaluate the embeddings against the following datasets: SentEval-1, SentEval-2, SentEval-3, SentEval-4, SentEval-5, SentEval-6, SentEval-7, SentEval-8, SentEval-9, SentEval-10, SentEval-11, SentEval-12, SentEval-13, SentEval-14, SentEval-15, SentEval-16, SentEval-17, SentEval-18, SentEval-1\n\nAnswer B:\nDownstream tasks include text classification, natural language inference, paraphrase detection, and semantic similarity. Explanation: The evaluation encompasses a variety of downstream and probing tasks. Downstream tasks include text classification, natural language inference, paraphrase detection, and semantic similarity. Probing tasks use single sentence embedding as input, are designed to probe sentence-level linguistic phenomena, from superficial properties of sentences to syntactic information to semantic acceptability.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.08868",
    "question": "what classifiers did they train?",
    "gold_answer": "",
    "answer_a": "How did they evaluate their models?\n< Explanation: We trained a supervised machine learning model, based on a range of linguistic features, that can reliably classify texts according to their difficulty level. Our model obtained an accuracy of 81.3% and an F-score of 0.8, which is comparable to the state of the art in English and is considerably higher than previously reported results for other languages. We further studied the utility of our features with single sentences instead of full texts since sentences are a common linguistic unit in language learning exercises. We trained a separate model on sentence-level data with five classes, which yielded 63.",
    "answer_b": "a multinomial logistic regression model with ridge estimator. Explanation: We explored different classification algorithms for this task using the machine learning toolkit WEKA BIBREF24 . These included: (1) a multinomial logistic regression model with ridge estimator, (2) a multilayer perceptron, (3) a support vector machine learner, Sequential Minimal Optimization (SMO), and (4) a decision tree (J48). For each of these, the default parameter settings have been used as implemented in WEKA.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat classifiers did they train?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow did they evaluate their models?\n< Explanation: We trained a supervised machine learning model, based on a range of linguistic features, that can reliably classify texts according to their difficulty level. Our model obtained an accuracy of 81.3% and an F-score of 0.8, which is comparable to the state of the art in English and is considerably higher than previously reported results for other languages. We further studied the utility of our features with single sentences instead of full texts since sentences are a common linguistic unit in language learning exercises. We trained a separate model on sentence-level data with five classes, which yielded 63.\n\nAnswer B:\na multinomial logistic regression model with ridge estimator. Explanation: We explored different classification algorithms for this task using the machine learning toolkit WEKA BIBREF24 . These included: (1) a multinomial logistic regression model with ridge estimator, (2) a multilayer perceptron, (3) a support vector machine learner, Sequential Minimal Optimization (SMO), and (4) a decision tree (J48). For each of these, the default parameter settings have been used as implemented in WEKA.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.08868",
    "question": "what dataset did they use?",
    "gold_answer": "",
    "answer_a": "subset of COCTAILL. Explanation: Our dataset is a subset of COCTAILL, a corpus of course books covering five CEFR levels (A1-C1) BIBREF19 .",
    "answer_b": "Explanation: We used the Swedish corpus of the Swedish Language Learning Platform (SCLP) (SCLP 2017) to train our models. The corpus consists of 10,000 sentences, 10,000 words, and 10,000 characters. The corpus was downloaded from the SCLP website and the sentences were manually annotated with the Swedish LEXICOR software. The corpus was further cleaned by removing sentences with more than 1000 words, sentences with more than 1000 characters, and sentences with more than 100",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat dataset did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nsubset of COCTAILL. Explanation: Our dataset is a subset of COCTAILL, a corpus of course books covering five CEFR levels (A1-C1) BIBREF19 .\n\nAnswer B:\nExplanation: We used the Swedish corpus of the Swedish Language Learning Platform (SCLP) (SCLP 2017) to train our models. The corpus consists of 10,000 sentences, 10,000 words, and 10,000 characters. The corpus was downloaded from the SCLP website and the sentences were manually annotated with the Swedish LEXICOR software. The corpus was further cleaned by removing sentences with more than 1000 words, sentences with more than 1000 characters, and sentences with more than 100\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.08868",
    "question": "what combination of features helped improve the classification?",
    "gold_answer": "",
    "answer_a": "Using all the 61 features helped them improve the classification. Explanation: In our experiments, using only word-frequency features was almost as predictive as a combination of all features for the document level, but the latter made more accurate predictions for sentences, resulting in a 7% difference in accuracy.",
    "answer_b": "Explanation: We found that using a combination of different features, compared to using lexical features alone, resulted in 7% improvement in classification accuracy at the sentence level, whereas at the document level, lexical features were more dominant. We also found that using a combination of different features, compared to using lexical features alone, resulted in 7% improvement in classification accuracy at the sentence level, whereas at the document level, lexical features were more dominant. We also found that using a combination of different features, compared to using lexical features alone, resulted in 7% improvement in classification accuracy at the sentence level, whereas at the document level, lexical",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat combination of features helped improve the classification?\n\nGround Truth Answer:\n\n\nAnswer A:\nUsing all the 61 features helped them improve the classification. Explanation: In our experiments, using only word-frequency features was almost as predictive as a combination of all features for the document level, but the latter made more accurate predictions for sentences, resulting in a 7% difference in accuracy.\n\nAnswer B:\nExplanation: We found that using a combination of different features, compared to using lexical features alone, resulted in 7% improvement in classification accuracy at the sentence level, whereas at the document level, lexical features were more dominant. We also found that using a combination of different features, compared to using lexical features alone, resulted in 7% improvement in classification accuracy at the sentence level, whereas at the document level, lexical features were more dominant. We also found that using a combination of different features, compared to using lexical features alone, resulted in 7% improvement in classification accuracy at the sentence level, whereas at the document level, lexical\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.08868",
    "question": "what linguistics features did they apply?",
    "gold_answer": "",
    "answer_a": "length-based. Explanation: We developed our features based on information both from previous literature BIBREF9 , BIBREF3 , BIBREF13 , BIBREF4 , BIBREF8 and a grammar book for Swedish L2 learners BIBREF20 . The set of features can be divided in the following five subgroups: length-based, lexical, morphological, syntactic and semantic features (Table TABREF6 ).",
    "answer_b": "Explanation: We used the following features: (1) the number of words, (2) the number of sentences, (3) the number of sentences per paragraph, (4) the number of sentences per page, (5) the number of sentences per page per paragraph, (6) the number of sentences per page per paragraph per paragraph, (7) the number of sentences per page per paragraph per paragraph per paragraph, (8) the number of sentences per page per paragraph per paragraph per paragraph per paragraph, (9) the number of sentences per page per paragraph per paragraph per paragraph per paragraph per paragraph, (10)",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat linguistics features did they apply?\n\nGround Truth Answer:\n\n\nAnswer A:\nlength-based. Explanation: We developed our features based on information both from previous literature BIBREF9 , BIBREF3 , BIBREF13 , BIBREF4 , BIBREF8 and a grammar book for Swedish L2 learners BIBREF20 . The set of features can be divided in the following five subgroups: length-based, lexical, morphological, syntactic and semantic features (Table TABREF6 ).\n\nAnswer B:\nExplanation: We used the following features: (1) the number of words, (2) the number of sentences, (3) the number of sentences per paragraph, (4) the number of sentences per page, (5) the number of sentences per page per paragraph, (6) the number of sentences per page per paragraph per paragraph, (7) the number of sentences per page per paragraph per paragraph per paragraph, (8) the number of sentences per page per paragraph per paragraph per paragraph per paragraph, (9) the number of sentences per page per paragraph per paragraph per paragraph per paragraph per paragraph, (10)\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.08868",
    "question": "what is the state of the art in English?",
    "gold_answer": "",
    "answer_a": "BIBREF9 , BIBREF12. Explanation: Although a direct comparison with other studies is difficult because of the target language, the nature of the datasets and the number of classes used, in terms of absolute numbers, our model achieves comparable performance with the state-of-the-art systems for English BIBREF9 , BIBREF12 .",
    "answer_b": "What is the difference between the two approaches?\n< Explanation: The state of the art in English is the LEX model, which is a supervised machine learning model that uses a range of features to predict the LIX score. The LEX model was trained on a corpus of 1000 sentences and achieved an accuracy of 80.5%. The LEX model was then used to predict the LIX score for 1000 new sentences, which resulted in an accuracy of 81.3%. The LEX model was also used to predict the LIX score for 1000 new sentences from",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the state of the art in English?\n\nGround Truth Answer:\n\n\nAnswer A:\nBIBREF9 , BIBREF12. Explanation: Although a direct comparison with other studies is difficult because of the target language, the nature of the datasets and the number of classes used, in terms of absolute numbers, our model achieves comparable performance with the state-of-the-art systems for English BIBREF9 , BIBREF12 .\n\nAnswer B:\nWhat is the difference between the two approaches?\n< Explanation: The state of the art in English is the LEX model, which is a supervised machine learning model that uses a range of features to predict the LIX score. The LEX model was trained on a corpus of 1000 sentences and achieved an accuracy of 80.5%. The LEX model was then used to predict the LIX score for 1000 new sentences, which resulted in an accuracy of 81.3%. The LEX model was also used to predict the LIX score for 1000 new sentences from\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.01340",
    "question": "What type of model were the features used in?",
    "gold_answer": "",
    "answer_a": "What are the features?\n< Explanation: We use the following features to identify the online trolls: (1) thematic features: the number of words, the number of unique words, the number of unique words per tweet, the number of unique words per tweet and the number of unique words per tweet in the text. (2) profiling features: the number of hashtags, the number of hashtags per tweet, the number of hashtags per tweet and the number of hashtags per tweet in the text. (3) the number of mentions, the number of mentions per tweet, the number of mentions per tweet and the number of mentions per tweet in the",
    "answer_b": "character-based Bidirectional Gated Recurrent neural network. Explanation: In order to evaluate our feature set, we use Random Selection, Majority Class, and bag-of-words baselines. In the bag-of-words baseline, we aggregate all the tweets of a user into one document. A previous work BIBREF30 showed that IRA trolls were playing a hashtag game which is a popular word game played on Twitter, where users add a hashtag to their tweets and then answer an implied question BIBREF31. IRA trolls used this game in a similar way but focusing more on offending or attacking others; an example from IRA tweets: \"#OffendEveryoneIn4Words undocumented immigrants are ILLEGALS\". Thus, we use as a baseline Tweet2vec BIBREF32 which is a a character-based Bidirectional Gated Recurrent neural network reads tweets and predicts their hashtags. We aim to assess if the tweets hashtags can help identifying the IRA tweets. The model reads the tweets in a form of character one-hot encodings and uses them for training with their hashtags as labels. To train the model, we use our collected dataset which consists of $\\sim $3.7M tweets. To represent the tweets in this baseline, we use the decoded embedding produced by the model and we feed them to the Logistic Regression classifier.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of model were the features used in?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the features?\n< Explanation: We use the following features to identify the online trolls: (1) thematic features: the number of words, the number of unique words, the number of unique words per tweet, the number of unique words per tweet and the number of unique words per tweet in the text. (2) profiling features: the number of hashtags, the number of hashtags per tweet, the number of hashtags per tweet and the number of hashtags per tweet in the text. (3) the number of mentions, the number of mentions per tweet, the number of mentions per tweet and the number of mentions per tweet in the\n\nAnswer B:\ncharacter-based Bidirectional Gated Recurrent neural network. Explanation: In order to evaluate our feature set, we use Random Selection, Majority Class, and bag-of-words baselines. In the bag-of-words baseline, we aggregate all the tweets of a user into one document. A previous work BIBREF30 showed that IRA trolls were playing a hashtag game which is a popular word game played on Twitter, where users add a hashtag to their tweets and then answer an implied question BIBREF31. IRA trolls used this game in a similar way but focusing more on offending or attacking others; an example from IRA tweets: \"#OffendEveryoneIn4Words undocumented immigrants are ILLEGALS\". Thus, we use as a baseline Tweet2vec BIBREF32 which is a a character-based Bidirectional Gated Recurrent neural network reads tweets and predicts their hashtags. We aim to assess if the tweets hashtags can help identifying the IRA tweets. The model reads the tweets in a form of character one-hot encodings and uses them for training with their hashtags as labels. To train the model, we use our collected dataset which consists of $\\sim $3.7M tweets. To represent the tweets in this baseline, we use the decoded embedding produced by the model and we feed them to the Logistic Regression classifier.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.01340",
    "question": "What unsupervised approach was used to deduce the thematic information?",
    "gold_answer": "",
    "answer_a": "What are the textual features used?\n< Explanation: We first used the TF-IDF vectorizer to extract the textual features. Then, we used the bag of words model to extract the thematic information. The bag of words model is a simple model that uses the frequency of the words in the text as the basis for the model. The TF-IDF vectorizer is a powerful tool that can be used to extract the textual features. The TF-IDF vectorizer is a powerful tool that can be used to extract the textual features. The TF-IDF vectorizer is a powerful tool that can be used to extract the textual features. The TF",
    "answer_b": "Latent Dirichlet Allocation (LDA). Explanation: Later on, Twitter declared that they had detected a suspicious campaign originated in Russia by an organization named Internet Research Agency (IRA), and targeted the US to affect the results of the 2016 presidential elections.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat unsupervised approach was used to deduce the thematic information?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the textual features used?\n< Explanation: We first used the TF-IDF vectorizer to extract the textual features. Then, we used the bag of words model to extract the thematic information. The bag of words model is a simple model that uses the frequency of the words in the text as the basis for the model. The TF-IDF vectorizer is a powerful tool that can be used to extract the textual features. The TF-IDF vectorizer is a powerful tool that can be used to extract the textual features. The TF-IDF vectorizer is a powerful tool that can be used to extract the textual features. The TF\n\nAnswer B:\nLatent Dirichlet Allocation (LDA). Explanation: Later on, Twitter declared that they had detected a suspicious campaign originated in Russia by an organization named Internet Research Agency (IRA), and targeted the US to affect the results of the 2016 presidential elections.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.01340",
    "question": "What profile features are used?",
    "gold_answer": "",
    "answer_a": "What are the best performing ones?\n< Explanation: We use the following profile features to identify the trolls: 1) the number of tweets with the same profile as the target account; 2) the number of tweets with the same profile as the target account and the number of tweets with the same profile as the target account and the number of tweets with the same profile as the target account and the number of tweets with the same profile as the target account and the number of tweets with the same profile as the target account and the number of tweets with the same profile as the target account and the number of tweets with the same profile as the",
    "answer_b": "Part-of-speech tags (POS). Explanation: Later on, Twitter declared that they had detected a suspicious campaign originated in Russia by an organization named Internet Research Agency (IRA), and targeted the US to affect the results of the 2016 presidential elections.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat profile features are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the best performing ones?\n< Explanation: We use the following profile features to identify the trolls: 1) the number of tweets with the same profile as the target account; 2) the number of tweets with the same profile as the target account and the number of tweets with the same profile as the target account and the number of tweets with the same profile as the target account and the number of tweets with the same profile as the target account and the number of tweets with the same profile as the target account and the number of tweets with the same profile as the target account and the number of tweets with the same profile as the\n\nAnswer B:\nPart-of-speech tags (POS). Explanation: Later on, Twitter declared that they had detected a suspicious campaign originated in Russia by an organization named Internet Research Agency (IRA), and targeted the US to affect the results of the 2016 presidential elections.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.01340",
    "question": "What textual features are used?",
    "gold_answer": "",
    "answer_a": "What are the benefits of using them?\n< Explanation: We use the following textual features: (1) the number of words in the tweet, (2) the number of hashtags, (3) the number of mentions, (4) the number of emojis, (5) the number of punctuations, (6) the number of special characters, (7) the number of URLs, (8) the number of URLs, (9) the number of URLs, (10) the number of URLs, (11) the number of URLs, (12) the number of URLs, (13) the number",
    "answer_b": "eight Plutchik's emotions. Explanation: Based on our thematic information, we model the users textual features w.r.t. each of these themes.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat textual features are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the benefits of using them?\n< Explanation: We use the following textual features: (1) the number of words in the tweet, (2) the number of hashtags, (3) the number of mentions, (4) the number of emojis, (5) the number of punctuations, (6) the number of special characters, (7) the number of URLs, (8) the number of URLs, (9) the number of URLs, (10) the number of URLs, (11) the number of URLs, (12) the number of URLs, (13) the number\n\nAnswer B:\neight Plutchik's emotions. Explanation: Based on our thematic information, we model the users textual features w.r.t. each of these themes.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.09749",
    "question": "what other representations do they compare with?",
    "gold_answer": "",
    "answer_a": "What are the results?\nThe experiments show that KeyVec outperforms the baseline models in both tasks. The results are encouraging and show that the proposed model can be used to learn distributed representations for text documents.\n\nThe experiments show that KeyVec outperforms the baseline models in both tasks. The results are encouraging and show that the proposed model can be used to learn distributed representations for text documents. The results are encouraging and show that the proposed model can be used to learn distributed representations for text documents. The results are encouraging and show that the proposed model can be used to learn distributed representations for text documents. The results are encouraging and show that",
    "answer_b": "word2vec averaging. Explanation: Table TABREF15 presents P@10, MAP and MRR results of our KeyVec model and competing embedding methods in academic paper retrieval. word2vec averaging generates an embedding for a document by averaging the word2vec vectors of its constituent words. In the experiment, we used two different versions of word2vec: one from public release, and the other one trained specifically on our own academic corpus (113 GB). From Table TABREF15 , we observe that as a document-embedding model, Paragraph Vector gave better retrieval results than word2vec averagings did.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat other representations do they compare with?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\nThe experiments show that KeyVec outperforms the baseline models in both tasks. The results are encouraging and show that the proposed model can be used to learn distributed representations for text documents.\n\nThe experiments show that KeyVec outperforms the baseline models in both tasks. The results are encouraging and show that the proposed model can be used to learn distributed representations for text documents. The results are encouraging and show that the proposed model can be used to learn distributed representations for text documents. The results are encouraging and show that the proposed model can be used to learn distributed representations for text documents. The results are encouraging and show that\n\nAnswer B:\nword2vec averaging. Explanation: Table TABREF15 presents P@10, MAP and MRR results of our KeyVec model and competing embedding methods in academic paper retrieval. word2vec averaging generates an embedding for a document by averaging the word2vec vectors of its constituent words. In the experiment, we used two different versions of word2vec: one from public release, and the other one trained specifically on our own academic corpus (113 GB). From Table TABREF15 , we observe that as a document-embedding model, Paragraph Vector gave better retrieval results than word2vec averagings did.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.09749",
    "question": "what empirical evaluations performed?",
    "gold_answer": "",
    "answer_a": "document retrieval. Explanation: To verify the effectiveness, we evaluate the KeyVec model on two text understanding tasks that take continuous distributed vectors as the representations for documents: document retrieval and document clustering.",
    "answer_b": "< Explanation: We evaluate the performance of KeyVec on two different tasks: (1) document understanding and (2) text classification. We use the Stanford CoreNLP toolkit to train and evaluate the models. We use the Stanford CoreNLP toolkit to train and evaluate the models. We use the Stanford CoreNLP toolkit to train and evaluate the models. We use the Stanford CoreNLP toolkit to train and evaluate the models. We use the Stanford CoreNLP toolkit to train and evaluate the models. We use the Stanford CoreNLP toolkit to train and evaluate the models. We use the Stanford CoreNLP toolkit to train",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat empirical evaluations performed?\n\nGround Truth Answer:\n\n\nAnswer A:\ndocument retrieval. Explanation: To verify the effectiveness, we evaluate the KeyVec model on two text understanding tasks that take continuous distributed vectors as the representations for documents: document retrieval and document clustering.\n\nAnswer B:\n< Explanation: We evaluate the performance of KeyVec on two different tasks: (1) document understanding and (2) text classification. We use the Stanford CoreNLP toolkit to train and evaluate the models. We use the Stanford CoreNLP toolkit to train and evaluate the models. We use the Stanford CoreNLP toolkit to train and evaluate the models. We use the Stanford CoreNLP toolkit to train and evaluate the models. We use the Stanford CoreNLP toolkit to train and evaluate the models. We use the Stanford CoreNLP toolkit to train and evaluate the models. We use the Stanford CoreNLP toolkit to train\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.09749",
    "question": "which document understanding tasks did they evaluate on?",
    "gold_answer": "",
    "answer_a": "document retrieval. Explanation: To verify the effectiveness, we evaluate the KeyVec model on two text understanding tasks that take continuous distributed vectors as the representations for documents: document retrieval and document clustering.",
    "answer_b": "What are the results?\nThe experiments are conducted on two different document understanding tasks: (1) text classification, and (2) text summarization. For text classification, we use the Stanford Sentiment Treebank (SST) as the training set and the Stanford Sentiment Treebank (SST) as the test set. For text summarization, we use the Wikipedia article summarization task as the training set and the Wikipedia article summarization task as the test set. The results are shown in Table 1. The results show that KeyVec models outperform the baseline models in both tasks. The results are also encouraging for the downstream",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhich document understanding tasks did they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\ndocument retrieval. Explanation: To verify the effectiveness, we evaluate the KeyVec model on two text understanding tasks that take continuous distributed vectors as the representations for documents: document retrieval and document clustering.\n\nAnswer B:\nWhat are the results?\nThe experiments are conducted on two different document understanding tasks: (1) text classification, and (2) text summarization. For text classification, we use the Stanford Sentiment Treebank (SST) as the training set and the Stanford Sentiment Treebank (SST) as the test set. For text summarization, we use the Wikipedia article summarization task as the training set and the Wikipedia article summarization task as the test set. The results are shown in Table 1. The results show that KeyVec models outperform the baseline models in both tasks. The results are also encouraging for the downstream\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.09749",
    "question": "what dataset was used?",
    "gold_answer": "",
    "answer_a": "669 academic papers published by IEEE. Explanation: In the experiments, our document pool contained 669 academic papers published by IEEE, from which top- INLINEFORM0 relevant papers are retrieved. We created 70 search queries, each composed of the text in a Wikipedia page on a field of study (e.g., https://en.wikipedia.org/wiki/Deep_learning). We retrieved relevant papers based on cosine similarity between document embeddings of 100 dimensions for Wikipedia pages and academic papers.",
    "answer_b": "< Explanation: We use the 20 Newsgroups dataset (Newsgroups) from the UCI repository (Udell et al., 2005) to evaluate the performance of KeyVec. The dataset consists of 20,000 documents, each containing 1000 words. The documents are labeled as either spam or non-spam. The dataset is divided into training and test sets. The training set consists of 10,000 documents, and the test set consists of 1,000 documents. The training set is used to train the model, and",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat dataset was used?\n\nGround Truth Answer:\n\n\nAnswer A:\n669 academic papers published by IEEE. Explanation: In the experiments, our document pool contained 669 academic papers published by IEEE, from which top- INLINEFORM0 relevant papers are retrieved. We created 70 search queries, each composed of the text in a Wikipedia page on a field of study (e.g., https://en.wikipedia.org/wiki/Deep_learning). We retrieved relevant papers based on cosine similarity between document embeddings of 100 dimensions for Wikipedia pages and academic papers.\n\nAnswer B:\n< Explanation: We use the 20 Newsgroups dataset (Newsgroups) from the UCI repository (Udell et al., 2005) to evaluate the performance of KeyVec. The dataset consists of 20,000 documents, each containing 1000 words. The documents are labeled as either spam or non-spam. The dataset is divided into training and test sets. The training set consists of 10,000 documents, and the test set consists of 1,000 documents. The training set is used to train the model, and\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.09244",
    "question": "What private companies are members of consortium?",
    "gold_answer": "",
    "answer_a": "Yes, the consortium is composed of the following companies:\npublic sector\nprivate sector\nuniversity\nresearch institute\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university",
    "answer_b": "Creditinfo, Grammatek, Mideind and Tiro. Explanation: FLOAT SELECTED: Table 1: Members of the S\u00cdM consortium for Icelandic LT",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat private companies are members of consortium?\n\nGround Truth Answer:\n\n\nAnswer A:\nYes, the consortium is composed of the following companies:\npublic sector\nprivate sector\nuniversity\nresearch institute\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\nresearch university\n\nAnswer B:\nCreditinfo, Grammatek, Mideind and Tiro. Explanation: FLOAT SELECTED: Table 1: Members of the S\u00cdM consortium for Icelandic LT\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.09244",
    "question": "What concrete software is planned to be developed by the end of the programme?",
    "gold_answer": "",
    "answer_a": "A lot of new software will be developed in all areas of the programme, some will be extensions of already available Greynir software.. Explanation: As Greynir is still in active development, it will be used as a base for the LT project's development.",
    "answer_b": "Explanation: The core projects of the programme are described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat concrete software is planned to be developed by the end of the programme?\n\nGround Truth Answer:\n\n\nAnswer A:\nA lot of new software will be developed in all areas of the programme, some will be extensions of already available Greynir software.. Explanation: As Greynir is still in active development, it will be used as a base for the LT project's development.\n\nAnswer B:\nExplanation: The core projects of the programme are described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in detail in the following sections. The software development work is described in\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.09244",
    "question": "What other national language technology programs are described in the paper?",
    "gold_answer": "",
    "answer_a": "Please give a brief overview.\nAnswer: The paper describes other national language technology programmes and gives an overview over the history of language technology in Iceland. These include the Icelandic Language Technology Programme (ILT), the Icelandic Language Technology Programme for the 21st Century (ILT21), the Icelandic Language Technology Programme for the 21st Century (ILT21), the Icelandic Language Technology Programme for the 21st Century (ILT21), the Icelandic Language Technology Programme for the 21st Century (ILT21), the Icelandic Language Technology Programme for the 21st Century (ILT21), the Icelandic Language Technology",
    "answer_b": "STEVIN programme in the Netherlands. Explanation: We studied three of these national programmes: the STEVIN programme in the Netherlands which ran between 2004 and 2011, the Plan for the Advancement of Language Technology in Spain, and, in particular, the Estonian LT programmes that have been running since 2006.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat other national language technology programs are described in the paper?\n\nGround Truth Answer:\n\n\nAnswer A:\nPlease give a brief overview.\nAnswer: The paper describes other national language technology programmes and gives an overview over the history of language technology in Iceland. These include the Icelandic Language Technology Programme (ILT), the Icelandic Language Technology Programme for the 21st Century (ILT21), the Icelandic Language Technology Programme for the 21st Century (ILT21), the Icelandic Language Technology Programme for the 21st Century (ILT21), the Icelandic Language Technology Programme for the 21st Century (ILT21), the Icelandic Language Technology Programme for the 21st Century (ILT21), the Icelandic Language Technology\n\nAnswer B:\nSTEVIN programme in the Netherlands. Explanation: We studied three of these national programmes: the STEVIN programme in the Netherlands which ran between 2004 and 2011, the Plan for the Advancement of Language Technology in Spain, and, in particular, the Estonian LT programmes that have been running since 2006.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.09244",
    "question": "When did language technology start in Iceland?",
    "gold_answer": "",
    "answer_a": "Around year 2000. Explanation: The history of Icelandic LT is usually considered to have begun around the turn of the century, even though a couple of LT resources and products were developed in the years leading up to that.",
    "answer_b": "Yes, but not in the way we think of it today. In the 1980s, the Icelandic government decided to develop a national language, and the first language was Icelandic. The government also decided to develop a national language technology programme, and the first project was to develop a speech recognition system. The speech recognition system was developed by the University of Iceland, and it was used to develop a speech recognition system for the Icelandic language. The speech recognition system was used to develop a speech recognition system for the Icelandic language. The speech recognition system was used to develop a speech recognition system for the Icelandic language. The speech recognition system",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhen did language technology start in Iceland?\n\nGround Truth Answer:\n\n\nAnswer A:\nAround year 2000. Explanation: The history of Icelandic LT is usually considered to have begun around the turn of the century, even though a couple of LT resources and products were developed in the years leading up to that.\n\nAnswer B:\nYes, but not in the way we think of it today. In the 1980s, the Icelandic government decided to develop a national language, and the first language was Icelandic. The government also decided to develop a national language technology programme, and the first project was to develop a speech recognition system. The speech recognition system was developed by the University of Iceland, and it was used to develop a speech recognition system for the Icelandic language. The speech recognition system was used to develop a speech recognition system for the Icelandic language. The speech recognition system was used to develop a speech recognition system for the Icelandic language. The speech recognition system\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.04491",
    "question": "what domain do the opinions fall under?",
    "gold_answer": "",
    "answer_a": "Explanation: The opinions are divided into three categories: positive, negative and neutral. The positive category includes opinions that are positive in nature, the negative category includes opinions that are negative in nature and the neutral category includes opinions that are neither positive nor negative in nature. The opinions are collected from the Twitter API and are stored in a JSON file. The opinions are then filtered to remove the negative and neutral opinions and the remaining opinions are stored in a CSV file. The opinions are then analyzed using the Rhetorical Structure Theory (RST) to extract aspects from the opinions. The RST is a theory that explains the structure of a text and",
    "answer_b": "computers. Explanation: We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat domain do the opinions fall under?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The opinions are divided into three categories: positive, negative and neutral. The positive category includes opinions that are positive in nature, the negative category includes opinions that are negative in nature and the neutral category includes opinions that are neither positive nor negative in nature. The opinions are collected from the Twitter API and are stored in a JSON file. The opinions are then filtered to remove the negative and neutral opinions and the remaining opinions are stored in a CSV file. The opinions are then analyzed using the Rhetorical Structure Theory (RST) to extract aspects from the opinions. The RST is a theory that explains the structure of a text and\n\nAnswer B:\ncomputers. Explanation: We used Bing Liu's dataset BIBREF20 for evaluation. It contains three review datasets of three domains: computers, wireless routers, and speakers as in Table TABREF16 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.04491",
    "question": "what dataset was used?",
    "gold_answer": "",
    "answer_a": "Explanation: We used the Opinion Sentiment Dataset (OSD) BIBREF11 to collect opinion texts. The dataset contains 1000 opinions, each opinion is a sentence from a text. The dataset is divided into 1000 parts, each part is a sentence from a text. Each opinion is associated with a sentiment score, which is a number between -1 and 1, where 1 indicates a positive opinion and -1 indicates a negative opinion. The dataset is available at http://www.cs.cornell.edu/~james/OSD/.\n\nThe dataset contains 1000 opinions,",
    "answer_b": "SNAP Amazon Dataset. Explanation: As a sentiment analysis model, we used the Bag of Word vectorization method with a Logistic Regression classifier trained on 1.2 million (1, 3 and 5-star rating only) of Electronic reviews from SNAP Amazon Dataset BIBREF19 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat dataset was used?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We used the Opinion Sentiment Dataset (OSD) BIBREF11 to collect opinion texts. The dataset contains 1000 opinions, each opinion is a sentence from a text. The dataset is divided into 1000 parts, each part is a sentence from a text. Each opinion is associated with a sentiment score, which is a number between -1 and 1, where 1 indicates a positive opinion and -1 indicates a negative opinion. The dataset is available at http://www.cs.cornell.edu/~james/OSD/.\n\nThe dataset contains 1000 opinions,\n\nAnswer B:\nSNAP Amazon Dataset. Explanation: As a sentiment analysis model, we used the Bag of Word vectorization method with a Logistic Regression classifier trained on 1.2 million (1, 3 and 5-star rating only) of Electronic reviews from SNAP Amazon Dataset BIBREF19 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.07491",
    "question": "what are the existing datasets for this task?",
    "gold_answer": "",
    "answer_a": "How large is the dataset?\n<blockquote>\n<font size=\"2\">Existing datasets are mostly composed of a small number of labeled concepts, and are not representative of the full set of concepts in the domain. </font>\n</blockquote>\nExisting datasets are mostly composed of a small number of labeled concepts, and are not representative of the full set of concepts in the domain. The dataset we use is a subset of the dataset of the 2015 NLP Challenge, which was used to train the controversiality classifier. The dataset contains 1000 concepts, and is labeled for 1000 concepts. The",
    "answer_b": "480 concepts previously analyzed in BIBREF1, BIBREF4. Explanation: Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4. 240 are positive examples, titles from the Wikipedia list of controversial issues, and 240 are negative examples chosen at random and exclusive of the positives.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the existing datasets for this task?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow large is the dataset?\n<blockquote>\n<font size=\"2\">Existing datasets are mostly composed of a small number of labeled concepts, and are not representative of the full set of concepts in the domain. </font>\n</blockquote>\nExisting datasets are mostly composed of a small number of labeled concepts, and are not representative of the full set of concepts in the domain. The dataset we use is a subset of the dataset of the 2015 NLP Challenge, which was used to train the controversiality classifier. The dataset contains 1000 concepts, and is labeled for 1000 concepts. The\n\nAnswer B:\n480 concepts previously analyzed in BIBREF1, BIBREF4. Explanation: Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4. 240 are positive examples, titles from the Wikipedia list of controversial issues, and 240 are negative examples chosen at random and exclusive of the positives.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.07491",
    "question": "what is the size of the introduced dataset?",
    "gold_answer": "",
    "answer_a": "608 controversial Wikipedia concepts. Explanation: We consider three datasets, two of which are a contribution of this work.",
    "answer_b": "How is it different from existing datasets?\n<blockquote>Our dataset contains 1,000 concepts, of which 500 are labeled as controversial. The remaining 500 are labeled as neutral. The dataset is significantly larger than existing datasets, and grades concepts on a 0-10 scale, rather than treating controversiality as a binary label. </blockquote>\n<blockquote>Our dataset contains 1,000 concepts, of which 500 are labeled as controversial. The remaining 500 are labeled as neutral. The dataset is significantly larger than existing datasets, and grades concepts on a",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the size of the introduced dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\n608 controversial Wikipedia concepts. Explanation: We consider three datasets, two of which are a contribution of this work.\n\nAnswer B:\nHow is it different from existing datasets?\n<blockquote>Our dataset contains 1,000 concepts, of which 500 are labeled as controversial. The remaining 500 are labeled as neutral. The dataset is significantly larger than existing datasets, and grades concepts on a 0-10 scale, rather than treating controversiality as a binary label. </blockquote>\n<blockquote>Our dataset contains 1,000 concepts, of which 500 are labeled as controversial. The remaining 500 are labeled as neutral. The dataset is significantly larger than existing datasets, and grades concepts on a\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.07491",
    "question": "how was labeling done?",
    "gold_answer": "",
    "answer_a": "The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. After that, annotations were normalized to controversiality scores on an integer scale of 0 - 10. Explanation: The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10.",
    "answer_b": "What is the dataset?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Figure 1</td>\n</tr>\n</table> Explanation: We label the 1000 concepts in the dataset with a 0-10 scale, where 0 indicates that the concept is not controversial, and 10 indicates that the concept is controversial. We use the following criteria to label the concepts: (1) the concept is not controversial if it is not in the Wikipedia list of controversial concepts, (2) the concept is controversial if it is in the Wikipedia list of controversial concepts",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow was labeling done?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. After that, annotations were normalized to controversiality scores on an integer scale of 0 - 10. Explanation: The annotation instructions were: \u201cGiven a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.\u201d. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10.\n\nAnswer B:\nWhat is the dataset?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Figure 1</td>\n</tr>\n</table> Explanation: We label the 1000 concepts in the dataset with a 0-10 scale, where 0 indicates that the concept is not controversial, and 10 indicates that the concept is controversial. We use the following criteria to label the concepts: (1) the concept is not controversial if it is not in the Wikipedia list of controversial concepts, (2) the concept is controversial if it is in the Wikipedia list of controversial concepts\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.07491",
    "question": "where does their dataset come from?",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n<blockquote>\n<font size=\"2\">The dataset is available at <a href=\"https://github.com/alexander-kravets/controversy\" target=\"_blank\">https://github.com/alexander-kravets/controversy</a>. The dataset is a subset of the dataset of <a href=\"https://github.com/alexander-kravets/controversy\" target=\"_blank\">https://github.com/alexander-kravets/controversy</a> and contains 1000 concepts. The dataset is labeled",
    "answer_b": "Wikipedia list of controversial issues. Explanation: Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhere does their dataset come from?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n<blockquote>\n<font size=\"2\">The dataset is available at <a href=\"https://github.com/alexander-kravets/controversy\" target=\"_blank\">https://github.com/alexander-kravets/controversy</a>. The dataset is a subset of the dataset of <a href=\"https://github.com/alexander-kravets/controversy\" target=\"_blank\">https://github.com/alexander-kravets/controversy</a> and contains 1000 concepts. The dataset is labeled\n\nAnswer B:\nWikipedia list of controversial issues. Explanation: Dataset II is based on a more recent version of the Wikipedia list of controversial issues (May 2017).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.07491",
    "question": "what are the baselines?",
    "gold_answer": "",
    "answer_a": "How do they compare to the baselines?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Baseline 1</td>\n<td align=\"left\">Baseline 2</td>\n<td align=\"left\">Baseline 3</td>\n<td align=\"left\">Baseline 4</td>\n<td align=\"left\">Baseline 5</td>\n<td align=\"left\">Baseline 6</td>\n<td align=\"left\">Baseline 7</td>\n<td align=\"left\">Baseline 8</td>\n<td align=\"left\">Baseline 9</td>\n<td align=\"left\">Baseline",
    "answer_b": "Nearest neighbors (NN) Estimator. Explanation: Nearest neighbors (NN) Estimator: We used the pre-trained GloVe embeddings BIBREF11 of concepts to implement a nearest-neighbor estimator as follows.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the baselines?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they compare to the baselines?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Baseline 1</td>\n<td align=\"left\">Baseline 2</td>\n<td align=\"left\">Baseline 3</td>\n<td align=\"left\">Baseline 4</td>\n<td align=\"left\">Baseline 5</td>\n<td align=\"left\">Baseline 6</td>\n<td align=\"left\">Baseline 7</td>\n<td align=\"left\">Baseline 8</td>\n<td align=\"left\">Baseline 9</td>\n<td align=\"left\">Baseline\n\nAnswer B:\nNearest neighbors (NN) Estimator. Explanation: Nearest neighbors (NN) Estimator: We used the pre-trained GloVe embeddings BIBREF11 of concepts to implement a nearest-neighbor estimator as follows.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.07491",
    "question": "what tools did they use?",
    "gold_answer": "",
    "answer_a": "nearest-neighbor estimator. Explanation: Nearest neighbors (NN) Estimator: We used the pre-trained GloVe embeddings BIBREF11 of concepts to implement a nearest-neighbor estimator as follows.",
    "answer_b": "(see explanation below)\nWe use a simple and language-independent machine-learning tool to train a classifier that predicts controversiality for a given concept. The classifier is trained on a dataset of 1000 concepts labeled for controversiality. The classifier is trained on a corpus of 10000 articles, and is trained on the Wikipedia metadata of these articles. The classifier is trained on a corpus of 10000 articles, and is trained on the Wikipedia metadata of these articles. The classifier is trained on a corpus of 10000 articles, and is trained on the Wikipedia metadata of these articles.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat tools did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nnearest-neighbor estimator. Explanation: Nearest neighbors (NN) Estimator: We used the pre-trained GloVe embeddings BIBREF11 of concepts to implement a nearest-neighbor estimator as follows.\n\nAnswer B:\n(see explanation below)\nWe use a simple and language-independent machine-learning tool to train a classifier that predicts controversiality for a given concept. The classifier is trained on a dataset of 1000 concepts labeled for controversiality. The classifier is trained on a corpus of 10000 articles, and is trained on the Wikipedia metadata of these articles. The classifier is trained on a corpus of 10000 articles, and is trained on the Wikipedia metadata of these articles. The classifier is trained on a corpus of 10000 articles, and is trained on the Wikipedia metadata of these articles.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.11850",
    "question": "What is the performance of NJM?",
    "gold_answer": "",
    "answer_a": "NJM vas selected as the funniest caption among the three options 22.59% of the times, and NJM captions posted to Bokete averaged 3.23 stars. Explanation: In this subsection, we present the experimental results along with a discussion. Table TABREF10 shows the experimental results of the questionnaire. A total of 16 personal questionnaires were completed. Table TABREF10 shows the percentages of captions of each rank for each method of caption generation considered herein. Captions generated by humans were ranked \u201cfunniest\u201d 67.99% of the time, followed by the NJM at 22.59%. The baseline captions, STAIR caption, were ranked \u201cfunniest\u201d 9.41% of the time. These results suggest that captions generated by the NJM are less funny than those generated by humans. However, the NJM is ranked much higher than STAIR caption.",
    "answer_b": "How does it compare to the baseline?\n< Explanation: The results of the experiments are shown in Table 1. The results show that the proposed method is more effective than the baseline. The results of the experiments are shown in Table 1. The results show that the proposed method is more effective than the baseline. The results of the experiments are shown in Table 1. The results show that the proposed method is more effective than the baseline. The results of the experiments are shown in Table 1. The results show that the proposed method is more effective than the baseline. The results of the experiments are shown in Table 1. The",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the performance of NJM?\n\nGround Truth Answer:\n\n\nAnswer A:\nNJM vas selected as the funniest caption among the three options 22.59% of the times, and NJM captions posted to Bokete averaged 3.23 stars. Explanation: In this subsection, we present the experimental results along with a discussion. Table TABREF10 shows the experimental results of the questionnaire. A total of 16 personal questionnaires were completed. Table TABREF10 shows the percentages of captions of each rank for each method of caption generation considered herein. Captions generated by humans were ranked \u201cfunniest\u201d 67.99% of the time, followed by the NJM at 22.59%. The baseline captions, STAIR caption, were ranked \u201cfunniest\u201d 9.41% of the time. These results suggest that captions generated by the NJM are less funny than those generated by humans. However, the NJM is ranked much higher than STAIR caption.\n\nAnswer B:\nHow does it compare to the baseline?\n< Explanation: The results of the experiments are shown in Table 1. The results show that the proposed method is more effective than the baseline. The results of the experiments are shown in Table 1. The results show that the proposed method is more effective than the baseline. The results of the experiments are shown in Table 1. The results show that the proposed method is more effective than the baseline. The results of the experiments are shown in Table 1. The results show that the proposed method is more effective than the baseline. The results of the experiments are shown in Table 1. The\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.11850",
    "question": "How are the results evaluated?",
    "gold_answer": "",
    "answer_a": "The captions are ranked by humans in order of \"funniness\".. Explanation: We use a questionnaire as the evaluation method.",
    "answer_b": "What is the baseline?\n< Explanation: We evaluate the effectiveness of the proposed method by comparing the results obtained using the proposed method and those obtained using MS COCO Pre-trained CNN+LSTM, which is the baseline and idiot created by humans. We use the Funny Score to evaluate the effectiveness of the proposed method. The Funny Score is a weighted sum of the evaluation results of the proposed method and MS COCO Pre-trained CNN+LSTM. The Funny Score is calculated by the following formula: where is the Funny Score of the proposed method, is the Funny Score of MS COCO Pre-trained CNN+LSTM, and is the Funny",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are the results evaluated?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe captions are ranked by humans in order of \"funniness\".. Explanation: We use a questionnaire as the evaluation method.\n\nAnswer B:\nWhat is the baseline?\n< Explanation: We evaluate the effectiveness of the proposed method by comparing the results obtained using the proposed method and those obtained using MS COCO Pre-trained CNN+LSTM, which is the baseline and idiot created by humans. We use the Funny Score to evaluate the effectiveness of the proposed method. The Funny Score is a weighted sum of the evaluation results of the proposed method and MS COCO Pre-trained CNN+LSTM. The Funny Score is calculated by the following formula: where is the Funny Score of the proposed method, is the Funny Score of MS COCO Pre-trained CNN+LSTM, and is the Funny\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.11850",
    "question": "How big is the self-collected corpus?",
    "gold_answer": "",
    "answer_a": "What is the size of the corpus?\n< Explanation: The BoketeDB is a collection of funny captions posted on Bokete, which is an image Ogiri website. The corpus contains 10,000 images and 10,000 captions. The corpus is divided into 1000 images and 1000 captions. The corpus is divided into 1000 images and 1000 captions. The corpus is divided into 1000 images and 1000 captions. The corpus is divided into 1000 images and 1",
    "answer_b": "999,571 funny captions for 70,981 images. Explanation: We have downloaded pairs of images and funny captions in order to construct a Bokete Database (BoketeDB). As of March 2018, 60M funny captions and 3.4M images have been posted on the Bokete Ogiri website. In the present study, we consider 999,571 funny captions for 70,981 images. A number of pair between image and funny caption is posted in temporal order on the Ogiri website Bokete. We collected images and funny captions to make corresponding image and caption pairs. Thus, we obtained a database for generating funny captions like an image caption one.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big is the self-collected corpus?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the corpus?\n< Explanation: The BoketeDB is a collection of funny captions posted on Bokete, which is an image Ogiri website. The corpus contains 10,000 images and 10,000 captions. The corpus is divided into 1000 images and 1000 captions. The corpus is divided into 1000 images and 1000 captions. The corpus is divided into 1000 images and 1000 captions. The corpus is divided into 1000 images and 1\n\nAnswer B:\n999,571 funny captions for 70,981 images. Explanation: We have downloaded pairs of images and funny captions in order to construct a Bokete Database (BoketeDB). As of March 2018, 60M funny captions and 3.4M images have been posted on the Bokete Ogiri website. In the present study, we consider 999,571 funny captions for 70,981 images. A number of pair between image and funny caption is posted in temporal order on the Ogiri website Bokete. We collected images and funny captions to make corresponding image and caption pairs. Thus, we obtained a database for generating funny captions like an image caption one.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.11850",
    "question": "How is the funny score calculated?",
    "gold_answer": "",
    "answer_a": "What is the Funny Score's effect on the model?\n< Explanation: The Funny Score is calculated by the following formula: where is the number of times the image caption is funny, and is the number of times the funny caption is funny. The Funny Score is a weighted sum of the number of times the image caption is funny and the number of times the funny caption is funny. The Funny Score is a measure of the effectiveness of the model. The higher the Funny Score, the more effective the model is. The Funny Score is calculated by the following formula: where is the number of times the image caption is funny, and is the number",
    "answer_b": "Based on the number of stars users assign funny captions, an LSTM calculates the loss value L as an average of each mini-batch and returns  L when the number of stars is less than 100, otherwise L-1.0. Explanation: The Bokete Ogiri website uses the number of stars to evaluate the degree of funniness of a caption. The user evaluates the \u201cfunniness\u201d of a posted caption and assigns one to three stars to the caption. Therefore, funnier captions tend to be assigned a lot of stars. We focus on the number of stars in order to propose an effective training method, in which the Funny Score enables us to evaluate the funniness of a caption. Based on the results of our pre-experiment, a Funny Score of 100 stars is treated as a threshold. In other words, the Funny Score outputs a loss value INLINEFORM0 when #star is less than 100. In contrast, the Funny Score returns INLINEFORM1 when #star is over 100. The loss value INLINEFORM2 is calculated with the LSTM as an average of each mini-batch.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is the funny score calculated?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the Funny Score's effect on the model?\n< Explanation: The Funny Score is calculated by the following formula: where is the number of times the image caption is funny, and is the number of times the funny caption is funny. The Funny Score is a weighted sum of the number of times the image caption is funny and the number of times the funny caption is funny. The Funny Score is a measure of the effectiveness of the model. The higher the Funny Score, the more effective the model is. The Funny Score is calculated by the following formula: where is the number of times the image caption is funny, and is the number\n\nAnswer B:\nBased on the number of stars users assign funny captions, an LSTM calculates the loss value L as an average of each mini-batch and returns  L when the number of stars is less than 100, otherwise L-1.0. Explanation: The Bokete Ogiri website uses the number of stars to evaluate the degree of funniness of a caption. The user evaluates the \u201cfunniness\u201d of a posted caption and assigns one to three stars to the caption. Therefore, funnier captions tend to be assigned a lot of stars. We focus on the number of stars in order to propose an effective training method, in which the Funny Score enables us to evaluate the funniness of a caption. Based on the results of our pre-experiment, a Funny Score of 100 stars is treated as a threshold. In other words, the Funny Score outputs a loss value INLINEFORM0 when #star is less than 100. In contrast, the Funny Score returns INLINEFORM1 when #star is over 100. The loss value INLINEFORM2 is calculated with the LSTM as an average of each mini-batch.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1710.06923",
    "question": "Which dataset do they use?",
    "gold_answer": "",
    "answer_a": "Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013. Explanation: We present the results of our experiments with both the Evo-Devo and the Machine Learning mechanisms described earlier using the U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12 .",
    "answer_b": "How do they evaluate the performance of the adaptation?\n<sep> We use the Stanford Question Answering Dataset (SQuAD) [1] for our experiments. SQuAD is a collection of 10,000 questions and answers, and is a standard benchmark for question answering. The dataset is available at http://squad.sourceforge.net/. We use the questions and answers from SQuAD to train the general-purpose speech engine. We use the answers to train the question answering system. We use the questions to train the speech engine. We use the answers to evaluate the performance of the adaptation. We use the questions and",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dataset do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nAnnual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013. Explanation: We present the results of our experiments with both the Evo-Devo and the Machine Learning mechanisms described earlier using the U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013 BIBREF12 .\n\nAnswer B:\nHow do they evaluate the performance of the adaptation?\n<sep> We use the Stanford Question Answering Dataset (SQuAD) [1] for our experiments. SQuAD is a collection of 10,000 questions and answers, and is a standard benchmark for question answering. The dataset is available at http://squad.sourceforge.net/. We use the questions and answers from SQuAD to train the general-purpose speech engine. We use the answers to train the question answering system. We use the questions to train the speech engine. We use the answers to evaluate the performance of the adaptation. We use the questions and\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1710.06923",
    "question": "Which of their proposed domain adaptation methods proves best overall?",
    "gold_answer": "",
    "answer_a": "What is the performance of the proposed methods?\n<sep><solution> We evaluate the performance of the proposed methods on the domain of the question answering task. We compare the performance of the proposed methods with the performance of the general-purpose speech engine trained on the general corpus. We also compare the performance of the proposed methods with the performance of the general-purpose speech engine trained on the domain-specific corpus. We also compare the performance of the proposed methods with the performance of the general-purpose speech engine trained on the general corpus and the domain-specific corpus. We also compare the performance of the proposed methods with the performance of the general-purpose speech engine trained on",
    "answer_b": "Machine learning approach. Explanation: We ran our Evo-Devo mechanism with the 486 ASR sentences (see Table TABREF14 ) and measured the accuracy after each repair. On an average we have achieved about 5 to 10% improvements in the accuracy of the sentences.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich of their proposed domain adaptation methods proves best overall?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of the proposed methods?\n<sep><solution> We evaluate the performance of the proposed methods on the domain of the question answering task. We compare the performance of the proposed methods with the performance of the general-purpose speech engine trained on the general corpus. We also compare the performance of the proposed methods with the performance of the general-purpose speech engine trained on the domain-specific corpus. We also compare the performance of the proposed methods with the performance of the general-purpose speech engine trained on the general corpus and the domain-specific corpus. We also compare the performance of the proposed methods with the performance of the general-purpose speech engine trained on\n\nAnswer B:\nMachine learning approach. Explanation: We ran our Evo-Devo mechanism with the 486 ASR sentences (see Table TABREF14 ) and measured the accuracy after each repair. On an average we have achieved about 5 to 10% improvements in the accuracy of the sentences.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1811.04791",
    "question": "With how many languages do they experiment in the multilingual setup?",
    "gold_answer": "",
    "answer_a": "Explanation: We use the same 10 languages as in the previous work, but we also include 10 languages that are not in the previous work. We use the same 10 languages as in the previous work, but we also include 10 languages that are not in the previous work. We use the same 10 languages as in the previous work, but we also include 10 languages that are not in the previous work. We use the same 10 languages as in the previous work, but we also include 10 languages that are not in the previous work. We use the same 1",
    "answer_b": "ten languages. Explanation: Next, we explore how multilingual annotated data can be used to improve feature extraction for a zero-resource target language. We train multilingual bnfs on between one and ten languages from the GlobalPhone collection and evaluate on six other languages (simulating different zero-resource targets).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWith how many languages do they experiment in the multilingual setup?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We use the same 10 languages as in the previous work, but we also include 10 languages that are not in the previous work. We use the same 10 languages as in the previous work, but we also include 10 languages that are not in the previous work. We use the same 10 languages as in the previous work, but we also include 10 languages that are not in the previous work. We use the same 10 languages as in the previous work, but we also include 10 languages that are not in the previous work. We use the same 1\n\nAnswer B:\nten languages. Explanation: Next, we explore how multilingual annotated data can be used to improve feature extraction for a zero-resource target language. We train multilingual bnfs on between one and ten languages from the GlobalPhone collection and evaluate on six other languages (simulating different zero-resource targets).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1811.04791",
    "question": "How do they extract target language bottleneck features?",
    "gold_answer": "",
    "answer_a": "train a tdnn BIBREF36 with block softmax. Explanation: We train a tdnn BIBREF36 with block softmax BIBREF37 , i.e. all hidden layers are shared between languages, but there is a separate output layer for each language. For each training instance only the error at the corresponding language's output layer is used to update the weights. This architecture is illustrated in Figure FIGREF17 . The tdnn has six 625-dimensional hidden layers followed by a 39-dimensional bottleneck layer with ReLU activations and batch normalization. Each language then has its own 625-dimensional affine and a softmax layer.",
    "answer_b": "What are the features?\n< Explanation: We first extract a target language bottleneck feature using a model trained on other languages. We then use this feature to train a model that learns to extract features from the target language. We then compare the two models on the target language and on the other language. We find that the target language model outperforms the other language model on both the target language and the other language. We also find that the target language model outperforms the other language model on the target language. We find that the target language model outperforms the other language model on the other language. We find that the target language",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they extract target language bottleneck features?\n\nGround Truth Answer:\n\n\nAnswer A:\ntrain a tdnn BIBREF36 with block softmax. Explanation: We train a tdnn BIBREF36 with block softmax BIBREF37 , i.e. all hidden layers are shared between languages, but there is a separate output layer for each language. For each training instance only the error at the corresponding language's output layer is used to update the weights. This architecture is illustrated in Figure FIGREF17 . The tdnn has six 625-dimensional hidden layers followed by a 39-dimensional bottleneck layer with ReLU activations and batch normalization. Each language then has its own 625-dimensional affine and a softmax layer.\n\nAnswer B:\nWhat are the features?\n< Explanation: We first extract a target language bottleneck feature using a model trained on other languages. We then use this feature to train a model that learns to extract features from the target language. We then compare the two models on the target language and on the other language. We find that the target language model outperforms the other language model on both the target language and the other language. We also find that the target language model outperforms the other language model on the target language. We find that the target language model outperforms the other language model on the other language. We find that the target language\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1811.04791",
    "question": "Which dataset do they use?",
    "gold_answer": "",
    "answer_a": "GlobalPhone corpus. Explanation: We use the GlobalPhone corpus of speech read from news articles BIBREF20 .",
    "answer_b": "How do they evaluate?\n< Explanation: We use the 2016-2017 National English Corpus (NEC) BIBREF10, which is a large, publicly available dataset of English speech recorded in the United States. The dataset consists of 1,000 hours of audio, with 100 hours of training and 900 hours of testing. The dataset is divided into two parts: training and testing. The training set is used to train a speech recognition model, and the testing set is used to evaluate the model. The dataset is available at http://www.cs.corn",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dataset do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nGlobalPhone corpus. Explanation: We use the GlobalPhone corpus of speech read from news articles BIBREF20 .\n\nAnswer B:\nHow do they evaluate?\n< Explanation: We use the 2016-2017 National English Corpus (NEC) BIBREF10, which is a large, publicly available dataset of English speech recorded in the United States. The dataset consists of 1,000 hours of audio, with 100 hours of training and 900 hours of testing. The dataset is divided into two parts: training and testing. The training set is used to train a speech recognition model, and the testing set is used to evaluate the model. The dataset is available at http://www.cs.corn\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1811.04791",
    "question": "Which intrisic measures do they use do evaluate obtained representations?",
    "gold_answer": "",
    "answer_a": "same-different. Explanation: In doing so, we also provide the first set of results on identical systems evaluated using both the same-different and ABX evaluation measures.",
    "answer_b": "What are they?\n< Explanation: We use the following two intrinsic measures to evaluate the learned representations:\n\n1) Average log likelihood of the learned representations over all utterances in the training set. This is the most common measure used in the literature, and it is also the one used in this work. We use this measure to evaluate the learned representations in the following way: for each utterance, we compute the average log likelihood of the learned representations over all utterances in the training set. We then compute the average log likelihood of the learned representations over all utterances in the test set. The average log likelihood of the learned representations is then",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich intrisic measures do they use do evaluate obtained representations?\n\nGround Truth Answer:\n\n\nAnswer A:\nsame-different. Explanation: In doing so, we also provide the first set of results on identical systems evaluated using both the same-different and ABX evaluation measures.\n\nAnswer B:\nWhat are they?\n< Explanation: We use the following two intrinsic measures to evaluate the learned representations:\n\n1) Average log likelihood of the learned representations over all utterances in the training set. This is the most common measure used in the literature, and it is also the one used in this work. We use this measure to evaluate the learned representations in the following way: for each utterance, we compute the average log likelihood of the learned representations over all utterances in the training set. We then compute the average log likelihood of the learned representations over all utterances in the test set. The average log likelihood of the learned representations is then\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01749",
    "question": "What results are obtained by their model?",
    "gold_answer": "",
    "answer_a": "What is the performance of the model on the datasets?\n<blockquote>Our model achieves state-of-the-art results on the datasets. The best results are obtained on the News20 dataset, which has 2000 examples, and the best results are obtained on the News200 dataset, which has 20000 examples. The best results are obtained on the News2000 dataset, which has 200000 examples. The best results are obtained on the News20000 dataset, which has 2000000 examples. The best results are obtained",
    "answer_b": "Our model outperforms PG-MMR when trained and tested on the Multi-News dataset. Explanation: Our model outperforms PG-MMR when trained and tested on the Multi-News dataset.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat results are obtained by their model?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of the model on the datasets?\n<blockquote>Our model achieves state-of-the-art results on the datasets. The best results are obtained on the News20 dataset, which has 2000 examples, and the best results are obtained on the News200 dataset, which has 20000 examples. The best results are obtained on the News2000 dataset, which has 200000 examples. The best results are obtained on the News20000 dataset, which has 2000000 examples. The best results are obtained\n\nAnswer B:\nOur model outperforms PG-MMR when trained and tested on the Multi-News dataset. Explanation: Our model outperforms PG-MMR when trained and tested on the Multi-News dataset.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01749",
    "question": "What sources do the news come from?",
    "gold_answer": "",
    "answer_a": "How many examples are in the dataset?\n< Explanation: The dataset contains 10,000 news articles from 1000 different news sources. Each article is 1000 words long and contains 10000 words of text. The dataset is divided into 10000 training examples and 10000 test examples. The training examples are used to train the model and the test examples are used to evaluate the model. The dataset is available at https://github.com/duanxu1991/MultiNews.\n\nThe dataset contains 10,0",
    "answer_b": "1500 news sites. Explanation: Our dataset also comes from a diverse set of news sources; over 1,500 sites appear as source documents 5 times or greater, as opposed to previous news datasets (DUC comes from 2 sources, CNNDM comes from CNN and Daily Mail respectively, and even the Newsroom dataset BIBREF6 covers only 38 news sources).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat sources do the news come from?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow many examples are in the dataset?\n< Explanation: The dataset contains 10,000 news articles from 1000 different news sources. Each article is 1000 words long and contains 10000 words of text. The dataset is divided into 10000 training examples and 10000 test examples. The training examples are used to train the model and the test examples are used to evaluate the model. The dataset is available at https://github.com/duanxu1991/MultiNews.\n\nThe dataset contains 10,0\n\nAnswer B:\n1500 news sites. Explanation: Our dataset also comes from a diverse set of news sources; over 1,500 sites appear as source documents 5 times or greater, as opposed to previous news datasets (DUC comes from 2 sources, CNNDM comes from CNN and Daily Mail respectively, and even the Newsroom dataset BIBREF6 covers only 38 news sources).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01749",
    "question": "What is the size of Multi-news dataset?",
    "gold_answer": "",
    "answer_a": "56216. Explanation: We split our dataset into training (80%, 44,972), validation (10%, 5,622), and test (10%, 5,622) sets.",
    "answer_b": "How many examples are there?\n< Explanation: The dataset contains 10,000 news articles, each with 1000 words. The dataset is divided into 1000 examples, each containing 1000 words. The dataset is divided into 1000 examples, each containing 1000 words. The dataset is divided into 1000 examples, each containing 1000 words. The dataset is divided into 1000 examples, each containing 1000 words. The dataset is divided into 1000 examples, each",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the size of Multi-news dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\n56216. Explanation: We split our dataset into training (80%, 44,972), validation (10%, 5,622), and test (10%, 5,622) sets.\n\nAnswer B:\nHow many examples are there?\n< Explanation: The dataset contains 10,000 news articles, each with 1000 words. The dataset is divided into 1000 examples, each containing 1000 words. The dataset is divided into 1000 examples, each containing 1000 words. The dataset is divided into 1000 examples, each containing 1000 words. The dataset is divided into 1000 examples, each containing 1000 words. The dataset is divided into 1000 examples, each\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02334",
    "question": "Which vocabulary size was the better performer?",
    "gold_answer": "",
    "answer_a": "Why?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td>1000</td>\n<td>10000</td>\n<td>100000</td>\n<td>1000000</td>\n<td>10000000</td>\n</tr>\n</table>\nExplanation: We compare the performance of the best-performing models on the English-German translation task with different vocabulary sizes. We find that the best-performing models on the English-German translation task are the ones with the largest vocabulary sizes. The best-per",
    "answer_b": "Source BPE vocabulary size is 32000; target BPE vocabulary size is 8000.. Explanation: FLOAT SELECTED: Table 2: BLEU scores from German-English (De-En) and English-German (En-De) experiments on NewsTest2013 (NT13) and NewsTest2014 (NT14) along with their vocabulary sizes, class imbalance D, and mean sequence length \u00b5.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich vocabulary size was the better performer?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhy?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td>1000</td>\n<td>10000</td>\n<td>100000</td>\n<td>1000000</td>\n<td>10000000</td>\n</tr>\n</table>\nExplanation: We compare the performance of the best-performing models on the English-German translation task with different vocabulary sizes. We find that the best-performing models on the English-German translation task are the ones with the largest vocabulary sizes. The best-per\n\nAnswer B:\nSource BPE vocabulary size is 32000; target BPE vocabulary size is 8000.. Explanation: FLOAT SELECTED: Table 2: BLEU scores from German-English (De-En) and English-German (En-De) experiments on NewsTest2013 (NT13) and NewsTest2014 (NT14) along with their vocabulary sizes, class imbalance D, and mean sequence length \u00b5.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02334",
    "question": "Which languages are explored?",
    "gold_answer": "",
    "answer_a": "How are they balanced?\n< Explanation: We consider 10 languages: English, French, German, Spanish, Italian, Portuguese, Arabic, Chinese, Japanese, and Korean. We use the same training and test data for all languages. We train the models on the training data and test on the test data. We use the same training and test data for all languages. We train the models on the training data and test on the test data. We use the same training and test data for all languages. We train the models on the training data and test on the test data. We use the same training and test data for all languages",
    "answer_b": "German (De) and English (En). Explanation: We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich languages are explored?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they balanced?\n< Explanation: We consider 10 languages: English, French, German, Spanish, Italian, Portuguese, Arabic, Chinese, Japanese, and Korean. We use the same training and test data for all languages. We train the models on the training data and test on the test data. We use the same training and test data for all languages. We train the models on the training data and test on the test data. We use the same training and test data for all languages. We train the models on the training data and test on the test data. We use the same training and test data for all languages\n\nAnswer B:\nGerman (De) and English (En). Explanation: We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02334",
    "question": "What datasets are used in the paper?",
    "gold_answer": "",
    "answer_a": "< Explanation: We use the following datasets: (1) English to French (EN-FR) and (2) English to German (EN-GD). We use the same training and test sets for both languages. We use the same training and test sets for both languages. We use the same training and test sets for both languages. We use the same training and test sets for both languages. We use the same training and test sets for both languages. We use the same training and test sets for both languages. We use the same training and test sets for both languages. We use the same training and test sets for both languages",
    "answer_b": "Europarl v9 parallel data set. Explanation: We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets are used in the paper?\n\nGround Truth Answer:\n\n\nAnswer A:\n< Explanation: We use the following datasets: (1) English to French (EN-FR) and (2) English to German (EN-GD). We use the same training and test sets for both languages. We use the same training and test sets for both languages. We use the same training and test sets for both languages. We use the same training and test sets for both languages. We use the same training and test sets for both languages. We use the same training and test sets for both languages. We use the same training and test sets for both languages. We use the same training and test sets for both languages\n\nAnswer B:\nEuroparl v9 parallel data set. Explanation: We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02334",
    "question": "What vocabulary sizes are explored?",
    "gold_answer": "",
    "answer_a": "Word 64k, 64k; Word 32k, 32k; BPE 64k, 64k; BPE 16k, 16k; BPE 8k, 8k; BPE 4k, 4k; BPE 2k, 2k; BPE 1k, 1k; Chars De:176; En:172; BPE 32k, 8k; BPE 8k, 32k. Explanation: FLOAT SELECTED: Table 2: BLEU scores from German-English (De-En) and English-German (En-De) experiments on NewsTest2013 (NT13) and NewsTest2014 (NT14) along with their vocabulary sizes, class imbalance D, and mean sequence length \u00b5.",
    "answer_b": "How are they compared?\n< Explanation: We consider three different vocabulary sizes: 100, 1000 and 10000 words. We compare the performance of the three models on the 10000 word model and the 100 word model. We also compare the performance of the three models on the 1000 word model. We find that the 10000 word model performs better than the 100 word model and the 1000 word model. We also find that the 10000 word model performs better than",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat vocabulary sizes are explored?\n\nGround Truth Answer:\n\n\nAnswer A:\nWord 64k, 64k; Word 32k, 32k; BPE 64k, 64k; BPE 16k, 16k; BPE 8k, 8k; BPE 4k, 4k; BPE 2k, 2k; BPE 1k, 1k; Chars De:176; En:172; BPE 32k, 8k; BPE 8k, 32k. Explanation: FLOAT SELECTED: Table 2: BLEU scores from German-English (De-En) and English-German (En-De) experiments on NewsTest2013 (NT13) and NewsTest2014 (NT14) along with their vocabulary sizes, class imbalance D, and mean sequence length \u00b5.\n\nAnswer B:\nHow are they compared?\n< Explanation: We consider three different vocabulary sizes: 100, 1000 and 10000 words. We compare the performance of the three models on the 10000 word model and the 100 word model. We also compare the performance of the three models on the 1000 word model. We find that the 10000 word model performs better than the 100 word model and the 1000 word model. We also find that the 10000 word model performs better than\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02334",
    "question": "What vocabulary size was the best performer?",
    "gold_answer": "",
    "answer_a": "Source BPE vocabulary size is 32000; target BPE vocabulary size is 8000.. Explanation: FLOAT SELECTED: Table 2: BLEU scores from German-English (De-En) and English-German (En-De) experiments on NewsTest2013 (NT13) and NewsTest2014 (NT14) along with their vocabulary sizes, class imbalance D, and mean sequence length \u00b5.",
    "answer_b": "What was the best performance?\n< Explanation: We find that the best performance is achieved with a vocabulary size of 1000. This is the size of the English Wikipedia. We also find that the best performance is achieved with a classification model with 1000 classes. This is the size of the English Wikipedia. We find that the best performance is achieved with a classification model with 1000 classes. This is the size of the English Wikipedia. We find that the best performance is achieved with a classification model with 1000 classes. This is the size of the English Wikipedia. We find that",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat vocabulary size was the best performer?\n\nGround Truth Answer:\n\n\nAnswer A:\nSource BPE vocabulary size is 32000; target BPE vocabulary size is 8000.. Explanation: FLOAT SELECTED: Table 2: BLEU scores from German-English (De-En) and English-German (En-De) experiments on NewsTest2013 (NT13) and NewsTest2014 (NT14) along with their vocabulary sizes, class imbalance D, and mean sequence length \u00b5.\n\nAnswer B:\nWhat was the best performance?\n< Explanation: We find that the best performance is achieved with a vocabulary size of 1000. This is the size of the English Wikipedia. We also find that the best performance is achieved with a classification model with 1000 classes. This is the size of the English Wikipedia. We find that the best performance is achieved with a classification model with 1000 classes. This is the size of the English Wikipedia. We find that the best performance is achieved with a classification model with 1000 classes. This is the size of the English Wikipedia. We find that\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02334",
    "question": "What datasets do they look at?",
    "gold_answer": "",
    "answer_a": "How do they measure performance?\n< Explanation: We use the following datasets: (1) the English to French NMT dataset (EN-F) from the Corpus of Formal English (COFE) BIBREF1 , (2) the English to German NMT dataset (EN-G) from the Corpus of Formal German (COFG) BIBREF2 , (3) the English to Spanish NMT dataset (EN-S) from the Corpus of Formal Spanish (COFS) BIBREF3 , (4) the English to Turkish NMT dataset (EN-T) from the Corpus of Formal Turkish (COFT) BIB",
    "answer_b": "Europarl v9. Explanation: We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets do they look at?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they measure performance?\n< Explanation: We use the following datasets: (1) the English to French NMT dataset (EN-F) from the Corpus of Formal English (COFE) BIBREF1 , (2) the English to German NMT dataset (EN-G) from the Corpus of Formal German (COFG) BIBREF2 , (3) the English to Spanish NMT dataset (EN-S) from the Corpus of Formal Spanish (COFS) BIBREF3 , (4) the English to Turkish NMT dataset (EN-T) from the Corpus of Formal Turkish (COFT) BIB\n\nAnswer B:\nEuroparl v9. Explanation: We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We use 1.8M sentences of this corpus and build models in English to German and vice versa. To segment initial words (i.e. before any subword processing) we use the Moses word tokenizer and detokenizer. We evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02334",
    "question": "Which vocab sizes did they analyze?",
    "gold_answer": "",
    "answer_a": "Word vocabulary sizes: 32000, 64000; BPE vocabulary sizes: 1000, 2000, 4000, 8000, 16000, 32000, 64000; Chars vocabulary sizes: 172, 176.. Explanation: FLOAT SELECTED: Table 2: BLEU scores from German-English (De-En) and English-German (En-De) experiments on NewsTest2013 (NT13) and NewsTest2014 (NT14) along with their vocabulary sizes, class imbalance D, and mean sequence length \u00b5.",
    "answer_b": "What was the effect of vocabulary sizes on NMT performance?\n< Explanation: We analyze the effect of vocabulary sizes on NMT performance. We use the same dataset as in the previous work (Section 2.2) and use the same model architecture (Section 2.3). We use the same hyperparameters (Section 2.4). We use the same evaluation metrics (Section 2.5). We use the same evaluation settings (Section 2.6). We use the same evaluation settings (Section 2.7). We use the same evaluation settings (Section 2.8). We use the same evaluation settings",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich vocab sizes did they analyze?\n\nGround Truth Answer:\n\n\nAnswer A:\nWord vocabulary sizes: 32000, 64000; BPE vocabulary sizes: 1000, 2000, 4000, 8000, 16000, 32000, 64000; Chars vocabulary sizes: 172, 176.. Explanation: FLOAT SELECTED: Table 2: BLEU scores from German-English (De-En) and English-German (En-De) experiments on NewsTest2013 (NT13) and NewsTest2014 (NT14) along with their vocabulary sizes, class imbalance D, and mean sequence length \u00b5.\n\nAnswer B:\nWhat was the effect of vocabulary sizes on NMT performance?\n< Explanation: We analyze the effect of vocabulary sizes on NMT performance. We use the same dataset as in the previous work (Section 2.2) and use the same model architecture (Section 2.3). We use the same hyperparameters (Section 2.4). We use the same evaluation metrics (Section 2.5). We use the same evaluation settings (Section 2.6). We use the same evaluation settings (Section 2.7). We use the same evaluation settings (Section 2.8). We use the same evaluation settings\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.11046",
    "question": "Why is improvement on OntoNotes significantly smaller compared to improvement on WNUT 2017?",
    "gold_answer": "",
    "answer_a": "suggesting that cross-context patterns were even more crucial for emerging contexts and entities than familiar entities. Explanation: More substantial improvements were achieved for WNUT 2017 emerging NER, suggesting that cross-context patterns were even more crucial for emerging contexts and entities than familiar entities, which might often be memorized by their surface forms.",
    "answer_b": "What is the reason for this?\n< Explanation: The results of the two datasets are shown in Table 1. The results show that the model outperforms the baseline by 1.4% and 4.6% on OntoNotes 5.0 and WNUT 2017, respectively. The results are also comparable to the state-of-the-art models on the other NER datasets. The model outperforms the baseline by 1.4% and 4.6% on OntoNotes 5.0 and WNUT 2017, respectively. The results are also comparable to",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhy is improvement on OntoNotes significantly smaller compared to improvement on WNUT 2017?\n\nGround Truth Answer:\n\n\nAnswer A:\nsuggesting that cross-context patterns were even more crucial for emerging contexts and entities than familiar entities. Explanation: More substantial improvements were achieved for WNUT 2017 emerging NER, suggesting that cross-context patterns were even more crucial for emerging contexts and entities than familiar entities, which might often be memorized by their surface forms.\n\nAnswer B:\nWhat is the reason for this?\n< Explanation: The results of the two datasets are shown in Table 1. The results show that the model outperforms the baseline by 1.4% and 4.6% on OntoNotes 5.0 and WNUT 2017, respectively. The results are also comparable to the state-of-the-art models on the other NER datasets. The model outperforms the baseline by 1.4% and 4.6% on OntoNotes 5.0 and WNUT 2017, respectively. The results are also comparable to\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.11046",
    "question": "How is \"complexity\" and \"confusability\" of entity mentions defined in this work?",
    "gold_answer": "",
    "answer_a": "What is the difference between the two?\n<sep><solution>Complexity: The complexity of an entity is defined as the number of words in the entity. For example, the entity \"The\" has complexity 1, while the entity \"The cat\" has complexity 2. Confusability: The confusion of an entity is defined as the number of words in the entity. For example, the entity \"The\" has confusion 1, while the entity \"The cat\" has confusion 2. </solution>\n\nComplexity: The complexity of an entity is defined as the number of words in the entity. For example, the",
    "answer_b": "Complexity is defined by examples of a singular named entity (e.g. work-of-art and creative-work entities) being represented by multiple surface forms. Mapping all of these forms to a single NE requires a complex understanding of the variations, some of which are genre-specific. Confusability is defined by examples when it becomes more difficult to disambiguate named entities that share the same surface form, such as the \"language\" versus \"NORP\" distinction represented by the surface forms Dutch and English.. Explanation: For example, work-of-art/creative-work entities could in principle take any surface forms \u2013 unseen, the same as a person name, abbreviated, or written with unreliable capitalizations on social media.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is \"complexity\" and \"confusability\" of entity mentions defined in this work?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the two?\n<sep><solution>Complexity: The complexity of an entity is defined as the number of words in the entity. For example, the entity \"The\" has complexity 1, while the entity \"The cat\" has complexity 2. Confusability: The confusion of an entity is defined as the number of words in the entity. For example, the entity \"The\" has confusion 1, while the entity \"The cat\" has confusion 2. </solution>\n\nComplexity: The complexity of an entity is defined as the number of words in the entity. For example, the\n\nAnswer B:\nComplexity is defined by examples of a singular named entity (e.g. work-of-art and creative-work entities) being represented by multiple surface forms. Mapping all of these forms to a single NE requires a complex understanding of the variations, some of which are genre-specific. Confusability is defined by examples when it becomes more difficult to disambiguate named entities that share the same surface form, such as the \"language\" versus \"NORP\" distinction represented by the surface forms Dutch and English.. Explanation: For example, work-of-art/creative-work entities could in principle take any surface forms \u2013 unseen, the same as a person name, abbreviated, or written with unreliable capitalizations on social media.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.11046",
    "question": "What are the baseline models?",
    "gold_answer": "",
    "answer_a": "What are the results?\n<sep><table cellspacing=\"0\" cellpadding=\"0\" border=\"0\" width=\"100%\" align=\"center\">\n<tr>\n<td align=\"center\">\nFigure 1: Results of the baseline models. The first column shows the performance of the baseline models on the WNUT 2017 dataset. The second column shows the performance of the baseline models on the OntoNotes 5.0 dataset. The third column shows the performance of the baseline models on the WNUT 2017 dataset with the addition of a gazetteer. The fourth column shows the performance of the baseline",
    "answer_b": "BiLSTM-CNN. Explanation: Previous studies have tried to stack multiple LSTMs for sequence-labeling NER BIBREF1. As they follow the trend of stacking forward and backward LSTMs independently, the Baseline-BiLSTM-CNN is only able to learn higher-level representations of past or future per se.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the baseline models?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n<sep><table cellspacing=\"0\" cellpadding=\"0\" border=\"0\" width=\"100%\" align=\"center\">\n<tr>\n<td align=\"center\">\nFigure 1: Results of the baseline models. The first column shows the performance of the baseline models on the WNUT 2017 dataset. The second column shows the performance of the baseline models on the OntoNotes 5.0 dataset. The third column shows the performance of the baseline models on the WNUT 2017 dataset with the addition of a gazetteer. The fourth column shows the performance of the baseline\n\nAnswer B:\nBiLSTM-CNN. Explanation: Previous studies have tried to stack multiple LSTMs for sequence-labeling NER BIBREF1. As they follow the trend of stacking forward and backward LSTMs independently, the Baseline-BiLSTM-CNN is only able to learn higher-level representations of past or future per se.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01076",
    "question": "What text classification tasks are considered?",
    "gold_answer": "",
    "answer_a": "What are the performance metrics?\n<blockquote>Text classification tasks: We consider three text classification tasks: sentiment classification, question classification, and question classification with a single answer. We use the Stanford Sentiment Treebank (SST) as the sentiment classification task and the Stanford Question Answering Dataset (SQuAD) as the question classification task. We use the Stanford Question Answering Dataset (SQuAD) as the single-answer task. We use the Stanford Question Answering Dataset (SQuAD) as the single-answer task. We use the Stanford Question Answering Dataset (SQuAD) as the single-answer task. We use",
    "answer_b": "news classification. Explanation: We use publicly available text classification datasets from BIBREF22 to evaluate our models (http://goo.gl/JyCnZq). This collection of datasets includes text classification datasets from diverse domains such as news classification (AGNews), sentiment analysis (Yelp, Amazon), Wikipedia article classification (DBPedia), and questions and answers categorization (Yahoo).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat text classification tasks are considered?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the performance metrics?\n<blockquote>Text classification tasks: We consider three text classification tasks: sentiment classification, question classification, and question classification with a single answer. We use the Stanford Sentiment Treebank (SST) as the sentiment classification task and the Stanford Question Answering Dataset (SQuAD) as the question classification task. We use the Stanford Question Answering Dataset (SQuAD) as the single-answer task. We use the Stanford Question Answering Dataset (SQuAD) as the single-answer task. We use the Stanford Question Answering Dataset (SQuAD) as the single-answer task. We use\n\nAnswer B:\nnews classification. Explanation: We use publicly available text classification datasets from BIBREF22 to evaluate our models (http://goo.gl/JyCnZq). This collection of datasets includes text classification datasets from diverse domains such as news classification (AGNews), sentiment analysis (Yelp, Amazon), Wikipedia article classification (DBPedia), and questions and answers categorization (Yahoo).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01076",
    "question": "What is episodic memory?",
    "gold_answer": "",
    "answer_a": "module that stores previously seen examples throughout its lifetime. Explanation: Our model is augmented with an episodic memory module that stores previously seen examples throughout its lifetime. The episodic memory module is used for sparse experience replay and local adaptation to prevent catastrophic forgetting and encourage positive transfer.",
    "answer_b": "How does it work?\n<sep>Episodic memory is a memory that is stored in a memory system that is able to store and retrieve information from a stream of text examples. It is a memory that is able to store and retrieve information from a stream of text examples. It is a memory that is able to store and retrieve information from a stream of text examples. It is a memory that is able to store and retrieve information from a stream of text examples. It is a memory that is able to store and retrieve information from a stream of text examples. It is a memory that is able to store and retrieve information from a stream",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is episodic memory?\n\nGround Truth Answer:\n\n\nAnswer A:\nmodule that stores previously seen examples throughout its lifetime. Explanation: Our model is augmented with an episodic memory module that stores previously seen examples throughout its lifetime. The episodic memory module is used for sparse experience replay and local adaptation to prevent catastrophic forgetting and encourage positive transfer.\n\nAnswer B:\nHow does it work?\n<sep>Episodic memory is a memory that is stored in a memory system that is able to store and retrieve information from a stream of text examples. It is a memory that is able to store and retrieve information from a stream of text examples. It is a memory that is able to store and retrieve information from a stream of text examples. It is a memory that is able to store and retrieve information from a stream of text examples. It is a memory that is able to store and retrieve information from a stream of text examples. It is a memory that is able to store and retrieve information from a stream\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.09919",
    "question": "What is the performance of the model on English, Spanish and Arabic?",
    "gold_answer": "",
    "answer_a": "on PAN 2018 dataset, the accuracy is 82.31% for English, 80.22% for Spanish and 80.50% for Arabic. Explanation: FLOAT SELECTED: Table 3: Accuracy on PAN 2018 test set.",
    "answer_b": "How does the performance change when concatenating LSA-reduced n-gram features with the learned neural representation of a user?\n< Explanation: The performance of the model on English, Spanish and Arabic is shown in Table TABREF10. The results show that the proposed model achieves state-of-the-art performance on English and has competitive results on Spanish and Arabic. The performance of the model on Arabic is slightly better than English and slightly worse than Spanish. The performance of the model on Spanish is slightly better than English and slightly worse than Arabic. The performance of the model on English is slightly better than Spanish and slightly worse than Arabic. The performance",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the performance of the model on English, Spanish and Arabic?\n\nGround Truth Answer:\n\n\nAnswer A:\non PAN 2018 dataset, the accuracy is 82.31% for English, 80.22% for Spanish and 80.50% for Arabic. Explanation: FLOAT SELECTED: Table 3: Accuracy on PAN 2018 test set.\n\nAnswer B:\nHow does the performance change when concatenating LSA-reduced n-gram features with the learned neural representation of a user?\n< Explanation: The performance of the model on English, Spanish and Arabic is shown in Table TABREF10. The results show that the proposed model achieves state-of-the-art performance on English and has competitive results on Spanish and Arabic. The performance of the model on Arabic is slightly better than English and slightly worse than Spanish. The performance of the model on Spanish is slightly better than English and slightly worse than Arabic. The performance of the model on English is slightly better than Spanish and slightly worse than Arabic. The performance\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.09919",
    "question": "How is this model different from a LSTM?",
    "gold_answer": "",
    "answer_a": "It's a recurrent neural network with n-gram model. Explanation: Model architecture ::: RNN with N-gram Model\nFor this model (denoted RNNwA + n-gram on results), n-gram features are collected with the same method described in BIBREF3. At the beginning, word level and character level n-gram features are obtained and concatenated. Then they are normalized with tf-idf transformation. For reducing the number of features and sparsity in n-gram vectors, tuples that have frequency less than 2 are ignored. For character level n-gram $N$ is selected as $3,4$, and 5 and for word level n-gram, $N$ is $1,2$ for Spanish and Arabic; $1,2,3$ for English. The dimension of the vector is reduced by LSA to 300. Then the vector is concatenated with neural representation which is produced right after tweet level attention in RNNwA model. The resultant representation is fed to a fully- connected layer that produces predictions.",
    "answer_b": "What is the difference between the two models?\n< Explanation: The proposed model is different from a LSTM in that it uses a RNNwA model with attention mechanism. The RNNwA model is a variant of the RNN model that uses attention mechanism to learn the context of a word. The RNNwA model is a generalization of the RNN model that can be used to learn the context of a word in a sentence. The RNNwA model is trained using a sequence of words to predict the next word in the sequence. The RNNwA model is trained using a sequence of words to predict the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is this model different from a LSTM?\n\nGround Truth Answer:\n\n\nAnswer A:\nIt's a recurrent neural network with n-gram model. Explanation: Model architecture ::: RNN with N-gram Model\nFor this model (denoted RNNwA + n-gram on results), n-gram features are collected with the same method described in BIBREF3. At the beginning, word level and character level n-gram features are obtained and concatenated. Then they are normalized with tf-idf transformation. For reducing the number of features and sparsity in n-gram vectors, tuples that have frequency less than 2 are ignored. For character level n-gram $N$ is selected as $3,4$, and 5 and for word level n-gram, $N$ is $1,2$ for Spanish and Arabic; $1,2,3$ for English. The dimension of the vector is reduced by LSA to 300. Then the vector is concatenated with neural representation which is produced right after tweet level attention in RNNwA model. The resultant representation is fed to a fully- connected layer that produces predictions.\n\nAnswer B:\nWhat is the difference between the two models?\n< Explanation: The proposed model is different from a LSTM in that it uses a RNNwA model with attention mechanism. The RNNwA model is a variant of the RNN model that uses attention mechanism to learn the context of a word. The RNNwA model is a generalization of the RNN model that can be used to learn the context of a word in a sentence. The RNNwA model is trained using a sequence of words to predict the next word in the sequence. The RNNwA model is trained using a sequence of words to predict the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.10670",
    "question": "What does the cache consist of?",
    "gold_answer": "",
    "answer_a": "static public cache stores the most frequent states. Explanation: In order to support this mechanism, we use a two-layered cached FST for decoding. The first layer is public cache which represents $T_P$. It is a static cache created by pre-initialization. The second layer is the private cache, which is owned by a particular user and constructed on-the-fly.",
    "answer_b": "How is it shared?\n< Explanation: The cache consists of two layers. The first layer is a public cache that stores most of the static part of the graph. This is shared globally among all users. The second layer is a private cache that caches the graph that represents the personalized language model, which is only shared by the utterances from a particular user. We also propose two simple yet effective pre-initialization methods, one based on breadth-first search, and another based on a data-driven exploration of decoder states using previous utterances. Experiments with a calling speech recognition task using a personalized contact list demonstrate that the proposed public cache reduces decoding",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat does the cache consist of?\n\nGround Truth Answer:\n\n\nAnswer A:\nstatic public cache stores the most frequent states. Explanation: In order to support this mechanism, we use a two-layered cached FST for decoding. The first layer is public cache which represents $T_P$. It is a static cache created by pre-initialization. The second layer is the private cache, which is owned by a particular user and constructed on-the-fly.\n\nAnswer B:\nHow is it shared?\n< Explanation: The cache consists of two layers. The first layer is a public cache that stores most of the static part of the graph. This is shared globally among all users. The second layer is a private cache that caches the graph that represents the personalized language model, which is only shared by the utterances from a particular user. We also propose two simple yet effective pre-initialization methods, one based on breadth-first search, and another based on a data-driven exploration of decoder states using previous utterances. Experiments with a calling speech recognition task using a personalized contact list demonstrate that the proposed public cache reduces decoding\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.10670",
    "question": "What languages is the model tested on?",
    "gold_answer": "",
    "answer_a": "How many users are in the dataset?\n< Explanation: We use the following languages: English, French, German, Spanish, Italian, Portuguese, and Chinese. We use 1000 users, each with 100 utterances. We use a 1000-word vocabulary, and we use a 1000-word model. We use a 1000-word model for all languages. We use a 1000-word model for all languages. We use a 1000-word model for all languages. We use a 1000-word model for all languages",
    "answer_b": "English. Explanation: Experiments are performed on two data sets. The first contains 7,500 utterances from the calling domain from Facebook employees. This includes commands like \u201cPlease call Jun Liu now\". The second consists of approximately 10,000 utterances from other common domains, such as weather, time, and music.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat languages is the model tested on?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow many users are in the dataset?\n< Explanation: We use the following languages: English, French, German, Spanish, Italian, Portuguese, and Chinese. We use 1000 users, each with 100 utterances. We use a 1000-word vocabulary, and we use a 1000-word model. We use a 1000-word model for all languages. We use a 1000-word model for all languages. We use a 1000-word model for all languages. We use a 1000-word model for all languages\n\nAnswer B:\nEnglish. Explanation: Experiments are performed on two data sets. The first contains 7,500 utterances from the calling domain from Facebook employees. This includes commands like \u201cPlease call Jun Liu now\". The second consists of approximately 10,000 utterances from other common domains, such as weather, time, and music.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.10670",
    "question": "What is a personalized language model?",
    "gold_answer": "",
    "answer_a": "How is it used?\n< Explanation: The personalized language model (PLM) is a language model that is trained on a large corpus of utterances and then used to generate a personalized transcript. The PLM is trained on a corpus of utterances and then used to generate a personalized transcript. The PLM is trained on a corpus of utterances and then used to generate a personalized transcript. The PLM is trained on a corpus of utterances and then used to generate a personalized transcript. The PLM is trained on a corpus of utterances and then used to generate a personalized transcript. The PLM is trained on a corpus of",
    "answer_b": "A model that contains the expected user-specific entities.. Explanation: One way to improve performance is such cases is through the use of a personal language model (LM) which contains the expected user-specific entities.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is a personalized language model?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it used?\n< Explanation: The personalized language model (PLM) is a language model that is trained on a large corpus of utterances and then used to generate a personalized transcript. The PLM is trained on a corpus of utterances and then used to generate a personalized transcript. The PLM is trained on a corpus of utterances and then used to generate a personalized transcript. The PLM is trained on a corpus of utterances and then used to generate a personalized transcript. The PLM is trained on a corpus of utterances and then used to generate a personalized transcript. The PLM is trained on a corpus of\n\nAnswer B:\nA model that contains the expected user-specific entities.. Explanation: One way to improve performance is such cases is through the use of a personal language model (LM) which contains the expected user-specific entities.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1902.06734",
    "question": "Is the dataset used in other work?",
    "gold_answer": "",
    "answer_a": "If so, what?\n<blockquote>Our dataset is based on the Twitter dataset of <a href=\"https://www.kaggle.com/c/twitter-hate-speech-dataset\" target=\"_blank\">Twitter Hate Speech Dataset</a>. The dataset contains 16k tweets from 1000 users. The dataset is available at <a href=\"https://drive.google.com/file/d/0Bx68800000000000000000000000000000000000000000",
    "answer_b": "Yes, in Waseem and Hovy (2016). Explanation: We experiment with the dataset of Waseem and Hovy c53cecce142c48628b3883d13155261c, containing tweets manually annotated for hate speech. The authors retrieved around $136k$ tweets over a period of two months. They bootstrapped their collection process with a search for commonly used slurs and expletives related to religious, sexual, gender and ethnic minorities. From the results, they identified terms and references to entities that frequently showed up in hateful tweets. Based on this sample, they used a public Twitter api to collect the entire corpus of ca. $136k$ tweets. After having manually annotated a randomly sampled subset of $16,914$ tweets under the categories racism, sexism or none themselves, they asked an expert to review their annotations in order to mitigate against any biases.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nIs the dataset used in other work?\n\nGround Truth Answer:\n\n\nAnswer A:\nIf so, what?\n<blockquote>Our dataset is based on the Twitter dataset of <a href=\"https://www.kaggle.com/c/twitter-hate-speech-dataset\" target=\"_blank\">Twitter Hate Speech Dataset</a>. The dataset contains 16k tweets from 1000 users. The dataset is available at <a href=\"https://drive.google.com/file/d/0Bx68800000000000000000000000000000000000000000\n\nAnswer B:\nYes, in Waseem and Hovy (2016). Explanation: We experiment with the dataset of Waseem and Hovy c53cecce142c48628b3883d13155261c, containing tweets manually annotated for hate speech. The authors retrieved around $136k$ tweets over a period of two months. They bootstrapped their collection process with a search for commonly used slurs and expletives related to religious, sexual, gender and ethnic minorities. From the results, they identified terms and references to entities that frequently showed up in hateful tweets. Based on this sample, they used a public Twitter api to collect the entire corpus of ca. $136k$ tweets. After having manually annotated a randomly sampled subset of $16,914$ tweets under the categories racism, sexism or none themselves, they asked an expert to review their annotations in order to mitigate against any biases.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1902.06734",
    "question": "What is the drawback to methods that rely on textual cues?",
    "gold_answer": "",
    "answer_a": "How do they compare to the current state of the art?\n< Explanation: The current state-of-the-art approaches to hate speech detection are oblivious to user and community information and rely entirely on textual (i.e., lexical and semantic) cues. In this paper, we propose a novel approach to this problem that incorporates community-based profiling features of Twitter users. Experimenting with a dataset of 16k tweets, we show that our methods significantly outperform the current state of the art in hate speech detection. Further, we conduct a qualitative analysis of model characteristics. We release our code, pre-trained models and all the resources used in the public",
    "answer_b": "tweets that are part of a larger hateful discourse or contain links to hateful content while not explicitly having textual cues. Explanation: Table 3 shows examples of hateful tweets from the dataset that are misclassified by the lr method, but are correctly classified upon the addition of author profiling features, i.e., by the lr + auth method. It is worth noting that some of the wins scored by the latter are on tweets that are part of a larger hateful discourse or contain links to hateful content while not explicitly having textual cues that are indicative of hate speech per se. The addition of author profiling features may then be viewed as a proxy for wider discourse information, thus allowing us to correctly resolve the cases where lexical and semantic features alone are insufficient.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the drawback to methods that rely on textual cues?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they compare to the current state of the art?\n< Explanation: The current state-of-the-art approaches to hate speech detection are oblivious to user and community information and rely entirely on textual (i.e., lexical and semantic) cues. In this paper, we propose a novel approach to this problem that incorporates community-based profiling features of Twitter users. Experimenting with a dataset of 16k tweets, we show that our methods significantly outperform the current state of the art in hate speech detection. Further, we conduct a qualitative analysis of model characteristics. We release our code, pre-trained models and all the resources used in the public\n\nAnswer B:\ntweets that are part of a larger hateful discourse or contain links to hateful content while not explicitly having textual cues. Explanation: Table 3 shows examples of hateful tweets from the dataset that are misclassified by the lr method, but are correctly classified upon the addition of author profiling features, i.e., by the lr + auth method. It is worth noting that some of the wins scored by the latter are on tweets that are part of a larger hateful discourse or contain links to hateful content while not explicitly having textual cues that are indicative of hate speech per se. The addition of author profiling features may then be viewed as a proxy for wider discourse information, thus allowing us to correctly resolve the cases where lexical and semantic features alone are insufficient.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1902.06734",
    "question": "What community-based profiling features are used?",
    "gold_answer": "",
    "answer_a": "How are they evaluated?\n< Explanation: We use the following community-based profiling features: (1) the number of tweets from the same user, (2) the number of tweets from the same community, (3) the number of tweets from the same user in the same community, (4) the number of tweets from the same user in the same community and the same community, (5) the number of tweets from the same user in the same community and the same community, (6) the number of tweets from the same user in the same community and the same community and the same community, (7) the number of tweets",
    "answer_b": "The features are the outputs from node2vec when run on a community graph where nodes are users and edges are connections if one user follows the other on Twitter.. Explanation: In order to leverage community-based information for the authors whose tweets form our dataset, we create an undirected unlabeled community graph wherein nodes are the authors and edges are the connections between them. An edge is instantiated between two authors $u$ and $v$ if $u$ follows $v$ on Twitter or vice versa.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat community-based profiling features are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they evaluated?\n< Explanation: We use the following community-based profiling features: (1) the number of tweets from the same user, (2) the number of tweets from the same community, (3) the number of tweets from the same user in the same community, (4) the number of tweets from the same user in the same community and the same community, (5) the number of tweets from the same user in the same community and the same community, (6) the number of tweets from the same user in the same community and the same community and the same community, (7) the number of tweets\n\nAnswer B:\nThe features are the outputs from node2vec when run on a community graph where nodes are users and edges are connections if one user follows the other on Twitter.. Explanation: In order to leverage community-based information for the authors whose tweets form our dataset, we create an undirected unlabeled community graph wherein nodes are the authors and edges are the connections between them. An edge is instantiated between two authors $u$ and $v$ if $u$ follows $v$ on Twitter or vice versa.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.08540",
    "question": "what user traits are taken into account?",
    "gold_answer": "",
    "answer_a": "The hierarchical personal values lexicon with 50  sets of words and phrases that represent the user's value.. Explanation: In order to get a representation of a user's values, we turn to the hierarchical personal values lexicon from BIBREF24 . In this lexicon, there are 50 value dimensions, represented as sets of words and phrases that characterize that value.",
    "answer_b": "Explanation: We use a set of 1000 user profiles to train a neural network model to predict which activities a user has performed. We use the following user traits: gender, age, occupation, and education. We use the following activities: walking, running, cycling, swimming, biking, hiking, camping, hiking, biking, and cycling. We use the following activities: eating, drinking, drinking, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating,",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat user traits are taken into account?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe hierarchical personal values lexicon with 50  sets of words and phrases that represent the user's value.. Explanation: In order to get a representation of a user's values, we turn to the hierarchical personal values lexicon from BIBREF24 . In this lexicon, there are 50 value dimensions, represented as sets of words and phrases that characterize that value.\n\nAnswer B:\nExplanation: We use a set of 1000 user profiles to train a neural network model to predict which activities a user has performed. We use the following user traits: gender, age, occupation, and education. We use the following activities: walking, running, cycling, swimming, biking, hiking, camping, hiking, biking, and cycling. We use the following activities: eating, drinking, drinking, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating, eating,\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.08540",
    "question": "how many activities are in the dataset?",
    "gold_answer": "",
    "answer_a": "29,494. Explanation: FLOAT SELECTED: Table 2: Number of human activity queries from multiple sources.",
    "answer_b": "What are the characteristics of the activities?\nThere are 1,000 activities in the dataset. The activities are divided into 10 clusters, each containing 10 activities. The characteristics of the activities are as follows: 1) the activities are divided into 10 clusters, each containing 10 activities; 2) the activities are divided into 10 clusters, each containing 10 activities; 3) the activities are divided into 10 clusters, each containing 10 activities; 4) the activities are divided into 10 clusters, each containing 10 activities;",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow many activities are in the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\n29,494. Explanation: FLOAT SELECTED: Table 2: Number of human activity queries from multiple sources.\n\nAnswer B:\nWhat are the characteristics of the activities?\nThere are 1,000 activities in the dataset. The activities are divided into 10 clusters, each containing 10 activities. The characteristics of the activities are as follows: 1) the activities are divided into 10 clusters, each containing 10 activities; 2) the activities are divided into 10 clusters, each containing 10 activities; 3) the activities are divided into 10 clusters, each containing 10 activities; 4) the activities are divided into 10 clusters, each containing 10 activities;\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.08540",
    "question": "who annotated the datset?",
    "gold_answer": "",
    "answer_a": "How did they annotate it?\n< Explanation: We collected a dataset of 10,000 posts from Twitter users, containing 1,000 posts per user. We then used a pre-trained word embedding model to extract the semantic meaning of each tweet. We then used a neural network to predict which cluster each tweet belongs to. We used a clustering algorithm to group the tweets into 10 clusters. We then used the cluster labels to train a neural network to predict which tweets belong to which cluster. We used a pre-trained word embedding model to extract the semantic meaning of each tweet. We then used a neural network",
    "answer_b": "1000 people. Explanation: In order to get an even richer set of human activities, we also ask a set of 1,000 people across the United States to list any five activities that they had done in the past week. We collect our responses using Amazon Mechanical Turk, and manually verify that all responses are reasonable.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwho annotated the datset?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow did they annotate it?\n< Explanation: We collected a dataset of 10,000 posts from Twitter users, containing 1,000 posts per user. We then used a pre-trained word embedding model to extract the semantic meaning of each tweet. We then used a neural network to predict which cluster each tweet belongs to. We used a clustering algorithm to group the tweets into 10 clusters. We then used the cluster labels to train a neural network to predict which tweets belong to which cluster. We used a pre-trained word embedding model to extract the semantic meaning of each tweet. We then used a neural network\n\nAnswer B:\n1000 people. Explanation: In order to get an even richer set of human activities, we also ask a set of 1,000 people across the United States to list any five activities that they had done in the past week. We collect our responses using Amazon Mechanical Turk, and manually verify that all responses are reasonable.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.08540",
    "question": "how were the data instances chosen?",
    "gold_answer": "",
    "answer_a": "What are the user demographics?\n<sep><xml>\n<dataset>\n<user>\n<id>1</id>\n<name>John</name>\n<gender>Male</gender>\n<age>25</age>\n<occupation>Engineer</occupation>\n<interests>Reading, Travel, Cooking</interests>\n<interests>Travel, Cooking</interests>\n<interests>Reading, Travel, Cooking</interests>\n<interests>Reading, Travel, Cooking</interests>\n<interests>Reading, Travel, Cooking</interests>\n<interests>Reading, Travel, Cooking</interest",
    "answer_b": "query contains a first-person, past-tense verb within a phrase that describes a common activity that people do. Explanation: Therefore, in order to target only those tweets that are rich in human activity content, we formulate a set of queries that allows us to use the Twitter Search API to find instances of users tweeting about common human activities. Each query contains a first-person, past-tense verb within a phrase that describes a common activity that people do.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow were the data instances chosen?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the user demographics?\n<sep><xml>\n<dataset>\n<user>\n<id>1</id>\n<name>John</name>\n<gender>Male</gender>\n<age>25</age>\n<occupation>Engineer</occupation>\n<interests>Reading, Travel, Cooking</interests>\n<interests>Travel, Cooking</interests>\n<interests>Reading, Travel, Cooking</interests>\n<interests>Reading, Travel, Cooking</interests>\n<interests>Reading, Travel, Cooking</interests>\n<interests>Reading, Travel, Cooking</interest\n\nAnswer B:\nquery contains a first-person, past-tense verb within a phrase that describes a common activity that people do. Explanation: Therefore, in order to target only those tweets that are rich in human activity content, we formulate a set of queries that allows us to use the Twitter Search API to find instances of users tweeting about common human activities. Each query contains a first-person, past-tense verb within a phrase that describes a common activity that people do.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.08540",
    "question": "what social media platform was the data collected from?",
    "gold_answer": "",
    "answer_a": "Explanation: We collected data from Twitter, Instagram, and Reddit. We used the Twitter API to extract tweets from the last 30 days for each of these platforms. We then used the Twitter API to extract the user profile information for each of these users. We used the Twitter API to extract the user's self-description and the number of tweets they had written in the last 30 days. We then used the Twitter API to extract the user's location and the number of followers they had. We used the Twitter API to extract the user's posts and the number of likes they had. We then used the Twitter API to extract the",
    "answer_b": "Twitter. Explanation: Therefore, in order to target only those tweets that are rich in human activity content, we formulate a set of queries that allows us to use the Twitter Search API to find instances of users tweeting about common human activities.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat social media platform was the data collected from?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We collected data from Twitter, Instagram, and Reddit. We used the Twitter API to extract tweets from the last 30 days for each of these platforms. We then used the Twitter API to extract the user profile information for each of these users. We used the Twitter API to extract the user's self-description and the number of tweets they had written in the last 30 days. We then used the Twitter API to extract the user's location and the number of followers they had. We used the Twitter API to extract the user's posts and the number of likes they had. We then used the Twitter API to extract the\n\nAnswer B:\nTwitter. Explanation: Therefore, in order to target only those tweets that are rich in human activity content, we formulate a set of queries that allows us to use the Twitter Search API to find instances of users tweeting about common human activities.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.04887",
    "question": "What conclusions do the authors draw from their experiments?",
    "gold_answer": "",
    "answer_a": "among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models. Explanation: We observed that among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models. Tweet length affects the task prediction accuracies, but we found that all models behave similarly under variation in tweet length. Finally while LDA is insensitive to input word order, CNN, LSTM and BLSTM are extremely sensitive to word order.",
    "answer_b": "They conclude that the representation learning models are not able to model the properties of the tweets.\n<blockquote>Our results show that the representation learning models are not able to model the properties of the tweets. In fact, the models are not able to model the properties of the tweets at all. The models are able to model the properties of the tweets in a very limited way, and the models are not able to model the properties of the tweets at all. In fact, the models are not able to model the properties of the tweets at all. The models are able to model the properties of the tweets in a very limited way, and the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat conclusions do the authors draw from their experiments?\n\nGround Truth Answer:\n\n\nAnswer A:\namong supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models. Explanation: We observed that among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models. Tweet length affects the task prediction accuracies, but we found that all models behave similarly under variation in tweet length. Finally while LDA is insensitive to input word order, CNN, LSTM and BLSTM are extremely sensitive to word order.\n\nAnswer B:\nThey conclude that the representation learning models are not able to model the properties of the tweets.\n<blockquote>Our results show that the representation learning models are not able to model the properties of the tweets. In fact, the models are not able to model the properties of the tweets at all. The models are able to model the properties of the tweets in a very limited way, and the models are not able to model the properties of the tweets at all. In fact, the models are not able to model the properties of the tweets at all. The models are able to model the properties of the tweets in a very limited way, and the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.04887",
    "question": "In what way does each classifier evaluate one of the syntactic or social properties which are salient for a tweet?",
    "gold_answer": "",
    "answer_a": "What are the properties that are evaluated?\n< Explanation: The classifiers are trained on the tweet representation and the properties are evaluated on the test set. The classifiers are trained on the tweet representation and the properties are evaluated on the test set. The classifiers are trained on the tweet representation and the properties are evaluated on the test set. The classifiers are trained on the tweet representation and the properties are evaluated on the test set. The classifiers are trained on the tweet representation and the properties are evaluated on the test set. The classifiers are trained on the tweet representation and the properties are evaluated on the test set. The classifiers are trained on the tweet representation and",
    "answer_b": "if we cannot train a classifier to predict a property based on its tweet representation, then this property is not encoded in this representation. Explanation: We assume that if we cannot train a classifier to predict a property based on its tweet representation, then this property is not encoded in this representation. For example, the model which preserves the tweet length should perform well in predicting the length given the representation generated from the model. Though these elementary property prediction tasks are not directly related to any downstream application, knowing that the model is good at modeling a particular property (e.g., the social properties) indicates that it could excel in correlated applications (e.g., user profiling task).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nIn what way does each classifier evaluate one of the syntactic or social properties which are salient for a tweet?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the properties that are evaluated?\n< Explanation: The classifiers are trained on the tweet representation and the properties are evaluated on the test set. The classifiers are trained on the tweet representation and the properties are evaluated on the test set. The classifiers are trained on the tweet representation and the properties are evaluated on the test set. The classifiers are trained on the tweet representation and the properties are evaluated on the test set. The classifiers are trained on the tweet representation and the properties are evaluated on the test set. The classifiers are trained on the tweet representation and the properties are evaluated on the test set. The classifiers are trained on the tweet representation and\n\nAnswer B:\nif we cannot train a classifier to predict a property based on its tweet representation, then this property is not encoded in this representation. Explanation: We assume that if we cannot train a classifier to predict a property based on its tweet representation, then this property is not encoded in this representation. For example, the model which preserves the tweet length should perform well in predicting the length given the representation generated from the model. Though these elementary property prediction tasks are not directly related to any downstream application, knowing that the model is good at modeling a particular property (e.g., the social properties) indicates that it could excel in correlated applications (e.g., user profiling task).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.10006",
    "question": "How is a per-word reward tuned with the perceptron algorithm?",
    "gold_answer": "",
    "answer_a": "What is the perceptron algorithm?\n< Explanation: We use the perceptron algorithm to tune the reward. The reward is a per-word scalar, and we use the perceptron algorithm to find the best possible reward for each word. The reward is computed as the difference between the log-likelihood of the translation and the log-likelihood of the best translation. The reward is then multiplied by the number of times the word is used in the translation. The reward is then multiplied by the number of times the word is used in the translation. The reward is then multiplied by the number of times the word is used in the translation. The reward is",
    "answer_b": "Optimal per-word reward is found using SGD, which in this case is the same as the perceptron algorithm. Explanation: A variety of methods, like minimum error rate training BIBREF14 , BIBREF5 , are possible, but keeping with the globally-normalized negative log-likelihood, we obtain, for the constant word reward, the gradient: INLINEFORM3\n\nwhere INLINEFORM0 is the 1-best translation. Then the stochastic gradient descent update is just the familiar perceptron rule: INLINEFORM1\n\nalthough below, we update on a batch of sentences rather than a single sentence.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is a per-word reward tuned with the perceptron algorithm?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the perceptron algorithm?\n< Explanation: We use the perceptron algorithm to tune the reward. The reward is a per-word scalar, and we use the perceptron algorithm to find the best possible reward for each word. The reward is computed as the difference between the log-likelihood of the translation and the log-likelihood of the best translation. The reward is then multiplied by the number of times the word is used in the translation. The reward is then multiplied by the number of times the word is used in the translation. The reward is then multiplied by the number of times the word is used in the translation. The reward is\n\nAnswer B:\nOptimal per-word reward is found using SGD, which in this case is the same as the perceptron algorithm. Explanation: A variety of methods, like minimum error rate training BIBREF14 , BIBREF5 , are possible, but keeping with the globally-normalized negative log-likelihood, we obtain, for the constant word reward, the gradient: INLINEFORM3\n\nwhere INLINEFORM0 is the 1-best translation. Then the stochastic gradient descent update is just the familiar perceptron rule: INLINEFORM1\n\nalthough below, we update on a batch of sentences rather than a single sentence.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.10006",
    "question": "What methods are used to correct the brevity problem?",
    "gold_answer": "",
    "answer_a": "tuned word reward. Explanation: The results of tuning the word reward, INLINEFORM0 , as described in Section SECREF6 , is shown in the second section of Tables TABREF10 , TABREF11 , and TABREF12 . In contrast to our baseline systems, our tuned word reward always fixes the brevity problem (length ratios are approximately 1.0), and generally fixes the beam problem. An optimized word reward score always leads to improvements in METEOR scores over any of the best baselines.",
    "answer_b": "What is the perceptron algorithm used?\n< Explanation: We use the following methods to correct the brevity problem: (1) a simple per-word reward, (2) a simple per-word penalty, (3) a simple per-word penalty plus a per-word reward, and (4) a simple per-word penalty plus a per-word reward plus a per-word penalty. We compare these methods to the perceptron algorithm, which is a simple way to tune the reward. We use the perceptron algorithm to tune the reward, and we compare the results to the simple per-word reward. We also compare the results to the simple per",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat methods are used to correct the brevity problem?\n\nGround Truth Answer:\n\n\nAnswer A:\ntuned word reward. Explanation: The results of tuning the word reward, INLINEFORM0 , as described in Section SECREF6 , is shown in the second section of Tables TABREF10 , TABREF11 , and TABREF12 . In contrast to our baseline systems, our tuned word reward always fixes the brevity problem (length ratios are approximately 1.0), and generally fixes the beam problem. An optimized word reward score always leads to improvements in METEOR scores over any of the best baselines.\n\nAnswer B:\nWhat is the perceptron algorithm used?\n< Explanation: We use the following methods to correct the brevity problem: (1) a simple per-word reward, (2) a simple per-word penalty, (3) a simple per-word penalty plus a per-word reward, and (4) a simple per-word penalty plus a per-word reward plus a per-word penalty. We compare these methods to the perceptron algorithm, which is a simple way to tune the reward. We use the perceptron algorithm to tune the reward, and we compare the results to the simple per-word reward. We also compare the results to the simple per\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.10006",
    "question": "Why does wider beam search hurt NMT?",
    "gold_answer": "",
    "answer_a": "How does it hurt?\n< Explanation: The main reason for the loss of translation quality is that the beam search is not able to find the best translation. The beam search is a greedy search, and it is greedy in the sense that it always chooses the best translation at each step. The best translation is the one that is closest to the target. The beam search is greedy because it is greedy in the sense that it always chooses the best translation at each step. The best translation is the one that is closest to the target. The best translation is the one that is closest to the target. The best translation is the one that is closest",
    "answer_b": "Using a wider beam increases the probability of a shorter translation to remain in the top k variants and eventually score higher than any longer and more accurate translation variant. Explanation: More generally, consider beam search: at time step INLINEFORM0 , only the top INLINEFORM1 partial or complete translations are retained while the rest are pruned.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhy does wider beam search hurt NMT?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow does it hurt?\n< Explanation: The main reason for the loss of translation quality is that the beam search is not able to find the best translation. The beam search is a greedy search, and it is greedy in the sense that it always chooses the best translation at each step. The best translation is the one that is closest to the target. The beam search is greedy because it is greedy in the sense that it always chooses the best translation at each step. The best translation is the one that is closest to the target. The best translation is the one that is closest to the target. The best translation is the one that is closest\n\nAnswer B:\nUsing a wider beam increases the probability of a shorter translation to remain in the top k variants and eventually score higher than any longer and more accurate translation variant. Explanation: More generally, consider beam search: at time step INLINEFORM0 , only the top INLINEFORM1 partial or complete translations are retained while the rest are pruned.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1702.02584",
    "question": "What linguistic model does the conventional method use?",
    "gold_answer": "",
    "answer_a": "How does it learn features?\n< Explanation: The conventional method uses a simple rule-based approach to learn features. It first extracts the word embeddings from the word embeddings database and then uses the word embeddings to learn features. The features are then used to classify the utterances. The conventional method is based on the following steps: (1) word embeddings: the word embeddings database contains 100000 word embeddings, each of which is a 300-dimensional vector. The word embeddings are learned by training a neural network on the word embeddings database. (2) feature extraction: the word embeddings are used to extract features from",
    "answer_b": "Random Forest to perform humor recognition by using the following two groups of features:  latent semantic structural features and semantic distance features.. Explanation: Following yang-EtAl:2015:EMNLP2, we applied Random Forest BIBREF12 to perform humor recognition by using the following two groups of features. The first group are latent semantic structural features covering the following 4 categories: Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4). The second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to this sentence (found by using a k-Nearest Neighbors (kNN) method), and each sentence's averaged Word2Vec representations ( INLINEFORM0 ).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat linguistic model does the conventional method use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow does it learn features?\n< Explanation: The conventional method uses a simple rule-based approach to learn features. It first extracts the word embeddings from the word embeddings database and then uses the word embeddings to learn features. The features are then used to classify the utterances. The conventional method is based on the following steps: (1) word embeddings: the word embeddings database contains 100000 word embeddings, each of which is a 300-dimensional vector. The word embeddings are learned by training a neural network on the word embeddings database. (2) feature extraction: the word embeddings are used to extract features from\n\nAnswer B:\nRandom Forest to perform humor recognition by using the following two groups of features:  latent semantic structural features and semantic distance features.. Explanation: Following yang-EtAl:2015:EMNLP2, we applied Random Forest BIBREF12 to perform humor recognition by using the following two groups of features. The first group are latent semantic structural features covering the following 4 categories: Incongruity (2), Ambiguity (6), Interpersonal Effect (4), and Phonetic Pattern (4). The second group are semantic distance features, including the humor label classes from 5 sentences in the training set that are closest to this sentence (found by using a k-Nearest Neighbors (kNN) method), and each sentence's averaged Word2Vec representations ( INLINEFORM0 ).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1702.02584",
    "question": "What is novel about the newly emerging CNN method, in comparison to well-established conventional method?",
    "gold_answer": "",
    "answer_a": "one layer of convolution on top of word embedding vectors, achieves excellent results on multiple tasks. Explanation: Convolutional Neural Networks (CNNs) have recently been successfully used in several text categorization tasks (e.g., review rating, sentiment recognition, and question type recognition). Kim2014,Johnson2015,Zhang2015 suggested that using a simple CNN setup, which entails one layer of convolution on top of word embedding vectors, achieves excellent results on multiple tasks.",
    "answer_b": "Explanation: The CNN method is based on a convolutional neural network (CNN) that is trained on a large corpus of humorous utterances. The CNN is trained on a large corpus of humorous utterances, which is then used to train a classifier. The CNN is trained on a large corpus of humorous utterances, which is then used to train a classifier. The CNN is trained on a large corpus of humorous utterances, which is then used to train a classifier. The CNN is trained on a large corpus of humorous utterances, which is then used to train a classifier. The CNN is trained on a large corpus of humorous utterances",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is novel about the newly emerging CNN method, in comparison to well-established conventional method?\n\nGround Truth Answer:\n\n\nAnswer A:\none layer of convolution on top of word embedding vectors, achieves excellent results on multiple tasks. Explanation: Convolutional Neural Networks (CNNs) have recently been successfully used in several text categorization tasks (e.g., review rating, sentiment recognition, and question type recognition). Kim2014,Johnson2015,Zhang2015 suggested that using a simple CNN setup, which entails one layer of convolution on top of word embedding vectors, achieves excellent results on multiple tasks.\n\nAnswer B:\nExplanation: The CNN method is based on a convolutional neural network (CNN) that is trained on a large corpus of humorous utterances. The CNN is trained on a large corpus of humorous utterances, which is then used to train a classifier. The CNN is trained on a large corpus of humorous utterances, which is then used to train a classifier. The CNN is trained on a large corpus of humorous utterances, which is then used to train a classifier. The CNN is trained on a large corpus of humorous utterances, which is then used to train a classifier. The CNN is trained on a large corpus of humorous utterances\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1702.02584",
    "question": "What lexical cues are used for humor recogition?",
    "gold_answer": "",
    "answer_a": "Incongruity. Explanation: In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website. BIBREF3 explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style. In addition, Word2Vec BIBREF4 distributed representations were utilized in the model building.",
    "answer_b": "Explanation: The first step of our approach is to extract the lexical cues from the utterances. We use the word-level features extracted from the TED talk utterances to build a lexicon. The lexicon consists of 1000 words, which are used to build a dictionary. The dictionary is then used to extract the lexical cues from the utterances. The lexical cues are the words that are used to describe humor. The words are extracted from the utterances and are used to build a dictionary. The dictionary is then used to extract the lexical cues from the utterances. The lexical cues are the words that are used to describe",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat lexical cues are used for humor recogition?\n\nGround Truth Answer:\n\n\nAnswer A:\nIncongruity. Explanation: In a recent work BIBREF3 , a new corpus was constructed from the Pun of the Day website. BIBREF3 explained and computed latent semantic structure features based on the following four aspects: (a) Incongruity, (b) Ambiguity, (c) Interpersonal Effect, and (d) Phonetic Style. In addition, Word2Vec BIBREF4 distributed representations were utilized in the model building.\n\nAnswer B:\nExplanation: The first step of our approach is to extract the lexical cues from the utterances. We use the word-level features extracted from the TED talk utterances to build a lexicon. The lexicon consists of 1000 words, which are used to build a dictionary. The dictionary is then used to extract the lexical cues from the utterances. The lexical cues are the words that are used to describe humor. The words are extracted from the utterances and are used to build a dictionary. The dictionary is then used to extract the lexical cues from the utterances. The lexical cues are the words that are used to describe\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1702.02584",
    "question": "How are the positive instances annotated? e.g. by annotators, or by laughter from the audience?",
    "gold_answer": "",
    "answer_a": "What is the annotation process?\n< Explanation: The positive instances are annotated by the audience. The annotators are trained to label the positive instances as either \u201cyes\u201d or \u201cno\u201d and the negative instances as \u201cyes\u201d or \u201cno\u201d. The annotators are trained to label the positive instances as either \u201cyes\u201d or \u201cno\u201d and the negative instances as \u201cyes\u201d or \u201cno\u201d. The annotators are trained to label the positive instances as either \u201cyes\u201d or \u201cno\u201d and the negative instances as \u201cyes\u201d or \u201cno\u201d. The annotators are trained to label the positive instances as either \u201cyes\u201d or \u201c",
    "answer_b": "Laughter from the audience.. Explanation: In this study, sentences containing or immediately followed by `(Laughter)' were used as `Laughter' sentences, as shown in Figure FIGREF4 ; all other sentences were defined as `No-Laughter' sentences.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are the positive instances annotated? e.g. by annotators, or by laughter from the audience?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the annotation process?\n< Explanation: The positive instances are annotated by the audience. The annotators are trained to label the positive instances as either \u201cyes\u201d or \u201cno\u201d and the negative instances as \u201cyes\u201d or \u201cno\u201d. The annotators are trained to label the positive instances as either \u201cyes\u201d or \u201cno\u201d and the negative instances as \u201cyes\u201d or \u201cno\u201d. The annotators are trained to label the positive instances as either \u201cyes\u201d or \u201cno\u201d and the negative instances as \u201cyes\u201d or \u201cno\u201d. The annotators are trained to label the positive instances as either \u201cyes\u201d or \u201c\n\nAnswer B:\nLaughter from the audience.. Explanation: In this study, sentences containing or immediately followed by `(Laughter)' were used as `Laughter' sentences, as shown in Figure FIGREF4 ; all other sentences were defined as `No-Laughter' sentences.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.05453",
    "question": "How you incorporate commonsense into an LSTM?",
    "gold_answer": "",
    "answer_a": "What is the best model?\n<sep><solution>Our model is based on the Tri-LSTM model, which is a combination of a recurrent neural network (RNN) and a sequence-to-sequence (Seq2Seq) model. The Tri-LSTM model is composed of three RNN layers, each with a bidirectional LSTM cell. The first RNN layer is responsible for encoding the message, the second RNN layer is responsible for encoding the commonsense, and the third RNN layer is responsible for encoding the response. The output of the first RNN layer is fed into the second RNN layer, and the output of",
    "answer_b": "by employing an external memory module containing commonsense knowledge. Explanation: Due to the vastness of such kind of knowledge, we speculate that this goal is better suited by employing an external memory module containing commonsense knowledge rather than forcing the system to encode it in model parameters as in traditional methods.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow you incorporate commonsense into an LSTM?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the best model?\n<sep><solution>Our model is based on the Tri-LSTM model, which is a combination of a recurrent neural network (RNN) and a sequence-to-sequence (Seq2Seq) model. The Tri-LSTM model is composed of three RNN layers, each with a bidirectional LSTM cell. The first RNN layer is responsible for encoding the message, the second RNN layer is responsible for encoding the commonsense, and the third RNN layer is responsible for encoding the response. The output of the first RNN layer is fed into the second RNN layer, and the output of\n\nAnswer B:\nby employing an external memory module containing commonsense knowledge. Explanation: Due to the vastness of such kind of knowledge, we speculate that this goal is better suited by employing an external memory module containing commonsense knowledge rather than forcing the system to encode it in model parameters as in traditional methods.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.05453",
    "question": "Which domain are the conversations in?",
    "gold_answer": "",
    "answer_a": "open-domain. Explanation: Our work distinguishes itself from previous research in that we consider a large heterogeneous commonsense knowledge base in an open-domain retrieval-based dialogue setting.",
    "answer_b": "How are they evaluated?\n< Explanation: The experiments are conducted on the dataset of the 2016-2017 Stanford Conversational Dialog Challenge (SCDC). The dataset consists of 10,000 dialog pairs between humans and conversational agents. The dataset is divided into training, validation and test sets. The training set consists of 6,000 dialog pairs, the validation set consists of 1,000 dialog pairs and the test set consists of 1,000 dialog pairs. The dataset is available at https://github.com/StanfordCSRC/SCDC.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich domain are the conversations in?\n\nGround Truth Answer:\n\n\nAnswer A:\nopen-domain. Explanation: Our work distinguishes itself from previous research in that we consider a large heterogeneous commonsense knowledge base in an open-domain retrieval-based dialogue setting.\n\nAnswer B:\nHow are they evaluated?\n< Explanation: The experiments are conducted on the dataset of the 2016-2017 Stanford Conversational Dialog Challenge (SCDC). The dataset consists of 10,000 dialog pairs between humans and conversational agents. The dataset is divided into training, validation and test sets. The training set consists of 6,000 dialog pairs, the validation set consists of 1,000 dialog pairs and the test set consists of 1,000 dialog pairs. The dataset is available at https://github.com/StanfordCSRC/SCDC.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.05453",
    "question": "Which commonsense knowledge base are they using?",
    "gold_answer": "",
    "answer_a": "ConceptNet. Explanation: In our experiment, ConceptNet is used as the commonsense knowledge base.",
    "answer_b": "What is the size of the commonsense knowledge base?\n< Explanation: We use the commonsense knowledge base (CKB) BIBREF11 , which is a large commonsense knowledge base that is used to train the Tri-LSTM model. The CKB is a combination of 100 commonsense knowledge items, each of which is associated with a 1000-word commonsense passage. The CKB is available at http://www.cs.cornell.edu/~matt/CKB. The CKB is divided into 100 commonsense knowledge items, each of which is associated with a 1",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich commonsense knowledge base are they using?\n\nGround Truth Answer:\n\n\nAnswer A:\nConceptNet. Explanation: In our experiment, ConceptNet is used as the commonsense knowledge base.\n\nAnswer B:\nWhat is the size of the commonsense knowledge base?\n< Explanation: We use the commonsense knowledge base (CKB) BIBREF11 , which is a large commonsense knowledge base that is used to train the Tri-LSTM model. The CKB is a combination of 100 commonsense knowledge items, each of which is associated with a 1000-word commonsense passage. The CKB is available at http://www.cs.cornell.edu/~matt/CKB. The CKB is divided into 100 commonsense knowledge items, each of which is associated with a 1\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.06854",
    "question": "How did they obtain the dataset?",
    "gold_answer": "",
    "answer_a": "The authors crawled all areas listed an TripAdvisor's SiteIndex and gathered all links related to hotels. Using Selenium, they put a time gap between opening each page, to mimic human behaviour and avoid having their scraper being detected. They discarded pages without a review and for pages with a review, they collected the review's profile, the overall rating, the summary, the written text and subratings, where given.. Explanation: We first crawled all areas listed on TripAdvisor's SiteIndex. Each area link leads to another page containing different information, such as a list of accommodations, or restaurants; we gathered all links corresponding to hotels. Our robot then opened each of the hotel links and filtered out hotels without any review. In total, in July 2019, there were $365\\,056$ out of $2\\,502\\,140$ hotels with at least one review.",
    "answer_b": "What is the size of the dataset?\n< Explanation: We obtained the dataset from the TripAdvisor website. We used the 2015 dataset, which contains 50M reviews. We removed the reviews with less than 1000 characters and the reviews with less than 1000 stars. We also removed the reviews with less than 1000 stars in the hotel category. We also removed the reviews with less than 1000 characters in the restaurant category. We removed the reviews with less than 1000 characters in the hotel category. We removed the reviews with less than",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow did they obtain the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe authors crawled all areas listed an TripAdvisor's SiteIndex and gathered all links related to hotels. Using Selenium, they put a time gap between opening each page, to mimic human behaviour and avoid having their scraper being detected. They discarded pages without a review and for pages with a review, they collected the review's profile, the overall rating, the summary, the written text and subratings, where given.. Explanation: We first crawled all areas listed on TripAdvisor's SiteIndex. Each area link leads to another page containing different information, such as a list of accommodations, or restaurants; we gathered all links corresponding to hotels. Our robot then opened each of the hotel links and filtered out hotels without any review. In total, in July 2019, there were $365\\,056$ out of $2\\,502\\,140$ hotels with at least one review.\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: We obtained the dataset from the TripAdvisor website. We used the 2015 dataset, which contains 50M reviews. We removed the reviews with less than 1000 characters and the reviews with less than 1000 stars. We also removed the reviews with less than 1000 stars in the hotel category. We also removed the reviews with less than 1000 characters in the restaurant category. We removed the reviews with less than 1000 characters in the hotel category. We removed the reviews with less than\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.05506",
    "question": "What sized character n-grams do they use?",
    "gold_answer": "",
    "answer_a": "How do they construct them?\n< Explanation: We use the following character n-grams: (1) 1, (2) 2, (3) 3, (4) 4, (5) 5, (6) 6, (7) 7, (8) 8, (9) 9, (10) 10, (11) 11, (12) 12, (13) 13, (14) 14, (15) 15, (16) 16,",
    "answer_b": "cahr3-MS-vec. Explanation: Moreover, to investigate the effect of only char INLINEFORM0 -MS-vec, we ignore INLINEFORM1 in Equation EQREF5 . We refer to this setting as \u201cRemove word embeddings INLINEFORM2 \u201d in Table TABREF24 . Table TABREF24 shows cahr3-MS-vec and char4-MS-vec are superior to char2-MS-vec.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat sized character n-grams do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they construct them?\n< Explanation: We use the following character n-grams: (1) 1, (2) 2, (3) 3, (4) 4, (5) 5, (6) 6, (7) 7, (8) 8, (9) 9, (10) 10, (11) 11, (12) 12, (13) 13, (14) 14, (15) 15, (16) 16,\n\nAnswer B:\ncahr3-MS-vec. Explanation: Moreover, to investigate the effect of only char INLINEFORM0 -MS-vec, we ignore INLINEFORM1 in Equation EQREF5 . We refer to this setting as \u201cRemove word embeddings INLINEFORM2 \u201d in Table TABREF24 . Table TABREF24 shows cahr3-MS-vec and char4-MS-vec are superior to char2-MS-vec.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.05506",
    "question": "Which dataset do they evaluate on for headline generation?",
    "gold_answer": "",
    "answer_a": "English Gigaword corpus. Explanation: For headline generation, we used sentence-headline pairs extracted from the annotated English Gigaword corpus BIBREF35 in the same manner as BIBREF2 .",
    "answer_b": "What is the perplexity of their model on the dataset?\n< Explanation: We evaluate our model on the headline generation task. We use the dataset of headline generation from the Wikipedia article \u201cThe New York Times\u201d (Wikipedia 2016). The dataset contains 1000 headlines and 1000 corresponding summaries. The summaries are generated by the system and the headlines are generated by the human. We use the dataset of the first 1000 headlines and the first 1000 summaries. We use the first 1000 headlines and the first 1000 summaries as",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dataset do they evaluate on for headline generation?\n\nGround Truth Answer:\n\n\nAnswer A:\nEnglish Gigaword corpus. Explanation: For headline generation, we used sentence-headline pairs extracted from the annotated English Gigaword corpus BIBREF35 in the same manner as BIBREF2 .\n\nAnswer B:\nWhat is the perplexity of their model on the dataset?\n< Explanation: We evaluate our model on the headline generation task. We use the dataset of headline generation from the Wikipedia article \u201cThe New York Times\u201d (Wikipedia 2016). The dataset contains 1000 headlines and 1000 corresponding summaries. The summaries are generated by the system and the headlines are generated by the human. We use the dataset of the first 1000 headlines and the first 1000 summaries. We use the first 1000 headlines and the first 1000 summaries as\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.05506",
    "question": "What results do their embeddings obtain on machine translation?",
    "gold_answer": "",
    "answer_a": "What results do their embeddings obtain on headline generation?\n< Explanation: We compare our proposed method with the following baselines: (1) a baseline that uses only word embeddings (Wieting et al. 2016), (2) a baseline that uses only character n-grams (Wieting et al. 2016), (3) a baseline that uses only word embeddings and character n-grams (Wieting et al. 2016), (4) a baseline that uses only word embeddings and character n-grams and then combines them with word embeddings (Wieting et al",
    "answer_b": "BLEU score of 35.48 on En-Fr, 23.27 on En-De, 34.43 on Fr-En, 28.86 on De-En. Explanation: Tables TABREF31 and TABREF32 show the results of machine translation and headline generation, respectively.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat results do their embeddings obtain on machine translation?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat results do their embeddings obtain on headline generation?\n< Explanation: We compare our proposed method with the following baselines: (1) a baseline that uses only word embeddings (Wieting et al. 2016), (2) a baseline that uses only character n-grams (Wieting et al. 2016), (3) a baseline that uses only word embeddings and character n-grams (Wieting et al. 2016), (4) a baseline that uses only word embeddings and character n-grams and then combines them with word embeddings (Wieting et al\n\nAnswer B:\nBLEU score of 35.48 on En-Fr, 23.27 on En-De, 34.43 on Fr-En, 28.86 on De-En. Explanation: Tables TABREF31 and TABREF32 show the results of machine translation and headline generation, respectively.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.05506",
    "question": "How do they combine ordinary word embeddings and ones constructed from character n-grams?",
    "gold_answer": "",
    "answer_a": "They use a sum of charn-MS-vec and the standard word embedding as an input of an RNN. Explanation: As illustrated in this figure, our proposed method regards the sum of char3-MS-vec and the standard word embedding as an input of an RNN.",
    "answer_b": "What is the difference between the two?\n< Explanation: We first construct word embeddings from ordinary word embeddings. Then, we combine them with those constructed from character n-grams. We use the following equation to construct word embeddings from ordinary word embeddings:\n\nwhere is the embedding dimension, is the embedding dimension of the ordinary word embeddings, is the embedding dimension of the character n-grams, and is the embedding dimension of the character n-grams. We use the following equation to construct word embeddings from character n-grams:\n\nwhere is the embedding dimension of the character n-grams, is the embedding dimension of the ordinary word embeddings, and is the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they combine ordinary word embeddings and ones constructed from character n-grams?\n\nGround Truth Answer:\n\n\nAnswer A:\nThey use a sum of charn-MS-vec and the standard word embedding as an input of an RNN. Explanation: As illustrated in this figure, our proposed method regards the sum of char3-MS-vec and the standard word embedding as an input of an RNN.\n\nAnswer B:\nWhat is the difference between the two?\n< Explanation: We first construct word embeddings from ordinary word embeddings. Then, we combine them with those constructed from character n-grams. We use the following equation to construct word embeddings from ordinary word embeddings:\n\nwhere is the embedding dimension, is the embedding dimension of the ordinary word embeddings, is the embedding dimension of the character n-grams, and is the embedding dimension of the character n-grams. We use the following equation to construct word embeddings from character n-grams:\n\nwhere is the embedding dimension of the character n-grams, is the embedding dimension of the ordinary word embeddings, and is the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.00957",
    "question": "Which dataset do they use?",
    "gold_answer": "",
    "answer_a": "How many records are in it?\n< Explanation: We use the dataset of 17000 headlines and 2538 articles from the 2018 News300 dataset. The dataset is available at https://www.kaggle.com/c/news300. The dataset is split into training and testing sets. The training set contains 17000 headlines and 2538 articles, while the testing set contains 17000 headlines and 2538 articles. The dataset is available at https://www.kaggle.com/c/news300. The",
    "answer_b": "A crowdsourced twitter dataset containing 19358 tweets. Explanation: BIBREF4 crowdsourced the annotation of 19538 tweets they had curated, into various levels of their clickbait-y nature.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dataset do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow many records are in it?\n< Explanation: We use the dataset of 17000 headlines and 2538 articles from the 2018 News300 dataset. The dataset is available at https://www.kaggle.com/c/news300. The dataset is split into training and testing sets. The training set contains 17000 headlines and 2538 articles, while the testing set contains 17000 headlines and 2538 articles. The dataset is available at https://www.kaggle.com/c/news300. The\n\nAnswer B:\nA crowdsourced twitter dataset containing 19358 tweets. Explanation: BIBREF4 crowdsourced the annotation of 19538 tweets they had curated, into various levels of their clickbait-y nature.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.00957",
    "question": "By how much do they outperform previous state-of-the-art approaches?",
    "gold_answer": "",
    "answer_a": "BiLSTM for 0.02 F1,  Feature Engineering SotA  for 0.08 F1, and Concatenated NN Architecture for  0.24 F1.. Explanation: FLOAT SELECTED: Table 1: Model Performance Comparison",
    "answer_b": "What are the previous state-of-the-art approaches?\n< Explanation: We compare our model with the following approaches: (1) a baseline model that uses the title and article text as input to a neural network, (2) a baseline model that uses the title and article text as input to a neural network, (3) a baseline model that uses the title and article text as input to a neural network, (4) a baseline model that uses the title and article text as input to a neural network, (5) a baseline model that uses the title and article text as input to a neural network, (6) a baseline model",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much do they outperform previous state-of-the-art approaches?\n\nGround Truth Answer:\n\n\nAnswer A:\nBiLSTM for 0.02 F1,  Feature Engineering SotA  for 0.08 F1, and Concatenated NN Architecture for  0.24 F1.. Explanation: FLOAT SELECTED: Table 1: Model Performance Comparison\n\nAnswer B:\nWhat are the previous state-of-the-art approaches?\n< Explanation: We compare our model with the following approaches: (1) a baseline model that uses the title and article text as input to a neural network, (2) a baseline model that uses the title and article text as input to a neural network, (3) a baseline model that uses the title and article text as input to a neural network, (4) a baseline model that uses the title and article text as input to a neural network, (5) a baseline model that uses the title and article text as input to a neural network, (6) a baseline model\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.07231",
    "question": "What other scenarios can the bias mitigation methods be applied to?",
    "gold_answer": "",
    "answer_a": "How effective are they?\n< Explanation: We also experiment with three bias mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. We find that these methods can effectively reduce gender bias by 90-98% and can be extended to correct model bias in other scenarios. For example, we can fine-tune the model with a larger corpus to correct gender bias. We also find that the gender bias is not only due to abusive language, but also due to the gender of the model. For example, we find that the gender",
    "answer_b": "sentiment analysis. Explanation: Although this work focuses on gender terms, the methods we proposed can easily be extended to other identity problems like racial and to different tasks like sentiment analysis by following similar steps, and we hope to work on this in the future.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat other scenarios can the bias mitigation methods be applied to?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow effective are they?\n< Explanation: We also experiment with three bias mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. We find that these methods can effectively reduce gender bias by 90-98% and can be extended to correct model bias in other scenarios. For example, we can fine-tune the model with a larger corpus to correct gender bias. We also find that the gender bias is not only due to abusive language, but also due to the gender of the model. For example, we find that the gender\n\nAnswer B:\nsentiment analysis. Explanation: Although this work focuses on gender terms, the methods we proposed can easily be extended to other identity problems like racial and to different tasks like sentiment analysis by following similar steps, and we hope to work on this in the future.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.07231",
    "question": "Which of the three bias mitigation methods is most effective?",
    "gold_answer": "",
    "answer_a": "Gender Swap. Explanation: FLOAT SELECTED: Table 6: Results of bias mitigation methods on st dataset. \u2018O\u2019 indicates that the corresponding method is applied. See Section 5.3 for more analysis.",
    "answer_b": "How much is it effective?\n< Explanation: We first compare the performance of the three bias mitigation methods on the abusive language dataset. We find that debiased word embeddings are the most effective method, with an average of 90% reduction in gender bias. The gender swap data augmentation method is also effective, with an average of 80% reduction in gender bias. The fine-tuning method is also effective, with an average of 70% reduction in gender bias. The results are shown in Table 1. We also compare the performance of the three bias mitigation methods on the English Wikipedia dataset. We find that deb",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich of the three bias mitigation methods is most effective?\n\nGround Truth Answer:\n\n\nAnswer A:\nGender Swap. Explanation: FLOAT SELECTED: Table 6: Results of bias mitigation methods on st dataset. \u2018O\u2019 indicates that the corresponding method is applied. See Section 5.3 for more analysis.\n\nAnswer B:\nHow much is it effective?\n< Explanation: We first compare the performance of the three bias mitigation methods on the abusive language dataset. We find that debiased word embeddings are the most effective method, with an average of 90% reduction in gender bias. The gender swap data augmentation method is also effective, with an average of 80% reduction in gender bias. The fine-tuning method is also effective, with an average of 70% reduction in gender bias. The results are shown in Table 1. We also compare the performance of the three bias mitigation methods on the English Wikipedia dataset. We find that deb\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.07231",
    "question": "What model architectures are used?",
    "gold_answer": "",
    "answer_a": "How are they evaluated?\n< Explanation: We use the following models: (1) BERT (2) BERT-Base (3) BERT-Intermediate (4) BERT-Extra (5) BERT-Extra-Intermediate (6) BERT-Extra-Intermediate-Extra (7) BERT-Extra-Intermediate-Extra-Intermediate (8) BERT-Extra-Intermediate-Extra-Intermediate-Extra (9) BERT-Extra-Intermediate-Extra-Intermediate-Extra-Extra (10) BERT-Extra-Intermediate-Extra-Intermediate-Extra-Extra-Extra (11",
    "answer_b": "Convolutional Neural Network. Explanation: We explore three neural models used in previous works on abusive language classification: Convolutional Neural Network (CNN) BIBREF7 , Gated Recurrent Unit (GRU) BIBREF14 , and Bidirectional GRU with self-attention ( INLINEFORM0 -GRU) BIBREF8 , but with a simpler mechanism used in BIBREF15 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat model architectures are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they evaluated?\n< Explanation: We use the following models: (1) BERT (2) BERT-Base (3) BERT-Intermediate (4) BERT-Extra (5) BERT-Extra-Intermediate (6) BERT-Extra-Intermediate-Extra (7) BERT-Extra-Intermediate-Extra-Intermediate (8) BERT-Extra-Intermediate-Extra-Intermediate-Extra (9) BERT-Extra-Intermediate-Extra-Intermediate-Extra-Extra (10) BERT-Extra-Intermediate-Extra-Intermediate-Extra-Extra-Extra (11\n\nAnswer B:\nConvolutional Neural Network. Explanation: We explore three neural models used in previous works on abusive language classification: Convolutional Neural Network (CNN) BIBREF7 , Gated Recurrent Unit (GRU) BIBREF14 , and Bidirectional GRU with self-attention ( INLINEFORM0 -GRU) BIBREF8 , but with a simpler mechanism used in BIBREF15 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.07231",
    "question": "What pre-trained word embeddings are used?",
    "gold_answer": "",
    "answer_a": "word2vec. Explanation: We also compare different pre-trained embeddings, word2vec BIBREF10 trained on Google News corpus, FastText BIBREF16 ) trained on Wikipedia corpus, and randomly initialized embeddings (random) to analyze their effects on the biases.",
    "answer_b": "How are they evaluated?\n< Explanation: We use the pre-trained word embeddings from the BERT model BIBREF10 . We use the BERT model BIBREF10 as the base model for our experiments. We use the pre-trained word embeddings from the BERT model BIBREF10 as the base model for our experiments. We use the BERT model BIBREF10 as the base model for our experiments. We use the BERT model BIBREF10 as the base model for our experiments. We use the BERT model BIBREF10 as the base model for our experiments. We",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat pre-trained word embeddings are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nword2vec. Explanation: We also compare different pre-trained embeddings, word2vec BIBREF10 trained on Google News corpus, FastText BIBREF16 ) trained on Wikipedia corpus, and randomly initialized embeddings (random) to analyze their effects on the biases.\n\nAnswer B:\nHow are they evaluated?\n< Explanation: We use the pre-trained word embeddings from the BERT model BIBREF10 . We use the BERT model BIBREF10 as the base model for our experiments. We use the pre-trained word embeddings from the BERT model BIBREF10 as the base model for our experiments. We use the BERT model BIBREF10 as the base model for our experiments. We use the BERT model BIBREF10 as the base model for our experiments. We use the BERT model BIBREF10 as the base model for our experiments. We\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.07231",
    "question": "What metrics are used to measure gender biases?",
    "gold_answer": "",
    "answer_a": "What are the results?\n< Explanation: We use the following metrics to measure gender biases: (1) the proportion of male and female words in the model's output, (2) the proportion of male and female words in the training set, and (3) the proportion of male and female words in the training set and the model's output. We also use the following metrics to measure the bias of the model: (1) the proportion of male and female words in the model's output, (2) the proportion of male and female words in the training set, and (3) the proportion of male and female words in the",
    "answer_b": "False Positive Equality Difference. Explanation: For the evaluation metric, we use 1) AUC scores on the original test set (Orig. AUC), 2) AUC scores on the unbiased generated test set (Gen. AUC), and 3) the false positive/negative equality differences proposed in BIBREF1 which aggregates the difference between the overall false positive/negative rate and gender-specific false positive/negative rate. False Positive Equality Difference (FPED) and False Negative Equality Difference (FNED) are defined as below, where INLINEFORM0 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat metrics are used to measure gender biases?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n< Explanation: We use the following metrics to measure gender biases: (1) the proportion of male and female words in the model's output, (2) the proportion of male and female words in the training set, and (3) the proportion of male and female words in the training set and the model's output. We also use the following metrics to measure the bias of the model: (1) the proportion of male and female words in the model's output, (2) the proportion of male and female words in the training set, and (3) the proportion of male and female words in the\n\nAnswer B:\nFalse Positive Equality Difference. Explanation: For the evaluation metric, we use 1) AUC scores on the original test set (Orig. AUC), 2) AUC scores on the unbiased generated test set (Gen. AUC), and 3) the false positive/negative equality differences proposed in BIBREF1 which aggregates the difference between the overall false positive/negative rate and gender-specific false positive/negative rate. False Positive Equality Difference (FPED) and False Negative Equality Difference (FNED) are defined as below, where INLINEFORM0 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1712.02555",
    "question": "Which dataset(s) do they train on?",
    "gold_answer": "",
    "answer_a": "What are the evaluation metrics?\n< Explanation: We use the following datasets: (1) Penn Treebank (PTB) BIBREF10 , (2) Penn Treebank (PTB) BIBREF11 , (3) Penn Treebank (PTB) BIBREF12 , (4) Penn Treebank (PTB) BIBREF13 , (5) Penn Treebank (PTB) BIBREF14 , (6) Penn Treebank (PTB) BIBREF15 , (7) Penn Treebank (PTB) BIBREF16 , (",
    "answer_b": "Quora Question Pairs. Explanation: We conduct our experiments on the public benchmark dataset of \u201cQuora Question Pairs\u201d for the task of paraphrase identification.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dataset(s) do they train on?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the evaluation metrics?\n< Explanation: We use the following datasets: (1) Penn Treebank (PTB) BIBREF10 , (2) Penn Treebank (PTB) BIBREF11 , (3) Penn Treebank (PTB) BIBREF12 , (4) Penn Treebank (PTB) BIBREF13 , (5) Penn Treebank (PTB) BIBREF14 , (6) Penn Treebank (PTB) BIBREF15 , (7) Penn Treebank (PTB) BIBREF16 , (\n\nAnswer B:\nQuora Question Pairs. Explanation: We conduct our experiments on the public benchmark dataset of \u201cQuora Question Pairs\u201d for the task of paraphrase identification.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1712.02555",
    "question": "By how much does their model outperform state-of-the-art baselines?",
    "gold_answer": "",
    "answer_a": "What are the baselines?\n< Explanation: We compare our model with the following baselines: (1) BiLSTM (BIBREF10), (2) A$^*$ (BIBREF11), (3) Decision Tree (DT) (BIBREF12), (4) Decision Tree with BiLSTM (DT-BIBREF13), (5) Decision Tree with BiLSTM and BiLSTM (DT-BIBREF14), (6) Decision Tree with BiLSTM and BiLSTM with BiLSTM (DT-BIBREF15), (7) Decision",
    "answer_b": "0.78% over the best state-of-the-art baseline. Explanation: FLOAT SELECTED: Table 1: Performance Evaluation on \u201cQuora Question Pairs\u201d.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much does their model outperform state-of-the-art baselines?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the baselines?\n< Explanation: We compare our model with the following baselines: (1) BiLSTM (BIBREF10), (2) A$^*$ (BIBREF11), (3) Decision Tree (DT) (BIBREF12), (4) Decision Tree with BiLSTM (DT-BIBREF13), (5) Decision Tree with BiLSTM and BiLSTM (DT-BIBREF14), (6) Decision Tree with BiLSTM and BiLSTM with BiLSTM (DT-BIBREF15), (7) Decision\n\nAnswer B:\n0.78% over the best state-of-the-art baseline. Explanation: FLOAT SELECTED: Table 1: Performance Evaluation on \u201cQuora Question Pairs\u201d.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.02322",
    "question": "What is the model trained?",
    "gold_answer": "",
    "answer_a": "What is the architecture?\n< Explanation: We train a multi-source transduction model using a combination of a dense vector representation of each document and a dense vector representation of the user preferences. The model is trained using a combination of a dense vector representation of each document and a dense vector representation of the user preferences. The model is trained using a combination of a dense vector representation of each document and a dense vector representation of the user preferences. The model is trained using a combination of a dense vector representation of each document and a dense vector representation of the user preferences. The model is trained using a combination of a dense vector representation of each document and",
    "answer_b": "Condense-Abstract Framework, consisting of BiLSTM autoencoder and LSTM decoder with attention.. Explanation: Figure FIGREF5 illustrates our Condense-Abstract (CA) framework.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the model trained?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the architecture?\n< Explanation: We train a multi-source transduction model using a combination of a dense vector representation of each document and a dense vector representation of the user preferences. The model is trained using a combination of a dense vector representation of each document and a dense vector representation of the user preferences. The model is trained using a combination of a dense vector representation of each document and a dense vector representation of the user preferences. The model is trained using a combination of a dense vector representation of each document and a dense vector representation of the user preferences. The model is trained using a combination of a dense vector representation of each document and\n\nAnswer B:\nCondense-Abstract Framework, consisting of BiLSTM autoencoder and LSTM decoder with attention.. Explanation: Figure FIGREF5 illustrates our Condense-Abstract (CA) framework.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.02322",
    "question": "How large is the dataset used?",
    "gold_answer": "",
    "answer_a": "3731 movies containing around 372353 reviews. Explanation: We performed experiments on the Rotten Tomatoes dataset provided in BIBREF16. It contains 3,731 movies; for each movie we are given a large set of reviews (99.8 on average) written by professional critics and users and a gold-standard consensus, i.e. a summary written by an editor (see an example in Figure FIGREF1).",
    "answer_b": "What is the size of the dataset?\n< Explanation: The Rotten Tomatoes dataset contains 1000 reviews from 1000 users. We use 1000 reviews to train a model and 1000 reviews to test the model. The dataset is available at https://github.com/alexander-chen/ROBERTA-ROBOT. The dataset is divided into 1000 reviews for training and 1000 reviews for testing. The training set is used to train a model and the testing set is used to evaluate the model. The dataset is available at https://",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow large is the dataset used?\n\nGround Truth Answer:\n\n\nAnswer A:\n3731 movies containing around 372353 reviews. Explanation: We performed experiments on the Rotten Tomatoes dataset provided in BIBREF16. It contains 3,731 movies; for each movie we are given a large set of reviews (99.8 on average) written by professional critics and users and a gold-standard consensus, i.e. a summary written by an editor (see an example in Figure FIGREF1).\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: The Rotten Tomatoes dataset contains 1000 reviews from 1000 users. We use 1000 reviews to train a model and 1000 reviews to test the model. The dataset is available at https://github.com/alexander-chen/ROBERTA-ROBOT. The dataset is divided into 1000 reviews for training and 1000 reviews for testing. The training set is used to train a model and the testing set is used to evaluate the model. The dataset is available at https://\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.04579",
    "question": "How exactly do they weigh between different statistical models?",
    "gold_answer": "",
    "answer_a": "What are the weights?\n< Explanation: Weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are",
    "answer_b": "They define cWeight as weight obtained for each sentence using all the models where the sentences is in the summary of predicted by each model.. Explanation: For INLINEFORM2 summarization model, let the INLINEFORM3 sentences contained be:-\n\nINLINEFORM0\n\nNow for our list of sentences INLINEFORM0 we define cWeight as weight obtained for each sentence using INLINEFORM1 models.\n\nINLINEFORM0\n\nHere, INLINEFORM0 is a function which returns 1 if sentence is in summary of INLINEFORM1 model, otherwise zero. INLINEFORM2 is weight assigned to each model without training, INLINEFORM3",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow exactly do they weigh between different statistical models?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the weights?\n< Explanation: Weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are calculated by multiplying the weights of the different models. The weights are\n\nAnswer B:\nThey define cWeight as weight obtained for each sentence using all the models where the sentences is in the summary of predicted by each model.. Explanation: For INLINEFORM2 summarization model, let the INLINEFORM3 sentences contained be:-\n\nINLINEFORM0\n\nNow for our list of sentences INLINEFORM0 we define cWeight as weight obtained for each sentence using INLINEFORM1 models.\n\nINLINEFORM0\n\nHere, INLINEFORM0 is a function which returns 1 if sentence is in summary of INLINEFORM1 model, otherwise zero. INLINEFORM2 is weight assigned to each model without training, INLINEFORM3\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.04579",
    "question": "What showed to be the best performing combination of semantic and statistical model on the summarization task in terms of ROUGE score?",
    "gold_answer": "",
    "answer_a": "Explanation: We have used the following combination of models: (1) TF/IDF + TextRank + Jaccard + Cosine Similarity + WordNet + Facebook's InferSent + Glove Vectors + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec +",
    "answer_b": "Combination of Jaccard/Cosine Similarity Matrix, TextRank and  InferSent Based Model. Explanation: FLOAT SELECTED: Table 1: Average ROUGE-2 Scores for Different Combination of Models.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat showed to be the best performing combination of semantic and statistical model on the summarization task in terms of ROUGE score?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We have used the following combination of models: (1) TF/IDF + TextRank + Jaccard + Cosine Similarity + WordNet + Facebook's InferSent + Glove Vectors + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec + Word2Vec +\n\nAnswer B:\nCombination of Jaccard/Cosine Similarity Matrix, TextRank and  InferSent Based Model. Explanation: FLOAT SELECTED: Table 1: Average ROUGE-2 Scores for Different Combination of Models.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.10149",
    "question": "What QA system was used in this work?",
    "gold_answer": "",
    "answer_a": "What is the dataset used?\n<sep>Answer: The system used is the QA system developed by T-Mobile Austria. The dataset used is the customer care domain of T-Mobile Austria. The dataset contains 877 answers and 10000 questions. The answers are extracted from the customer care chat logs. The answers are then used to train the QA system. The system is trained using the answers and the question as input. The system is evaluated using the top-10 results of the system. The top-10 results are used to train a new QA system. The new QA system is trained using the top",
    "answer_b": "We implement our question answering system using state-of-the-art open source components.. Explanation: We implement our question answering system using state-of-the-art open source components. Our pipeline is based on the Rasa natural language understanding (NLU) framework BIBREF21 which offers two standard pipelines for text classification: spacy_sklearn and tensorflow_embedding. The main difference is that spacy_sklearn uses Spacy for feature extraction with pre-trained word embedding models and Scikit-learn BIBREF22 for text classification. In contrast, the tensorflow_embedding pipeline trains custom word embeddings for text similarity estimation using TensorFlow BIBREF23 as machine learning backend.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat QA system was used in this work?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the dataset used?\n<sep>Answer: The system used is the QA system developed by T-Mobile Austria. The dataset used is the customer care domain of T-Mobile Austria. The dataset contains 877 answers and 10000 questions. The answers are extracted from the customer care chat logs. The answers are then used to train the QA system. The system is trained using the answers and the question as input. The system is evaluated using the top-10 results of the system. The top-10 results are used to train a new QA system. The new QA system is trained using the top\n\nAnswer B:\nWe implement our question answering system using state-of-the-art open source components.. Explanation: We implement our question answering system using state-of-the-art open source components. Our pipeline is based on the Rasa natural language understanding (NLU) framework BIBREF21 which offers two standard pipelines for text classification: spacy_sklearn and tensorflow_embedding. The main difference is that spacy_sklearn uses Spacy for feature extraction with pre-trained word embedding models and Scikit-learn BIBREF22 for text classification. In contrast, the tensorflow_embedding pipeline trains custom word embeddings for text similarity estimation using TensorFlow BIBREF23 as machine learning backend.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.10149",
    "question": "How big is the test set used for evaluating the proposed re-ranking approach?",
    "gold_answer": "",
    "answer_a": "What is the size of the test set?\n<sep><font color=\"red\">Answer: We use the test set of 10000 answers from the customer care domain of T-Mobile Austria. The test set is available after deployment of the system. The test set is split into 80% training and 20% test. The test set is used to evaluate the performance of the proposed re-ranking approach. </font> Explanation: We use the test set of 10000 answers from the customer care domain of T-Mobile Austria. The test set is available after deployment of the system. The test",
    "answer_b": "3084 real user requests  assigned to suitable answers from the training corpus.. Explanation: The performance of the implemented QA system and of our re-ranking approach is assessed using a separate test corpus. It includes 3084 real user requests from a chat-log of T-Mobile Austria, which are assigned to suitable answers from the training corpus (at most three).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big is the test set used for evaluating the proposed re-ranking approach?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the test set?\n<sep><font color=\"red\">Answer: We use the test set of 10000 answers from the customer care domain of T-Mobile Austria. The test set is available after deployment of the system. The test set is split into 80% training and 20% test. The test set is used to evaluate the performance of the proposed re-ranking approach. </font> Explanation: We use the test set of 10000 answers from the customer care domain of T-Mobile Austria. The test set is available after deployment of the system. The test\n\nAnswer B:\n3084 real user requests  assigned to suitable answers from the training corpus.. Explanation: The performance of the implemented QA system and of our re-ranking approach is assessed using a separate test corpus. It includes 3084 real user requests from a chat-log of T-Mobile Austria, which are assigned to suitable answers from the training corpus (at most three).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.07828",
    "question": "What is the new metric?",
    "gold_answer": "",
    "answer_a": "They propose two new metrics. One, which they call the Neighbour Similarity Test, calculates how many shared characteristics there are between entities whose representations are neighbors in the embedding space. The second, which they call the Type and Category Test, is the same as the  Neighbour Similarity Test, except it uses entity types and categories in the place of individual entity characteristics.. Explanation: In this paper, we introduce two metrics inspired by The Identity of Indiscernibles BIBREF24 to gain insights over the distributional quality of the learned embeddings. The more characteristics two entities share, the more similar they are and so should be their vector representations. Considering the set of characteristics $C_K(s)=\\lbrace (p_1,o_1),\\dots ,(p_m,o_m)\\rbrace $ of a subject $s$ in a triple, we can define a metric that expresses the similarity among two entities $e_1,e_2$ as the Jaccard index between their sets of characteristics $C_K(e_1)$ and $C_K(e_2)$ . Given a set of entities $\\tilde{E}$ and their $N$ nearest neighbours in the vector space, the overall Neighbour Similarity Test (NST) metric is defined as:\n\n$$ NST(\\tilde{E},N,K) = \\frac{1}{N \\vert \\tilde{E} \\vert } \\sum _{e \\in \\tilde{E}} \\sum _{j=1}^N \\frac{\\vert C_K(e) \\cap C_K(n_j^{(e)}) \\vert }{\\vert C_K(e) \\cup C_K(n_j^{(e)}) \\vert }$$ (Eq. 19)\n\nwhere $n_j^{(e)}$ is the $j$ th nearest neighbour of $e$ in the vector space.\n\nThe second metric is the Type and Category Test (TCT), based on the assumption that two entities which share types and categories should be close in the vector space. This assumption is suggested by the human bias for which rdf:type and dct:subject would be predicates with a higher weight than the others. Although this does not happen, we compute it for a mere sake of comparison with the NST metric. The TCT formula is equal to Equation 19 except for sets $C_K(e)$ , which are replaced by sets of types and categories $TC_K(e)$ .",
    "answer_b": "How is it computed?\n< Explanation: We define the new metric as the cosine similarity between the embeddings of the two triples. We compute the cosine similarity between the embeddings of the two triples using the following formula: where is the embedding of the triple and is the embedding of the triple . We compute the cosine similarity between the embeddings of the two triples using the following formula: where is the embedding of the triple and is the embedding of the triple . We compute the cosine similarity between the embeddings of the two triples using the following formula: where is the embedding of the triple and is the embedding of the triple . We compute the cosine similarity between the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the new metric?\n\nGround Truth Answer:\n\n\nAnswer A:\nThey propose two new metrics. One, which they call the Neighbour Similarity Test, calculates how many shared characteristics there are between entities whose representations are neighbors in the embedding space. The second, which they call the Type and Category Test, is the same as the  Neighbour Similarity Test, except it uses entity types and categories in the place of individual entity characteristics.. Explanation: In this paper, we introduce two metrics inspired by The Identity of Indiscernibles BIBREF24 to gain insights over the distributional quality of the learned embeddings. The more characteristics two entities share, the more similar they are and so should be their vector representations. Considering the set of characteristics $C_K(s)=\\lbrace (p_1,o_1),\\dots ,(p_m,o_m)\\rbrace $ of a subject $s$ in a triple, we can define a metric that expresses the similarity among two entities $e_1,e_2$ as the Jaccard index between their sets of characteristics $C_K(e_1)$ and $C_K(e_2)$ . Given a set of entities $\\tilde{E}$ and their $N$ nearest neighbours in the vector space, the overall Neighbour Similarity Test (NST) metric is defined as:\n\n$$ NST(\\tilde{E},N,K) = \\frac{1}{N \\vert \\tilde{E} \\vert } \\sum _{e \\in \\tilde{E}} \\sum _{j=1}^N \\frac{\\vert C_K(e) \\cap C_K(n_j^{(e)}) \\vert }{\\vert C_K(e) \\cup C_K(n_j^{(e)}) \\vert }$$ (Eq. 19)\n\nwhere $n_j^{(e)}$ is the $j$ th nearest neighbour of $e$ in the vector space.\n\nThe second metric is the Type and Category Test (TCT), based on the assumption that two entities which share types and categories should be close in the vector space. This assumption is suggested by the human bias for which rdf:type and dct:subject would be predicates with a higher weight than the others. Although this does not happen, we compute it for a mere sake of comparison with the NST metric. The TCT formula is equal to Equation 19 except for sets $C_K(e)$ , which are replaced by sets of types and categories $TC_K(e)$ .\n\nAnswer B:\nHow is it computed?\n< Explanation: We define the new metric as the cosine similarity between the embeddings of the two triples. We compute the cosine similarity between the embeddings of the two triples using the following formula: where is the embedding of the triple and is the embedding of the triple . We compute the cosine similarity between the embeddings of the two triples using the following formula: where is the embedding of the triple and is the embedding of the triple . We compute the cosine similarity between the embeddings of the two triples using the following formula: where is the embedding of the triple and is the embedding of the triple . We compute the cosine similarity between the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.07828",
    "question": "How long do other state-of-the-art models take to process the same amount of data?",
    "gold_answer": "",
    "answer_a": "What is the difference between KG2Vec and the most scalable models?\n< Explanation: We compare our approach with the most scalable approaches to KG2Vec, namely the Skip-Gram model [1] and the Skip-Gram model with a pre-trained word2vec model [2]. We also compare our approach with the most scalable approaches to KG2Vec, namely the Skip-Gram model [1] and the Skip-Gram model with a pre-trained word2vec model [2]. We also compare our approach with the most scalable approaches to KG2Vec, namely the Skip-Gram model [1] and the Skip-Gram model",
    "answer_b": "RDF2Vec takes 123 minutes to generate random walks and an estimated 96 hours to train word2vec. KGloVe takes an estimated 12 hours to train GloVe. fastText takes an estimated 72 hours to train. Explanation: FLOAT SELECTED: Table 2 Runtime comparison of the single phases. Those with (*) are estimated runtimes.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long do other state-of-the-art models take to process the same amount of data?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between KG2Vec and the most scalable models?\n< Explanation: We compare our approach with the most scalable approaches to KG2Vec, namely the Skip-Gram model [1] and the Skip-Gram model with a pre-trained word2vec model [2]. We also compare our approach with the most scalable approaches to KG2Vec, namely the Skip-Gram model [1] and the Skip-Gram model with a pre-trained word2vec model [2]. We also compare our approach with the most scalable approaches to KG2Vec, namely the Skip-Gram model [1] and the Skip-Gram model\n\nAnswer B:\nRDF2Vec takes 123 minutes to generate random walks and an estimated 96 hours to train word2vec. KGloVe takes an estimated 12 hours to train GloVe. fastText takes an estimated 72 hours to train. Explanation: FLOAT SELECTED: Table 2 Runtime comparison of the single phases. Those with (*) are estimated runtimes.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.07828",
    "question": "What context is used when computing the embedding for an entity?",
    "gold_answer": "",
    "answer_a": "a subject, a predicate, and an object in a knowledge base. Explanation: Existing KGE approaches based on the skip-gram model such as RDF2Vec BIBREF4 submit paths built using random walks to a Word2Vec algorithm. Instead, we preprocess the input knowledge base by converting each triple into a small sentence of three words.",
    "answer_b": "How is the embedding computed?\n< Explanation: We use the following context for each entity: the entity itself, the entity's relation, and the entity's relation's relation. We use the entity itself as the context for the entity itself, the entity's relation as the context for the entity's relation, and the entity's relation's relation as the context for the entity's relation's relation. We use the entity itself as the context for the entity itself, the entity's relation as the context for the entity's relation, and the entity's relation's relation as the context for the entity's relation's relation. We use the entity itself",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat context is used when computing the embedding for an entity?\n\nGround Truth Answer:\n\n\nAnswer A:\na subject, a predicate, and an object in a knowledge base. Explanation: Existing KGE approaches based on the skip-gram model such as RDF2Vec BIBREF4 submit paths built using random walks to a Word2Vec algorithm. Instead, we preprocess the input knowledge base by converting each triple into a small sentence of three words.\n\nAnswer B:\nHow is the embedding computed?\n< Explanation: We use the following context for each entity: the entity itself, the entity's relation, and the entity's relation's relation. We use the entity itself as the context for the entity itself, the entity's relation as the context for the entity's relation, and the entity's relation's relation as the context for the entity's relation's relation. We use the entity itself as the context for the entity itself, the entity's relation as the context for the entity's relation, and the entity's relation's relation as the context for the entity's relation's relation. We use the entity itself\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.08419",
    "question": "What are the limitations of the currently used quantitative metrics? e.g. why are they not 'good'?",
    "gold_answer": "",
    "answer_a": "perplexity and BLEU score are not good enough and correlate very weakly with human judgments. Explanation: The current evaluation methods like perplexity and BLEU score are not good enough and correlate very weakly with human judgments.",
    "answer_b": "What are the limitations of the current models?\n<blockquote>Our current approach is based on the fact that the quality of the output is measured by the number of correct answers. This is a very naive approach and does not take into account the quality of the utterances. For example, if the user says \"I want to buy a car\", the model should output \"I want to buy a car\" as well. This is not a good approach because the model should be able to generate a variety of answers. We have also seen that the quality of the output is not measured by the number of correct answers. For example, if the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the limitations of the currently used quantitative metrics? e.g. why are they not 'good'?\n\nGround Truth Answer:\n\n\nAnswer A:\nperplexity and BLEU score are not good enough and correlate very weakly with human judgments. Explanation: The current evaluation methods like perplexity and BLEU score are not good enough and correlate very weakly with human judgments.\n\nAnswer B:\nWhat are the limitations of the current models?\n<blockquote>Our current approach is based on the fact that the quality of the output is measured by the number of correct answers. This is a very naive approach and does not take into account the quality of the utterances. For example, if the user says \"I want to buy a car\", the model should output \"I want to buy a car\" as well. This is not a good approach because the model should be able to generate a variety of answers. We have also seen that the quality of the output is not measured by the number of correct answers. For example, if the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.08419",
    "question": "What metrics are typically used to compare models?",
    "gold_answer": "",
    "answer_a": "BLeU. Explanation: Currently there exists no perfect quantitative method to compare two conversational agents. The field has to rely on qualitative measures or measures like BLeU and perplexity borrowed from machine translation.",
    "answer_b": "What are the challenges in doing so?\n<sep><br/ >Answer: We use the following metrics to compare models: F1 score, BLEU score, perplexity, and accuracy. We also use the following metrics to compare agents: accuracy, recall, precision, F1 score, and BLEU score. We also use the following metrics to compare agents: accuracy, recall, precision, F1 score, and perplexity. We also use the following metrics to compare agents: accuracy, recall, precision, F1 score, and BLEU score. We also use the following metrics to compare agents: accuracy, recall, precision",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat metrics are typically used to compare models?\n\nGround Truth Answer:\n\n\nAnswer A:\nBLeU. Explanation: Currently there exists no perfect quantitative method to compare two conversational agents. The field has to rely on qualitative measures or measures like BLeU and perplexity borrowed from machine translation.\n\nAnswer B:\nWhat are the challenges in doing so?\n<sep><br/ >Answer: We use the following metrics to compare models: F1 score, BLEU score, perplexity, and accuracy. We also use the following metrics to compare agents: accuracy, recall, precision, F1 score, and BLEU score. We also use the following metrics to compare agents: accuracy, recall, precision, F1 score, and perplexity. We also use the following metrics to compare agents: accuracy, recall, precision, F1 score, and BLEU score. We also use the following metrics to compare agents: accuracy, recall, precision\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.08419",
    "question": "What GAN and RL approaches are used?",
    "gold_answer": "",
    "answer_a": "What are the challenges?\n< Explanation: We discuss three approaches: GANs, RL and a combination of GANs and RL. We also discuss the challenges that arise in each of these approaches. We also discuss the current state of the art in each of these approaches. We also discuss the current state of the art in each of these approaches. We also discuss the current state of the art in each of these approaches. We also discuss the current state of the art in each of these approaches. We also discuss the current state of the art in each of these approaches. We also discuss the current state of the art in each of",
    "answer_b": "adversarial training for open-domain dialogue generation. Explanation: Reinforcement Learning based models\nAfter exploring the neural methods in a lot of detail, the researchers have also begun exploring, in the current decade, how to use the reinforcement learning methods in the dialogue and personal agents.\n\nInitial reinforcement methods\nOne of the first main papers that thought of using reinforcement learning for this came in 2005 by English et al BIBREF25 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat GAN and RL approaches are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the challenges?\n< Explanation: We discuss three approaches: GANs, RL and a combination of GANs and RL. We also discuss the challenges that arise in each of these approaches. We also discuss the current state of the art in each of these approaches. We also discuss the current state of the art in each of these approaches. We also discuss the current state of the art in each of these approaches. We also discuss the current state of the art in each of these approaches. We also discuss the current state of the art in each of these approaches. We also discuss the current state of the art in each of\n\nAnswer B:\nadversarial training for open-domain dialogue generation. Explanation: Reinforcement Learning based models\nAfter exploring the neural methods in a lot of detail, the researchers have also begun exploring, in the current decade, how to use the reinforcement learning methods in the dialogue and personal agents.\n\nInitial reinforcement methods\nOne of the first main papers that thought of using reinforcement learning for this came in 2005 by English et al BIBREF25 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.08419",
    "question": "What type of neural models are used?",
    "gold_answer": "",
    "answer_a": "Sequence to Sequence approaches for dialogue modelling. Explanation: Sequence to Sequence approaches for dialogue modelling\nThe problem with rule-based models was that they were often domain dependent and could not be easily ported to a new domain. They also depended on hand crafted rules which was both expensive and required domain expertise. Two factors which when combined spell doom for scalbility. All of this changed in 2015 when Vinyals et al proposed an approach BIBREF2 inspired from the recent progress in machine translation BIBREF1 . Vinyals et al used the sequence to sequence learning architecture for conversation agents. Their model was the first model which could be trained end-to-end, and could generate a new output utterance based on just the input sentence and no other hand crafted features.",
    "answer_b": "What are the advantages and disadvantages of using them?\n<sep><font color=\"#100000\">Answer: We discuss the following neural models: (i) Generative Adversarial Networks (GANs) (ii) Variational Autoencoders (VAEs) (iii) Recurrent Neural Networks (RNNs) (iv) Recurrent Neural Networks with Attention (RNN-AT) (v) Recurrent Neural Networks with Long-Short Term Memory (RNN-LSTM) (vi) Recurrent Neural Networks with Long-Short Term Memory with Attention (RNN-LSTM-AT) (v",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of neural models are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nSequence to Sequence approaches for dialogue modelling. Explanation: Sequence to Sequence approaches for dialogue modelling\nThe problem with rule-based models was that they were often domain dependent and could not be easily ported to a new domain. They also depended on hand crafted rules which was both expensive and required domain expertise. Two factors which when combined spell doom for scalbility. All of this changed in 2015 when Vinyals et al proposed an approach BIBREF2 inspired from the recent progress in machine translation BIBREF1 . Vinyals et al used the sequence to sequence learning architecture for conversation agents. Their model was the first model which could be trained end-to-end, and could generate a new output utterance based on just the input sentence and no other hand crafted features.\n\nAnswer B:\nWhat are the advantages and disadvantages of using them?\n<sep><font color=\"#100000\">Answer: We discuss the following neural models: (i) Generative Adversarial Networks (GANs) (ii) Variational Autoencoders (VAEs) (iii) Recurrent Neural Networks (RNNs) (iv) Recurrent Neural Networks with Attention (RNN-AT) (v) Recurrent Neural Networks with Long-Short Term Memory (RNN-LSTM) (vi) Recurrent Neural Networks with Long-Short Term Memory with Attention (RNN-LSTM-AT) (v\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.08419",
    "question": "What type of statistical models were used initially?",
    "gold_answer": "",
    "answer_a": "What were the challenges?\n<sep><EXPLANATION> We start with a brief overview of the statistical models that were used in the early days of conversational agents. We then discuss the challenges that the field faces and how they are addressed. Finally, we discuss the current state-of-the-art models and how they compare to the challenges faced. </SECTION> Explanation: We start with a brief overview of the statistical models that were used in the early days of conversational agents. We then discuss the challenges that the field faces and how they are addressed. Finally, we discuss the current state-of-the-art models and how they compare to the",
    "answer_b": "semi-continuous HMMs. Explanation: The speech recognition was done using n-gram statistical model which is then passed to a robust parser based on an extended Context Free Grammar allowing the system to skip unknown words and perform partial parsing.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of statistical models were used initially?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat were the challenges?\n<sep><EXPLANATION> We start with a brief overview of the statistical models that were used in the early days of conversational agents. We then discuss the challenges that the field faces and how they are addressed. Finally, we discuss the current state-of-the-art models and how they compare to the challenges faced. </SECTION> Explanation: We start with a brief overview of the statistical models that were used in the early days of conversational agents. We then discuss the challenges that the field faces and how they are addressed. Finally, we discuss the current state-of-the-art models and how they compare to the\n\nAnswer B:\nsemi-continuous HMMs. Explanation: The speech recognition was done using n-gram statistical model which is then passed to a robust parser based on an extended Context Free Grammar allowing the system to skip unknown words and perform partial parsing.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.08419",
    "question": "What was the proposed use of conversational agents in pioneering work?",
    "gold_answer": "",
    "answer_a": "spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries.. Explanation: Early Techniques\nInitially, the interactive dialogue systems were based on and limited to speaker independent recognition of isolated words and phrases or limited continuous speech such as digit strings. In August 1993, there came the ESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 ) which was aimed at allowing spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries.",
    "answer_b": "What was the state of the art?\n<blockquote>Conversational agents have become ubiquitous, ranging from goal-oriented systems for helping with reservations to chit-chat models found in modern virtual assistants. In this survey paper, we explore this fascinating field. We look at some of the pioneering work that defined the field and gradually move to the current state-of-the-art models. We look at statistical, neural, generative adversarial network based and reinforcement learning based approaches and how they evolved. Along the way we discuss various challenges that the field faces, lack of context in utterances, not having a good quantitative metric to compare models, lack of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat was the proposed use of conversational agents in pioneering work?\n\nGround Truth Answer:\n\n\nAnswer A:\nspontaneous conversational inquiries over the telephone for the train timetable and flight enquiries.. Explanation: Early Techniques\nInitially, the interactive dialogue systems were based on and limited to speaker independent recognition of isolated words and phrases or limited continuous speech such as digit strings. In August 1993, there came the ESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 ) which was aimed at allowing spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries.\n\nAnswer B:\nWhat was the state of the art?\n<blockquote>Conversational agents have become ubiquitous, ranging from goal-oriented systems for helping with reservations to chit-chat models found in modern virtual assistants. In this survey paper, we explore this fascinating field. We look at some of the pioneering work that defined the field and gradually move to the current state-of-the-art models. We look at statistical, neural, generative adversarial network based and reinforcement learning based approaches and how they evolved. Along the way we discuss various challenges that the field faces, lack of context in utterances, not having a good quantitative metric to compare models, lack of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.08419",
    "question": "What work pioneered the field of conversational agents?",
    "gold_answer": "",
    "answer_a": "ESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 ). Explanation: Initially, the interactive dialogue systems were based on and limited to speaker independent recognition of isolated words and phrases or limited continuous speech such as digit strings. In August 1993, there came the ESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 ) which was aimed at allowing spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries.",
    "answer_b": "What are the challenges they face?\n<sep><font color=\"#100080\">Answer: The first work in this area was done by the team at Google Brain. They used a deep neural network to generate responses to questions. They used a neural network to generate responses to questions. They used a neural network to generate responses to questions. They used a neural network to generate responses to questions. They used a neural network to generate responses to questions. They used a neural network to generate responses to questions. They used a neural network to generate responses to questions. They used a neural network to generate responses to questions. They used",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat work pioneered the field of conversational agents?\n\nGround Truth Answer:\n\n\nAnswer A:\nESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 ). Explanation: Initially, the interactive dialogue systems were based on and limited to speaker independent recognition of isolated words and phrases or limited continuous speech such as digit strings. In August 1993, there came the ESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 ) which was aimed at allowing spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries.\n\nAnswer B:\nWhat are the challenges they face?\n<sep><font color=\"#100080\">Answer: The first work in this area was done by the team at Google Brain. They used a deep neural network to generate responses to questions. They used a neural network to generate responses to questions. They used a neural network to generate responses to questions. They used a neural network to generate responses to questions. They used a neural network to generate responses to questions. They used a neural network to generate responses to questions. They used a neural network to generate responses to questions. They used a neural network to generate responses to questions. They used\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.08917",
    "question": "How does this research compare to research going on in the US and USSR at this time?",
    "gold_answer": "",
    "answer_a": "Is there any comparison made?\n<sep><explanation> We are exploring the historical significance of research in the field of machine translation conducted by Bulcsu Laszlo, Croatian linguist, who was a pioneer in machine translation in Yugoslavia during the 1950s. We are focused on two important seminal papers written by members of his research group from 1959 and 1962, as well as their legacy in establishing a Croatian machine translation program based around the Faculty of Humanities and Social Sciences of the University of Zagreb in the late 1950s and early 1960",
    "answer_b": "lagging only a couple of years behind the research of the superpowers. Explanation: However, the fact that they referred to them in a text published in 1959 (probably prepared for publishing in 1958, based on BIBREF11, where Laszlo and Petrovi\u0107 described that the group had started its work in 1958) leads us to the conclusion that the poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers (which invested heavily in this effort).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does this research compare to research going on in the US and USSR at this time?\n\nGround Truth Answer:\n\n\nAnswer A:\nIs there any comparison made?\n<sep><explanation> We are exploring the historical significance of research in the field of machine translation conducted by Bulcsu Laszlo, Croatian linguist, who was a pioneer in machine translation in Yugoslavia during the 1950s. We are focused on two important seminal papers written by members of his research group from 1959 and 1962, as well as their legacy in establishing a Croatian machine translation program based around the Faculty of Humanities and Social Sciences of the University of Zagreb in the late 1950s and early 1960\n\nAnswer B:\nlagging only a couple of years behind the research of the superpowers. Explanation: However, the fact that they referred to them in a text published in 1959 (probably prepared for publishing in 1958, based on BIBREF11, where Laszlo and Petrovi\u0107 described that the group had started its work in 1958) leads us to the conclusion that the poorly funded Croatian research was lagging only a couple of years behind the research of the superpowers (which invested heavily in this effort).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.08917",
    "question": "What is the reason this research was not adopted in the 1960s?",
    "gold_answer": "",
    "answer_a": "< Explanation: The Croatian group was not adopted in the 1960s because of the lack of interest in the field of machine translation in Yugoslavia. The group was not interested in the field of machine translation in the 1960s, and the main reason was that the field was not a priority in the Yugoslav educational system. The field of machine translation was not a priority in the Yugoslav educational system because the field was not a priority in the Yugoslav educational system. The field of machine translation was not a priority in the Yugoslav educational system because the field was not a priority in the Yugoslav educational",
    "answer_b": "the lack of funding. Explanation: It is also interesting to note that the Andreev group had a profound influence on the Czechoslovakian machine translation program BIBREF9, which unfortunately suffered a similar fate as the Yugoslav one due to the lack of funding.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the reason this research was not adopted in the 1960s?\n\nGround Truth Answer:\n\n\nAnswer A:\n< Explanation: The Croatian group was not adopted in the 1960s because of the lack of interest in the field of machine translation in Yugoslavia. The group was not interested in the field of machine translation in the 1960s, and the main reason was that the field was not a priority in the Yugoslav educational system. The field of machine translation was not a priority in the Yugoslav educational system because the field was not a priority in the Yugoslav educational system. The field of machine translation was not a priority in the Yugoslav educational system because the field was not a priority in the Yugoslav educational\n\nAnswer B:\nthe lack of funding. Explanation: It is also interesting to note that the Andreev group had a profound influence on the Czechoslovakian machine translation program BIBREF9, which unfortunately suffered a similar fate as the Yugoslav one due to the lack of funding.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.08917",
    "question": "What is included in the cybernetic methods mentioned?",
    "gold_answer": "",
    "answer_a": "compile a dictionary of words sorted from the end of the word to the beginning. Explanation: The first task is to compile a dictionary of words sorted from the end of the word to the beginning. This would enable the development of what is now called stemming and lemmatization modules: a knowledge base with suffixes so they can be trimmed, but also a systematic way to find the base of the word (lemmatization) (p. 121). The second task would be to make a word frequency table. This would enable focusing on a few thousand most frequent words and dropping the rest. This is currently a good industrial practice for building efficient natural language processing systems, and in 1962, it was a computational necessity. The last task was to create a good thesaurus, but such a thesaurus where every data point has a \"meaning\" as the key, and words (synonyms) as values.",
    "answer_b": "Explanation: The cybernetic methods of the Croatian group were based on the idea of a \u201cfeedback loop\u201d and the idea of a \u201cfeedback system\u201d. The feedback system is a system that receives information from the environment and uses it to make decisions. The feedback loop is a system that receives information from the environment and uses it to make decisions. The feedback system is a system that receives information from the environment and uses it to make decisions. The feedback system is a system that receives information from the environment and uses it to make decisions. The feedback system is a system that receives information from the environment and uses it to make decisions. The feedback",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is included in the cybernetic methods mentioned?\n\nGround Truth Answer:\n\n\nAnswer A:\ncompile a dictionary of words sorted from the end of the word to the beginning. Explanation: The first task is to compile a dictionary of words sorted from the end of the word to the beginning. This would enable the development of what is now called stemming and lemmatization modules: a knowledge base with suffixes so they can be trimmed, but also a systematic way to find the base of the word (lemmatization) (p. 121). The second task would be to make a word frequency table. This would enable focusing on a few thousand most frequent words and dropping the rest. This is currently a good industrial practice for building efficient natural language processing systems, and in 1962, it was a computational necessity. The last task was to create a good thesaurus, but such a thesaurus where every data point has a \"meaning\" as the key, and words (synonyms) as values.\n\nAnswer B:\nExplanation: The cybernetic methods of the Croatian group were based on the idea of a \u201cfeedback loop\u201d and the idea of a \u201cfeedback system\u201d. The feedback system is a system that receives information from the environment and uses it to make decisions. The feedback loop is a system that receives information from the environment and uses it to make decisions. The feedback system is a system that receives information from the environment and uses it to make decisions. The feedback system is a system that receives information from the environment and uses it to make decisions. The feedback system is a system that receives information from the environment and uses it to make decisions. The feedback\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.08917",
    "question": "What were the usual logical approaches of the time period?",
    "gold_answer": "",
    "answer_a": "What was the Croatian group's approach?\n<blockquote>They were interested in the logical approach of the time, which was based on the idea that the machine should be able to translate from one language to another, and that the machine should be able to translate from one language to another in a logical way, that is, in a way that is consistent with the rules of the language. They were interested in the logical approach of the time, which was based on the idea that the machine should be able to translate from one language to another, and that the machine should be able to translate from one language to another in a logical way, that",
    "answer_b": "They evaluated the majority of algorithms known at the time algorithms over meticulously prepared datasets, whose main strength was data cleaning, and by 1959 they have built a German-Russian machine translation prototype.. Explanation: In the USSR, there were four major approaches to machine translation in the late 1950s BIBREF7. The first one was the research at the Institute for Precise Mechanics and Computational Technology of the USSR Academy of Sciences. Their approach was mostly experimental and not much different from today's empirical methods. They evaluated the majority of algorithms known at the time algorithms over meticulously prepared datasets, whose main strength was data cleaning, and by 1959 they have built a German-Russian machine translation prototype. The second approach, as noted by Muli\u0107 BIBREF7, was championed by the team at the Steklov Mathematical Institute of the USSR Academy of Sciences led by A. A. Reformatsky. Their approach was mainly logical, and they extended the theoretical ideas of Bar-Hillel BIBREF2 to build three algorithms: French-Russian, English-Russian and Hungarian-Russian. The third and perhaps the most successful approach was the one by A. A. Lyapunov, O. S. Kulagina and R. L. Dobrushin. Their efforts resulted in the formation of the Mathematical Linguistics Seminar at the Faculty of Philology in Moscow in 1956 and in Leningrad in 1957. Their approach was mainly information-theoretic (but they also tried logic-based approaches BIBREF7), which was considered cybernetic at that time. This was the main role model for the Croatian efforts from 1957 onwards. The fourth, and perhaps most influential, was the approach at the Experimental Laboratory of the Leningrad University championed by N. D. Andreev BIBREF7. Here, the algorithms for Indonesian-Russian, Arabic-Russian, Hindu-Russian, Japanese-Russian, Burmese-Russian, Norwegian-Russian, English-Russian, Spanish-Russian and Turkish-Russian were being built. The main approach of Andreev's group was to use an intermediary language, which would capture the meanings BIBREF7. It was an approach similar to KL-ONE, which would be introduced in the West much later (in 1985) by Brachman and Schmolze BIBREF8. It is also interesting to note that the Andreev group had a profound influence on the Czechoslovakian machine translation program BIBREF9, which unfortunately suffered a similar fate as the Yugoslav one due to the lack of funding.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat were the usual logical approaches of the time period?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat was the Croatian group's approach?\n<blockquote>They were interested in the logical approach of the time, which was based on the idea that the machine should be able to translate from one language to another, and that the machine should be able to translate from one language to another in a logical way, that is, in a way that is consistent with the rules of the language. They were interested in the logical approach of the time, which was based on the idea that the machine should be able to translate from one language to another, and that the machine should be able to translate from one language to another in a logical way, that\n\nAnswer B:\nThey evaluated the majority of algorithms known at the time algorithms over meticulously prepared datasets, whose main strength was data cleaning, and by 1959 they have built a German-Russian machine translation prototype.. Explanation: In the USSR, there were four major approaches to machine translation in the late 1950s BIBREF7. The first one was the research at the Institute for Precise Mechanics and Computational Technology of the USSR Academy of Sciences. Their approach was mostly experimental and not much different from today's empirical methods. They evaluated the majority of algorithms known at the time algorithms over meticulously prepared datasets, whose main strength was data cleaning, and by 1959 they have built a German-Russian machine translation prototype. The second approach, as noted by Muli\u0107 BIBREF7, was championed by the team at the Steklov Mathematical Institute of the USSR Academy of Sciences led by A. A. Reformatsky. Their approach was mainly logical, and they extended the theoretical ideas of Bar-Hillel BIBREF2 to build three algorithms: French-Russian, English-Russian and Hungarian-Russian. The third and perhaps the most successful approach was the one by A. A. Lyapunov, O. S. Kulagina and R. L. Dobrushin. Their efforts resulted in the formation of the Mathematical Linguistics Seminar at the Faculty of Philology in Moscow in 1956 and in Leningrad in 1957. Their approach was mainly information-theoretic (but they also tried logic-based approaches BIBREF7), which was considered cybernetic at that time. This was the main role model for the Croatian efforts from 1957 onwards. The fourth, and perhaps most influential, was the approach at the Experimental Laboratory of the Leningrad University championed by N. D. Andreev BIBREF7. Here, the algorithms for Indonesian-Russian, Arabic-Russian, Hindu-Russian, Japanese-Russian, Burmese-Russian, Norwegian-Russian, English-Russian, Spanish-Russian and Turkish-Russian were being built. The main approach of Andreev's group was to use an intermediary language, which would capture the meanings BIBREF7. It was an approach similar to KL-ONE, which would be introduced in the West much later (in 1985) by Brachman and Schmolze BIBREF8. It is also interesting to note that the Andreev group had a profound influence on the Czechoslovakian machine translation program BIBREF9, which unfortunately suffered a similar fate as the Yugoslav one due to the lack of funding.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.07445",
    "question": "what language was the data in?",
    "gold_answer": "",
    "answer_a": "Explanation: We use the English Wikipedia corpus, which is available at https://www.wikiwand.com/w/index.php?title=Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:",
    "answer_b": "English. Explanation: We then asked three volunteers to rate simplifications with respect to Fluency (the extent to which the output is grammatical English), Adequacy (the extent to which the output has the same meaning as the input sentence), and Simplicity (the extent to which the output is simpler than the input sentence) using a five point Likert scale.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat language was the data in?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We use the English Wikipedia corpus, which is available at https://www.wikiwand.com/w/index.php?title=Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:Wikipedia:\n\nAnswer B:\nEnglish. Explanation: We then asked three volunteers to rate simplifications with respect to Fluency (the extent to which the output is grammatical English), Adequacy (the extent to which the output has the same meaning as the input sentence), and Simplicity (the extent to which the output is simpler than the input sentence) using a five point Likert scale.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.07445",
    "question": "what was the baseline?",
    "gold_answer": "",
    "answer_a": "What is the evaluation measure?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Baseline</td>\n<td align=\"left\">Automatic evaluation measure</td>\n</tr>\n<tr>\n<td align=\"left\">Simplification</td>\n<td align=\"left\">Accuracy</td>\n</tr>\n<tr>\n<td align=\"left\">Simplification</td>\n<td align=\"left\">Accuracy</td>\n</tr>\n<tr>\n<td align=\"left\">Simplification</td>\n<td align=\"left\">Accuracy</td>\n</tr>\n<tr>\n<td align=\"left\">Simplification",
    "answer_b": "Dress. Explanation: We compared our models, either tuned with BLEU (-B) or SARI (-S), against systems reported in BIBREF15 , namely Dress, a deep reinforcement learning model, Dress-Ls, a combination of Dress and a lexical simplification model BIBREF15 , Pbmt-R, a PBMT model with dissimilarity-based re-ranking BIBREF9 , Hybrid, a hybrid semantic-based model that combines a simplification model and a monolingual MT model BIBREF29 , and Sbmt-Sari, a SBMT model with simplification-specific components. BIBREF12 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat was the baseline?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the evaluation measure?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Baseline</td>\n<td align=\"left\">Automatic evaluation measure</td>\n</tr>\n<tr>\n<td align=\"left\">Simplification</td>\n<td align=\"left\">Accuracy</td>\n</tr>\n<tr>\n<td align=\"left\">Simplification</td>\n<td align=\"left\">Accuracy</td>\n</tr>\n<tr>\n<td align=\"left\">Simplification</td>\n<td align=\"left\">Accuracy</td>\n</tr>\n<tr>\n<td align=\"left\">Simplification\n\nAnswer B:\nDress. Explanation: We compared our models, either tuned with BLEU (-B) or SARI (-S), against systems reported in BIBREF15 , namely Dress, a deep reinforcement learning model, Dress-Ls, a combination of Dress and a lexical simplification model BIBREF15 , Pbmt-R, a PBMT model with dissimilarity-based re-ranking BIBREF9 , Hybrid, a hybrid semantic-based model that combines a simplification model and a monolingual MT model BIBREF29 , and Sbmt-Sari, a SBMT model with simplification-specific components. BIBREF12 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.07445",
    "question": "which automatic metrics were used in evaluation?",
    "gold_answer": "",
    "answer_a": "What are the human judgments?\n< Explanation: We use the following metrics to evaluate the performance of the model: BLEU (BibTeX link), METEOR (BibTeX link), ROUGE (BibTeX link), and the F1 score (BibTeX link). We also use the following human judgments: the number of sentences simplified, the number of sentences simplified in the original text, and the number of sentences simplified in the simplified text. We also use the following human judgments: the number of sentences simplified, the number of sentences simplified in the original text, and the number of sentences simplified in the simplified text.",
    "answer_b": "BLEU. Explanation: The results of the automatic evaluation are displayed in Table TABREF15 . We first discuss the results on Newsela that contains high-quality simplifications composed by professional editors. In terms of BLEU, all neural models achieved much higher scores than Pbmt-R and Hybrid. NseLstm-B scored highest with a BLEU score of 26.31. With regard to SARI, NseLstm-S scored best among neural models (29.58) and came close to the performance of Hybrid (30.00). This indicates that NSE offers an effective means to better encode complex sentences for sentence simplification.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhich automatic metrics were used in evaluation?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the human judgments?\n< Explanation: We use the following metrics to evaluate the performance of the model: BLEU (BibTeX link), METEOR (BibTeX link), ROUGE (BibTeX link), and the F1 score (BibTeX link). We also use the following human judgments: the number of sentences simplified, the number of sentences simplified in the original text, and the number of sentences simplified in the simplified text. We also use the following human judgments: the number of sentences simplified, the number of sentences simplified in the original text, and the number of sentences simplified in the simplified text.\n\nAnswer B:\nBLEU. Explanation: The results of the automatic evaluation are displayed in Table TABREF15 . We first discuss the results on Newsela that contains high-quality simplifications composed by professional editors. In terms of BLEU, all neural models achieved much higher scores than Pbmt-R and Hybrid. NseLstm-B scored highest with a BLEU score of 26.31. With regard to SARI, NseLstm-S scored best among neural models (29.58) and came close to the performance of Hybrid (30.00). This indicates that NSE offers an effective means to better encode complex sentences for sentence simplification.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.07445",
    "question": "how do humans judge the simplified sentences?",
    "gold_answer": "",
    "answer_a": "Rate simplifications with respect to Fluency, Adequacy, and Simplicity, using a five point Likert scale.. Explanation: We then asked three volunteers to rate simplifications with respect to Fluency (the extent to which the output is grammatical English), Adequacy (the extent to which the output has the same meaning as the input sentence), and Simplicity (the extent to which the output is simpler than the input sentence) using a five point Likert scale.",
    "answer_b": "What are the evaluation measures?\n< Explanation: We evaluate the performance of our model on the simplified sentences using the following metrics: BLEU, METEOR, and ROUGE. We also evaluate the performance of the original model on the original sentences using the following metrics: BLEU, METEOR, and ROUGE. We also evaluate the performance of the original model on the simplified sentences using the following metrics: BLEU, METEOR, and ROUGE. We also evaluate the performance of the original model on the original sentences using the following metrics: BLEU, METEOR, and ROUGE. We also evaluate the performance",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow do humans judge the simplified sentences?\n\nGround Truth Answer:\n\n\nAnswer A:\nRate simplifications with respect to Fluency, Adequacy, and Simplicity, using a five point Likert scale.. Explanation: We then asked three volunteers to rate simplifications with respect to Fluency (the extent to which the output is grammatical English), Adequacy (the extent to which the output has the same meaning as the input sentence), and Simplicity (the extent to which the output is simpler than the input sentence) using a five point Likert scale.\n\nAnswer B:\nWhat are the evaluation measures?\n< Explanation: We evaluate the performance of our model on the simplified sentences using the following metrics: BLEU, METEOR, and ROUGE. We also evaluate the performance of the original model on the original sentences using the following metrics: BLEU, METEOR, and ROUGE. We also evaluate the performance of the original model on the simplified sentences using the following metrics: BLEU, METEOR, and ROUGE. We also evaluate the performance of the original model on the original sentences using the following metrics: BLEU, METEOR, and ROUGE. We also evaluate the performance\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.07445",
    "question": "what datasets were used?",
    "gold_answer": "",
    "answer_a": "< Explanation: We use the following datasets: (1) the English to French Simplification dataset (ES) (BIBREF10), (2) the English to German Simplification dataset (EG) (BIBREF11), (3) the English to Spanish Simplification dataset (ES) (BIBREF12), (4) the English to Turkish Simplification dataset (ET) (BIBREF13), (5) the English to Polish Simplification dataset (EP) (BIBREF14), (6) the English to Russian Simplification dataset (ER) (BIBREF1",
    "answer_b": "Newsela BIBREF22. Explanation: Following BIBREF15 , we experiment on three simplification datasets, namely: (1) Newsela BIBREF22 , a high-quality simplification corpus of news articles composed by Newsela professional editors for children at multiple grade levels. We used the split of the data in BIBREF15 , i.e., 94,208/1,129/1,077 pairs for train/dev/test. (2) WikiSmall BIBREF10 , which contains aligned complex-simple sentence pairs from English Wikipedia (EW) and SEW. The dataset has 88,837/205/100 pairs for train/dev/test. (3) WikiLarge BIBREF15 , a larger corpus in which the training set is a mixture of three Wikipedia datasets in BIBREF10 , BIBREF11 , BIBREF23 , and the development and test sests are complex sentences taken from WikiSmall, each has 8 simplifications written by Amazon Mechanical Turk workers BIBREF12 . The dataset has 296,402/2,000/359 pairs for train/dev/test. Table TABREF7 provides statistics on the training sets.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat datasets were used?\n\nGround Truth Answer:\n\n\nAnswer A:\n< Explanation: We use the following datasets: (1) the English to French Simplification dataset (ES) (BIBREF10), (2) the English to German Simplification dataset (EG) (BIBREF11), (3) the English to Spanish Simplification dataset (ES) (BIBREF12), (4) the English to Turkish Simplification dataset (ET) (BIBREF13), (5) the English to Polish Simplification dataset (EP) (BIBREF14), (6) the English to Russian Simplification dataset (ER) (BIBREF1\n\nAnswer B:\nNewsela BIBREF22. Explanation: Following BIBREF15 , we experiment on three simplification datasets, namely: (1) Newsela BIBREF22 , a high-quality simplification corpus of news articles composed by Newsela professional editors for children at multiple grade levels. We used the split of the data in BIBREF15 , i.e., 94,208/1,129/1,077 pairs for train/dev/test. (2) WikiSmall BIBREF10 , which contains aligned complex-simple sentence pairs from English Wikipedia (EW) and SEW. The dataset has 88,837/205/100 pairs for train/dev/test. (3) WikiLarge BIBREF15 , a larger corpus in which the training set is a mixture of three Wikipedia datasets in BIBREF10 , BIBREF11 , BIBREF23 , and the development and test sests are complex sentences taken from WikiSmall, each has 8 simplifications written by Amazon Mechanical Turk workers BIBREF12 . The dataset has 296,402/2,000/359 pairs for train/dev/test. Table TABREF7 provides statistics on the training sets.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.03355",
    "question": "What previous approaches are presented for comparison?",
    "gold_answer": "",
    "answer_a": "Baseline system corresponds to considering the original document as the modernized version. They used two approaches SMT and NMT and compared to the baseline, SMT showed best results.. Explanation: Both SMT and NMT approaches were able to significantly improved the baseline.",
    "answer_b": "Is the proposed approach superior?\n<sep><br/ >Previous approaches to modernization include the use of machine translation, which is a powerful tool for generating a new version of a document. However, machine translation is not always reliable, and it is not always possible to generate a new version of a document that is as accurate as the original. In addition, machine translation is not always feasible for documents that are too long or too complex. Furthermore, machine translation is not always feasible for documents that are written in a foreign language. In this work, we propose a collaborative framework in which a scholar can work together with the machine to generate the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat previous approaches are presented for comparison?\n\nGround Truth Answer:\n\n\nAnswer A:\nBaseline system corresponds to considering the original document as the modernized version. They used two approaches SMT and NMT and compared to the baseline, SMT showed best results.. Explanation: Both SMT and NMT approaches were able to significantly improved the baseline.\n\nAnswer B:\nIs the proposed approach superior?\n<sep><br/ >Previous approaches to modernization include the use of machine translation, which is a powerful tool for generating a new version of a document. However, machine translation is not always reliable, and it is not always possible to generate a new version of a document that is as accurate as the original. In addition, machine translation is not always feasible for documents that are too long or too complex. Furthermore, machine translation is not always feasible for documents that are written in a foreign language. In this work, we propose a collaborative framework in which a scholar can work together with the machine to generate the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.03355",
    "question": "What kind of data is used to train the model?",
    "gold_answer": "",
    "answer_a": "How is it used?\n< Explanation: We use the following data to train the model: (i) the original document, (ii) the original document's translation, (iii) the machine-generated version of the document, and (iv) the human-generated version of the document. We use the original document and the machine-generated version of the document as the training data. The original document is used to train the model, and the machine-generated version of the document is used as the test data. The human-generated version of the document is used as the validation data. We use the original document and the human-generated version of the document as",
    "answer_b": "Modern and historical versions of literature like the Bible and a Spanish novel.. Explanation: The first corpus used in our experimental session was the Dutch Bible BIBREF1. This corpus consists in a collection of different versions of the Dutch Bible: a version from 1637, another from 1657, another from 1888 and another from 2010. Except for the 2010 version, which is missing the last books, all versions contain the same texts.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat kind of data is used to train the model?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it used?\n< Explanation: We use the following data to train the model: (i) the original document, (ii) the original document's translation, (iii) the machine-generated version of the document, and (iv) the human-generated version of the document. We use the original document and the machine-generated version of the document as the training data. The original document is used to train the model, and the machine-generated version of the document is used as the test data. The human-generated version of the document is used as the validation data. We use the original document and the human-generated version of the document as\n\nAnswer B:\nModern and historical versions of literature like the Bible and a Spanish novel.. Explanation: The first corpus used in our experimental session was the Dutch Bible BIBREF1. This corpus consists in a collection of different versions of the Dutch Bible: a version from 1637, another from 1657, another from 1888 and another from 2010. Except for the 2010 version, which is missing the last books, all versions contain the same texts.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.03355",
    "question": "What machine learning techniques are used in the model architecture?",
    "gold_answer": "",
    "answer_a": "Classical IMT approaches. Explanation: Classical IMT approaches relay on the statistical formalization of the MT problem. Given a source sentence $\\mathbf {x}$, SMT aims at finding its most likely translation $\\hat{\\mathbf {y}}$ BIBREF18:\n\nFor years, the prevailing approach to compute this expression have been phrase-based models BIBREF19. These models rely on a log-linear combination of different models BIBREF20: namely, phrase-based alignment models, reordering models and language models; among others BIBREF21, BIBREF22. However, more recently, this approach has shifted into neural models (see se:NMT).",
    "answer_b": "Explanation: We use a deep learning model to generate the new version of the document. The model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents, and the model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat machine learning techniques are used in the model architecture?\n\nGround Truth Answer:\n\n\nAnswer A:\nClassical IMT approaches. Explanation: Classical IMT approaches relay on the statistical formalization of the MT problem. Given a source sentence $\\mathbf {x}$, SMT aims at finding its most likely translation $\\hat{\\mathbf {y}}$ BIBREF18:\n\nFor years, the prevailing approach to compute this expression have been phrase-based models BIBREF19. These models rely on a log-linear combination of different models BIBREF20: namely, phrase-based alignment models, reordering models and language models; among others BIBREF21, BIBREF22. However, more recently, this approach has shifted into neural models (see se:NMT).\n\nAnswer B:\nExplanation: We use a deep learning model to generate the new version of the document. The model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents, and the model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents. The model is trained on a corpus of historical documents.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.03355",
    "question": "What language(s) is the model tested on?",
    "gold_answer": "",
    "answer_a": "How is the language model trained?\n< Explanation: We tested our approach on a simulated environment, where we generated a new version of a document in English. We used the following language models: (1) a BERT model trained on the English Wikipedia corpus, (2) a BERT model trained on the English Wikipedia corpus with a 1000 times larger vocabulary, and (3) a BERT model trained on the English Wikipedia corpus with a 1000 times larger vocabulary and a 1000 times larger vocabulary size. We used the same training data for all models. We trained the models using the",
    "answer_b": "Dutch and Spanish. Explanation: The first corpus used in our experimental session was the Dutch Bible BIBREF1. This corpus consists in a collection of different versions of the Dutch Bible: a version from 1637, another from 1657, another from 1888 and another from 2010. Except for the 2010 version, which is missing the last books, all versions contain the same texts. Moreover, since the authors mentioned that the translation from this last version is not very reliable and, considering that Dutch has not evolved significantly between 1637 and 1657, we decided to only use the 1637 version\u2014considering this as the original document\u2014and the 1888 version\u2014considering 19$^{\\mathrm {th}}$ century Dutch as modern Dutch.\n\nWe selected El Quijote BIBREF2 as our second corpus. This corpus contains the famous 17$^{\\mathrm {th}}$ century Spanish novel by Miguel de Cervantes, and its correspondent 21$^{\\mathrm {st}}$ century version. Finally, we used El Conde Lucanor BIBREF2 as a third corpus. This data set contains the original 14$^{\\mathrm {th}}$ century Spanish novel by Don Juan Manuel, and its correspondent 21$^{\\mathrm {st}}$ century version. Due to the small size of the corpus, we decided to use it only as a test. Additionally, unable to find a suitable training corpus, we used the systems built for El Quijote\u2014despite the original documents belonging to different time periods\u2014in order to modernize El Conde Lucanor.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat language(s) is the model tested on?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is the language model trained?\n< Explanation: We tested our approach on a simulated environment, where we generated a new version of a document in English. We used the following language models: (1) a BERT model trained on the English Wikipedia corpus, (2) a BERT model trained on the English Wikipedia corpus with a 1000 times larger vocabulary, and (3) a BERT model trained on the English Wikipedia corpus with a 1000 times larger vocabulary and a 1000 times larger vocabulary size. We used the same training data for all models. We trained the models using the\n\nAnswer B:\nDutch and Spanish. Explanation: The first corpus used in our experimental session was the Dutch Bible BIBREF1. This corpus consists in a collection of different versions of the Dutch Bible: a version from 1637, another from 1657, another from 1888 and another from 2010. Except for the 2010 version, which is missing the last books, all versions contain the same texts. Moreover, since the authors mentioned that the translation from this last version is not very reliable and, considering that Dutch has not evolved significantly between 1637 and 1657, we decided to only use the 1637 version\u2014considering this as the original document\u2014and the 1888 version\u2014considering 19$^{\\mathrm {th}}$ century Dutch as modern Dutch.\n\nWe selected El Quijote BIBREF2 as our second corpus. This corpus contains the famous 17$^{\\mathrm {th}}$ century Spanish novel by Miguel de Cervantes, and its correspondent 21$^{\\mathrm {st}}$ century version. Finally, we used El Conde Lucanor BIBREF2 as a third corpus. This data set contains the original 14$^{\\mathrm {th}}$ century Spanish novel by Don Juan Manuel, and its correspondent 21$^{\\mathrm {st}}$ century version. Due to the small size of the corpus, we decided to use it only as a test. Additionally, unable to find a suitable training corpus, we used the systems built for El Quijote\u2014despite the original documents belonging to different time periods\u2014in order to modernize El Conde Lucanor.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.09381",
    "question": "By how much did their model outperform baselines?",
    "gold_answer": "",
    "answer_a": "Answer with content missing: (Table 3) Best proposed result has F1 score of 0.844, 0.813, 0.870, 0.842, 0.844 compared to 0.855, 0.789, 0.852, 0.792, 0.833 on span, modality, degree, polarity and type respectively.. Explanation: Our initial submits RUN 4 and 5 outperformed the memorization baseline on every metric on every task.",
    "answer_b": "What were the baselines?\n< Explanation: We compare our model with the following baselines: (1) a baseline that uses only the shape information (shape-only) (2) a baseline that uses only the context words (context-only) (3) a baseline that uses both the shape and context information (shape-context) (4) a baseline that uses both the shape and context words (shape-context) (5) a baseline that uses both the shape and context words and the shape context (shape-context) (6) a baseline that uses both the shape and context words and the shape context and the context (shape-context)",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much did their model outperform baselines?\n\nGround Truth Answer:\n\n\nAnswer A:\nAnswer with content missing: (Table 3) Best proposed result has F1 score of 0.844, 0.813, 0.870, 0.842, 0.844 compared to 0.855, 0.789, 0.852, 0.792, 0.833 on span, modality, degree, polarity and type respectively.. Explanation: Our initial submits RUN 4 and 5 outperformed the memorization baseline on every metric on every task.\n\nAnswer B:\nWhat were the baselines?\n< Explanation: We compare our model with the following baselines: (1) a baseline that uses only the shape information (shape-only) (2) a baseline that uses only the context words (context-only) (3) a baseline that uses both the shape and context information (shape-context) (4) a baseline that uses both the shape and context words (shape-context) (5) a baseline that uses both the shape and context words and the shape context (shape-context) (6) a baseline that uses both the shape and context words and the shape context and the context (shape-context)\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.09381",
    "question": "Which baselines did they compare against?",
    "gold_answer": "",
    "answer_a": "memorization, median report, max report. Explanation: Our initial submits RUN 4 and 5 outperformed the memorization baseline on every metric on every task. The precision of event span identification is close to the max report.",
    "answer_b": "What is the performance of their approach?\n< Explanation: We compare our approach with the following baselines: (1) a baseline that uses only the shape information (shape-only) (2) a baseline that uses only the context words (context-only) (3) a baseline that uses both the shape and context information (shape-context) (4) a baseline that uses both the shape and context words (shape-context) (5) a baseline that uses both the shape and context words and the shape context (shape-context) (6) a baseline that uses both the shape and context words and the shape context and the context (shape",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich baselines did they compare against?\n\nGround Truth Answer:\n\n\nAnswer A:\nmemorization, median report, max report. Explanation: Our initial submits RUN 4 and 5 outperformed the memorization baseline on every metric on every task. The precision of event span identification is close to the max report.\n\nAnswer B:\nWhat is the performance of their approach?\n< Explanation: We compare our approach with the following baselines: (1) a baseline that uses only the shape information (shape-only) (2) a baseline that uses only the context words (context-only) (3) a baseline that uses both the shape and context information (shape-context) (4) a baseline that uses both the shape and context words (shape-context) (5) a baseline that uses both the shape and context words and the shape context (shape-context) (6) a baseline that uses both the shape and context words and the shape context and the context (shape\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.09381",
    "question": "What was their performance on this task?",
    "gold_answer": "",
    "answer_a": "How was it evaluated?\n< Explanation: We evaluated our approach on the following tasks: (1) event span extraction, (2) event span prediction, and (3) event span classification. We used the following metrics: (1) F1 score, (2) Recall, and (3) Precision. We also used the following metrics: (1) Recall, (2) Precision, and (3) F1 score. We also used the following metrics: (1) Recall, (2) Precision, and (3) F1 score. We also used the following metrics: (1) Recall, (2)",
    "answer_b": "Their average F1 score was 0.874 on span detection; 08115 on contextual modality detection; 0.8695 on degree detection; 0.839 on polarity detection; 0.844 on type detection. Explanation: FLOAT SELECTED: Table 3: System performance comparison. Note that Run4 means the window size is 4, Run5 means the window size is 5",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat was their performance on this task?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow was it evaluated?\n< Explanation: We evaluated our approach on the following tasks: (1) event span extraction, (2) event span prediction, and (3) event span classification. We used the following metrics: (1) F1 score, (2) Recall, and (3) Precision. We also used the following metrics: (1) Recall, (2) Precision, and (3) F1 score. We also used the following metrics: (1) Recall, (2) Precision, and (3) F1 score. We also used the following metrics: (1) Recall, (2)\n\nAnswer B:\nTheir average F1 score was 0.874 on span detection; 08115 on contextual modality detection; 0.8695 on degree detection; 0.839 on polarity detection; 0.844 on type detection. Explanation: FLOAT SELECTED: Table 3: System performance comparison. Note that Run4 means the window size is 4, Run5 means the window size is 5\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.09381",
    "question": "What dataset did they use to evaluate?",
    "gold_answer": "",
    "answer_a": "Clinical TempEval corpus. Explanation: We use the Clinical TempEval corpus as the evaluation dataset.",
    "answer_b": "How did they evaluate?\n< Explanation: We use the Clinical Event Extraction (CEE) dataset, which is a collection of 1000 clinical notes and pathology reports from the National Library of Medicine (NLM) database. The dataset contains 1000 notes and 1000 pathology reports. Each note and report is annotated with an event span and its attributes. The annotations are provided in the form of a JSON file. The annotations are in the form of a JSON object with the following fields: event, start, end, and attributes. The event is a string that represents the event type. The start and",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat dataset did they use to evaluate?\n\nGround Truth Answer:\n\n\nAnswer A:\nClinical TempEval corpus. Explanation: We use the Clinical TempEval corpus as the evaluation dataset.\n\nAnswer B:\nHow did they evaluate?\n< Explanation: We use the Clinical Event Extraction (CEE) dataset, which is a collection of 1000 clinical notes and pathology reports from the National Library of Medicine (NLM) database. The dataset contains 1000 notes and 1000 pathology reports. Each note and report is annotated with an event span and its attributes. The annotations are provided in the form of a JSON file. The annotations are in the form of a JSON object with the following fields: event, start, end, and attributes. The event is a string that represents the event type. The start and\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.09381",
    "question": "How did they obtain part-of-speech tags?",
    "gold_answer": "",
    "answer_a": "Answer with content missing: (We then use \u201dPerceptronTagger\u201d as our part-ofspeech tagger due to its fast tagging speed) PerceptronTagger.. Explanation: like below,",
    "answer_b": "What is the size of the dataset?\n< Explanation: We use the Stanford NLP Corpus (Stanford NLP) to obtain part-of-speech tags. The corpus contains 10,000,000 documents, each of which is annotated with 100,000 tokens. We use the Stanford NLP Corpus to obtain part-of-speech tags. The corpus contains 10,000,000 documents, each of which is annotated with 100,000 tokens. We use the Stanford NLP Corpus to obtain part-of-speech tags. The corpus",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow did they obtain part-of-speech tags?\n\nGround Truth Answer:\n\n\nAnswer A:\nAnswer with content missing: (We then use \u201dPerceptronTagger\u201d as our part-ofspeech tagger due to its fast tagging speed) PerceptronTagger.. Explanation: like below,\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: We use the Stanford NLP Corpus (Stanford NLP) to obtain part-of-speech tags. The corpus contains 10,000,000 documents, each of which is annotated with 100,000 tokens. We use the Stanford NLP Corpus to obtain part-of-speech tags. The corpus contains 10,000,000 documents, each of which is annotated with 100,000 tokens. We use the Stanford NLP Corpus to obtain part-of-speech tags. The corpus\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.05482",
    "question": "what was their system's f1 score?",
    "gold_answer": "",
    "answer_a": "0.6955. Explanation: FLOAT SELECTED: Table 2: Comparison with existing methods.",
    "answer_b": "How did they do?\n< Explanation: We use the emotion cause dataset (EMCA) which is a new emotion cause dataset released by the University of California, Irvine (UCI) in 2017. The dataset contains 10,000 text pairs, each pair consists of a question and its answer. The question is a short sentence containing the emotion cause, and the answer is a short sentence containing the emotion. The dataset contains 1,000 pairs of question-answer pairs. The dataset is divided into training, validation and test sets. The training set contains 500 pairs, the validation",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat was their system's f1 score?\n\nGround Truth Answer:\n\n\nAnswer A:\n0.6955. Explanation: FLOAT SELECTED: Table 2: Comparison with existing methods.\n\nAnswer B:\nHow did they do?\n< Explanation: We use the emotion cause dataset (EMCA) which is a new emotion cause dataset released by the University of California, Irvine (UCI) in 2017. The dataset contains 10,000 text pairs, each pair consists of a question and its answer. The question is a short sentence containing the emotion cause, and the answer is a short sentence containing the emotion. The dataset contains 1,000 pairs of question-answer pairs. The dataset is divided into training, validation and test sets. The training set contains 500 pairs, the validation\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.05482",
    "question": "what were the baselines?",
    "gold_answer": "",
    "answer_a": "RB (Rule based method). Explanation: We compare with the following baseline methods:\n\nRB (Rule based method): The rule based method proposed in BIBREF33 .\n\nCB (Common-sense based method): This is the knowledge based method proposed by BIBREF34 . We use the Chinese Emotion Cognition Lexicon BIBREF35 as the common-sense knowledge base. The lexicon contains more than 5,000 kinds of emotion stimulation and their corresponding reflection words.\n\nRB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base): This methods was previously proposed for emotion cause classification in BIBREF36 . It takes rules and facts in a knowledge base as features for classifier training. We train a SVM using features extracted from the rules defined in BIBREF33 and the Chinese Emotion Cognition Lexicon BIBREF35 .\n\nSVM: This is a SVM classifier using the unigram, bigram and trigram features. It is a baseline previously used in BIBREF24 , BIBREF31\n\nWord2vec: This is a SVM classifier using word representations learned by Word2vec BIBREF32 as features.\n\nMulti-kernel: This is the state-of-the-art method using the multi-kernel method BIBREF31 to identify the emotion cause. We use the best performance reported in their paper.\n\nCNN: The convolutional neural network for sentence classification BIBREF5 .\n\nMemnet: The deep memory network described in Section SECREF3 . Word embeddings are pre-trained by skip-grams. The number of hops is set to 3.",
    "answer_b": "How did they compare to the proposed method?\n< Explanation: We compare our proposed method with the following baselines: (1) a baseline using a simple word-level sequence representation (2) a baseline using a simple lexical representation (3) a baseline using a simple word-level sequence representation with a word embedding (4) a baseline using a simple lexical representation with a word embedding (5) a baseline using a simple word-level sequence representation with a word embedding and a simple lexical representation (6) a baseline using a simple lexical representation with a word embedding and a simple word-level sequence representation (7) a baseline using a simple word-level sequence",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat were the baselines?\n\nGround Truth Answer:\n\n\nAnswer A:\nRB (Rule based method). Explanation: We compare with the following baseline methods:\n\nRB (Rule based method): The rule based method proposed in BIBREF33 .\n\nCB (Common-sense based method): This is the knowledge based method proposed by BIBREF34 . We use the Chinese Emotion Cognition Lexicon BIBREF35 as the common-sense knowledge base. The lexicon contains more than 5,000 kinds of emotion stimulation and their corresponding reflection words.\n\nRB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base): This methods was previously proposed for emotion cause classification in BIBREF36 . It takes rules and facts in a knowledge base as features for classifier training. We train a SVM using features extracted from the rules defined in BIBREF33 and the Chinese Emotion Cognition Lexicon BIBREF35 .\n\nSVM: This is a SVM classifier using the unigram, bigram and trigram features. It is a baseline previously used in BIBREF24 , BIBREF31\n\nWord2vec: This is a SVM classifier using word representations learned by Word2vec BIBREF32 as features.\n\nMulti-kernel: This is the state-of-the-art method using the multi-kernel method BIBREF31 to identify the emotion cause. We use the best performance reported in their paper.\n\nCNN: The convolutional neural network for sentence classification BIBREF5 .\n\nMemnet: The deep memory network described in Section SECREF3 . Word embeddings are pre-trained by skip-grams. The number of hops is set to 3.\n\nAnswer B:\nHow did they compare to the proposed method?\n< Explanation: We compare our proposed method with the following baselines: (1) a baseline using a simple word-level sequence representation (2) a baseline using a simple lexical representation (3) a baseline using a simple word-level sequence representation with a word embedding (4) a baseline using a simple lexical representation with a word embedding (5) a baseline using a simple word-level sequence representation with a word embedding and a simple lexical representation (6) a baseline using a simple lexical representation with a word embedding and a simple word-level sequence representation (7) a baseline using a simple word-level sequence\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.05482",
    "question": "what emotion cause dataset was used?",
    "gold_answer": "",
    "answer_a": "simplified Chinese emotion cause corpus BIBREF31. Explanation: We conduct experiments on a simplified Chinese emotion cause corpus BIBREF31 , the only publicly available dataset on this task to the best of our knowledge.",
    "answer_b": "Explanation: We use the emotion cause dataset, which is a publicly available dataset of emotion cause identification. The dataset contains 10000 pairs of emotion cause and emotion pairs. The dataset is available at https://github.com/EmotionCauseDataset/EmotionCauseDataset. The dataset contains 10000 pairs of emotion cause and emotion pairs. The dataset is available at https://github.com/EmotionCauseDataset/EmotionCauseDataset. The dataset contains 10000 pairs of emotion cause and emotion pairs. The dataset is available at https://github.com/EmotionCauseDataset/Emotion",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat emotion cause dataset was used?\n\nGround Truth Answer:\n\n\nAnswer A:\nsimplified Chinese emotion cause corpus BIBREF31. Explanation: We conduct experiments on a simplified Chinese emotion cause corpus BIBREF31 , the only publicly available dataset on this task to the best of our knowledge.\n\nAnswer B:\nExplanation: We use the emotion cause dataset, which is a publicly available dataset of emotion cause identification. The dataset contains 10000 pairs of emotion cause and emotion pairs. The dataset is available at https://github.com/EmotionCauseDataset/EmotionCauseDataset. The dataset contains 10000 pairs of emotion cause and emotion pairs. The dataset is available at https://github.com/EmotionCauseDataset/EmotionCauseDataset. The dataset contains 10000 pairs of emotion cause and emotion pairs. The dataset is available at https://github.com/EmotionCauseDataset/Emotion\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.05482",
    "question": "what lexical features are extracted?",
    "gold_answer": "",
    "answer_a": "the distance between a clause and an emotion words. Explanation: As the distance between a clause and an emotion words is a very important feature according to BIBREF31 , we simply add this distance into the softmax function as an additional feature in our work.",
    "answer_b": "How are they used?\n< Explanation: We use the following lexical features to extract the emotion cause: (1) the word embedding of the emotion cause, (2) the word embedding of the emotion, (3) the word embedding of the emotion, (4) the word embedding of the emotion, (5) the word embedding of the emotion, (6) the word embedding of the emotion, (7) the word embedding of the emotion, (8) the word embedding of the emotion, (9) the word embedding of the emotion, (10) the word embedding of the emotion, (11) the word",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat lexical features are extracted?\n\nGround Truth Answer:\n\n\nAnswer A:\nthe distance between a clause and an emotion words. Explanation: As the distance between a clause and an emotion words is a very important feature according to BIBREF31 , we simply add this distance into the softmax function as an additional feature in our work.\n\nAnswer B:\nHow are they used?\n< Explanation: We use the following lexical features to extract the emotion cause: (1) the word embedding of the emotion cause, (2) the word embedding of the emotion, (3) the word embedding of the emotion, (4) the word embedding of the emotion, (5) the word embedding of the emotion, (6) the word embedding of the emotion, (7) the word embedding of the emotion, (8) the word embedding of the emotion, (9) the word embedding of the emotion, (10) the word embedding of the emotion, (11) the word\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.05482",
    "question": "what word level sequences features are extracted?",
    "gold_answer": "",
    "answer_a": "How are they computed?\n< Explanation: We use the word level sequence features to represent the emotion cause. The word level sequence features are computed by using the following formula: where is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word",
    "answer_b": "Concatenation of three prediction output vectors. Explanation: Hence, there are three prediction output vectors, namely, INLINEFORM2 , INLINEFORM3 , INLINEFORM4 : DISPLAYFORM0\n\nAt last, we concatenate the three vectors as INLINEFORM0 for the prediction by a softmax function: DISPLAYFORM0\n\nHere, the size of INLINEFORM0 is INLINEFORM1 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat word level sequences features are extracted?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they computed?\n< Explanation: We use the word level sequence features to represent the emotion cause. The word level sequence features are computed by using the following formula: where is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word embedding, is the word\n\nAnswer B:\nConcatenation of three prediction output vectors. Explanation: Hence, there are three prediction output vectors, namely, INLINEFORM2 , INLINEFORM3 , INLINEFORM4 : DISPLAYFORM0\n\nAt last, we concatenate the three vectors as INLINEFORM0 for the prediction by a softmax function: DISPLAYFORM0\n\nHere, the size of INLINEFORM0 is INLINEFORM1 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.05589",
    "question": "what are the recent models they compare with?",
    "gold_answer": "",
    "answer_a": "Explanation: We compare the following models:\n\n* LSTM with standard regularisation (LSTM-regularised) and\n\n* LSTM with standard regularisation (LSTM-regularised) and\n\n* LSTM with standard regularisation (LSTM-regularised) and\n\n* LSTM with standard regularisation (LSTM-regularised) and\n\n* LSTM with standard regularisation (LSTM-regularised) and\n\n* LSTM with standard regularisation (LSTM-regularised) and\n\n* LSTM with standard regularisation (LSTM-regularised) and\n\n* LSTM with standard regularisation (LSTM-regularised) and",
    "answer_b": "Recurrent Highway Networks. Explanation: In this paper, we use a black-box hyperparameter optimisation technique to control for hyperparameter effects while comparing the relative performance of language modelling architectures based on LSTMs, Recurrent Highway Networks BIBREF0 and NAS BIBREF1",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the recent models they compare with?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We compare the following models:\n\n* LSTM with standard regularisation (LSTM-regularised) and\n\n* LSTM with standard regularisation (LSTM-regularised) and\n\n* LSTM with standard regularisation (LSTM-regularised) and\n\n* LSTM with standard regularisation (LSTM-regularised) and\n\n* LSTM with standard regularisation (LSTM-regularised) and\n\n* LSTM with standard regularisation (LSTM-regularised) and\n\n* LSTM with standard regularisation (LSTM-regularised) and\n\n* LSTM with standard regularisation (LSTM-regularised) and\n\nAnswer B:\nRecurrent Highway Networks. Explanation: In this paper, we use a black-box hyperparameter optimisation technique to control for hyperparameter effects while comparing the relative performance of language modelling architectures based on LSTMs, Recurrent Highway Networks BIBREF0 and NAS BIBREF1\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.05589",
    "question": "what were their results on the hutter prize dataset?",
    "gold_answer": "",
    "answer_a": "Explanation: We evaluate the performance of the models on the Hutter Prize dataset, which is a subset of the Penn Treebank. The dataset contains 10,000 sentences, each of which is annotated with a 100-word chunk of text. The dataset is split into training and validation sets, and the model is trained on the training set and evaluated on the validation set. We use the following hyperparameters: batch size = 128, learning rate = 0.001, number of layers = 1, number of hidden units = 128, dropout = 0.2",
    "answer_b": "slightly off the state of the art. Explanation: In contrast to the previous datasets, our numbers on this task (reported in BPC, following convetion) are slightly off the state of the art.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat were their results on the hutter prize dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We evaluate the performance of the models on the Hutter Prize dataset, which is a subset of the Penn Treebank. The dataset contains 10,000 sentences, each of which is annotated with a 100-word chunk of text. The dataset is split into training and validation sets, and the model is trained on the training set and evaluated on the validation set. We use the following hyperparameters: batch size = 128, learning rate = 0.001, number of layers = 1, number of hidden units = 128, dropout = 0.2\n\nAnswer B:\nslightly off the state of the art. Explanation: In contrast to the previous datasets, our numbers on this task (reported in BPC, following convetion) are slightly off the state of the art.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.05589",
    "question": "what was their newly established state of the art results?",
    "gold_answer": "",
    "answer_a": "Explanation: We find that the standard LSTM architecture outperforms all other architectures, and that the standard LSTM architecture with a linear attention layer outperforms all other architectures. We also find that the standard LSTM architecture with a linear attention layer outperforms all other architectures. We find that the standard LSTM architecture with a linear attention layer outperforms all other architectures. We find that the standard LSTM architecture with a linear attention layer outperforms all other architectures. We find that the standard LSTM architecture with a linear attention layer outperforms all other architectures. We find that the standard LSTM architecture with a linear attention layer outperforms all other",
    "answer_b": "58.3 perplexity in PTB, and 65.9 perplexity in Wikitext-2. Explanation: We compare models on three datasets. The smallest of them is the Penn Treebank corpus by BIBREF13 with preprocessing from BIBREF14 . We also include another word level corpus: Wikitext-2 by BIBREF15 . It is about twice the size of Penn Treebank with a larger vocabulary and much lighter preprocessing. The third corpus is Enwik8 from the Hutter Prize dataset BIBREF16",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat was their newly established state of the art results?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We find that the standard LSTM architecture outperforms all other architectures, and that the standard LSTM architecture with a linear attention layer outperforms all other architectures. We also find that the standard LSTM architecture with a linear attention layer outperforms all other architectures. We find that the standard LSTM architecture with a linear attention layer outperforms all other architectures. We find that the standard LSTM architecture with a linear attention layer outperforms all other architectures. We find that the standard LSTM architecture with a linear attention layer outperforms all other architectures. We find that the standard LSTM architecture with a linear attention layer outperforms all other\n\nAnswer B:\n58.3 perplexity in PTB, and 65.9 perplexity in Wikitext-2. Explanation: We compare models on three datasets. The smallest of them is the Penn Treebank corpus by BIBREF13 with preprocessing from BIBREF14 . We also include another word level corpus: Wikitext-2 by BIBREF15 . It is about twice the size of Penn Treebank with a larger vocabulary and much lighter preprocessing. The third corpus is Enwik8 from the Hutter Prize dataset BIBREF16\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.05589",
    "question": "what regularisation methods did they look at?",
    "gold_answer": "",
    "answer_a": "dropout. Explanation: Dropout is applied to feedforward connections denoted by dashed arrows in the figure. From the bottom up: to embedded inputs (input dropout), to connections between layers (intra-layer dropout), to the combined and the down-projected outputs (output dropout). All these dropouts have random masks drawn independently per time step, in contrast to the dropout on recurrent states where the same mask is used for all time steps in the sequence.",
    "answer_b": "Explanation: We consider three different types of regularisation: (1) dropout, (2) attention, and (3) attention with dropout. We also consider a simple attention mechanism that combines the output of the LSTM with the input sequence. We use the following attention mechanisms: (1) attention with dropout, (2) attention with dropout and dropout, and (3) attention with dropout and dropout. We also consider a simple attention mechanism that combines the output of the LSTM with the input sequence. We use the following attention mechanisms: (1) attention with dropout, (2) attention with dropout and dropout, and (3) attention",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat regularisation methods did they look at?\n\nGround Truth Answer:\n\n\nAnswer A:\ndropout. Explanation: Dropout is applied to feedforward connections denoted by dashed arrows in the figure. From the bottom up: to embedded inputs (input dropout), to connections between layers (intra-layer dropout), to the combined and the down-projected outputs (output dropout). All these dropouts have random masks drawn independently per time step, in contrast to the dropout on recurrent states where the same mask is used for all time steps in the sequence.\n\nAnswer B:\nExplanation: We consider three different types of regularisation: (1) dropout, (2) attention, and (3) attention with dropout. We also consider a simple attention mechanism that combines the output of the LSTM with the input sequence. We use the following attention mechanisms: (1) attention with dropout, (2) attention with dropout and dropout, and (3) attention with dropout and dropout. We also consider a simple attention mechanism that combines the output of the LSTM with the input sequence. We use the following attention mechanisms: (1) attention with dropout, (2) attention with dropout and dropout, and (3) attention\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.05589",
    "question": "what architectures were reevaluated?",
    "gold_answer": "",
    "answer_a": "LSTMs. Explanation: In this paper, we use a black-box hyperparameter optimisation technique to control for hyperparameter effects while comparing the relative performance of language modelling architectures based on LSTMs, Recurrent Highway Networks BIBREF0 and NAS BIBREF1",
    "answer_b": "Explanation: We reevaluate the following architectures: LSTM, GRU, GRU-1, GRU-2, GRU-3, GRU-4, GRU-5, GRU-6, GRU-7, GRU-8, GRU-9, GRU-10, GRU-11, GRU-12, GRU-13, GRU-14, GRU-15, GRU-16, GRU-17, GRU-18, GRU-19, GRU-20, GRU-",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat architectures were reevaluated?\n\nGround Truth Answer:\n\n\nAnswer A:\nLSTMs. Explanation: In this paper, we use a black-box hyperparameter optimisation technique to control for hyperparameter effects while comparing the relative performance of language modelling architectures based on LSTMs, Recurrent Highway Networks BIBREF0 and NAS BIBREF1\n\nAnswer B:\nExplanation: We reevaluate the following architectures: LSTM, GRU, GRU-1, GRU-2, GRU-3, GRU-4, GRU-5, GRU-6, GRU-7, GRU-8, GRU-9, GRU-10, GRU-11, GRU-12, GRU-13, GRU-14, GRU-15, GRU-16, GRU-17, GRU-18, GRU-19, GRU-20, GRU-\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.14589",
    "question": "what baseline models are trained?",
    "gold_answer": "",
    "answer_a": "Transformer Big BIBREF26. Explanation: For all experiments, we use the Transformer Big BIBREF26 as implemented in Fairseq, with the hyperparameters of BIBREF27.",
    "answer_b": "How are they evaluated?\n< Explanation: We train a baseline model that uses a Transformer-based model (BIBREF10) and a CNN-based model (BIBREF11). We also train a Transformer-based model (BIBREF12) that uses a CNN-based model (BIBREF13) as a baseline. We train a Transformer-based model (BIBREF14) that uses a CNN-based model (BIBREF15) as a baseline. We train a Transformer-based model (BIBREF16) that uses a CNN-based model (BIBREF17) as a baseline",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat baseline models are trained?\n\nGround Truth Answer:\n\n\nAnswer A:\nTransformer Big BIBREF26. Explanation: For all experiments, we use the Transformer Big BIBREF26 as implemented in Fairseq, with the hyperparameters of BIBREF27.\n\nAnswer B:\nHow are they evaluated?\n< Explanation: We train a baseline model that uses a Transformer-based model (BIBREF10) and a CNN-based model (BIBREF11). We also train a Transformer-based model (BIBREF12) that uses a CNN-based model (BIBREF13) as a baseline. We train a Transformer-based model (BIBREF14) that uses a CNN-based model (BIBREF15) as a baseline. We train a Transformer-based model (BIBREF16) that uses a CNN-based model (BIBREF17) as a baseline\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.14589",
    "question": "what dataset was used?",
    "gold_answer": "",
    "answer_a": "WMT 2014. Explanation: After some initial work with the WMT 2014 data, we built a new training corpus named UGC (User Generated Content), closer to our domain, by combining: Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet (See Table TABREF31). UGC does not include Common Crawl (which contains many",
    "answer_b": "< Explanation: We use the French-English parallel corpus of Foursquare restaurant reviews (this https URL), and define a new task to encourage research on Neural Machine Translation robustness and domain adaptation, in a real-world scenario where better-quality MT would be greatly beneficial. We discuss the challenges of such user-generated content, and train good baseline models that build upon the latest techniques for MT robustness. We also perform an extensive evaluation (automatic and human) that shows significant improvements over existing online systems. Finally, we propose task-specific metrics based on sentiment analysis or translation accuracy of domain-specific polysemous words.\n\nWe use the French-",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat dataset was used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWMT 2014. Explanation: After some initial work with the WMT 2014 data, we built a new training corpus named UGC (User Generated Content), closer to our domain, by combining: Multi UN, OpenSubtitles, Wikipedia, Books, Tatoeba, TED talks, ParaCrawl and Gourmet (See Table TABREF31). UGC does not include Common Crawl (which contains many\n\nAnswer B:\n< Explanation: We use the French-English parallel corpus of Foursquare restaurant reviews (this https URL), and define a new task to encourage research on Neural Machine Translation robustness and domain adaptation, in a real-world scenario where better-quality MT would be greatly beneficial. We discuss the challenges of such user-generated content, and train good baseline models that build upon the latest techniques for MT robustness. We also perform an extensive evaluation (automatic and human) that shows significant improvements over existing online systems. Finally, we propose task-specific metrics based on sentiment analysis or translation accuracy of domain-specific polysemous words.\n\nWe use the French-\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.14589",
    "question": "what are the human evaluation metrics?",
    "gold_answer": "",
    "answer_a": "translation quality.. Explanation: Then we asked bilingual colleagues to rank the output of 4 models by their translation quality",
    "answer_b": "How are they evaluated?\n< Explanation: We evaluate the proposed metrics on the Foursquare dataset. We use the following metrics: (1) accuracy, (2) precision, (3) recall, (4) F1 score, (5) mean absolute error (MAE), (6) mean squared error (MSE), (7) mean absolute percentage error (MAPE), (8) mean absolute percentage point error (MAPPE), (9) mean absolute percentage point error (MAPPE), (10) mean absolute percentage point error (MAPPE), (11) mean absolute percentage point error (MAPPE",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the human evaluation metrics?\n\nGround Truth Answer:\n\n\nAnswer A:\ntranslation quality.. Explanation: Then we asked bilingual colleagues to rank the output of 4 models by their translation quality\n\nAnswer B:\nHow are they evaluated?\n< Explanation: We evaluate the proposed metrics on the Foursquare dataset. We use the following metrics: (1) accuracy, (2) precision, (3) recall, (4) F1 score, (5) mean absolute error (MAE), (6) mean squared error (MSE), (7) mean absolute percentage error (MAPE), (8) mean absolute percentage point error (MAPPE), (9) mean absolute percentage point error (MAPPE), (10) mean absolute percentage point error (MAPPE), (11) mean absolute percentage point error (MAPPE\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.14589",
    "question": "what automatic evaluation is performed?",
    "gold_answer": "",
    "answer_a": "BLEU BIBREF28. Explanation: In addition to BLEU, we do an indirect evaluation on an Aspect-Based Sentiment Analysis (ABSA) task, a human evaluation, and a task-related evaluation based on polysemous words.",
    "answer_b": "How is the accuracy of the metrics computed?\n< Explanation: We evaluate the performance of the baseline models on the Foursquare corpus using the following metrics: (1) accuracy of the translation, (2) accuracy of the sentiment analysis, and (3) accuracy of the domain-specific polysemous word translation. We also evaluate the performance of the baseline models on the Foursquare corpus using the following metrics: (1) accuracy of the translation, (2) accuracy of the sentiment analysis, and (3) accuracy of the domain-specific polysemous word translation. We also evaluate the performance of the baseline models on the Fours",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat automatic evaluation is performed?\n\nGround Truth Answer:\n\n\nAnswer A:\nBLEU BIBREF28. Explanation: In addition to BLEU, we do an indirect evaluation on an Aspect-Based Sentiment Analysis (ABSA) task, a human evaluation, and a task-related evaluation based on polysemous words.\n\nAnswer B:\nHow is the accuracy of the metrics computed?\n< Explanation: We evaluate the performance of the baseline models on the Foursquare corpus using the following metrics: (1) accuracy of the translation, (2) accuracy of the sentiment analysis, and (3) accuracy of the domain-specific polysemous word translation. We also evaluate the performance of the baseline models on the Foursquare corpus using the following metrics: (1) accuracy of the translation, (2) accuracy of the sentiment analysis, and (3) accuracy of the domain-specific polysemous word translation. We also evaluate the performance of the baseline models on the Fours\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.14589",
    "question": "what are the existing online systems?",
    "gold_answer": "",
    "answer_a": "How do they compare to the proposed metrics?\n< Explanation: We compare our proposed metrics with the following metrics: (1) MT robustness metrics: MT-robustness, MT-robustness-1, MT-robustness-2, MT-robustness-3, MT-robustness-4, MT-robustness-5, MT-robustness-6, MT-robustness-7, MT-robustness-8, MT-robustness-9, MT-robustness-10, MT-robustness-11, MT-",
    "answer_b": "Existing online systems compared in this work are Google Translate (Feb 2019) and DeepL (Feb 2019).. Explanation: As shown in Table TABREF54, these techniques can be combined to achieve the best results.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the existing online systems?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they compare to the proposed metrics?\n< Explanation: We compare our proposed metrics with the following metrics: (1) MT robustness metrics: MT-robustness, MT-robustness-1, MT-robustness-2, MT-robustness-3, MT-robustness-4, MT-robustness-5, MT-robustness-6, MT-robustness-7, MT-robustness-8, MT-robustness-9, MT-robustness-10, MT-robustness-11, MT-\n\nAnswer B:\nExisting online systems compared in this work are Google Translate (Feb 2019) and DeepL (Feb 2019).. Explanation: As shown in Table TABREF54, these techniques can be combined to achieve the best results.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1801.05617",
    "question": "What are their baselines?",
    "gold_answer": "",
    "answer_a": "an unoptimised linear-kernel SVM. Explanation: The optimised models are evaluated against two baseline systems: i) an unoptimised linear-kernel SVM (configured with default parameter settings) based on word INLINEFORM0 -grams only and, ii) a keyword-based system that marks posts as positive for cyberbullying if they contain a word from existing vocabulary lists composed by aggressive language and profanity terms.",
    "answer_b": "How do they compare?\n< Explanation: We compare our results with the following baselines: (1) a baseline using only keywords, (2) a baseline using only word unigrams, (3) a baseline using only word bigrams, (4) a baseline using only word trigrams, (5) a baseline using only word triplets, (6) a baseline using only word quadruples, (7) a baseline using only word quintuples, (8) a baseline using only word sextuples, (9) a baseline using only word septuples, (10) a baseline using only word octuples, (",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are their baselines?\n\nGround Truth Answer:\n\n\nAnswer A:\nan unoptimised linear-kernel SVM. Explanation: The optimised models are evaluated against two baseline systems: i) an unoptimised linear-kernel SVM (configured with default parameter settings) based on word INLINEFORM0 -grams only and, ii) a keyword-based system that marks posts as positive for cyberbullying if they contain a word from existing vocabulary lists composed by aggressive language and profanity terms.\n\nAnswer B:\nHow do they compare?\n< Explanation: We compare our results with the following baselines: (1) a baseline using only keywords, (2) a baseline using only word unigrams, (3) a baseline using only word bigrams, (4) a baseline using only word trigrams, (5) a baseline using only word triplets, (6) a baseline using only word quadruples, (7) a baseline using only word quintuples, (8) a baseline using only word sextuples, (9) a baseline using only word septuples, (10) a baseline using only word octuples, (\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1801.05617",
    "question": "How long is the test dataset for Dutch?",
    "gold_answer": "",
    "answer_a": "What is the size of the test dataset for English?\nFor the Dutch dataset, we collected 1000 posts from the social media platform Twitter. We selected 1000 posts from the Twitter timeline of a user who was identified as a bully. We selected 1000 posts from the Twitter timeline of a user who was identified as a victim. We selected 1000 posts from the Twitter timeline of a user who was identified as a bystander. We selected 1000 posts from the Twitter timeline of a user who was identified as a bystander. We selected 1000",
    "answer_b": "Random 10 percent out of 78381 posts.. Explanation: The English and Dutch corpus contain 113,698 and 78,387 posts, respectively.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long is the test dataset for Dutch?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the test dataset for English?\nFor the Dutch dataset, we collected 1000 posts from the social media platform Twitter. We selected 1000 posts from the Twitter timeline of a user who was identified as a bully. We selected 1000 posts from the Twitter timeline of a user who was identified as a victim. We selected 1000 posts from the Twitter timeline of a user who was identified as a bystander. We selected 1000 posts from the Twitter timeline of a user who was identified as a bystander. We selected 1000\n\nAnswer B:\nRandom 10 percent out of 78381 posts.. Explanation: The English and Dutch corpus contain 113,698 and 78,387 posts, respectively.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1801.05617",
    "question": "How long is the training dataset for English?",
    "gold_answer": "",
    "answer_a": "How long is the training dataset for Dutch?\nFor English, we collected 1000 posts from 1000 users. For Dutch, we collected 1000 posts from 1000 users. The training set was split into 80% for training and 20% for testing. The test set was used to evaluate the performance of the model. The test set was split into 80% for training and 20% for testing. The test set was used to evaluate the performance of the model. The test set was split into 80% for training and 2",
    "answer_b": "Random 90 percent out of 113698 posts.. Explanation: The English and Dutch corpus contain 113,698 and 78,387 posts, respectively.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long is the training dataset for English?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow long is the training dataset for Dutch?\nFor English, we collected 1000 posts from 1000 users. For Dutch, we collected 1000 posts from 1000 users. The training set was split into 80% for training and 20% for testing. The test set was used to evaluate the performance of the model. The test set was split into 80% for training and 20% for testing. The test set was used to evaluate the performance of the model. The test set was split into 80% for training and 2\n\nAnswer B:\nRandom 90 percent out of 113698 posts.. Explanation: The English and Dutch corpus contain 113,698 and 78,387 posts, respectively.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1801.05617",
    "question": "What features are used?",
    "gold_answer": "",
    "answer_a": "Word INLINEFORM0 -gram bag-of-words. Explanation: After pre-processing of the corpus, the following feature types were extracted:\n\nWord INLINEFORM0 -gram bag-of-words: binary features indicating the presence of word unigrams, bigrams and trigrams.\n\nCharacter INLINEFORM0 -gram bag-of-words: binary features indicating the presence of character bigrams, trigrams and fourgrams (without crossing word boundaries).",
    "answer_b": "How are they selected?\n< Explanation: We use the following features: 1) the number of words, 2) the number of characters, 3) the number of hashtags, 4) the number of mentions, 5) the number of URLs, 6) the number of URLs, 7) the number of URLs, 8) the number of URLs, 9) the number of URLs, 10) the number of URLs, 11) the number of URLs, 12) the number of URLs, 13) the number of URLs, 14) the number",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat features are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWord INLINEFORM0 -gram bag-of-words. Explanation: After pre-processing of the corpus, the following feature types were extracted:\n\nWord INLINEFORM0 -gram bag-of-words: binary features indicating the presence of word unigrams, bigrams and trigrams.\n\nCharacter INLINEFORM0 -gram bag-of-words: binary features indicating the presence of character bigrams, trigrams and fourgrams (without crossing word boundaries).\n\nAnswer B:\nHow are they selected?\n< Explanation: We use the following features: 1) the number of words, 2) the number of characters, 3) the number of hashtags, 4) the number of mentions, 5) the number of URLs, 6) the number of URLs, 7) the number of URLs, 8) the number of URLs, 9) the number of URLs, 10) the number of URLs, 11) the number of URLs, 12) the number of URLs, 13) the number of URLs, 14) the number\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1801.05617",
    "question": "What is the source of the data?",
    "gold_answer": "",
    "answer_a": "How is it collected?\n< Explanation: The data was collected from the Twitter API. The Twitter API is a service that provides access to the Twitter API, which is a set of APIs that allow developers to access Twitter\u2019s data. The Twitter API is a service that provides access to the Twitter API, which is a set of APIs that allow developers to access Twitter\u2019s data. The Twitter API is a service that provides access to the Twitter API, which is a set of APIs that allow developers to access Twitter\u2019s data. The Twitter API is a service that provides access to the Twitter API, which is a set of APIs that allow developers to",
    "answer_b": "social networking site ASKfm. Explanation: Two corpora were constructed by collecting data from the social networking site ASKfm, where users can create profiles and ask or answer questions, with the option of doing so anonymously.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the source of the data?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it collected?\n< Explanation: The data was collected from the Twitter API. The Twitter API is a service that provides access to the Twitter API, which is a set of APIs that allow developers to access Twitter\u2019s data. The Twitter API is a service that provides access to the Twitter API, which is a set of APIs that allow developers to access Twitter\u2019s data. The Twitter API is a service that provides access to the Twitter API, which is a set of APIs that allow developers to access Twitter\u2019s data. The Twitter API is a service that provides access to the Twitter API, which is a set of APIs that allow developers to\n\nAnswer B:\nsocial networking site ASKfm. Explanation: Two corpora were constructed by collecting data from the social networking site ASKfm, where users can create profiles and ask or answer questions, with the option of doing so anonymously.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.08067",
    "question": "What languages feature in the dataset?",
    "gold_answer": "",
    "answer_a": "English. Explanation: In this study, we focus mainly on the ISIS group as they are one of the leading terrorist groups that utilise social media to share their propaganda and recruit individuals. We analyze the propaganda material they publish in their online English magazine called Dabiq, and use data-mining techniques to computationally uncover contextual text and psychological properties associated with these groups.",
    "answer_b": "How many?\n< Explanation: The dataset contains 10,000 tweets from 1000 users. The dataset is divided into two parts: training and testing. The training set contains 7,000 tweets and the testing set contains 3,000 tweets. The dataset is available at https://github.com/abdelrahman-ahmed/Online-Radicalization-Data-Set.\n\nThe dataset contains 10,000 tweets from 1000 users. The dataset is divided into two parts: training and testing. The training set contains 7,0",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat languages feature in the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nEnglish. Explanation: In this study, we focus mainly on the ISIS group as they are one of the leading terrorist groups that utilise social media to share their propaganda and recruit individuals. We analyze the propaganda material they publish in their online English magazine called Dabiq, and use data-mining techniques to computationally uncover contextual text and psychological properties associated with these groups.\n\nAnswer B:\nHow many?\n< Explanation: The dataset contains 10,000 tweets from 1000 users. The dataset is divided into two parts: training and testing. The training set contains 7,000 tweets and the testing set contains 3,000 tweets. The dataset is available at https://github.com/abdelrahman-ahmed/Online-Radicalization-Data-Set.\n\nThe dataset contains 10,000 tweets from 1000 users. The dataset is divided into two parts: training and testing. The training set contains 7,0\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.08067",
    "question": "What textual, psychological and behavioural patterns are observed in radical users?",
    "gold_answer": "",
    "answer_a": "How do they differ from non-radical users?\n<blockquote>Textual patterns: The most common patterns are the use of emoticons, the use of the word \u201cterrorist\u201d and the use of the word \u201cterrorist\u201d in the context of a tweet. The use of emoticons is the most common pattern, with 60% of the tweets containing emoticons. The use of the word \u201cterrorist\u201d is the second most common pattern, with 20% of the tweets containing the word \u201cterrorist\u201d. The use of the word \u201cterrorist\u201d in the context of a tweet is the third most common",
    "answer_b": "They use a lot of \"us\" and \"them\" in their vocabulary. They use a lot of mentions, and they tend to be \"central\" in their network. They use a lot of violent words.. Explanation: We investigated which features contribute most to the classification task to distinguish between radical and non-radical tweets. We used the mean decrease impurity method of random forests BIBREF27 to identify the most important features in each feature category. The ten most important features are shown in Table TABREF22 . We found that the most important feature for distinguishing radical tweets is the psychological feature distance measure. This measures how similar the Twitter user is to the average psychological profile calculated from the propaganda magazine articles. Following this is the Us-them dichotomy which looks at the total number of pronouns used (I,they, we, you). This finding is in line with the tactics reported in the radicalization literature with regards to emphasizing the separation between the radical group and the world.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat textual, psychological and behavioural patterns are observed in radical users?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they differ from non-radical users?\n<blockquote>Textual patterns: The most common patterns are the use of emoticons, the use of the word \u201cterrorist\u201d and the use of the word \u201cterrorist\u201d in the context of a tweet. The use of emoticons is the most common pattern, with 60% of the tweets containing emoticons. The use of the word \u201cterrorist\u201d is the second most common pattern, with 20% of the tweets containing the word \u201cterrorist\u201d. The use of the word \u201cterrorist\u201d in the context of a tweet is the third most common\n\nAnswer B:\nThey use a lot of \"us\" and \"them\" in their vocabulary. They use a lot of mentions, and they tend to be \"central\" in their network. They use a lot of violent words.. Explanation: We investigated which features contribute most to the classification task to distinguish between radical and non-radical tweets. We used the mean decrease impurity method of random forests BIBREF27 to identify the most important features in each feature category. The ten most important features are shown in Table TABREF22 . We found that the most important feature for distinguishing radical tweets is the psychological feature distance measure. This measures how similar the Twitter user is to the average psychological profile calculated from the propaganda magazine articles. Following this is the Us-them dichotomy which looks at the total number of pronouns used (I,they, we, you). This finding is in line with the tactics reported in the radicalization literature with regards to emphasizing the separation between the radical group and the world.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.08067",
    "question": "Where is the propaganda material sourced from?",
    "gold_answer": "",
    "answer_a": "How is it distributed?\n< Explanation: The propaganda material is sourced from extremist groups. The content is distributed through social media platforms. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is",
    "answer_b": "online English magazine called Dabiq. Explanation: In this study, we focus mainly on the ISIS group as they are one of the leading terrorist groups that utilise social media to share their propaganda and recruit individuals. We analyze the propaganda material they publish in their online English magazine called Dabiq, and use data-mining techniques to computationally uncover contextual text and psychological properties associated with these groups.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhere is the propaganda material sourced from?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it distributed?\n< Explanation: The propaganda material is sourced from extremist groups. The content is distributed through social media platforms. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is also distributed through the Internet. The content is\n\nAnswer B:\nonline English magazine called Dabiq. Explanation: In this study, we focus mainly on the ISIS group as they are one of the leading terrorist groups that utilise social media to share their propaganda and recruit individuals. We analyze the propaganda material they publish in their online English magazine called Dabiq, and use data-mining techniques to computationally uncover contextual text and psychological properties associated with these groups.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.08067",
    "question": "Which behavioural features are used?",
    "gold_answer": "",
    "answer_a": "frequency of tweets posted. Explanation: This category consists of measuring behavioural features to capture different properties related to the user and their behaviour. This includes how active the user is (frequency of tweets posted) and the followers/following ratio. Additionally, we use features to capture users' interactions with others through using hashtags, and engagement in discussions using mention action. To capture this, we construct the mention interaction graph ( INLINEFORM0 ) from our dataset, such that INLINEFORM1 = INLINEFORM2 , where INLINEFORM3 represents the user nodes and INLINEFORM4 represents the set of edges. The graph INLINEFORM5 is a directed graph, where an edge INLINEFORM6 exists between two user nodes INLINEFORM7 and INLINEFORM8 , if user INLINEFORM9 mentions user INLINEFORM10 . After constructing the graph, we measure the degree of influence each user has over their network using different centrality measures, such as degree centrality, betweenness centrality, and HITS-Hub.",
    "answer_b": "What are the results of the experiments?\n< Explanation: We use the following features: (1) the number of tweets, (2) the number of hashtags, (3) the number of mentions, (4) the number of replies, (5) the number of likes, (6) the number of retweets, (7) the number of shares, (8) the number of mentions of the user, (9) the number of mentions of the group, (10) the number of mentions of the group and the user, (11) the number of mentions of the group and the user, (1",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich behavioural features are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nfrequency of tweets posted. Explanation: This category consists of measuring behavioural features to capture different properties related to the user and their behaviour. This includes how active the user is (frequency of tweets posted) and the followers/following ratio. Additionally, we use features to capture users' interactions with others through using hashtags, and engagement in discussions using mention action. To capture this, we construct the mention interaction graph ( INLINEFORM0 ) from our dataset, such that INLINEFORM1 = INLINEFORM2 , where INLINEFORM3 represents the user nodes and INLINEFORM4 represents the set of edges. The graph INLINEFORM5 is a directed graph, where an edge INLINEFORM6 exists between two user nodes INLINEFORM7 and INLINEFORM8 , if user INLINEFORM9 mentions user INLINEFORM10 . After constructing the graph, we measure the degree of influence each user has over their network using different centrality measures, such as degree centrality, betweenness centrality, and HITS-Hub.\n\nAnswer B:\nWhat are the results of the experiments?\n< Explanation: We use the following features: (1) the number of tweets, (2) the number of hashtags, (3) the number of mentions, (4) the number of replies, (5) the number of likes, (6) the number of retweets, (7) the number of shares, (8) the number of mentions of the user, (9) the number of mentions of the group, (10) the number of mentions of the group and the user, (11) the number of mentions of the group and the user, (1\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.08067",
    "question": "Which psychological features are used?",
    "gold_answer": "",
    "answer_a": "What are the results of the experiments?\n< Explanation: We use the following psychological features: (1) the number of tweets containing the word \u201cterrorist\u201d or \u201cextremist\u201d (number of tweets containing the word \u201cterrorist\u201d or \u201cextremist\u201d); (2) the number of tweets containing the word \u201cterrorist\u201d or \u201cextremist\u201d and the number of tweets containing the word \u201cterrorist\u201d or \u201cextremist\u201d and the number of tweets containing the word \u201cterrorist\u201d or \u201cextremist\u201d and the number of tweets containing the word \u201cterrorist\u201d or \u201cextrem",
    "answer_b": "Analytically thinking. Explanation: We utilise LIWC dictionaries to assign a score to a set of psychological, personality, and emotional categories. Mainly, we look at the following properties: (1) Summary variables: Analytically thinking which reflects formal, logical, and hierarchical thinking (high value), versus informal, personal, and narrative thinking (low value). Clout which reflects high expertise and confidence levels (high value), versus tentative, humble, and anxious levels (low value). Tone which reflects positive emotions (high value) versus more negative emotions such as anxiety, sadness, or anger (low value). Authentic which reflects whether the text is conveying honesty and disclosing (high value) versus more guarded, and distanced (low value). (2) Big five: Measures the five psychological properties (OCEAN), namely Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. (3) Emotional Analysis: Measures the positive emotions conveyed in the text, and the negative emotions (including anger, sadness, anxiety). (4) Personal Drives: Focuses on five personal drives, namely power, reward, risk, achievement, and affiliation. (5) Personal Pronouns: Counts the number of 1st, 2nd, and 3rd personal pronouns used. For each Twitter user, we calculate their psychological profiles across these categories. Additionally, using Minkowski distance measure, we calculate the distance between each of these profiles and the average values of the psychological properties created from the ISIS magazines.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich psychological features are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results of the experiments?\n< Explanation: We use the following psychological features: (1) the number of tweets containing the word \u201cterrorist\u201d or \u201cextremist\u201d (number of tweets containing the word \u201cterrorist\u201d or \u201cextremist\u201d); (2) the number of tweets containing the word \u201cterrorist\u201d or \u201cextremist\u201d and the number of tweets containing the word \u201cterrorist\u201d or \u201cextremist\u201d and the number of tweets containing the word \u201cterrorist\u201d or \u201cextremist\u201d and the number of tweets containing the word \u201cterrorist\u201d or \u201cextrem\n\nAnswer B:\nAnalytically thinking. Explanation: We utilise LIWC dictionaries to assign a score to a set of psychological, personality, and emotional categories. Mainly, we look at the following properties: (1) Summary variables: Analytically thinking which reflects formal, logical, and hierarchical thinking (high value), versus informal, personal, and narrative thinking (low value). Clout which reflects high expertise and confidence levels (high value), versus tentative, humble, and anxious levels (low value). Tone which reflects positive emotions (high value) versus more negative emotions such as anxiety, sadness, or anger (low value). Authentic which reflects whether the text is conveying honesty and disclosing (high value) versus more guarded, and distanced (low value). (2) Big five: Measures the five psychological properties (OCEAN), namely Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. (3) Emotional Analysis: Measures the positive emotions conveyed in the text, and the negative emotions (including anger, sadness, anxiety). (4) Personal Drives: Focuses on five personal drives, namely power, reward, risk, achievement, and affiliation. (5) Personal Pronouns: Counts the number of 1st, 2nd, and 3rd personal pronouns used. For each Twitter user, we calculate their psychological profiles across these categories. Additionally, using Minkowski distance measure, we calculate the distance between each of these profiles and the average values of the psychological properties created from the ISIS magazines.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.08067",
    "question": "Which textual features are used?",
    "gold_answer": "",
    "answer_a": "N-grams. Explanation: We use two methods to extract the radical language from the propaganda corpus. First we calculate tf-idf scores for each gram in the propaganda corpus. We use uni-grams, bi-grams, and tri-grams to capture phrases and context in which words are being used. We then select the top scoring grams to be used as features for the language model. N-grams and words frequency have been used in the literature to classify similar problems, such as hate-speech and extremist text and have proven successful BIBREF16",
    "answer_b": "What are the results of the experiments?\n< Explanation: We use the following features: (1) the number of words, (2) the number of hashtags, (3) the number of mentions, (4) the number of emojis, (5) the number of URLs, (6) the number of URLs, (7) the number of hashtags, (8) the number of mentions, (9) the number of emojis, (10) the number of URLs, (11) the number of hashtags, (12) the number of mentions, (13) the number of emojis, (14",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich textual features are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nN-grams. Explanation: We use two methods to extract the radical language from the propaganda corpus. First we calculate tf-idf scores for each gram in the propaganda corpus. We use uni-grams, bi-grams, and tri-grams to capture phrases and context in which words are being used. We then select the top scoring grams to be used as features for the language model. N-grams and words frequency have been used in the literature to classify similar problems, such as hate-speech and extremist text and have proven successful BIBREF16\n\nAnswer B:\nWhat are the results of the experiments?\n< Explanation: We use the following features: (1) the number of words, (2) the number of hashtags, (3) the number of mentions, (4) the number of emojis, (5) the number of URLs, (6) the number of URLs, (7) the number of hashtags, (8) the number of mentions, (9) the number of emojis, (10) the number of URLs, (11) the number of hashtags, (12) the number of mentions, (13) the number of emojis, (14\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.03943",
    "question": "what is the cold-start problem?",
    "gold_answer": "",
    "answer_a": "How does the proposed model address it?\n< Explanation: The cold-start problem is a common problem in machine learning. It occurs when a model is trained on a small amount of data and then applied to a new data set. The model is unable to learn the underlying patterns in the new data set. This is because the model is trained on a small amount of data and the new data set is much larger. The model is unable to learn the underlying patterns in the new data set. This is because the model is trained on a small amount of data and the new data set is much larger. The model is unable to learn the underlying patterns",
    "answer_b": "Dealing with hotels/items  that  appear infrequently or never in historical data and choosing appropriate weights for them is referred to as the \"cold start problem.\". Explanation: In practice, many hotels/items appear infrequently or never in historical data. Recommender systems typically have difficulty handling these items effectively due to the lack of relevant training data. Apart from the obvious negative impacts on searchability and sales, neglecting these items can introduce a feedback loop. That is, the less these items are recommended, or the more they are recommended in inappropriate circumstances, the more the data reinforces their apparent lack of popularity.\n\nDealing with such hotels/items and choosing appropriate weights for them is referred to as the \"cold start problem.\"",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the cold-start problem?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow does the proposed model address it?\n< Explanation: The cold-start problem is a common problem in machine learning. It occurs when a model is trained on a small amount of data and then applied to a new data set. The model is unable to learn the underlying patterns in the new data set. This is because the model is trained on a small amount of data and the new data set is much larger. The model is unable to learn the underlying patterns in the new data set. This is because the model is trained on a small amount of data and the new data set is much larger. The model is unable to learn the underlying patterns\n\nAnswer B:\nDealing with hotels/items  that  appear infrequently or never in historical data and choosing appropriate weights for them is referred to as the \"cold start problem.\". Explanation: In practice, many hotels/items appear infrequently or never in historical data. Recommender systems typically have difficulty handling these items effectively due to the lack of relevant training data. Apart from the obvious negative impacts on searchability and sales, neglecting these items can introduce a feedback loop. That is, the less these items are recommended, or the more they are recommended in inappropriate circumstances, the more the data reinforces their apparent lack of popularity.\n\nDealing with such hotels/items and choosing appropriate weights for them is referred to as the \"cold start problem.\"\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.03943",
    "question": "how was the experiment evaluated?",
    "gold_answer": "",
    "answer_a": "the average number of times the correct selection appears in the top k predictions. Explanation: In this section, we compare our model based on the hits@k metric in various scenarios. Hits@k measures the average number of times the correct selection appears in the top k predictions.",
    "answer_b": "What metrics were used?\n< Explanation: We evaluate our model using the following metrics: Recall, Precision, F1, and Mean Average Precision (MAP). Recall is the proportion of correct predictions to the total number of predictions. Precision is the proportion of correct predictions to the total number of true positives. F1 is the harmonic mean of precision and recall. MAP is the weighted average of precision and recall, where the weights are the number of true positives, true negatives, and false positives, respectively. We use the following weights: 0.2 for true positives, 0.3 for true negatives, and 0.5 for",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow was the experiment evaluated?\n\nGround Truth Answer:\n\n\nAnswer A:\nthe average number of times the correct selection appears in the top k predictions. Explanation: In this section, we compare our model based on the hits@k metric in various scenarios. Hits@k measures the average number of times the correct selection appears in the top k predictions.\n\nAnswer B:\nWhat metrics were used?\n< Explanation: We evaluate our model using the following metrics: Recall, Precision, F1, and Mean Average Precision (MAP). Recall is the proportion of correct predictions to the total number of predictions. Precision is the proportion of correct predictions to the total number of true positives. F1 is the harmonic mean of precision and recall. MAP is the weighted average of precision and recall, where the weights are the number of true positives, true negatives, and false positives, respectively. We use the following weights: 0.2 for true positives, 0.3 for true negatives, and 0.5 for\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.03943",
    "question": "what other applications did they experiment in?",
    "gold_answer": "",
    "answer_a": "None. Explanation: Although we mainly focus on learning embeddings for hotels, the same framework can be applied to general item embedding, such as product embedding on Amazon, Ebay, or Spotify.",
    "answer_b": "(optional)\n< Explanation: We also experiment with the hotel recommendation system to see how well the learned embeddings perform in other applications. We train a recommendation system on the hotel click data and use the learned embeddings to make recommendations. We show that the embeddings can be used to make recommendations for hotels that are not in the training set. We also show that the embeddings can be used to make recommendations for hotels that are in the training set but have low ratings. We show that the embeddings can be used to make recommendations for hotels that are in the training set but have high ratings. We show that the embeddings can be used to make recommendations for hotels",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat other applications did they experiment in?\n\nGround Truth Answer:\n\n\nAnswer A:\nNone. Explanation: Although we mainly focus on learning embeddings for hotels, the same framework can be applied to general item embedding, such as product embedding on Amazon, Ebay, or Spotify.\n\nAnswer B:\n(optional)\n< Explanation: We also experiment with the hotel recommendation system to see how well the learned embeddings perform in other applications. We train a recommendation system on the hotel click data and use the learned embeddings to make recommendations. We show that the embeddings can be used to make recommendations for hotels that are not in the training set. We also show that the embeddings can be used to make recommendations for hotels that are in the training set but have low ratings. We show that the embeddings can be used to make recommendations for hotels that are in the training set but have high ratings. We show that the embeddings can be used to make recommendations for hotels\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.03943",
    "question": "what dataset was used for training?",
    "gold_answer": "",
    "answer_a": "Our dataset contains more than 40M user click sessions. Explanation: Our dataset contains more than 40M user click sessions, which includes more than 1.1 million unique hotels. A click session is defined as a span of clicks performed by a user with no gap of more than 7 days. We randomly split the sessions into training, validation, and test with a ratio of 8:1:1.",
    "answer_b": "How many users were used?\n< Explanation: We use the dataset of 40 million user clicks from a leading online travel platform (TripAdvisor) to train our model. We use 10000 users for training and 30000 users for testing. We use the following attributes for each user: user ID, gender, age, city, country, and the number of clicks. We use the following attributes for each hotel: hotel ID, name, address, city, country, and the number of clicks. We use the following attributes for each amenity: amenity ID, name, address, city",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat dataset was used for training?\n\nGround Truth Answer:\n\n\nAnswer A:\nOur dataset contains more than 40M user click sessions. Explanation: Our dataset contains more than 40M user click sessions, which includes more than 1.1 million unique hotels. A click session is defined as a span of clicks performed by a user with no gap of more than 7 days. We randomly split the sessions into training, validation, and test with a ratio of 8:1:1.\n\nAnswer B:\nHow many users were used?\n< Explanation: We use the dataset of 40 million user clicks from a leading online travel platform (TripAdvisor) to train our model. We use 10000 users for training and 30000 users for testing. We use the following attributes for each user: user ID, gender, age, city, country, and the number of clicks. We use the following attributes for each hotel: hotel ID, name, address, city, country, and the number of clicks. We use the following attributes for each amenity: amenity ID, name, address, city\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12722",
    "question": "What were the results of their experiment?",
    "gold_answer": "",
    "answer_a": ".41, .31, and .31 Proportional $\\text{F}_1$ on Holders, Targets, and Polarity Expressions, respectively. Explanation: Casting the problem as a sequence labeling task, we train a model to jointly predict holders, targets and polar expressions. Below, we first describe the evaluation metrics and the experimental setup, before finally discussing the results.",
    "answer_b": "Is it a benchmark?\nYes, we report the first experimental results on the dataset, intended as a preliminary benchmark for further experiments. We report the results of the first experiment on the dataset, which is the first time that the dataset has been used for sentiment analysis. We report the results of the first experiment on the dataset, which is the first time that the dataset has been used for sentiment analysis. We report the results of the first experiment on the dataset, which is the first time that the dataset has been used for sentiment analysis. We report the results of the first experiment on the dataset, which is the first time that the dataset has",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat were the results of their experiment?\n\nGround Truth Answer:\n\n\nAnswer A:\n.41, .31, and .31 Proportional $\\text{F}_1$ on Holders, Targets, and Polarity Expressions, respectively. Explanation: Casting the problem as a sequence labeling task, we train a model to jointly predict holders, targets and polar expressions. Below, we first describe the evaluation metrics and the experimental setup, before finally discussing the results.\n\nAnswer B:\nIs it a benchmark?\nYes, we report the first experimental results on the dataset, intended as a preliminary benchmark for further experiments. We report the results of the first experiment on the dataset, which is the first time that the dataset has been used for sentiment analysis. We report the results of the first experiment on the dataset, which is the first time that the dataset has been used for sentiment analysis. We report the results of the first experiment on the dataset, which is the first time that the dataset has been used for sentiment analysis. We report the results of the first experiment on the dataset, which is the first time that the dataset has\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12722",
    "question": "How big is the dataset?",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n< Explanation: The dataset contains 10,000 reviews, each of which is annotated with a polar expression, a target and a holder of opinion. The dataset is divided into 1000 reviews, each of which is annotated with a polar expression, a target and a holder of opinion. The dataset is available at https://github.com/NoReC/NoReC_fine.\n\nThe dataset contains 10,000 reviews, each of which is annotated with a polar expression, a target and a holder of opinion. The dataset is divided into 1",
    "answer_b": "7451 sentences. Explanation: In this work, we describe the annotation of a fine-grained sentiment dataset for Norwegian, NoReC$_\\text{\\textit {fine}}$, the first such dataset available in Norwegian.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big is the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n< Explanation: The dataset contains 10,000 reviews, each of which is annotated with a polar expression, a target and a holder of opinion. The dataset is divided into 1000 reviews, each of which is annotated with a polar expression, a target and a holder of opinion. The dataset is available at https://github.com/NoReC/NoReC_fine.\n\nThe dataset contains 10,000 reviews, each of which is annotated with a polar expression, a target and a holder of opinion. The dataset is divided into 1\n\nAnswer B:\n7451 sentences. Explanation: In this work, we describe the annotation of a fine-grained sentiment dataset for Norwegian, NoReC$_\\text{\\textit {fine}}$, the first such dataset available in Norwegian.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12722",
    "question": "What are all the domains the corpus came from?",
    "gold_answer": "",
    "answer_a": "How many domains are there?\n< Explanation: The corpus contains 10 domains, each with 1000 reviews. The domains are: literature, games, music, products, movies, sports, travel, travel, travel, travel. The corpus is available at https://github.com/NoReC/NoReC_fine. The domain names are: literature, games, music, products, movies, sports, travel, travel, travel, travel. The domain names are in the following order: literature, games, music, products, movies, sports, travel, travel, travel, travel. The domain names are",
    "answer_b": "a wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants, etc.. Explanation: The underlying texts are taken from the Norwegian Review Corpus (NoReC) BIBREF0 \u2013 a corpus of professionally authored reviews from multiple news-sources and across a wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants, etc.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are all the domains the corpus came from?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow many domains are there?\n< Explanation: The corpus contains 10 domains, each with 1000 reviews. The domains are: literature, games, music, products, movies, sports, travel, travel, travel, travel. The corpus is available at https://github.com/NoReC/NoReC_fine. The domain names are: literature, games, music, products, movies, sports, travel, travel, travel, travel. The domain names are in the following order: literature, games, music, products, movies, sports, travel, travel, travel, travel. The domain names are\n\nAnswer B:\na wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants, etc.. Explanation: The underlying texts are taken from the Norwegian Review Corpus (NoReC) BIBREF0 \u2013 a corpus of professionally authored reviews from multiple news-sources and across a wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants, etc.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.00786",
    "question": "How big is benefit in experiments of this editing approach compared to generating entire SQL from scratch?",
    "gold_answer": "",
    "answer_a": "improvement of 7% question match accuracy and 11% interaction match accuracy. Explanation: Experiment results show that by generating from the previous query, our model delivers an improvement of 7% question match accuracy and 11% interaction match accuracy over the previous state-of-the-art.",
    "answer_b": "What is the difference between the two approaches?\n< Explanation: We evaluate our approach on the SParC dataset and compare it with the state-of-the-art baselines. The baselines are generated by the following steps: (1) the user utterance is fed into a SQL generator, (2) the SQL generator generates a SQL query, (3) the user is asked to edit the generated query, and (4) the edited query is fed into the SQL generator again. The baselines are evaluated on the following four tasks: (1) the task of generating a SQL query from scratch, (2) the task of generating",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big is benefit in experiments of this editing approach compared to generating entire SQL from scratch?\n\nGround Truth Answer:\n\n\nAnswer A:\nimprovement of 7% question match accuracy and 11% interaction match accuracy. Explanation: Experiment results show that by generating from the previous query, our model delivers an improvement of 7% question match accuracy and 11% interaction match accuracy over the previous state-of-the-art.\n\nAnswer B:\nWhat is the difference between the two approaches?\n< Explanation: We evaluate our approach on the SParC dataset and compare it with the state-of-the-art baselines. The baselines are generated by the following steps: (1) the user utterance is fed into a SQL generator, (2) the SQL generator generates a SQL query, (3) the user is asked to edit the generated query, and (4) the edited query is fed into the SQL generator again. The baselines are evaluated on the following four tasks: (1) the task of generating a SQL query from scratch, (2) the task of generating\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.00786",
    "question": "What are state-of-the-art baselines?",
    "gold_answer": "",
    "answer_a": "guo2019towards who achieve state-of-the-art performance. Explanation: Spider. We compare with the results as reported in yu2018syntaxsqlnet. Furthermore, we also include recent results from lee2019recursive who propose to use recursive decoding procedure, bogin2019representing introducing graph neural networks for encoding schemas, and guo2019towards who achieve state-of-the-art performance by using an intermediate representation to bridge natural language questions and SQL queries.",
    "answer_b": "What is the difference between their approach and ours?\n< Explanation: Table 1: Comparison of baselines and our approach. The first column shows the number of tokens in the generated SQL. The second column shows the number of tokens in the generated SQL after editing. The third column shows the number of tokens in the generated SQL after editing and the number of tokens in the generated SQL after editing and the number of tokens in the generated SQL after editing and the number of tokens in the generated SQL after editing and the number of tokens in the generated SQL after editing and the number of tokens in the generated SQL after editing and the number of tokens in",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are state-of-the-art baselines?\n\nGround Truth Answer:\n\n\nAnswer A:\nguo2019towards who achieve state-of-the-art performance. Explanation: Spider. We compare with the results as reported in yu2018syntaxsqlnet. Furthermore, we also include recent results from lee2019recursive who propose to use recursive decoding procedure, bogin2019representing introducing graph neural networks for encoding schemas, and guo2019towards who achieve state-of-the-art performance by using an intermediate representation to bridge natural language questions and SQL queries.\n\nAnswer B:\nWhat is the difference between their approach and ours?\n< Explanation: Table 1: Comparison of baselines and our approach. The first column shows the number of tokens in the generated SQL. The second column shows the number of tokens in the generated SQL after editing. The third column shows the number of tokens in the generated SQL after editing and the number of tokens in the generated SQL after editing and the number of tokens in the generated SQL after editing and the number of tokens in the generated SQL after editing and the number of tokens in the generated SQL after editing and the number of tokens in the generated SQL after editing and the number of tokens in\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.03087",
    "question": "Which dialogue data do they use to evaluate on?",
    "gold_answer": "",
    "answer_a": "< Explanation: We use the following data sets: (1) the 2017 Stanford Dialogue Challenge (SDC) dataset, which contains 1000 utterances from 1000 users, and (2) the 2018 Stanford Dialogue Challenge (SDC-2018) dataset, which contains 1000 utterances from 1000 users. We use the SDC-2018 dataset for our experiments, and the SDC dataset for our self-play experiments. We also use the 2018 SDC-201",
    "answer_b": "Datasets from PersonaChat and Wizard of Wikipedia tasks.. Explanation: We perform experiments on two tasks, PersonaChat and Wizard of Wikipedia, which evaluate different aspects of conversational ability.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dialogue data do they use to evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\n< Explanation: We use the following data sets: (1) the 2017 Stanford Dialogue Challenge (SDC) dataset, which contains 1000 utterances from 1000 users, and (2) the 2018 Stanford Dialogue Challenge (SDC-2018) dataset, which contains 1000 utterances from 1000 users. We use the SDC-2018 dataset for our experiments, and the SDC dataset for our self-play experiments. We also use the 2018 SDC-201\n\nAnswer B:\nDatasets from PersonaChat and Wizard of Wikipedia tasks.. Explanation: We perform experiments on two tasks, PersonaChat and Wizard of Wikipedia, which evaluate different aspects of conversational ability.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.03087",
    "question": "How much faster are pairwise annotations than other annotations?",
    "gold_answer": "",
    "answer_a": "by 5 times. Explanation: FLOAT SELECTED: Figure 5: Relative cost effectiveness of potential collection methods: Likert and ACUTE-EVAL human-model chat and self-chat pairwise tests. Our methods obtain statistical significance with fewer person hours; Likert fails in this case.",
    "answer_b": "What is the difference in time between the two?\n< Explanation: We find that the time difference between the two methods is 10-15 minutes. We also find that the pairwise method is 10-15 times faster than the multi-turn method. This is because the multi-turn method requires a human judge to evaluate the entire dialogue, whereas the pairwise method only requires a human judge to evaluate one speaker. We also find that the pairwise method is 10-15 times faster than the multi-turn method. This is because the multi-turn method requires a human judge to evaluate the entire dialogue, whereas the pairwise method",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much faster are pairwise annotations than other annotations?\n\nGround Truth Answer:\n\n\nAnswer A:\nby 5 times. Explanation: FLOAT SELECTED: Figure 5: Relative cost effectiveness of potential collection methods: Likert and ACUTE-EVAL human-model chat and self-chat pairwise tests. Our methods obtain statistical significance with fewer person hours; Likert fails in this case.\n\nAnswer B:\nWhat is the difference in time between the two?\n< Explanation: We find that the time difference between the two methods is 10-15 minutes. We also find that the pairwise method is 10-15 times faster than the multi-turn method. This is because the multi-turn method requires a human judge to evaluate the entire dialogue, whereas the pairwise method only requires a human judge to evaluate one speaker. We also find that the pairwise method is 10-15 times faster than the multi-turn method. This is because the multi-turn method requires a human judge to evaluate the entire dialogue, whereas the pairwise method\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02105",
    "question": "How much improvement is there in the BLEU score?",
    "gold_answer": "",
    "answer_a": "What is the baseline?\n< Explanation: We compare our methods to the baseline of the original approach of BIBREF10, which uses a pre-trained language model to cluster sentences into domains. We also compare our methods to the baseline of the original approach of BIBREF10, which uses a pre-trained language model to cluster sentences into domains. We also compare our methods to the baseline of the original approach of BIBREF10, which uses a pre-trained language model to cluster sentences into domains. We also compare our methods to the baseline of the original approach of BIBREF10, which uses a pre-trained language",
    "answer_b": "Average SacreBLEU score accross all domains is improved from 40.88 to 41.26.. Explanation: FLOAT SELECTED: Table 6: SacreBLEU scores for the data selection experiments. Highest scores per column are marked in bold.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much improvement is there in the BLEU score?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the baseline?\n< Explanation: We compare our methods to the baseline of the original approach of BIBREF10, which uses a pre-trained language model to cluster sentences into domains. We also compare our methods to the baseline of the original approach of BIBREF10, which uses a pre-trained language model to cluster sentences into domains. We also compare our methods to the baseline of the original approach of BIBREF10, which uses a pre-trained language model to cluster sentences into domains. We also compare our methods to the baseline of the original approach of BIBREF10, which uses a pre-trained language\n\nAnswer B:\nAverage SacreBLEU score accross all domains is improved from 40.88 to 41.26.. Explanation: FLOAT SELECTED: Table 6: SacreBLEU scores for the data selection experiments. Highest scores per column are marked in bold.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02105",
    "question": "What is the established approach used for comparison?",
    "gold_answer": "",
    "answer_a": "What is the difference between the two?\n< Explanation: We compare our approach with the following two approaches: (1) a baseline approach that uses a pre-trained language model to cluster sentences by domain, and (2) a baseline approach that uses a pre-trained language model to cluster sentences by domain and then selects the most relevant sentences for translation. We also compare our approach with the following two approaches: (3) a baseline approach that uses a pre-trained language model to cluster sentences by domain and then selects the most relevant sentences for translation, and (4) a baseline approach that uses a pre-trained language model to cluster sentences by domain and",
    "answer_b": "method of BIBREF4. Explanation: Our data selection methods enable to train NMT models that outperform those trained using the well-established cross-entropy difference method of BIBREF4 across five diverse domains, achieving a recall of more than 95% in all cases with respect to an oracle that selects the \u201ctrue\u201d in-domain data.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the established approach used for comparison?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the two?\n< Explanation: We compare our approach with the following two approaches: (1) a baseline approach that uses a pre-trained language model to cluster sentences by domain, and (2) a baseline approach that uses a pre-trained language model to cluster sentences by domain and then selects the most relevant sentences for translation. We also compare our approach with the following two approaches: (3) a baseline approach that uses a pre-trained language model to cluster sentences by domain and then selects the most relevant sentences for translation, and (4) a baseline approach that uses a pre-trained language model to cluster sentences by domain and\n\nAnswer B:\nmethod of BIBREF4. Explanation: Our data selection methods enable to train NMT models that outperform those trained using the well-established cross-entropy difference method of BIBREF4 across five diverse domains, achieving a recall of more than 95% in all cases with respect to an oracle that selects the \u201ctrue\u201d in-domain data.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02105",
    "question": "What are the five domains?",
    "gold_answer": "",
    "answer_a": "subtitles, medical text (PDF documents from the European Medicines Agency), legal text (legislative text of the European Union), translations of the Koran, and IT-related text (manuals and localization files of open-source software). Explanation: To evaluate the unsupervised domain clustering we used the multi-domain corpus proposed by BIBREF8 which includes textual data in five diverse domains: subtitles, medical text (PDF documents from the European Medicines Agency), legal text (legislative text of the European Union), translations of the Koran, and IT-related text (manuals and localization files of open-source software).",
    "answer_b": "How are they selected?\n< Explanation: We consider five domains: English, French, German, Spanish and Chinese. We use the following criteria to select the domains:\n\n1. Domain labels: We use the domain labels from the Stanford NLP corpus, which are available in the form of a JSON file. We use the following criteria to select the domains:\n\na. The domain label is a string in the form of \"domain_name\" where \"domain_name\" is a string in the form of \"domain_name\" where \"domain_name\" is a string in the form of \"domain_name\" where \"domain_name\" is a string in",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the five domains?\n\nGround Truth Answer:\n\n\nAnswer A:\nsubtitles, medical text (PDF documents from the European Medicines Agency), legal text (legislative text of the European Union), translations of the Koran, and IT-related text (manuals and localization files of open-source software). Explanation: To evaluate the unsupervised domain clustering we used the multi-domain corpus proposed by BIBREF8 which includes textual data in five diverse domains: subtitles, medical text (PDF documents from the European Medicines Agency), legal text (legislative text of the European Union), translations of the Koran, and IT-related text (manuals and localization files of open-source software).\n\nAnswer B:\nHow are they selected?\n< Explanation: We consider five domains: English, French, German, Spanish and Chinese. We use the following criteria to select the domains:\n\n1. Domain labels: We use the domain labels from the Stanford NLP corpus, which are available in the form of a JSON file. We use the following criteria to select the domains:\n\na. The domain label is a string in the form of \"domain_name\" where \"domain_name\" is a string in the form of \"domain_name\" where \"domain_name\" is a string in the form of \"domain_name\" where \"domain_name\" is a string in\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02105",
    "question": "Which pre-trained language models are used?",
    "gold_answer": "",
    "answer_a": "How are they evaluated?\n< Explanation: We use the pre-trained language models from the BERT project BIBREF11 . We use the BERT model with the following hyperparameters: BERT-base, 128 hidden units, 10000 training examples, 0.1 dropout, 100 epochs, and 10000 epochs of training. We use the same hyperparameters for all models. We use the same dataset for all models. We use the same dataset for all models. We use the same dataset for all models. We use the same dataset for all models. We",
    "answer_b": "BERT. Explanation: For MLM-based models we use BERT BIBREF10, DistilBERT BIBREF18 and RoBERTa BIBREF11 (in both the base and large versions).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich pre-trained language models are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they evaluated?\n< Explanation: We use the pre-trained language models from the BERT project BIBREF11 . We use the BERT model with the following hyperparameters: BERT-base, 128 hidden units, 10000 training examples, 0.1 dropout, 100 epochs, and 10000 epochs of training. We use the same hyperparameters for all models. We use the same dataset for all models. We use the same dataset for all models. We use the same dataset for all models. We use the same dataset for all models. We\n\nAnswer B:\nBERT. Explanation: For MLM-based models we use BERT BIBREF10, DistilBERT BIBREF18 and RoBERTa BIBREF11 (in both the base and large versions).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.01720",
    "question": "What are the hyperparameter setting of the MTL model?",
    "gold_answer": "",
    "answer_a": "What is the performance of the model on the two datasets?\n< Explanation: FLOAT: hyperparameters of the model: 1. number of tasks: 2. number of hidden units: 3. number of layers: 4. number of filters: 5. number of neurons: 6. number of neurons in the shared layer: 7. number of neurons in the attention layer: 8. number of neurons in the gating layer: 9. number of neurons in the output layer: 10. number of neurons in the shared layer: 11. number of neurons in the attention layer:",
    "answer_b": "size of word embeddings is 200, size of position embedding is 100, the number of attention heads in transformer block is 6, the number of attention block Is 2, dropout of multi-head attention is 0.7, minibatch size is 64, the initiall learning rate is .001. In fake news detection, the dropout rate is 0.3 and lambda is 0.6.. Explanation: Parameters - hyper-parameters configurations of our model: for each task, we strictly turn all the hyper-parameters on the validation dataset, and we achieve the best performance via a small grid search. The sizes of word embeddings and position embeddings are set to 200 and 100. In transformer encoder, attention heads and blocks are set to 6 and 2 respectively, and the dropout of multi-head attention is set to 0.7. Moreover, the minibatch size is 64; the initial learning rate is set to 0.001, the dropout rate to 0.3, and $\\lambda $ to 0.6 for fake news detection.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the hyperparameter setting of the MTL model?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of the model on the two datasets?\n< Explanation: FLOAT: hyperparameters of the model: 1. number of tasks: 2. number of hidden units: 3. number of layers: 4. number of filters: 5. number of neurons: 6. number of neurons in the shared layer: 7. number of neurons in the attention layer: 8. number of neurons in the gating layer: 9. number of neurons in the output layer: 10. number of neurons in the shared layer: 11. number of neurons in the attention layer:\n\nAnswer B:\nsize of word embeddings is 200, size of position embedding is 100, the number of attention heads in transformer block is 6, the number of attention block Is 2, dropout of multi-head attention is 0.7, minibatch size is 64, the initiall learning rate is .001. In fake news detection, the dropout rate is 0.3 and lambda is 0.6.. Explanation: Parameters - hyper-parameters configurations of our model: for each task, we strictly turn all the hyper-parameters on the validation dataset, and we achieve the best performance via a small grid search. The sizes of word embeddings and position embeddings are set to 200 and 100. In transformer encoder, attention heads and blocks are set to 6 and 2 respectively, and the dropout of multi-head attention is set to 0.7. Moreover, the minibatch size is 64; the initial learning rate is set to 0.001, the dropout rate to 0.3, and $\\lambda $ to 0.6 for fake news detection.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.01720",
    "question": "What architecture does the rest of the multi-task learning setup use?",
    "gold_answer": "",
    "answer_a": "shared features in the shared layer are equally sent to their respective tasks without filtering. Explanation: However, prominent drawback to these methods and even typical multi-task learning methods, like the shared-private model, is that the shared features in the shared layer are equally sent to their respective tasks without filtering, which causes that some useless and even adverse features are mixed in different tasks, as shown in Figure FIGREF2(a).",
    "answer_b": "What is the difference between the two architectures?\n< Explanation: The rest of the multi-task learning setup is the same as the one in the previous work. The only difference is that the shared feature flows are selected by the gate mechanism and attention mechanism. The gate mechanism is used to filter out the shared feature flows that are not useful for the specific task. The attention mechanism is used to select the shared feature flows that are useful for the specific task. The shared feature flows are then fed into the corresponding task-specific model. The task-specific model is trained to maximize the similarity between the shared feature flows and the target task-specific features. The task",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat architecture does the rest of the multi-task learning setup use?\n\nGround Truth Answer:\n\n\nAnswer A:\nshared features in the shared layer are equally sent to their respective tasks without filtering. Explanation: However, prominent drawback to these methods and even typical multi-task learning methods, like the shared-private model, is that the shared features in the shared layer are equally sent to their respective tasks without filtering, which causes that some useless and even adverse features are mixed in different tasks, as shown in Figure FIGREF2(a).\n\nAnswer B:\nWhat is the difference between the two architectures?\n< Explanation: The rest of the multi-task learning setup is the same as the one in the previous work. The only difference is that the shared feature flows are selected by the gate mechanism and attention mechanism. The gate mechanism is used to filter out the shared feature flows that are not useful for the specific task. The attention mechanism is used to select the shared feature flows that are useful for the specific task. The shared feature flows are then fed into the corresponding task-specific model. The task-specific model is trained to maximize the similarity between the shared feature flows and the target task-specific features. The task\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.01720",
    "question": "How is the selected sharing layer trained?",
    "gold_answer": "",
    "answer_a": "What is the training procedure?\n< Explanation: The selected sharing layer is trained by two steps: (1) the shared feature flows are extracted from the input sentence and the shared feature flows are fed into the shared layer; (2) the shared layer is trained to select the shared feature flows from the input sentence. The training procedure is as follows: (1) the shared feature flows are extracted from the input sentence and the shared feature flows are fed into the shared layer; (2) the shared layer is trained to select the shared feature flows from the input sentence. The training procedure is as follows: (1) the shared feature flows",
    "answer_b": "The selected sharing layer is trained jointly on the tasks of stance detection and fake news detection. Explanation: We propose a novel sifted multi-task learning method on the ground of shared-private model to jointly train the tasks of stance detection and fake news detection, filter original outputs of shared layer by a selected sharing layer.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is the selected sharing layer trained?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the training procedure?\n< Explanation: The selected sharing layer is trained by two steps: (1) the shared feature flows are extracted from the input sentence and the shared feature flows are fed into the shared layer; (2) the shared layer is trained to select the shared feature flows from the input sentence. The training procedure is as follows: (1) the shared feature flows are extracted from the input sentence and the shared feature flows are fed into the shared layer; (2) the shared layer is trained to select the shared feature flows from the input sentence. The training procedure is as follows: (1) the shared feature flows\n\nAnswer B:\nThe selected sharing layer is trained jointly on the tasks of stance detection and fake news detection. Explanation: We propose a novel sifted multi-task learning method on the ground of shared-private model to jointly train the tasks of stance detection and fake news detection, filter original outputs of shared layer by a selected sharing layer.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.10090",
    "question": "what were the length constraints they set?",
    "gold_answer": "",
    "answer_a": "search to translations longer than 0.25 times the source sentence length. Explanation: We first constrained search to translations longer than 0.25 times the source sentence length and thus excluded the empty translation from the search space.",
    "answer_b": "How did they do this?\n< Explanation: We set the minimum length of the translation to be 100 words. We also set the maximum length of the translation to be 1000 words. We also set the maximum length of the translation to be 10000 words. We set the maximum length of the translation to be 100000 words. We set the maximum length of the translation to be 1000000 words. We set the maximum length of the translation to be 10000000 words. We set the maximum length of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat were the length constraints they set?\n\nGround Truth Answer:\n\n\nAnswer A:\nsearch to translations longer than 0.25 times the source sentence length. Explanation: We first constrained search to translations longer than 0.25 times the source sentence length and thus excluded the empty translation from the search space.\n\nAnswer B:\nHow did they do this?\n< Explanation: We set the minimum length of the translation to be 100 words. We also set the maximum length of the translation to be 1000 words. We also set the maximum length of the translation to be 10000 words. We set the maximum length of the translation to be 100000 words. We set the maximum length of the translation to be 1000000 words. We set the maximum length of the translation to be 10000000 words. We set the maximum length of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.10090",
    "question": "what is the test set size?",
    "gold_answer": "",
    "answer_a": "2,169 sentences. Explanation: We conduct all our experiments in this section on the entire English-German WMT news-test2015 test set (2,169 sentences) with a Transformer base BIBREF13 model trained with Tensor2Tensor BIBREF14 on parallel WMT18 data excluding ParaCrawl.",
    "answer_b": "How is it divided?\n< Explanation: The test set consists of 1000 sentences, each of which is 1000 words long. The test set is divided into 1000 sentences, each of which is 1000 words long. The test set is divided into 1000 sentences, each of which is 1000 words long. The test set is divided into 1000 sentences, each of which is 1000 words long. The test set is divided into 1000 sentences, each of which is 100",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the test set size?\n\nGround Truth Answer:\n\n\nAnswer A:\n2,169 sentences. Explanation: We conduct all our experiments in this section on the entire English-German WMT news-test2015 test set (2,169 sentences) with a Transformer base BIBREF13 model trained with Tensor2Tensor BIBREF14 on parallel WMT18 data excluding ParaCrawl.\n\nAnswer B:\nHow is it divided?\n< Explanation: The test set consists of 1000 sentences, each of which is 1000 words long. The test set is divided into 1000 sentences, each of which is 1000 words long. The test set is divided into 1000 sentences, each of which is 1000 words long. The test set is divided into 1000 sentences, each of which is 1000 words long. The test set is divided into 1000 sentences, each of which is 100\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.05644",
    "question": "what are the evaluation metrics used?",
    "gold_answer": "",
    "answer_a": "How are they evaluated?\n< Explanation: We evaluate the performance of Meta-NLG on the MultiWoz dataset using the following metrics: BLEU, METEOR, and F1. BLEU is the most commonly used metric for evaluating NLG performance. It is defined as the fraction of tokens that are correctly predicted by the model. METEOR is a metric that measures the quality of the generated sentences by considering the similarity between the generated and the reference sentences. F1 is a metric that measures the overall performance of the model by considering the precision and the recall. We use the following thresholds for the evaluation: 0.5",
    "answer_b": "BLEU-4. Explanation: As in earlier NLG researches, we use the BLEU-4 score BIBREF26 and the slot error rate (ERR) as evaluation metrics.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the evaluation metrics used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they evaluated?\n< Explanation: We evaluate the performance of Meta-NLG on the MultiWoz dataset using the following metrics: BLEU, METEOR, and F1. BLEU is the most commonly used metric for evaluating NLG performance. It is defined as the fraction of tokens that are correctly predicted by the model. METEOR is a metric that measures the quality of the generated sentences by considering the similarity between the generated and the reference sentences. F1 is a metric that measures the overall performance of the model by considering the precision and the recall. We use the following thresholds for the evaluation: 0.5\n\nAnswer B:\nBLEU-4. Explanation: As in earlier NLG researches, we use the BLEU-4 score BIBREF26 and the slot error rate (ERR) as evaluation metrics.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.05644",
    "question": "what other training procedures were explored?",
    "gold_answer": "",
    "answer_a": "Scratch-NLG. Explanation: We included different model settings as baseline:\n\nScratch-NLG: Train INLINEFORM0 with only low-resource target task data, ignoring all high-resource source task data.\n\nMTL-NLG: Train INLINEFORM0 using a multi-task learning paradigm with source task data, then fine-tune on the low-resource target task.\n\nZero-NLG: Train INLINEFORM0 using multi-task learning (MTL) with source task data, then directly test on a target task without a fine-tuning step.",
    "answer_b": "Explanation: We also compare Meta-NLG with other training procedures, including the following:\n\n1) a baseline approach that uses a fixed set of training examples (Baseline) to train the model.\n\n2) a baseline approach that uses a fixed set of training examples (Baseline2) to train the model.\n\n3) a baseline approach that uses a fixed set of training examples (Baseline3) to train the model.\n\n4) a baseline approach that uses a fixed set of training examples (Baseline4) to train the model.\n\n5) a baseline approach that uses a fixed set of training examples (Baseline5) to train the model.\n\n6)",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat other training procedures were explored?\n\nGround Truth Answer:\n\n\nAnswer A:\nScratch-NLG. Explanation: We included different model settings as baseline:\n\nScratch-NLG: Train INLINEFORM0 with only low-resource target task data, ignoring all high-resource source task data.\n\nMTL-NLG: Train INLINEFORM0 using a multi-task learning paradigm with source task data, then fine-tune on the low-resource target task.\n\nZero-NLG: Train INLINEFORM0 using multi-task learning (MTL) with source task data, then directly test on a target task without a fine-tuning step.\n\nAnswer B:\nExplanation: We also compare Meta-NLG with other training procedures, including the following:\n\n1) a baseline approach that uses a fixed set of training examples (Baseline) to train the model.\n\n2) a baseline approach that uses a fixed set of training examples (Baseline2) to train the model.\n\n3) a baseline approach that uses a fixed set of training examples (Baseline3) to train the model.\n\n4) a baseline approach that uses a fixed set of training examples (Baseline4) to train the model.\n\n5) a baseline approach that uses a fixed set of training examples (Baseline5) to train the model.\n\n6)\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.07154",
    "question": "What baseline did they use?",
    "gold_answer": "",
    "answer_a": "What is the performance of their model?\n< Explanation: We compare our model with the following baselines: (1) a baseline that uses the raw text as input, (2) a baseline that uses the raw text as input and a pre-trained BERT model, (3) a baseline that uses the raw text as input and a pre-trained BERT model with a pre-trained BERT model as a feature extractor, (4) a baseline that uses the raw text as input and a pre-trained BERT model with a pre-trained BERT model as a feature extractor and (5) a baseline that uses the raw text as input",
    "answer_b": "we compare the label accuracy of \u201cSUPPORTS\u201d label against a supervised approach \u2013 HexaF. Explanation: Although our unsupervised model doesn't support all the labels, to show the effectiveness of the approach, we compare the label accuracy of \u201cSUPPORTS\u201d label against a supervised approach \u2013 HexaF.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat baseline did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of their model?\n< Explanation: We compare our model with the following baselines: (1) a baseline that uses the raw text as input, (2) a baseline that uses the raw text as input and a pre-trained BERT model, (3) a baseline that uses the raw text as input and a pre-trained BERT model with a pre-trained BERT model as a feature extractor, (4) a baseline that uses the raw text as input and a pre-trained BERT model with a pre-trained BERT model as a feature extractor and (5) a baseline that uses the raw text as input\n\nAnswer B:\nwe compare the label accuracy of \u201cSUPPORTS\u201d label against a supervised approach \u2013 HexaF. Explanation: Although our unsupervised model doesn't support all the labels, to show the effectiveness of the approach, we compare the label accuracy of \u201cSUPPORTS\u201d label against a supervised approach \u2013 HexaF.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.07154",
    "question": "What is the threshold?",
    "gold_answer": "",
    "answer_a": "0.76. Explanation: We compare the results against two different classification thresholds.",
    "answer_b": "How is it computed?\n< Explanation: We use the following threshold to classify the claims: if the predicted label is \"SUPPORTS\" or \"MANUAL_REVIEW\", then the claim is classified as \"SUPPORTS\" or \"MANUAL_REVIEW\", respectively. Otherwise, the claim is classified as \"SUPPORTS\" or \"MANUAL_REVIEW\" respectively. The threshold is computed as follows: if the predicted label is \"SUPPORTS\" or \"MANUAL_REVIEW\", then the claim is classified as \"SUPPORTS\" or \"MANUAL_REVIEW\", respectively. Otherwise, the claim is classified as \"",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the threshold?\n\nGround Truth Answer:\n\n\nAnswer A:\n0.76. Explanation: We compare the results against two different classification thresholds.\n\nAnswer B:\nHow is it computed?\n< Explanation: We use the following threshold to classify the claims: if the predicted label is \"SUPPORTS\" or \"MANUAL_REVIEW\", then the claim is classified as \"SUPPORTS\" or \"MANUAL_REVIEW\", respectively. Otherwise, the claim is classified as \"SUPPORTS\" or \"MANUAL_REVIEW\" respectively. The threshold is computed as follows: if the predicted label is \"SUPPORTS\" or \"MANUAL_REVIEW\", then the claim is classified as \"SUPPORTS\" or \"MANUAL_REVIEW\", respectively. Otherwise, the claim is classified as \"\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.07154",
    "question": "How was the masking done?",
    "gold_answer": "",
    "answer_a": "What is the purpose of the masking?\n< Explanation: We mask the named entities in the claims with the corresponding named entities in the claims. The named entities in the claims are the same as the named entities in the claims. The named entities in the claims are the same as the named entities in the claims. The named entities in the claims are the same as the named entities in the claims. The named entities in the claims are the same as the named entities in the claims. The named entities in the claims are the same as the named entities in the claims. The named entities in the claims are the same as the named entities in the",
    "answer_b": "The named entities are then used to generate the questions by masking the entities for the subsequent stage.. Explanation: The named entities are then used to generate the questions by masking the entities for the subsequent stage.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow was the masking done?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the purpose of the masking?\n< Explanation: We mask the named entities in the claims with the corresponding named entities in the claims. The named entities in the claims are the same as the named entities in the claims. The named entities in the claims are the same as the named entities in the claims. The named entities in the claims are the same as the named entities in the claims. The named entities in the claims are the same as the named entities in the claims. The named entities in the claims are the same as the named entities in the claims. The named entities in the claims are the same as the named entities in the\n\nAnswer B:\nThe named entities are then used to generate the questions by masking the entities for the subsequent stage.. Explanation: The named entities are then used to generate the questions by masking the entities for the subsequent stage.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.07154",
    "question": "How large is the FEVER dataset?",
    "gold_answer": "",
    "answer_a": "around 185k claims from the corpus of 5.4M Wikipedia articles. Explanation: FEVER is the largest dataset and contains around 185k claims from the corpus of 5.4M Wikipedia articles.",
    "answer_b": "What is the size of the test set?\n< Explanation: The FEVER dataset contains 10,000 claims and 10,000 answers. The claims are labeled as \"SUPPORTS\" or \"MANUAL_REVIEW\". The test set contains 1,000 claims and 1,000 answers. The test set is used to evaluate the performance of the classifier. The test set is divided into 1000 training and 1000 test sets. The training set is used to train the classifier and the test set is used to evaluate the performance of the classifier",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow large is the FEVER dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\naround 185k claims from the corpus of 5.4M Wikipedia articles. Explanation: FEVER is the largest dataset and contains around 185k claims from the corpus of 5.4M Wikipedia articles.\n\nAnswer B:\nWhat is the size of the test set?\n< Explanation: The FEVER dataset contains 10,000 claims and 10,000 answers. The claims are labeled as \"SUPPORTS\" or \"MANUAL_REVIEW\". The test set contains 1,000 claims and 1,000 answers. The test set is used to evaluate the performance of the classifier. The test set is divided into 1000 training and 1000 test sets. The training set is used to train the classifier and the test set is used to evaluate the performance of the classifier\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.09501",
    "question": "How do they obtain structured data?",
    "gold_answer": "",
    "answer_a": "What is the dataset?\n<sep><font color=\"darkgreen\">Answer: We use the Basketball Game Report (BGR) dataset, which is a collection of basketball game reports from the 2016-2017 season. The dataset contains 10,000 game reports, each with a unique identifier and a structured record containing the following information: (1) the player name (e.g., Lebron James), (2) the number of points scored (e.g., 20), (3) the number of assists (e.g., 10), and (4) the team",
    "answer_b": "The structured data is obtained from the box-score tables.. Explanation: To obtain our data, we first split each game report into individual sentences, and, for each sentence, find its corresponding data in the box-score table as the content record.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they obtain structured data?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the dataset?\n<sep><font color=\"darkgreen\">Answer: We use the Basketball Game Report (BGR) dataset, which is a collection of basketball game reports from the 2016-2017 season. The dataset contains 10,000 game reports, each with a unique identifier and a structured record containing the following information: (1) the player name (e.g., Lebron James), (2) the number of points scored (e.g., 20), (3) the number of assists (e.g., 10), and (4) the team\n\nAnswer B:\nThe structured data is obtained from the box-score tables.. Explanation: To obtain our data, we first split each game report into individual sentences, and, for each sentence, find its corresponding data in the box-score table as the content record.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.09501",
    "question": "Which prior approaches for style transfer do they test with?",
    "gold_answer": "",
    "answer_a": "Multi-Attribute Style Transfer. Explanation: Multi-Attribute Style Transfer (MAST) BIBREF11 . We compare with the most recent style transfer approach that models multiple attributes.",
    "answer_b": "What is the baseline?\n< Explanation: We compare our method with the following baselines: (1) a rule-based baseline that uses a set of rules to generate a sentence that matches the reference, (2) a style transfer baseline that uses a set of style transfer rules to generate a sentence that matches the reference, and (3) a style transfer baseline that uses a set of style transfer rules to generate a sentence that matches the reference. We also compare our method with the following baselines: (1) a rule-based baseline that uses a set of rules to generate a sentence that matches the reference, (2) a style transfer",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich prior approaches for style transfer do they test with?\n\nGround Truth Answer:\n\n\nAnswer A:\nMulti-Attribute Style Transfer. Explanation: Multi-Attribute Style Transfer (MAST) BIBREF11 . We compare with the most recent style transfer approach that models multiple attributes.\n\nAnswer B:\nWhat is the baseline?\n< Explanation: We compare our method with the following baselines: (1) a rule-based baseline that uses a set of rules to generate a sentence that matches the reference, (2) a style transfer baseline that uses a set of style transfer rules to generate a sentence that matches the reference, and (3) a style transfer baseline that uses a set of style transfer rules to generate a sentence that matches the reference. We also compare our method with the following baselines: (1) a rule-based baseline that uses a set of rules to generate a sentence that matches the reference, (2) a style transfer\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.09501",
    "question": "Which competing objectives for their unsupevised method do they use?",
    "gold_answer": "",
    "answer_a": "What is the difference between their objective and the rule-based baseline?\n<sep><solution>Our unsupervised method uses a novel objective to minimize the difference between the generated and reference sentences. The objective is to minimize the difference between the generated and reference sentences, while the rule-based baseline minimizes the difference between the generated and reference sentences, but does not consider the content of the reference sentence. Our objective is to minimize the difference between the generated and reference sentences, while the rule-based baseline minimizes the difference between the generated and reference sentences, but does not consider the content of the reference sentence. Our objective is to minimize the difference between",
    "answer_b": "A combination of Content Objective and Style Objective. Explanation: The model is learned with two competing objectives of reconstructing the auxiliary sentence (for content fidelity) and the reference sentence (for style preservation).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich competing objectives for their unsupevised method do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between their objective and the rule-based baseline?\n<sep><solution>Our unsupervised method uses a novel objective to minimize the difference between the generated and reference sentences. The objective is to minimize the difference between the generated and reference sentences, while the rule-based baseline minimizes the difference between the generated and reference sentences, but does not consider the content of the reference sentence. Our objective is to minimize the difference between the generated and reference sentences, while the rule-based baseline minimizes the difference between the generated and reference sentences, but does not consider the content of the reference sentence. Our objective is to minimize the difference between\n\nAnswer B:\nA combination of Content Objective and Style Objective. Explanation: The model is learned with two competing objectives of reconstructing the auxiliary sentence (for content fidelity) and the reference sentence (for style preservation).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.09501",
    "question": "Which content coverage constraints do they design?",
    "gold_answer": "",
    "answer_a": "What are the explicit constraints?\n< Explanation: We propose a novel content coverage constraint to ensure that the generated sentence is faithful to the original record. The constraint is based on the notion of content coverage, which is a measure of how much the generated sentence is consistent with the original record. We define the content coverage of a sentence as the ratio of the number of words in the sentence to the number of words in the original record. We assume that the generated sentence is a subsequence of the original record, and that the generated sentence is faithful to the original record. We define the explicit content coverage constraint as the ratio of the number of words",
    "answer_b": "Answer with content missing: (Content Coverage Constraint section) We thus devise an additional learning constraint based on the nature of content description\u2014each data tuple in the content record should usually be mentioned exactly once in the generated sentence.\nThe copy mechanism over content record x enables a simple yet effective way to encourage the behavior. Intuitively, we want each tuple to be copied once and only once on average.. Explanation: Model",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich content coverage constraints do they design?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the explicit constraints?\n< Explanation: We propose a novel content coverage constraint to ensure that the generated sentence is faithful to the original record. The constraint is based on the notion of content coverage, which is a measure of how much the generated sentence is consistent with the original record. We define the content coverage of a sentence as the ratio of the number of words in the sentence to the number of words in the original record. We assume that the generated sentence is a subsequence of the original record, and that the generated sentence is faithful to the original record. We define the explicit content coverage constraint as the ratio of the number of words\n\nAnswer B:\nAnswer with content missing: (Content Coverage Constraint section) We thus devise an additional learning constraint based on the nature of content description\u2014each data tuple in the content record should usually be mentioned exactly once in the generated sentence.\nThe copy mechanism over content record x enables a simple yet effective way to encourage the behavior. Intuitively, we want each tuple to be copied once and only once on average.. Explanation: Model\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1705.02023",
    "question": "what were the evaluation metrics?",
    "gold_answer": "",
    "answer_a": "Explanation: The evaluation metrics are the F1 score and the accuracy. The F1 score is calculated by the formula: F1 = 2 * (precision * recall) / (precision + recall). The accuracy is calculated by the formula: accuracy = (TP + TN) / (TP + TN + FP + FN). The precision is calculated by the formula: precision = TP / (TP + FP). The recall is calculated by the formula: recall = TP / (TP + FN). The F1 score is calculated by the formula: F1 = 2 * (precision * recall) / (precision + recall). The",
    "answer_b": "macro-average recall. Explanation: Official ranking: Our system is ranked fourth over 38 systems in terms of macro-average recall.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat were the evaluation metrics?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The evaluation metrics are the F1 score and the accuracy. The F1 score is calculated by the formula: F1 = 2 * (precision * recall) / (precision + recall). The accuracy is calculated by the formula: accuracy = (TP + TN) / (TP + TN + FP + FN). The precision is calculated by the formula: precision = TP / (TP + FP). The recall is calculated by the formula: recall = TP / (TP + FN). The F1 score is calculated by the formula: F1 = 2 * (precision * recall) / (precision + recall). The\n\nAnswer B:\nmacro-average recall. Explanation: Official ranking: Our system is ranked fourth over 38 systems in terms of macro-average recall.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1705.02023",
    "question": "how many sentiment labels do they explore?",
    "gold_answer": "",
    "answer_a": "Explanation: The system is trained on 10000 tweets and the accuracy is 67.4%. The system is ranked fourth in SemEval-2017 Task4 over 38 systems with 67.4%. The system is trained on 10000 tweets and the accuracy is 67.4%. The system is ranked fourth in SemEval-2017 Task4 over 38 systems with 67.4%. The system is trained on 10000 tweets and the accuracy is 67.4%. The system is ranked fourth in",
    "answer_b": "3. Explanation: Thus, we have ten classifiers, we count the number of classifiers which give the positive, negative and neutral sentiment label to each tweet and select the sentiment label which have the highest number of votes.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow many sentiment labels do they explore?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The system is trained on 10000 tweets and the accuracy is 67.4%. The system is ranked fourth in SemEval-2017 Task4 over 38 systems with 67.4%. The system is trained on 10000 tweets and the accuracy is 67.4%. The system is ranked fourth in SemEval-2017 Task4 over 38 systems with 67.4%. The system is trained on 10000 tweets and the accuracy is 67.4%. The system is ranked fourth in\n\nAnswer B:\n3. Explanation: Thus, we have ten classifiers, we count the number of classifiers which give the positive, negative and neutral sentiment label to each tweet and select the sentiment label which have the highest number of votes.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.06203",
    "question": "Which dataset do they use for text altering attributes matching to image parts?",
    "gold_answer": "",
    "answer_a": "CUB bird. Explanation: As shown in Table TABREF11, our method has the highest IS and MP values on both the CUB and COCO datasets compared with the state-of-the-art approaches, which demonstrates that (1) our method can produce high-quality manipulated results, and (2) our method can better generate new attributes matching the given text, and also effectively reconstruct text-irrelevant contents of the original image.",
    "answer_b": "What is the metric they use to evaluate the results?\n< Explanation: We use the CUB dataset BIBREF11 and the COCO dataset BIBREF12 to evaluate the proposed method. The CUB dataset contains 1000 images and the COCO dataset contains 10000 images. The images in both datasets are of the same size and resolution. The images are divided into 1000 images for training and 10000 images for testing. The images are labeled with the text describing the desired attributes. The images are also labeled with the corresponding semantic words. The semantic",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dataset do they use for text altering attributes matching to image parts?\n\nGround Truth Answer:\n\n\nAnswer A:\nCUB bird. Explanation: As shown in Table TABREF11, our method has the highest IS and MP values on both the CUB and COCO datasets compared with the state-of-the-art approaches, which demonstrates that (1) our method can produce high-quality manipulated results, and (2) our method can better generate new attributes matching the given text, and also effectively reconstruct text-irrelevant contents of the original image.\n\nAnswer B:\nWhat is the metric they use to evaluate the results?\n< Explanation: We use the CUB dataset BIBREF11 and the COCO dataset BIBREF12 to evaluate the proposed method. The CUB dataset contains 1000 images and the COCO dataset contains 10000 images. The images in both datasets are of the same size and resolution. The images are divided into 1000 images for training and 10000 images for testing. The images are labeled with the text describing the desired attributes. The images are also labeled with the corresponding semantic words. The semantic\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.05807",
    "question": "What statistical test is performed?",
    "gold_answer": "",
    "answer_a": "t-test. Explanation: We also show the p-value by comparing the result of 10 random tests for both our model and the state-of-the-art model in the t-test .",
    "answer_b": "What is the significance of the results?\nThe proposed model is compared with the state-of-the-art models on three benchmark datasets. The results show that the proposed model outperforms the state-of-the-art models on all three datasets. The results are statistically significant at the 5% level. The proposed model is also compared with the state-of-the-art models on the IMDB dataset. The results show that the proposed model outperforms the state-of-the-art models on the IMDB dataset. The results are statistically significant at the 5% level. The proposed model is compared with the state-of-the-art models on the Yelp13",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat statistical test is performed?\n\nGround Truth Answer:\n\n\nAnswer A:\nt-test. Explanation: We also show the p-value by comparing the result of 10 random tests for both our model and the state-of-the-art model in the t-test .\n\nAnswer B:\nWhat is the significance of the results?\nThe proposed model is compared with the state-of-the-art models on three benchmark datasets. The results show that the proposed model outperforms the state-of-the-art models on all three datasets. The results are statistically significant at the 5% level. The proposed model is also compared with the state-of-the-art models on the IMDB dataset. The results show that the proposed model outperforms the state-of-the-art models on the IMDB dataset. The results are statistically significant at the 5% level. The proposed model is compared with the state-of-the-art models on the Yelp13\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1904.02954",
    "question": "Which downstream tasks are used for evaluation in this paper?",
    "gold_answer": "",
    "answer_a": "Argument component detection. Explanation: We trained this architecture for the following datasets: Arguments: Argument component detection (major claim, claim, premise) in 402 persuasive essays BIBREF7 . Development and test set were 80 randomly selected essays each. ACE Entities/Events: ACE 2005 dataset BIBREF8 consists of 599 annotated documents from six different domains (newswire, broadcast news, broadcast conversations, blogs, forums, and speeches). We train the architecture to either detect events or to detect entities in these documents. We used 90 randomly selected documents each for the development and test set. POS: We use the part-of-speech tags from Universal Dependencies v. 1.3 for English with the provided data splits. We reduced the training set to the first 500 sentences to increase the difficulty for the network. The development and test set were kept unchanged. Chunking: CoNLL 2000 shared task dataset on chunking. NER: CoNLL 2003 shared task on named entity recognition. GENIA NER: The Bio-Entity Recognition Task at JNLPBA BIBREF9 annotated Medline abstracts with information on bio-entities (like protein or DNA-names). The dataset consists of 2000 abstracts for training (we used 400 of those as development set) and the test set contains 404 abstracts. WNUT16: WNUT16 was a shared task on Named Entity Recognition over Twitter BIBREF10 . Training data are 2,394 annotated tweets, development data are 1,000 tweets, and test data are 3,856 tweets.",
    "answer_b": "What are the performance metrics used?\n< Explanation: We evaluate the performance of the proposed language model on the following downstream tasks: (1) sentiment analysis, (2) named entity recognition, (3) question answering, (4) question answering with multiple-choice answers, (5) question answering with multiple-choice answers and multiple-answer questions, (6) question answering with multiple-choice answers and multiple-answer questions with multiple-choice answers, (7) question answering with multiple-choice answers and multiple-answer questions with multiple-answer questions, (8) question answering with multiple-choice answers and multiple-answer questions with multiple-answer questions with multiple-answer questions, (9",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich downstream tasks are used for evaluation in this paper?\n\nGround Truth Answer:\n\n\nAnswer A:\nArgument component detection. Explanation: We trained this architecture for the following datasets: Arguments: Argument component detection (major claim, claim, premise) in 402 persuasive essays BIBREF7 . Development and test set were 80 randomly selected essays each. ACE Entities/Events: ACE 2005 dataset BIBREF8 consists of 599 annotated documents from six different domains (newswire, broadcast news, broadcast conversations, blogs, forums, and speeches). We train the architecture to either detect events or to detect entities in these documents. We used 90 randomly selected documents each for the development and test set. POS: We use the part-of-speech tags from Universal Dependencies v. 1.3 for English with the provided data splits. We reduced the training set to the first 500 sentences to increase the difficulty for the network. The development and test set were kept unchanged. Chunking: CoNLL 2000 shared task dataset on chunking. NER: CoNLL 2003 shared task on named entity recognition. GENIA NER: The Bio-Entity Recognition Task at JNLPBA BIBREF9 annotated Medline abstracts with information on bio-entities (like protein or DNA-names). The dataset consists of 2000 abstracts for training (we used 400 of those as development set) and the test set contains 404 abstracts. WNUT16: WNUT16 was a shared task on Named Entity Recognition over Twitter BIBREF10 . Training data are 2,394 annotated tweets, development data are 1,000 tweets, and test data are 3,856 tweets.\n\nAnswer B:\nWhat are the performance metrics used?\n< Explanation: We evaluate the performance of the proposed language model on the following downstream tasks: (1) sentiment analysis, (2) named entity recognition, (3) question answering, (4) question answering with multiple-choice answers, (5) question answering with multiple-choice answers and multiple-answer questions, (6) question answering with multiple-choice answers and multiple-answer questions with multiple-choice answers, (7) question answering with multiple-choice answers and multiple-answer questions with multiple-answer questions, (8) question answering with multiple-choice answers and multiple-answer questions with multiple-answer questions with multiple-answer questions, (9\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1904.02954",
    "question": "Which datasets are used for evaluation?",
    "gold_answer": "",
    "answer_a": "What are the performance metrics used?\n< Explanation: We evaluate the performance of the proposed method on the following datasets: (1) the Penn Treebank (PTB) BIBREF10, (2) the Stanford Sentiment Treebank (SST) BIBREF11, (3) the Stanford CoreNLP Sentiment Treebank (SCNTB) BIBREF12, (4) the Stanford CoreNLP Sentiment Treebank (SCNTB) BIBREF13, (5) the Stanford CoreNLP Sentiment Treebank (SCNTB) BIBREF14,",
    "answer_b": "Argument detection, ACE 2005, Universal Dependencies part-of-speech tags, CoNLL 2000 chunking shared task, CoNLL 2003 named entity recognition shared task, GENIA NER Bio-Entity Recognition, WNUT16 Twitter named entity recognition shared task, Stanford Sentiment Treebank, Penn TreeBank constituency parsing, Stanford Natural Language Inference corpus. Explanation: We trained this architecture for the following datasets: Arguments: Argument component detection (major claim, claim, premise) in 402 persuasive essays BIBREF7 . Development and test set were 80 randomly selected essays each. ACE Entities/Events: ACE 2005 dataset BIBREF8 consists of 599 annotated documents from six different domains (newswire, broadcast news, broadcast conversations, blogs, forums, and speeches). We train the architecture to either detect events or to detect entities in these documents. We used 90 randomly selected documents each for the development and test set. POS: We use the part-of-speech tags from Universal Dependencies v. 1.3 for English with the provided data splits. We reduced the training set to the first 500 sentences to increase the difficulty for the network. The development and test set were kept unchanged. Chunking: CoNLL 2000 shared task dataset on chunking. NER: CoNLL 2003 shared task on named entity recognition. GENIA NER: The Bio-Entity Recognition Task at JNLPBA BIBREF9 annotated Medline abstracts with information on bio-entities (like protein or DNA-names). The dataset consists of 2000 abstracts for training (we used 400 of those as development set) and the test set contains 404 abstracts. WNUT16: WNUT16 was a shared task on Named Entity Recognition over Twitter BIBREF10 . Training data are 2,394 annotated tweets, development data are 1,000 tweets, and test data are 3,856 tweets.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich datasets are used for evaluation?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the performance metrics used?\n< Explanation: We evaluate the performance of the proposed method on the following datasets: (1) the Penn Treebank (PTB) BIBREF10, (2) the Stanford Sentiment Treebank (SST) BIBREF11, (3) the Stanford CoreNLP Sentiment Treebank (SCNTB) BIBREF12, (4) the Stanford CoreNLP Sentiment Treebank (SCNTB) BIBREF13, (5) the Stanford CoreNLP Sentiment Treebank (SCNTB) BIBREF14,\n\nAnswer B:\nArgument detection, ACE 2005, Universal Dependencies part-of-speech tags, CoNLL 2000 chunking shared task, CoNLL 2003 named entity recognition shared task, GENIA NER Bio-Entity Recognition, WNUT16 Twitter named entity recognition shared task, Stanford Sentiment Treebank, Penn TreeBank constituency parsing, Stanford Natural Language Inference corpus. Explanation: We trained this architecture for the following datasets: Arguments: Argument component detection (major claim, claim, premise) in 402 persuasive essays BIBREF7 . Development and test set were 80 randomly selected essays each. ACE Entities/Events: ACE 2005 dataset BIBREF8 consists of 599 annotated documents from six different domains (newswire, broadcast news, broadcast conversations, blogs, forums, and speeches). We train the architecture to either detect events or to detect entities in these documents. We used 90 randomly selected documents each for the development and test set. POS: We use the part-of-speech tags from Universal Dependencies v. 1.3 for English with the provided data splits. We reduced the training set to the first 500 sentences to increase the difficulty for the network. The development and test set were kept unchanged. Chunking: CoNLL 2000 shared task dataset on chunking. NER: CoNLL 2003 shared task on named entity recognition. GENIA NER: The Bio-Entity Recognition Task at JNLPBA BIBREF9 annotated Medline abstracts with information on bio-entities (like protein or DNA-names). The dataset consists of 2000 abstracts for training (we used 400 of those as development set) and the test set contains 404 abstracts. WNUT16: WNUT16 was a shared task on Named Entity Recognition over Twitter BIBREF10 . Training data are 2,394 annotated tweets, development data are 1,000 tweets, and test data are 3,856 tweets.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.03839",
    "question": "What does the human-in-the-loop do to help their system?",
    "gold_answer": "",
    "answer_a": "What is the role of the system?\n< Explanation: The human-in-the-loop is the user who provides the text to the system. The system then uses the user-provided text to generate a story. The human-in-the-loop is then responsible for reviewing the generated story to determine if it is biased or not. The system can then offer counter-evidences of opposite gender also being associated with the same occupation in the context of user-provided geography and timespan. The system thus enables text de-biasing by assisting a human-in-the-loop. The system can not only act as a text pre-processor before training any AI model",
    "answer_b": "identifying which biases to focus on and how to paraphrase or modify the sentence to de-bias it. Explanation: Since multiple types of biases may exist in the given text, the former de-biasing system requires identifying which biases to focus on and how to paraphrase or modify the sentence to de-bias it. These notions can often be subjective and it might be desirable to have a human-in-the-loop. This is the focus of the latter de-biasing system as well as the approach taken by us in the paper.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat does the human-in-the-loop do to help their system?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the role of the system?\n< Explanation: The human-in-the-loop is the user who provides the text to the system. The system then uses the user-provided text to generate a story. The human-in-the-loop is then responsible for reviewing the generated story to determine if it is biased or not. The system can then offer counter-evidences of opposite gender also being associated with the same occupation in the context of user-provided geography and timespan. The system thus enables text de-biasing by assisting a human-in-the-loop. The system can not only act as a text pre-processor before training any AI model\n\nAnswer B:\nidentifying which biases to focus on and how to paraphrase or modify the sentence to de-bias it. Explanation: Since multiple types of biases may exist in the given text, the former de-biasing system requires identifying which biases to focus on and how to paraphrase or modify the sentence to de-bias it. These notions can often be subjective and it might be desirable to have a human-in-the-loop. This is the focus of the latter de-biasing system as well as the approach taken by us in the paper.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.03839",
    "question": "Which dataset do they use to train their model?",
    "gold_answer": "",
    "answer_a": "A dataset they created that contains occupation and names data.. Explanation: Our dataset comprises of the following - 1) Occupation Data 2) Names Data.",
    "answer_b": "How does it differ from the existing datasets?\n<sep> Dataset: The dataset used in this paper is the dataset of the 2016 US Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dataset do they use to train their model?\n\nGround Truth Answer:\n\n\nAnswer A:\nA dataset they created that contains occupation and names data.. Explanation: Our dataset comprises of the following - 1) Occupation Data 2) Names Data.\n\nAnswer B:\nHow does it differ from the existing datasets?\n<sep> Dataset: The dataset used in this paper is the dataset of the 2016 US Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census Bureau Census\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02214",
    "question": "What are existing baseline models on these benchmark datasets?",
    "gold_answer": "",
    "answer_a": "How do they compare to the proposed model?\n<blockquote>Baseline models: We compare our proposed model with the following baseline models: (1) a simple word2vec model (2) a simple word2vec model with a word embedding size of 100 (3) a simple word2vec model with a word embedding size of 200 (4) a simple word2vec model with a word embedding size of 300 (5) a simple word2vec model with a word embedding size of 400 (6) a simple word2vec model with a word embedding size of 5",
    "answer_b": "Seq2seq. Explanation: We compare the proposed approach with several competitive baselines that can be categorized into two classes: generative approaches and retrieval-based approaches.\n\nExperiments ::: Model Comparison ::: Generative Approaches ::: Seq2seq:\nStandard sequence-to-sequence model with attention mechanism BIBREF39, BIBREF40.\n\nExperiments ::: Model Comparison ::: Generative Approaches ::: GPT2-FT:\nTo examine the effect of leveraging the pre-trained language model for the task of dialogue generation, we directly fine-tune the GPT-2 model on the dialogue data without any designed adaptations.\n\nExperiments ::: Model Comparison ::: Generative Approaches ::: Speaker:\nModel proposed by BIBREF16 which incorporates distributed style embeddings into the structure of decoding cells to control the generation process.\n\nExperiments ::: Model Comparison ::: Generative Approaches ::: ECM:\nModel proposed by BIBREF11 which uses memory modules to control the stylistic expressions in the generated responses.\n\nExperiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Skeleton-to-Response (SR):\nModel proposed by BIBREF27 which modifies the retrieved response based on the lexical difference between the input and the retrieved query. This approach does not take the style aspect into consideration.\n\nExperiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Style Transfer (RST):\nFor this approach, we apply the state-of-the-art style transfer BIBREF23 model on the retrieved response. This approach does not consider the input query information during the transfer process.\n\nExperiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Reranking (RRe):\nGiven the input query, a style classifier is used to rerank the top 10 retrieved responses. The response with the highest score on the desired style is selected.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are existing baseline models on these benchmark datasets?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they compare to the proposed model?\n<blockquote>Baseline models: We compare our proposed model with the following baseline models: (1) a simple word2vec model (2) a simple word2vec model with a word embedding size of 100 (3) a simple word2vec model with a word embedding size of 200 (4) a simple word2vec model with a word embedding size of 300 (5) a simple word2vec model with a word embedding size of 400 (6) a simple word2vec model with a word embedding size of 5\n\nAnswer B:\nSeq2seq. Explanation: We compare the proposed approach with several competitive baselines that can be categorized into two classes: generative approaches and retrieval-based approaches.\n\nExperiments ::: Model Comparison ::: Generative Approaches ::: Seq2seq:\nStandard sequence-to-sequence model with attention mechanism BIBREF39, BIBREF40.\n\nExperiments ::: Model Comparison ::: Generative Approaches ::: GPT2-FT:\nTo examine the effect of leveraging the pre-trained language model for the task of dialogue generation, we directly fine-tune the GPT-2 model on the dialogue data without any designed adaptations.\n\nExperiments ::: Model Comparison ::: Generative Approaches ::: Speaker:\nModel proposed by BIBREF16 which incorporates distributed style embeddings into the structure of decoding cells to control the generation process.\n\nExperiments ::: Model Comparison ::: Generative Approaches ::: ECM:\nModel proposed by BIBREF11 which uses memory modules to control the stylistic expressions in the generated responses.\n\nExperiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Skeleton-to-Response (SR):\nModel proposed by BIBREF27 which modifies the retrieved response based on the lexical difference between the input and the retrieved query. This approach does not take the style aspect into consideration.\n\nExperiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Style Transfer (RST):\nFor this approach, we apply the state-of-the-art style transfer BIBREF23 model on the retrieved response. This approach does not consider the input query information during the transfer process.\n\nExperiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Reranking (RRe):\nGiven the input query, a style classifier is used to rerank the top 10 retrieved responses. The response with the highest score on the desired style is selected.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02214",
    "question": "On what two languages is experimented on?",
    "gold_answer": "",
    "answer_a": "Chinese and English. Explanation: We conduct extensive experiments on three dialogue datasets: gender-specific (Chinese) dataset, emotion-specific (Chinese) dataset, and sentiment-specific (English) dataset.",
    "answer_b": "What are the results?\n< Explanation: We evaluate the proposed approach on two languages: English and Chinese. We use the following datasets: (1) English: the Corpus of Contemporary English (CoCE) BIBREF1 , (2) Chinese: the Chinese Dialogue Corpus (CCDC) BIBREF2 . We use the following evaluation metrics: (1) BLEU BIBREF3 , (2) F1 BIBREF4 , (3) ROUGE BIBREF5 . We also use the following evaluation metrics: (1) F1 BIBREF6 , (2) ROUGE BIBREF7",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nOn what two languages is experimented on?\n\nGround Truth Answer:\n\n\nAnswer A:\nChinese and English. Explanation: We conduct extensive experiments on three dialogue datasets: gender-specific (Chinese) dataset, emotion-specific (Chinese) dataset, and sentiment-specific (English) dataset.\n\nAnswer B:\nWhat are the results?\n< Explanation: We evaluate the proposed approach on two languages: English and Chinese. We use the following datasets: (1) English: the Corpus of Contemporary English (CoCE) BIBREF1 , (2) Chinese: the Chinese Dialogue Corpus (CCDC) BIBREF2 . We use the following evaluation metrics: (1) BLEU BIBREF3 , (2) F1 BIBREF4 , (3) ROUGE BIBREF5 . We also use the following evaluation metrics: (1) F1 BIBREF6 , (2) ROUGE BIBREF7\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02214",
    "question": "What three benchmark datasets are used?",
    "gold_answer": "",
    "answer_a": "How are they evaluated?\n< Explanation: We evaluate the proposed approach on three benchmark datasets from two languages. The first dataset is from the Chinese language, the second one is from the English language, and the third one is from the Japanese language. The datasets are collected from the following sources: (1) the Chinese Wikipedia dataset (Wikipedia Chinese) BIBREF10 ; (2) the English Wikipedia dataset (Wikipedia English) BIBREF11 ; (3) the Japanese Wikipedia dataset (Wikipedia Japanese) BIBREF12 . The datasets are collected from the Wikipedia platform and are publicly available. The datasets are",
    "answer_b": "gender-specific (Chinese) dataset. Explanation: We conduct extensive experiments on three dialogue datasets: gender-specific (Chinese) dataset, emotion-specific (Chinese) dataset, and sentiment-specific (English) dataset.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat three benchmark datasets are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they evaluated?\n< Explanation: We evaluate the proposed approach on three benchmark datasets from two languages. The first dataset is from the Chinese language, the second one is from the English language, and the third one is from the Japanese language. The datasets are collected from the following sources: (1) the Chinese Wikipedia dataset (Wikipedia Chinese) BIBREF10 ; (2) the English Wikipedia dataset (Wikipedia English) BIBREF11 ; (3) the Japanese Wikipedia dataset (Wikipedia Japanese) BIBREF12 . The datasets are collected from the Wikipedia platform and are publicly available. The datasets are\n\nAnswer B:\ngender-specific (Chinese) dataset. Explanation: We conduct extensive experiments on three dialogue datasets: gender-specific (Chinese) dataset, emotion-specific (Chinese) dataset, and sentiment-specific (English) dataset.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.05652",
    "question": "What IS versification?",
    "gold_answer": "",
    "answer_a": "the ratios of iambic lines ending with a stressed syllable (\u201cThe view of earthly glory: men might say\u201d) to lines ending with an extra unstressed one (\u201cTill this time pomp was single, but now married\u201d). Explanation: Spedding supported this with data from the domain of versification, namely the ratios of iambic lines ending with a stressed syllable (\u201cThe view of earthly glory: men might say\u201d) to lines ending with an extra unstressed one (\u201cTill this time pomp was single, but now married\u201d), pointing out that the distribution of values across scenes is strongly bimodal.",
    "answer_b": "How is it defined?\n<sep><br/ >Definition: The act of writing a verse in a line of poetry, usually in a rhyme scheme, and in a meter, is called versification. The act of writing a verse in a line of poetry, usually in a rhyme scheme, and in a meter, is called versification. The act of writing a verse in a line of poetry, usually in a rhyme scheme, and in a meter, is called versification. The act of writing a verse in a line of poetry, usually in a rhyme scheme, and in a meter, is called versification. The act of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat IS versification?\n\nGround Truth Answer:\n\n\nAnswer A:\nthe ratios of iambic lines ending with a stressed syllable (\u201cThe view of earthly glory: men might say\u201d) to lines ending with an extra unstressed one (\u201cTill this time pomp was single, but now married\u201d). Explanation: Spedding supported this with data from the domain of versification, namely the ratios of iambic lines ending with a stressed syllable (\u201cThe view of earthly glory: men might say\u201d) to lines ending with an extra unstressed one (\u201cTill this time pomp was single, but now married\u201d), pointing out that the distribution of values across scenes is strongly bimodal.\n\nAnswer B:\nHow is it defined?\n<sep><br/ >Definition: The act of writing a verse in a line of poetry, usually in a rhyme scheme, and in a meter, is called versification. The act of writing a verse in a line of poetry, usually in a rhyme scheme, and in a meter, is called versification. The act of writing a verse in a line of poetry, usually in a rhyme scheme, and in a meter, is called versification. The act of writing a verse in a line of poetry, usually in a rhyme scheme, and in a meter, is called versification. The act of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.05652",
    "question": "How confident is the conclusion about Shakespeare vs Flectcher?",
    "gold_answer": "",
    "answer_a": "What is the difference between the two?\n<sep><br/ >Answer: The results show that the probability of Shakespeare\u2019s attribution is 0.61, while the probability of Fletcher\u2019s attribution is 0.39. The difference between the two attribution probabilities is significant (p < 0.001). The results also show that the probability of Fletcher\u2019s attribution is higher than the probability of Shakespeare\u2019s attribution. Explanation: The results show that the probability of Shakespeare\u2019s attribution is 0.61, while the probability of Fletcher\u2019s attribution is 0.39. The difference between the two attribution probabilities is",
    "answer_b": "high reliability. Explanation: Combined versification-based and word-based models trained on 17th century English drama yield a high accuracy of authorship recognition. We can thus state with high reliability that H8 is a result of collaboration between William Shakespeare and John Fletcher, while the participation of Philip Massinger is rather unlikely.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow confident is the conclusion about Shakespeare vs Flectcher?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the two?\n<sep><br/ >Answer: The results show that the probability of Shakespeare\u2019s attribution is 0.61, while the probability of Fletcher\u2019s attribution is 0.39. The difference between the two attribution probabilities is significant (p < 0.001). The results also show that the probability of Fletcher\u2019s attribution is higher than the probability of Shakespeare\u2019s attribution. Explanation: The results show that the probability of Shakespeare\u2019s attribution is 0.61, while the probability of Fletcher\u2019s attribution is 0.39. The difference between the two attribution probabilities is\n\nAnswer B:\nhigh reliability. Explanation: Combined versification-based and word-based models trained on 17th century English drama yield a high accuracy of authorship recognition. We can thus state with high reliability that H8 is a result of collaboration between William Shakespeare and John Fletcher, while the participation of Philip Massinger is rather unlikely.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.05652",
    "question": "What are the modifications by Thomas Merriam?",
    "gold_answer": "",
    "answer_a": "Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa.. Explanation: Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare\u2019s and Fletcher\u2019s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27. Eisen, Riberio, Segarra, and Egan BIBREF28 used Word adjacency networks BIBREF29 to analyze the frequencies of collocations of selected function words in particular scenes of the play. In contrast to Spedding, they reattribute several scenes back to Shakespeare. Details on Spedding\u2019s attribution as well as the ones mentioned in this paragraph are given in Table TABREF3.",
    "answer_b": "Explanation: Thomas Merriam (1680-1748) is credited with the first edition of the play, which was published in 1701. He also published a new version of the play in 1703, which was published in 1704. Merriam\u2019s edition is considered to be the most accurate and authoritative version of the play. Merriam\u2019s edition is also the most widely used edition of the play. Merriam\u2019s edition is also the most widely used edition of the play. Merriam\u2019s edition is also the most widely used edition of the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the modifications by Thomas Merriam?\n\nGround Truth Answer:\n\n\nAnswer A:\nThomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa.. Explanation: Thomas Merriam proposed a modification to Spedding\u2019s original attribution concerning re-attribution of several parts of supposedly Fletcher\u2019s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare\u2019s and Fletcher\u2019s plays BIBREF25, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable BIBREF26 or principal component analysis of 64 most frequent words BIBREF27. Eisen, Riberio, Segarra, and Egan BIBREF28 used Word adjacency networks BIBREF29 to analyze the frequencies of collocations of selected function words in particular scenes of the play. In contrast to Spedding, they reattribute several scenes back to Shakespeare. Details on Spedding\u2019s attribution as well as the ones mentioned in this paragraph are given in Table TABREF3.\n\nAnswer B:\nExplanation: Thomas Merriam (1680-1748) is credited with the first edition of the play, which was published in 1701. He also published a new version of the play in 1703, which was published in 1704. Merriam\u2019s edition is considered to be the most accurate and authoritative version of the play. Merriam\u2019s edition is also the most widely used edition of the play. Merriam\u2019s edition is also the most widely used edition of the play. Merriam\u2019s edition is also the most widely used edition of the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1703.10090",
    "question": "What sources of less sensitive data are available?",
    "gold_answer": "",
    "answer_a": "Explanation: We discuss the following sources of less sensitive data: (i) clinical notes from the National Health Information Database (NHID) (ii) clinical notes from the National Health Information Exchange (NHIE) (iii) clinical notes from the National Health Information Exchange for Clinical Practice (NHICP) (iv) clinical notes from the National Health Information Exchange for Clinical Practice (NHICP) (v) clinical notes from the National Health Information Exchange for Clinical Practice (NHICP) (vi) clinical notes from the National Health Information Exchange for Clinical Practice (NHICP) (vii) clinical notes from the National Health",
    "answer_b": "MIMICII(I), THYME, results from i2b2 and ShARe/CLEF shared task, MiPACQ, Blulab, EMC Dutch Clinical Corpus, 2010 i2b2/VA, VetCompass. Explanation: An often used corpus is MIMICII(I)",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat sources of less sensitive data are available?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We discuss the following sources of less sensitive data: (i) clinical notes from the National Health Information Database (NHID) (ii) clinical notes from the National Health Information Exchange (NHIE) (iii) clinical notes from the National Health Information Exchange for Clinical Practice (NHICP) (iv) clinical notes from the National Health Information Exchange for Clinical Practice (NHICP) (v) clinical notes from the National Health Information Exchange for Clinical Practice (NHICP) (vi) clinical notes from the National Health Information Exchange for Clinical Practice (NHICP) (vii) clinical notes from the National Health\n\nAnswer B:\nMIMICII(I), THYME, results from i2b2 and ShARe/CLEF shared task, MiPACQ, Blulab, EMC Dutch Clinical Corpus, 2010 i2b2/VA, VetCompass. Explanation: An often used corpus is MIMICII(I)\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1703.10090",
    "question": "Other than privacy, what are the other major ethical challenges in clinical data?",
    "gold_answer": "",
    "answer_a": "Texts produced in the clinical settings do not always tell a complete or accurate patient story (e.g. due to time constraints or due to patient treatment in different hospitals), yet important decisions can be based on them. As language is situated, a lot of information may be implicit, such as the circumstances in which treatment decisions are made. Explanation: However, discrimination can occur when individuals or groups receive unfair treatment as a result of automated processing, which might be a result of biases in the data that were used to train models.",
    "answer_b": "Please provide examples.\n<sep><br/ >Answer: The second challenge is the lack of transparency in the data. The data is not publicly available, and the data scientists are not transparent about the data they use. The third challenge is the lack of accountability. The data scientists are not accountable for the data they use. The fourth challenge is the lack of transparency in the data. The data is not publicly available, and the data scientists are not transparent about the data they use. The fifth challenge is the lack of accountability. The data scientists are not accountable for the data they use. The sixth challenge is the lack of transparency in the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nOther than privacy, what are the other major ethical challenges in clinical data?\n\nGround Truth Answer:\n\n\nAnswer A:\nTexts produced in the clinical settings do not always tell a complete or accurate patient story (e.g. due to time constraints or due to patient treatment in different hospitals), yet important decisions can be based on them. As language is situated, a lot of information may be implicit, such as the circumstances in which treatment decisions are made. Explanation: However, discrimination can occur when individuals or groups receive unfair treatment as a result of automated processing, which might be a result of biases in the data that were used to train models.\n\nAnswer B:\nPlease provide examples.\n<sep><br/ >Answer: The second challenge is the lack of transparency in the data. The data is not publicly available, and the data scientists are not transparent about the data they use. The third challenge is the lack of accountability. The data scientists are not accountable for the data they use. The fourth challenge is the lack of transparency in the data. The data is not publicly available, and the data scientists are not transparent about the data they use. The fifth challenge is the lack of accountability. The data scientists are not accountable for the data they use. The sixth challenge is the lack of transparency in the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.10039",
    "question": "what evaluation metrics were used?",
    "gold_answer": "",
    "answer_a": "Explanation: We evaluate our model on the WIKIOG dataset, which consists of over 1.75 million document-outline pairs. We use the following evaluation metrics: Recall, Precision, F1, and BLEU. Recall is the proportion of the total number of documents that are correctly identified as outlines. Precision is the proportion of the number of correctly identified outlines that are actually outlines. F1 is the harmonic mean of precision and recall. BLEU is a metric that measures the quality of the generated outlines relative to the ground truth outlines. BLEU is calculated by comparing the generated outlines to the ground truth outlines and is defined as",
    "answer_b": "EM-outline, EM-sec, Rouge. Explanation: EM INLINEFORM0 : evaluates the overall accuracy of the generated outline based on exact matching.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat evaluation metrics were used?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We evaluate our model on the WIKIOG dataset, which consists of over 1.75 million document-outline pairs. We use the following evaluation metrics: Recall, Precision, F1, and BLEU. Recall is the proportion of the total number of documents that are correctly identified as outlines. Precision is the proportion of the number of correctly identified outlines that are actually outlines. F1 is the harmonic mean of precision and recall. BLEU is a metric that measures the quality of the generated outlines relative to the ground truth outlines. BLEU is calculated by comparing the generated outlines to the ground truth outlines and is defined as\n\nAnswer B:\nEM-outline, EM-sec, Rouge. Explanation: EM INLINEFORM0 : evaluates the overall accuracy of the generated outline based on exact matching.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.10039",
    "question": "what state of the art models did they compare with?",
    "gold_answer": "",
    "answer_a": "What is the performance of their model?\n< Explanation: We compare our model with the following state-of-the-art models: (1) the state-of-the-art sequential generation model, named SeqGen, which is based on the Markov chain model and is trained on the WIKIOG dataset. (2) the state-of-the-art sequential generation model, named SeqGen2, which is based on the Markov chain model and is trained on the WIKIOG dataset. (3) the state-of-the-art sequential generation model, named SeqGen3, which is based on the Markov chain model and is trained on the",
    "answer_b": "IG CRF+GHD. Explanation: FLOAT SELECTED: Table 4: Comparisons between our HiStGen and step-wise baselines in terms of EMoutline (%).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat state of the art models did they compare with?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of their model?\n< Explanation: We compare our model with the following state-of-the-art models: (1) the state-of-the-art sequential generation model, named SeqGen, which is based on the Markov chain model and is trained on the WIKIOG dataset. (2) the state-of-the-art sequential generation model, named SeqGen2, which is based on the Markov chain model and is trained on the WIKIOG dataset. (3) the state-of-the-art sequential generation model, named SeqGen3, which is based on the Markov chain model and is trained on the\n\nAnswer B:\nIG CRF+GHD. Explanation: FLOAT SELECTED: Table 4: Comparisons between our HiStGen and step-wise baselines in terms of EMoutline (%).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1704.06851",
    "question": "How to extract affect attributes from the sentence?",
    "gold_answer": "",
    "answer_a": "What are the affect categories?\n< Explanation: We use the Stanford Sentiment Treebank (SST) BIBREF10 to extract affective attributes from the sentence. We use the Stanford Sentiment Treebank (SST) BIBREF10 to extract affective attributes from the sentence. We use the Stanford Sentiment Treebank (SST) BIBREF10 to extract affective attributes from the sentence. We use the Stanford Sentiment Treebank (SST) BIBREF10 to extract affective attributes from the sentence. We use the Stanford Sentiment Treebank (SST) BIBREF",
    "answer_b": "Using a dictionary of emotional words, LIWC, they perform keyword spotting.. Explanation: Specifically for model training, the affect category is derived from features generated using keyword spotting from a dictionary of emotional words, such as the LIWC (Linguistic Inquiry and Word Count) tool BIBREF11 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow to extract affect attributes from the sentence?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the affect categories?\n< Explanation: We use the Stanford Sentiment Treebank (SST) BIBREF10 to extract affective attributes from the sentence. We use the Stanford Sentiment Treebank (SST) BIBREF10 to extract affective attributes from the sentence. We use the Stanford Sentiment Treebank (SST) BIBREF10 to extract affective attributes from the sentence. We use the Stanford Sentiment Treebank (SST) BIBREF10 to extract affective attributes from the sentence. We use the Stanford Sentiment Treebank (SST) BIBREF\n\nAnswer B:\nUsing a dictionary of emotional words, LIWC, they perform keyword spotting.. Explanation: Specifically for model training, the affect category is derived from features generated using keyword spotting from a dictionary of emotional words, such as the LIWC (Linguistic Inquiry and Word Count) tool BIBREF11 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.06815",
    "question": "Which BERT-based baselines do they compare to?",
    "gold_answer": "",
    "answer_a": "What are their results?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Baseline 1</td>\n<td align=\"left\">Baseline 2</td>\n<td align=\"left\">Baseline 3</td>\n<td align=\"left\">Baseline 4</td>\n<td align=\"left\">Baseline 5</td>\n<td align=\"left\">Baseline 6</td>\n<td align=\"left\">Baseline 7</td>\n<td align=\"left\">Baseline 8</td>\n<td align=\"left\">Baseline 9</td>\n<td align=\"left\">Baseline 10</",
    "answer_b": "BERT. We add a linear layer on top of BERT and we fine-tune it. Explanation: We depart from BERT BIBREF12, and we design three baselines.\n\nBERT. We add a linear layer on top of BERT and we fine-tune it, as suggested in BIBREF12. For the FLC task, we feed the final hidden representation for each token to a layer $L_{g_2}$ that makes a 19-way classification: does this token belong to one of the eighteen propaganda techniques or to none of them (cf. Figure FIGREF7-a). For the SLC task, we feed the final hidden representation for the special [CLS] token, which BERT uses to represent the full sentence, to a two-dimensional layer $L_{g_1}$ to make a binary classification.\n\nBERT-Joint. We use the layers for both tasks in the BERT baseline, $L_{g_1}$ and $L_{g_2}$, and we train for both FLC and SLC jointly (cf. Figure FIGREF7-b).\n\nBERT-Granularity. We modify BERT-Joint to transfer information from SLC directly to FLC. Instead of using only the $L_{g_2}$ layer for FLC, we concatenate $L_{g_1}$ and $L_{g_2}$, and we add an extra 19-dimensional classification layer $L_{g_{1,2}}$ on top of that concatenation to perform the prediction for FLC (cf. Figure FIGREF7-c).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich BERT-based baselines do they compare to?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are their results?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Baseline 1</td>\n<td align=\"left\">Baseline 2</td>\n<td align=\"left\">Baseline 3</td>\n<td align=\"left\">Baseline 4</td>\n<td align=\"left\">Baseline 5</td>\n<td align=\"left\">Baseline 6</td>\n<td align=\"left\">Baseline 7</td>\n<td align=\"left\">Baseline 8</td>\n<td align=\"left\">Baseline 9</td>\n<td align=\"left\">Baseline 10</\n\nAnswer B:\nBERT. We add a linear layer on top of BERT and we fine-tune it. Explanation: We depart from BERT BIBREF12, and we design three baselines.\n\nBERT. We add a linear layer on top of BERT and we fine-tune it, as suggested in BIBREF12. For the FLC task, we feed the final hidden representation for each token to a layer $L_{g_2}$ that makes a 19-way classification: does this token belong to one of the eighteen propaganda techniques or to none of them (cf. Figure FIGREF7-a). For the SLC task, we feed the final hidden representation for the special [CLS] token, which BERT uses to represent the full sentence, to a two-dimensional layer $L_{g_1}$ to make a binary classification.\n\nBERT-Joint. We use the layers for both tasks in the BERT baseline, $L_{g_1}$ and $L_{g_2}$, and we train for both FLC and SLC jointly (cf. Figure FIGREF7-b).\n\nBERT-Granularity. We modify BERT-Joint to transfer information from SLC directly to FLC. Instead of using only the $L_{g_2}$ layer for FLC, we concatenate $L_{g_1}$ and $L_{g_2}$, and we add an extra 19-dimensional classification layer $L_{g_{1,2}}$ on top of that concatenation to perform the prediction for FLC (cf. Figure FIGREF7-c).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.06815",
    "question": "What are the propaganda types?",
    "gold_answer": "",
    "answer_a": "annotated according to eighteen persuasion techniques BIBREF4. Explanation: We retrieved 451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques BIBREF4, ranging from leveraging on the emotions of the audience \u2014such as using loaded language or appeal to authority BIBREF5 and slogans BIBREF6\u2014 to using logical fallacies \u2014such as straw men BIBREF7 (misrepresenting someone's opinion), hidden ad-hominem fallacies, and red herring BIBREF8 (presenting irrelevant data).",
    "answer_b": "How are they detected?\n<sep><br/ >The propaganda techniques are: 1) the use of emoticons, 2) the use of hyperlinks, 3) the use of emoticons and hyperlinks, 4) the use of emoticons, hyperlinks, and images, 5) the use of emoticons, hyperlinks, images, and videos, 6) the use of emoticons, hyperlinks, images, videos, and images, 7) the use of emoticons, hyperlinks, images, videos, and images, 8) the use of emoticons, hyperlinks,",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the propaganda types?\n\nGround Truth Answer:\n\n\nAnswer A:\nannotated according to eighteen persuasion techniques BIBREF4. Explanation: We retrieved 451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques BIBREF4, ranging from leveraging on the emotions of the audience \u2014such as using loaded language or appeal to authority BIBREF5 and slogans BIBREF6\u2014 to using logical fallacies \u2014such as straw men BIBREF7 (misrepresenting someone's opinion), hidden ad-hominem fallacies, and red herring BIBREF8 (presenting irrelevant data).\n\nAnswer B:\nHow are they detected?\n<sep><br/ >The propaganda techniques are: 1) the use of emoticons, 2) the use of hyperlinks, 3) the use of emoticons and hyperlinks, 4) the use of emoticons, hyperlinks, and images, 5) the use of emoticons, hyperlinks, images, and videos, 6) the use of emoticons, hyperlinks, images, videos, and images, 7) the use of emoticons, hyperlinks, images, videos, and images, 8) the use of emoticons, hyperlinks,\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.06815",
    "question": "What datasets did they use in their experiment?",
    "gold_answer": "",
    "answer_a": "How did they evaluate their results?\n< Explanation: We use the following datasets: (1) the 2016 US Presidential election dataset, which contains 100,000 tweets from the 2016 US Presidential election, and (2) the 2018 Brazilian election dataset, which contains 100,000 tweets from the 2018 Brazilian election. We use the Twitter API to download the data. We use the Twitter API to download the data. We use the Twitter API to download the data. We use the Twitter API to download the data. We",
    "answer_b": "retrieved 451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques. Explanation: We retrieved 451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques BIBREF4, ranging from leveraging on the emotions of the audience \u2014such as using loaded language or appeal to authority BIBREF5 and slogans BIBREF6\u2014 to using logical fallacies \u2014such as straw men BIBREF7 (misrepresenting someone's opinion), hidden ad-hominem fallacies, and red herring BIBREF8 (presenting irrelevant data).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets did they use in their experiment?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow did they evaluate their results?\n< Explanation: We use the following datasets: (1) the 2016 US Presidential election dataset, which contains 100,000 tweets from the 2016 US Presidential election, and (2) the 2018 Brazilian election dataset, which contains 100,000 tweets from the 2018 Brazilian election. We use the Twitter API to download the data. We use the Twitter API to download the data. We use the Twitter API to download the data. We use the Twitter API to download the data. We\n\nAnswer B:\nretrieved 451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques. Explanation: We retrieved 451 news articles from 48 news outlets, both propagandistic and non-propagandistic according to Media Bias/Fact Check, which professionals annotators annotated according to eighteen persuasion techniques BIBREF4, ranging from leveraging on the emotions of the audience \u2014such as using loaded language or appeal to authority BIBREF5 and slogans BIBREF6\u2014 to using logical fallacies \u2014such as straw men BIBREF7 (misrepresenting someone's opinion), hidden ad-hominem fallacies, and red herring BIBREF8 (presenting irrelevant data).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1704.08390",
    "question": "What size ngram models performed best? e.g. bigram, trigram, etc.",
    "gold_answer": "",
    "answer_a": "?\n<sep><table cellspacing=\"0\" cellpadding=\"0\" width=\"600\" border=\"0\" align=\"center\">\n<tr>\n<td>\nFigure 1: Performance of the system in Subtask A and B\n</td>\n</tr>\n<tr>\n<td>\nFigure 2: Performance of the system in Subtask C\n</td>\n</tr>\n<tr>\n<td>\nFigure 3: Performance of the system in Subtask D\n</td>\n</tr>\n<tr>\n<td>\nFigure 4: Performance of the system in Subtask E\n</td>\n</tr>\n<tr>\n<td>\nFigure 5",
    "answer_b": "bigram. Explanation: Contrary to what we observed in the development data, the bigram language model actually performed somewhat better than the trigram language model.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat size ngram models performed best? e.g. bigram, trigram, etc.\n\nGround Truth Answer:\n\n\nAnswer A:\n?\n<sep><table cellspacing=\"0\" cellpadding=\"0\" width=\"600\" border=\"0\" align=\"center\">\n<tr>\n<td>\nFigure 1: Performance of the system in Subtask A and B\n</td>\n</tr>\n<tr>\n<td>\nFigure 2: Performance of the system in Subtask C\n</td>\n</tr>\n<tr>\n<td>\nFigure 3: Performance of the system in Subtask D\n</td>\n</tr>\n<tr>\n<td>\nFigure 4: Performance of the system in Subtask E\n</td>\n</tr>\n<tr>\n<td>\nFigure 5\n\nAnswer B:\nbigram. Explanation: Contrary to what we observed in the development data, the bigram language model actually performed somewhat better than the trigram language model.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1704.08390",
    "question": "How were the ngram models used to generate predictions on the data?",
    "gold_answer": "",
    "answer_a": "What were the results?\n<blockquote>Our system used a 1000 word n-gram model to generate predictions for each tweet. The model was trained on the 2016 Twitter dataset, which contained 100000 tweets. The model was trained using the 2017 Twitter dataset, which contained 100000 tweets. The model was trained using the 2018 Twitter dataset, which contained 100000 tweets. The model was trained using the 2019 Twitter dataset, which contained 100000 tweets",
    "answer_b": "The n-gram models were used to calculate the logarithm of the probability for each tweet. Explanation: A trigram Language Model (LM) predicts the conditional probability of the next word using the following approximation: DISPLAYFORM0",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow were the ngram models used to generate predictions on the data?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat were the results?\n<blockquote>Our system used a 1000 word n-gram model to generate predictions for each tweet. The model was trained on the 2016 Twitter dataset, which contained 100000 tweets. The model was trained using the 2017 Twitter dataset, which contained 100000 tweets. The model was trained using the 2018 Twitter dataset, which contained 100000 tweets. The model was trained using the 2019 Twitter dataset, which contained 100000 tweets\n\nAnswer B:\nThe n-gram models were used to calculate the logarithm of the probability for each tweet. Explanation: A trigram Language Model (LM) predicts the conditional probability of the next word using the following approximation: DISPLAYFORM0\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1704.08390",
    "question": "What package was used to build the ngram language models?",
    "gold_answer": "",
    "answer_a": "Explanation: The n-gram language models were built using the following packages: NLTK (version 3.4.1) and SpaCy (version 2.0.0). The models were trained using the following parameters: ngram_size=1, ngram_range=(1,1), ngram_range=(1,1), ngram_range=(1,1), ngram_range=(1,1), ngram_range=(1,1), ngram_range=(1,1), ngram_range=(1,1), ngram_range=(1,1), ngram_range=(1,1), ngram_range",
    "answer_b": "KenLM Toolkit. Explanation: Once we had the corpora ready, we used the KenLM Toolkit to train the N-gram language models on each corpus.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat package was used to build the ngram language models?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The n-gram language models were built using the following packages: NLTK (version 3.4.1) and SpaCy (version 2.0.0). The models were trained using the following parameters: ngram_size=1, ngram_range=(1,1), ngram_range=(1,1), ngram_range=(1,1), ngram_range=(1,1), ngram_range=(1,1), ngram_range=(1,1), ngram_range=(1,1), ngram_range=(1,1), ngram_range=(1,1), ngram_range\n\nAnswer B:\nKenLM Toolkit. Explanation: Once we had the corpora ready, we used the KenLM Toolkit to train the N-gram language models on each corpus.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1704.08390",
    "question": "What rank did the language model system achieve in the task evaluation?",
    "gold_answer": "",
    "answer_a": "4th place on SubtaskA; 1st place on Subtask B. Explanation: FLOAT SELECTED: Table 3: Evaluation results (bold) and post-evaluation results based on evaluation dir data. The trigram LM trained on the news data ranked 4th place on Subtask A and 1st place on Subtask B.",
    "answer_b": "Explanation: The system achieved the highest score of 1.00 on the task evaluation. The system achieved the highest score of 1.00 on the task evaluation. The system achieved the highest score of 1.00 on the task evaluation. The system achieved the highest score of 1.00 on the task evaluation. The system achieved the highest score of 1.00 on the task evaluation. The system achieved the highest score of 1.00 on the task evaluation. The system achieved the highest score of 1.00 on the task evaluation. The system achieved the highest score of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat rank did the language model system achieve in the task evaluation?\n\nGround Truth Answer:\n\n\nAnswer A:\n4th place on SubtaskA; 1st place on Subtask B. Explanation: FLOAT SELECTED: Table 3: Evaluation results (bold) and post-evaluation results based on evaluation dir data. The trigram LM trained on the news data ranked 4th place on Subtask A and 1st place on Subtask B.\n\nAnswer B:\nExplanation: The system achieved the highest score of 1.00 on the task evaluation. The system achieved the highest score of 1.00 on the task evaluation. The system achieved the highest score of 1.00 on the task evaluation. The system achieved the highest score of 1.00 on the task evaluation. The system achieved the highest score of 1.00 on the task evaluation. The system achieved the highest score of 1.00 on the task evaluation. The system achieved the highest score of 1.00 on the task evaluation. The system achieved the highest score of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1704.08390",
    "question": "What were subtasks A and B?",
    "gold_answer": "",
    "answer_a": "What was the evaluation metric?\n< Explanation: Subtask A: Learning a Sense of Humor. The system was trained on the 2016 dataset and tested on the 2017 dataset. The system was trained on the 2016 dataset and tested on the 2017 dataset. The system was trained on the 2016 dataset and tested on the 2017 dataset. The system was trained on the 2016 dataset and tested on the 2017 dataset. The system was trained on the 2016 dataset and tested on the",
    "answer_b": "For Subtask A, the system goes through the sorted list of tweets in a hashtag file and compares each pair of tweets.. Explanation: For Subtask A, the system goes through the sorted list of tweets in a hashtag file and compares each pair of tweets.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat were subtasks A and B?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat was the evaluation metric?\n< Explanation: Subtask A: Learning a Sense of Humor. The system was trained on the 2016 dataset and tested on the 2017 dataset. The system was trained on the 2016 dataset and tested on the 2017 dataset. The system was trained on the 2016 dataset and tested on the 2017 dataset. The system was trained on the 2016 dataset and tested on the 2017 dataset. The system was trained on the 2016 dataset and tested on the\n\nAnswer B:\nFor Subtask A, the system goes through the sorted list of tweets in a hashtag file and compares each pair of tweets.. Explanation: For Subtask A, the system goes through the sorted list of tweets in a hashtag file and compares each pair of tweets.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.03642",
    "question": "How does counterfactual data augmentation affect gender bias in predictions and performance?",
    "gold_answer": "",
    "answer_a": "mitigates these contextual biases. Explanation: Counterfactual Data Augmentation (CDA) mitigates these contextual biases. CDA consists of replacing masculine words in a sentence with their corresponding feminine words and vice versa for all sentences in a corpus, then training on the union of the original and augmented corpora .",
    "answer_b": "What is the effect of hard debiasing on gender bias in predictions and performance?\n< Explanation: We first introduce a new dataset, WikiGenderBias, which contains sentences from Wikipedia that are specifically curated to analyze gender bias in relation extraction systems. We use this dataset to evaluate the performance of two state-of-the-art systems for relation extraction, namely, Relation Extraction with Neural Networks (RENN) and Relation Extraction with Neural Networks (RENN) with a human annotated test set. We also introduce a new dataset, WikiGenderBias, which contains sentences specifically curated to analyze gender bias in relation extraction systems. We use this dataset to evaluate the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does counterfactual data augmentation affect gender bias in predictions and performance?\n\nGround Truth Answer:\n\n\nAnswer A:\nmitigates these contextual biases. Explanation: Counterfactual Data Augmentation (CDA) mitigates these contextual biases. CDA consists of replacing masculine words in a sentence with their corresponding feminine words and vice versa for all sentences in a corpus, then training on the union of the original and augmented corpora .\n\nAnswer B:\nWhat is the effect of hard debiasing on gender bias in predictions and performance?\n< Explanation: We first introduce a new dataset, WikiGenderBias, which contains sentences from Wikipedia that are specifically curated to analyze gender bias in relation extraction systems. We use this dataset to evaluate the performance of two state-of-the-art systems for relation extraction, namely, Relation Extraction with Neural Networks (RENN) and Relation Extraction with Neural Networks (RENN) with a human annotated test set. We also introduce a new dataset, WikiGenderBias, which contains sentences specifically curated to analyze gender bias in relation extraction systems. We use this dataset to evaluate the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.03642",
    "question": "How does hard debiasing affect gender bias in prediction and performance?",
    "gold_answer": "",
    "answer_a": "mitigating the difference in F1 scores for all relations. Explanation: Hard-Debiased Word Embeddings was also extremely effective at mitigating the difference in F1 scores for all relations. While gender-swapping did slightly better at decreasing that difference for the spouse relation, debiased embeddings mitigated bias better for the birthDate and hypernym relations. We note that using debiased embeddings increases absolute scores just like gender-swapping, though it increases them slightly less.",
    "answer_b": "What is the effect of counterfactual data augmentation?\n< Explanation: We first introduce a simple debiasing technique, hard debiasing, to our system. We use the word embeddings of the system to create a new embedding for each word in the system. We then use this new embedding to create a new embedding for each word in the test set. We then compare the embeddings of the test set to the embeddings of the training set. We find that the embeddings of the test set are significantly different from the embeddings of the training set. We then use this new embedding to create a new embedding for each word in the test set.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does hard debiasing affect gender bias in prediction and performance?\n\nGround Truth Answer:\n\n\nAnswer A:\nmitigating the difference in F1 scores for all relations. Explanation: Hard-Debiased Word Embeddings was also extremely effective at mitigating the difference in F1 scores for all relations. While gender-swapping did slightly better at decreasing that difference for the spouse relation, debiased embeddings mitigated bias better for the birthDate and hypernym relations. We note that using debiased embeddings increases absolute scores just like gender-swapping, though it increases them slightly less.\n\nAnswer B:\nWhat is the effect of counterfactual data augmentation?\n< Explanation: We first introduce a simple debiasing technique, hard debiasing, to our system. We use the word embeddings of the system to create a new embedding for each word in the system. We then use this new embedding to create a new embedding for each word in the test set. We then compare the embeddings of the test set to the embeddings of the training set. We find that the embeddings of the test set are significantly different from the embeddings of the training set. We then use this new embedding to create a new embedding for each word in the test set.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.03642",
    "question": "How does name anonymization affect gender bias in predictions and performance?",
    "gold_answer": "",
    "answer_a": "Name Anonymization surprisingly substantially increases F1 score gap for the hypernym relation, but slightly decreases F1 score gap for all other relations.. Explanation: Name Anonymization surprisingly substantially increases F1 score gap for the hypernym relation, but slightly decreases F1 score gap for all other relations. Name Anonymization appears to be effective at debiasing all relations aside from hypernym, though not as effective as either Gender-Swapping or using Debiased Embeddings. These results indicate that entity bias likely does not contribute very much the gender bias in the models' original predictions.",
    "answer_b": "What is the effect of hard debiasing for word embeddings?\n< Explanation: We first introduce a simple method for name anonymization, which is to replace the name with a random string of characters. We then introduce a simple method for hard debiasing for word embeddings, which is to replace the word with a random string of characters. We then introduce a simple method for counterfactual data augmentation, which is to replace the word with a random string of characters. We then introduce a simple method for name anonymization, which is to replace the name with a random string of characters. We then introduce a simple method for hard debias",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does name anonymization affect gender bias in predictions and performance?\n\nGround Truth Answer:\n\n\nAnswer A:\nName Anonymization surprisingly substantially increases F1 score gap for the hypernym relation, but slightly decreases F1 score gap for all other relations.. Explanation: Name Anonymization surprisingly substantially increases F1 score gap for the hypernym relation, but slightly decreases F1 score gap for all other relations. Name Anonymization appears to be effective at debiasing all relations aside from hypernym, though not as effective as either Gender-Swapping or using Debiased Embeddings. These results indicate that entity bias likely does not contribute very much the gender bias in the models' original predictions.\n\nAnswer B:\nWhat is the effect of hard debiasing for word embeddings?\n< Explanation: We first introduce a simple method for name anonymization, which is to replace the name with a random string of characters. We then introduce a simple method for hard debiasing for word embeddings, which is to replace the word with a random string of characters. We then introduce a simple method for counterfactual data augmentation, which is to replace the word with a random string of characters. We then introduce a simple method for name anonymization, which is to replace the name with a random string of characters. We then introduce a simple method for hard debias\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.03642",
    "question": "How are the sentences in WikiGenderBias curated?",
    "gold_answer": "",
    "answer_a": "What is the human annotated test set?\n< Explanation: We first created a dataset of 1000 sentences from Wikipedia articles that were annotated for gender bias. We then manually annotated 1000 sentences for gender bias. We then removed 1000 sentences from the dataset that were not annotated for gender bias. We then manually annotated 1000 sentences for gender bias. We then removed 1000 sentences from the dataset that were not annotated for gender bias. We then manually annotated 1000 sentences for gender bias. We then removed 1000 sentences from the dataset that",
    "answer_b": "Distant Supervision assumption. Explanation: To generate WikiGenderBias, we use a variant of the Distant Supervision assumption: for a given relation between two entities, assume that any sentence from an article written about one of those entities that mentions the other entity expresses the relation.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are the sentences in WikiGenderBias curated?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the human annotated test set?\n< Explanation: We first created a dataset of 1000 sentences from Wikipedia articles that were annotated for gender bias. We then manually annotated 1000 sentences for gender bias. We then removed 1000 sentences from the dataset that were not annotated for gender bias. We then manually annotated 1000 sentences for gender bias. We then removed 1000 sentences from the dataset that were not annotated for gender bias. We then manually annotated 1000 sentences for gender bias. We then removed 1000 sentences from the dataset that\n\nAnswer B:\nDistant Supervision assumption. Explanation: To generate WikiGenderBias, we use a variant of the Distant Supervision assumption: for a given relation between two entities, assume that any sentence from an article written about one of those entities that mentions the other entity expresses the relation.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.01214",
    "question": "what crowdsourcing platform did they use?",
    "gold_answer": "",
    "answer_a": "Explanation: We used the crowdsourcing platform GitHub to collect the data. We collected 1000 documents from the GitHub repository \u201cfact-checker\u201d and 1000 documents from the GitHub repository \u201cfact-checker-2\u201d. We then manually annotated the documents with the following tasks: document retrieval, evidence extraction, stance detection, and claim validation. We manually annotated the documents with the following tasks: document retrieval, evidence extraction, stance detection, and claim validation. We manually annotated the documents with the following tasks: document retrieval, evidence extraction, stance detection, and claim validation. We manually annotated the documents with the following tasks",
    "answer_b": "Amazon Mechanical Turk. Explanation: We asked crowd workers on Amazon Mechanical Turk to annotate whether an ETS agrees with the claim, refutes it, or has no stance towards the claim.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat crowdsourcing platform did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We used the crowdsourcing platform GitHub to collect the data. We collected 1000 documents from the GitHub repository \u201cfact-checker\u201d and 1000 documents from the GitHub repository \u201cfact-checker-2\u201d. We then manually annotated the documents with the following tasks: document retrieval, evidence extraction, stance detection, and claim validation. We manually annotated the documents with the following tasks: document retrieval, evidence extraction, stance detection, and claim validation. We manually annotated the documents with the following tasks: document retrieval, evidence extraction, stance detection, and claim validation. We manually annotated the documents with the following tasks\n\nAnswer B:\nAmazon Mechanical Turk. Explanation: We asked crowd workers on Amazon Mechanical Turk to annotate whether an ETS agrees with the claim, refutes it, or has no stance towards the claim.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.01214",
    "question": "where does their data come from?",
    "gold_answer": "",
    "answer_a": "Snopes. Explanation: Snopes is a large-scale fact-checking platform that employs human fact-checkers to validate claims.",
    "answer_b": "Is it publicly available?\n< Explanation: We collected our corpus from the web using the fact-checking tool FactCheckBot. The tool is available at https://github.com/abraham/FACTCHECKBOT. The corpus contains 1,000,000 documents, of which 100,000 are in English and 900,000 are in Spanish. The corpus is annotated with 100,000 documents, of which 90,000 are in English and 10,000 are in Spanish. The corpus is divided into",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhere does their data come from?\n\nGround Truth Answer:\n\n\nAnswer A:\nSnopes. Explanation: Snopes is a large-scale fact-checking platform that employs human fact-checkers to validate claims.\n\nAnswer B:\nIs it publicly available?\n< Explanation: We collected our corpus from the web using the fact-checking tool FactCheckBot. The tool is available at https://github.com/abraham/FACTCHECKBOT. The corpus contains 1,000,000 documents, of which 100,000 are in English and 900,000 are in Spanish. The corpus is annotated with 100,000 documents, of which 90,000 are in English and 10,000 are in Spanish. The corpus is divided into\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.01214",
    "question": "which existing corpora do they compare with?",
    "gold_answer": "",
    "answer_a": "What are their results?\nThey compare with the following corpora: (1) the 2016 Fact-Check Corpus (FCC) BIBREF1 , (2) the 2017 Fact-Check Corpus (FCC) BIBREF2 , (3) the 2018 Fact-Check Corpus (FCC) BIBREF3 , (4) the 2019 Fact-Check Corpus (FCC) BIBREF4 , (5) the 2020 Fact-Check Corpus (FCC) BIBREF5 , (6) the",
    "answer_b": "FEVER shared task BIBREF7 and the Fake News Challenge BIBREF8. Explanation: For evidence extraction, stance detection, and claim validation we evaluate the performance of high-scoring systems from the FEVER shared task BIBREF7 and the Fake News Challenge BIBREF8 as well as the Bidirectional Transformer model BERT BIBREF9 on our data.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhich existing corpora do they compare with?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are their results?\nThey compare with the following corpora: (1) the 2016 Fact-Check Corpus (FCC) BIBREF1 , (2) the 2017 Fact-Check Corpus (FCC) BIBREF2 , (3) the 2018 Fact-Check Corpus (FCC) BIBREF3 , (4) the 2019 Fact-Check Corpus (FCC) BIBREF4 , (5) the 2020 Fact-Check Corpus (FCC) BIBREF5 , (6) the\n\nAnswer B:\nFEVER shared task BIBREF7 and the Fake News Challenge BIBREF8. Explanation: For evidence extraction, stance detection, and claim validation we evaluate the performance of high-scoring systems from the FEVER shared task BIBREF7 and the Fake News Challenge BIBREF8 as well as the Bidirectional Transformer model BERT BIBREF9 on our data.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.01214",
    "question": "what is the size of their corpus?",
    "gold_answer": "",
    "answer_a": "How is it divided?\n< Explanation: The corpus contains 1,000,000 documents, of which 100,000 are annotated. The remaining 900,000 documents are unannotated. The corpus is divided into 100,000 annotated documents, 10,000 unannotated documents, and 80,000 unannotated documents. The unannotated documents are used to train a baseline model for each task. The unannotated documents are then used to train a baseline model for each task. The baseline",
    "answer_b": "6,422. Explanation: Our corpus consists of 6,422 validated claims with comprehensive annotations based on the data collected by Snopes fact-checkers and our crowd-workers.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the size of their corpus?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it divided?\n< Explanation: The corpus contains 1,000,000 documents, of which 100,000 are annotated. The remaining 900,000 documents are unannotated. The corpus is divided into 100,000 annotated documents, 10,000 unannotated documents, and 80,000 unannotated documents. The unannotated documents are used to train a baseline model for each task. The unannotated documents are then used to train a baseline model for each task. The baseline\n\nAnswer B:\n6,422. Explanation: Our corpus consists of 6,422 validated claims with comprehensive annotations based on the data collected by Snopes fact-checkers and our crowd-workers.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.01214",
    "question": "which architectures did they experiment with?",
    "gold_answer": "",
    "answer_a": "Is there any difference in performance between the baselines and the models they experimented with?\n< Explanation: We experimented with a number of different architectures for the document retrieval task, including a simple neural network, a transformer-based model, and a transformer-based model with a pre-trained language model. We also experimented with a number of different architectures for the evidence extraction task, including a simple neural network, a transformer-based model, and a transformer-based model with a pre-trained language model. We also experimented with a number of different architectures for the stance detection task, including a simple neural network, a transformer-based model, and a transformer-based model with a",
    "answer_b": "For stance detection they used MLP, for evidence extraction they used Tf-idf and BiLSTM, for claim validation they used MLP, BiLSTM and SVM. Explanation: Experiments and error analysis ::: Stance detection ::: Models and Results",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhich architectures did they experiment with?\n\nGround Truth Answer:\n\n\nAnswer A:\nIs there any difference in performance between the baselines and the models they experimented with?\n< Explanation: We experimented with a number of different architectures for the document retrieval task, including a simple neural network, a transformer-based model, and a transformer-based model with a pre-trained language model. We also experimented with a number of different architectures for the evidence extraction task, including a simple neural network, a transformer-based model, and a transformer-based model with a pre-trained language model. We also experimented with a number of different architectures for the stance detection task, including a simple neural network, a transformer-based model, and a transformer-based model with a\n\nAnswer B:\nFor stance detection they used MLP, for evidence extraction they used Tf-idf and BiLSTM, for claim validation they used MLP, BiLSTM and SVM. Explanation: Experiments and error analysis ::: Stance detection ::: Models and Results\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.01214",
    "question": "what domains are present in the corpus?",
    "gold_answer": "",
    "answer_a": "corpus covers multiple domains, including discussion blogs, news, and social media. Explanation: The corpus covers multiple domains, including discussion blogs, news, and social media, which are often found responsible for the creation and distribution of unreliable information.",
    "answer_b": "Explanation: We present a corpus of 10,000 documents, each containing 1000 words. The documents are divided into 10 domains, each containing 1000 words. The domains are: 1) news, 2) politics, 3) science, 4) health, 5) entertainment, 6) sports, 7) crime, 8) religion, 9) education, and 10) other. Each document is annotated with 1000 keywords, and the keywords are used to create 1000 document-level facts.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat domains are present in the corpus?\n\nGround Truth Answer:\n\n\nAnswer A:\ncorpus covers multiple domains, including discussion blogs, news, and social media. Explanation: The corpus covers multiple domains, including discussion blogs, news, and social media, which are often found responsible for the creation and distribution of unreliable information.\n\nAnswer B:\nExplanation: We present a corpus of 10,000 documents, each containing 1000 words. The documents are divided into 10 domains, each containing 1000 words. The domains are: 1) news, 2) politics, 3) science, 4) health, 5) entertainment, 6) sports, 7) crime, 8) religion, 9) education, and 10) other. Each document is annotated with 1000 keywords, and the keywords are used to create 1000 document-level facts.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.01214",
    "question": "what was the inter-annotator agreement?",
    "gold_answer": "",
    "answer_a": "For stance annotation the inter-annotator agreement was 0.7, for FGE annotation inter-annotator agreement was 0.55. Explanation: Stance annotation.",
    "answer_b": "How was it measured?\n< Explanation: We measured the inter-annotator agreement using the kappa statistic. The kappa statistic is a measure of agreement between two raters. It is defined as the proportion of pairs of raters that agree on the same item, divided by the total number of pairs of raters. The kappa statistic is a measure of agreement between two raters. It is defined as the proportion of pairs of raters that agree on the same item, divided by the total number of pairs of raters. The kappa statistic is a measure of agreement between two raters. It is defined as the proportion of pairs of rat",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat was the inter-annotator agreement?\n\nGround Truth Answer:\n\n\nAnswer A:\nFor stance annotation the inter-annotator agreement was 0.7, for FGE annotation inter-annotator agreement was 0.55. Explanation: Stance annotation.\n\nAnswer B:\nHow was it measured?\n< Explanation: We measured the inter-annotator agreement using the kappa statistic. The kappa statistic is a measure of agreement between two raters. It is defined as the proportion of pairs of raters that agree on the same item, divided by the total number of pairs of raters. The kappa statistic is a measure of agreement between two raters. It is defined as the proportion of pairs of raters that agree on the same item, divided by the total number of pairs of raters. The kappa statistic is a measure of agreement between two raters. It is defined as the proportion of pairs of rat\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1701.03578",
    "question": "Which metrics are used for quantitative analysis?",
    "gold_answer": "",
    "answer_a": "perplexity. Explanation: Figure 1 shows the validation curve while training. Perplexity values from various model output are plotted.",
    "answer_b": "What is the difference between the two metrics?\n< Explanation: We use the following metrics to evaluate the performance of the personalized language model: (1) BLEU (Bilingual Evaluation Framework) BIBREF10 , (2) ROUGE (Recall-Oriented Under-Confidence Evaluation) BIBREF11 , and (3) METEOR (Metric for Evaluating the Reliability of Named Entity Recognition) BIBREF12 . The BLEU metric is used to evaluate the quality of the generated text, while the ROUGE metric is used to evaluate the quality of the generated text in terms of the number",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich metrics are used for quantitative analysis?\n\nGround Truth Answer:\n\n\nAnswer A:\nperplexity. Explanation: Figure 1 shows the validation curve while training. Perplexity values from various model output are plotted.\n\nAnswer B:\nWhat is the difference between the two metrics?\n< Explanation: We use the following metrics to evaluate the performance of the personalized language model: (1) BLEU (Bilingual Evaluation Framework) BIBREF10 , (2) ROUGE (Recall-Oriented Under-Confidence Evaluation) BIBREF11 , and (3) METEOR (Metric for Evaluating the Reliability of Named Entity Recognition) BIBREF12 . The BLEU metric is used to evaluate the quality of the generated text, while the ROUGE metric is used to evaluate the quality of the generated text in terms of the number\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1701.03578",
    "question": "What dataset did they use?",
    "gold_answer": "",
    "answer_a": "How did they evaluate the performance of the personalized language model?\n< Explanation: We use the Dialogue Dataset (DD) [1] to evaluate the performance of the personalized language model. The DD is a dialogue dataset that contains 10000 utterances from 1000 users. Each utterance is a dialogue between a user and a computer. The dialogue is divided into 10000 utterances, each of which is a dialogue between a user and a computer. Each utterance is a dialogue between a user and a computer. The dialogue is divided into 10000 utterances, each",
    "answer_b": "Workshop on Statistical Machine Translation (WMT) data. Explanation: In experiments, we trained a general language model with huge corpus such as an Workshop on Statistical Machine Translation (WMT) data and a movie script data by using powerful computing machines, and then transferred the model to target environment for updating to be a personalized language model.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat dataset did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow did they evaluate the performance of the personalized language model?\n< Explanation: We use the Dialogue Dataset (DD) [1] to evaluate the performance of the personalized language model. The DD is a dialogue dataset that contains 10000 utterances from 1000 users. Each utterance is a dialogue between a user and a computer. The dialogue is divided into 10000 utterances, each of which is a dialogue between a user and a computer. Each utterance is a dialogue between a user and a computer. The dialogue is divided into 10000 utterances, each\n\nAnswer B:\nWorkshop on Statistical Machine Translation (WMT) data. Explanation: In experiments, we trained a general language model with huge corpus such as an Workshop on Statistical Machine Translation (WMT) data and a movie script data by using powerful computing machines, and then transferred the model to target environment for updating to be a personalized language model.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1701.03578",
    "question": "What metric did they use for qualitative evaluation?",
    "gold_answer": "",
    "answer_a": "Sample model output. Explanation: FLOAT SELECTED: Table 2: Sample model output of general language model and personalized language model. The general language model used WMT\u201914 data, personalized language model 1 used \u201cFriends\u201d drama data, and personalized language model 2 used the English bible data. Scheme 1 to scheme 3 are relearn whole, surplus layer and fixed-n layer, respectively. The output was generated with the given input sequence, \u201cIt is possible, however\u201d",
    "answer_b": "What metric did they use for quantitative evaluation?\n< Explanation: We use the following metrics to evaluate the personalized language model. The qualitative evaluation is based on the user's opinion. The quantitative evaluation is based on the user's rating. The user's rating is a numerical value between 1 and 5. The higher the rating, the more satisfied the user is with the personalized language model. The user's rating is calculated by the user himself. The user's rating is calculated by the user himself. The user's rating is calculated by the user himself. The user's rating is calculated by the user himself. The user's rating is calculated",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat metric did they use for qualitative evaluation?\n\nGround Truth Answer:\n\n\nAnswer A:\nSample model output. Explanation: FLOAT SELECTED: Table 2: Sample model output of general language model and personalized language model. The general language model used WMT\u201914 data, personalized language model 1 used \u201cFriends\u201d drama data, and personalized language model 2 used the English bible data. Scheme 1 to scheme 3 are relearn whole, surplus layer and fixed-n layer, respectively. The output was generated with the given input sequence, \u201cIt is possible, however\u201d\n\nAnswer B:\nWhat metric did they use for quantitative evaluation?\n< Explanation: We use the following metrics to evaluate the personalized language model. The qualitative evaluation is based on the user's opinion. The quantitative evaluation is based on the user's rating. The user's rating is a numerical value between 1 and 5. The higher the rating, the more satisfied the user is with the personalized language model. The user's rating is calculated by the user himself. The user's rating is calculated by the user himself. The user's rating is calculated by the user himself. The user's rating is calculated by the user himself. The user's rating is calculated\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1701.03578",
    "question": "What metric did they use for quantitative evaluation?",
    "gold_answer": "",
    "answer_a": "perplexity. Explanation: Figure 1 shows the validation curve while training. Perplexity values from various model output are plotted.",
    "answer_b": "How was it calculated?\n< Explanation: We use the F1 score to evaluate the quality of the personalized language model. The F1 score is defined as follows: where P is the predicted probability and Q is the true probability. The F1 score is a measure of the quality of the model. The higher the F1 score, the better the model is. The F1 score is calculated by averaging the scores of the top 100 predictions. The F1 score is calculated by averaging the scores of the top 100 predictions. The F1 score is calculated by averaging the scores of the top 100",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat metric did they use for quantitative evaluation?\n\nGround Truth Answer:\n\n\nAnswer A:\nperplexity. Explanation: Figure 1 shows the validation curve while training. Perplexity values from various model output are plotted.\n\nAnswer B:\nHow was it calculated?\n< Explanation: We use the F1 score to evaluate the quality of the personalized language model. The F1 score is defined as follows: where P is the predicted probability and Q is the true probability. The F1 score is a measure of the quality of the model. The higher the F1 score, the better the model is. The F1 score is calculated by averaging the scores of the top 100 predictions. The F1 score is calculated by averaging the scores of the top 100 predictions. The F1 score is calculated by averaging the scores of the top 100\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1701.03578",
    "question": "Which similarity metrics are used for quantitative analysis?",
    "gold_answer": "",
    "answer_a": "What is the difference between the two metrics?\n< Explanation: We use the cosine similarity metric to measure the similarity between the personalized language model and the personal language model. The cosine similarity is defined as follows: where is the vector of the personalized language model and is the vector of the personal language model. The cosine similarity is a measure of the similarity between two vectors. The cosine similarity is a scalar value between -1 and 1, where 1 indicates that the vectors are identical, and -1 indicates that the vectors are completely opposite. The cosine similarity is a measure of the similarity between two vectors. The cosine similarity is a scalar value",
    "answer_b": "Cross entropy between word distribution of model output and word distribution of target data.. Explanation: So, we use a simple and efficient metric to measure the similarity between the user style and the output of the personalized model.\n\nAn output of a personalized language model can be measured by calculating the cross entropy between the word distribution of the model output and that of the target data. Word distribution can be acquired by normalizing a word histogram which is calculated based on word counts in the target corpus. Equation (3) shows the metric formulation.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich similarity metrics are used for quantitative analysis?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the two metrics?\n< Explanation: We use the cosine similarity metric to measure the similarity between the personalized language model and the personal language model. The cosine similarity is defined as follows: where is the vector of the personalized language model and is the vector of the personal language model. The cosine similarity is a measure of the similarity between two vectors. The cosine similarity is a scalar value between -1 and 1, where 1 indicates that the vectors are identical, and -1 indicates that the vectors are completely opposite. The cosine similarity is a measure of the similarity between two vectors. The cosine similarity is a scalar value\n\nAnswer B:\nCross entropy between word distribution of model output and word distribution of target data.. Explanation: So, we use a simple and efficient metric to measure the similarity between the user style and the output of the personalized model.\n\nAn output of a personalized language model can be measured by calculating the cross entropy between the word distribution of the model output and that of the target data. Word distribution can be acquired by normalizing a word histogram which is calculated based on word counts in the target corpus. Equation (3) shows the metric formulation.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.09233",
    "question": "What is the best performing model?",
    "gold_answer": "",
    "answer_a": "An ensemble of N-Channels ConvNet and XGboost regressor model. Explanation: FLOAT SELECTED: Table 3: EI-reg task results.",
    "answer_b": "How is it compared to other models?\n<blockquote>Our system was evaluated on the testing sets of the tasks outperforming all other approaches for the Arabic version of valence intensity regression task and valence ordinal classification task. The best performing model is the one with the highest accuracy of 0.938. The best performing model is the one with the highest F1 score of 0.938. The best performing model is the one with the highest precision of 0.938. The best performing model is the one with the highest recall of 0.938. The best performing model is",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the best performing model?\n\nGround Truth Answer:\n\n\nAnswer A:\nAn ensemble of N-Channels ConvNet and XGboost regressor model. Explanation: FLOAT SELECTED: Table 3: EI-reg task results.\n\nAnswer B:\nHow is it compared to other models?\n<blockquote>Our system was evaluated on the testing sets of the tasks outperforming all other approaches for the Arabic version of valence intensity regression task and valence ordinal classification task. The best performing model is the one with the highest accuracy of 0.938. The best performing model is the one with the highest F1 score of 0.938. The best performing model is the one with the highest precision of 0.938. The best performing model is the one with the highest recall of 0.938. The best performing model is\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1902.08830",
    "question": "what languages did they evaluate on?",
    "gold_answer": "",
    "answer_a": "Arabic, Chinese, French, English, and German. Explanation: We present a novel knowledge-lean, cognitively motivated Bayesian model which learns categories and their structured features jointly from large natural language text corpora in five diverse languages: Arabic, Chinese, English, French, and German.",
    "answer_b": "Explanation: We evaluated our model on 10 languages: English, French, German, Spanish, Arabic, Chinese, Japanese, Hindi, Korean, and Arabic. We used the same model for each language, and we used the same data for each language. We used the same data for each language, and we used the same data for each language. We used the same data for each language, and we used the same data for each language. We used the same data for each language, and we used the same data for each language. We used the same data for each language, and we used the same data for each language.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat languages did they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\nArabic, Chinese, French, English, and German. Explanation: We present a novel knowledge-lean, cognitively motivated Bayesian model which learns categories and their structured features jointly from large natural language text corpora in five diverse languages: Arabic, Chinese, English, French, and German.\n\nAnswer B:\nExplanation: We evaluated our model on 10 languages: English, French, German, Spanish, Arabic, Chinese, Japanese, Hindi, Korean, and Arabic. We used the same model for each language, and we used the same data for each language. We used the same data for each language, and we used the same data for each language. We used the same data for each language, and we used the same data for each language. We used the same data for each language, and we used the same data for each language. We used the same data for each language, and we used the same data for each language.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.08357",
    "question": "How long is the vocabulary of subwords?",
    "gold_answer": "",
    "answer_a": "500. Explanation: During the experiment, we find that 500 is the best vocabulary size for both segmentation algorithms, and BPE is better than ULM in our setting.",
    "answer_b": "What is the size of the vocabulary of subwords?\n< Explanation: The subword vocabulary is 10000 words, which is 10000 words in total. The size of the subword vocabulary is 10000 words, which is 10000 words in total. The subword vocabulary is 10000 words, which is 10000 words in total. The subword vocabulary is 10000 words, which is 10000 words in total. The subword vocabulary is 10000",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long is the vocabulary of subwords?\n\nGround Truth Answer:\n\n\nAnswer A:\n500. Explanation: During the experiment, we find that 500 is the best vocabulary size for both segmentation algorithms, and BPE is better than ULM in our setting.\n\nAnswer B:\nWhat is the size of the vocabulary of subwords?\n< Explanation: The subword vocabulary is 10000 words, which is 10000 words in total. The size of the subword vocabulary is 10000 words, which is 10000 words in total. The subword vocabulary is 10000 words, which is 10000 words in total. The subword vocabulary is 10000 words, which is 10000 words in total. The subword vocabulary is 10000\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1801.04433",
    "question": "what rnn classifiers were used?",
    "gold_answer": "",
    "answer_a": "Explanation: The proposed model is composed of three RNN classifiers, each with a different architecture. The first classifier is a simple RNN that uses the word frequency vectors as input. The second classifier is a RNN that uses the word frequency vectors and the user's gender as input. The third classifier is a RNN that uses the word frequency vectors, the user's gender, and the user's age as input. The final classifier is a RNN that uses the word frequency vectors, the user's gender, age, and the user's location as input. The final classifier is a RNN that uses the word frequency vectors, the",
    "answer_b": "LSTM classifier with no additional features, Neutral & Sexism, Neutral & Racism, Racism & Sexism and Neutral, Racism & Sexism.. Explanation: The details of each experiment, including the resulting size of each embedding can be found in Table TABREF10 , with the latter denoted `input dimension' in the table.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat rnn classifiers were used?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The proposed model is composed of three RNN classifiers, each with a different architecture. The first classifier is a simple RNN that uses the word frequency vectors as input. The second classifier is a RNN that uses the word frequency vectors and the user's gender as input. The third classifier is a RNN that uses the word frequency vectors, the user's gender, and the user's age as input. The final classifier is a RNN that uses the word frequency vectors, the user's gender, age, and the user's location as input. The final classifier is a RNN that uses the word frequency vectors, the\n\nAnswer B:\nLSTM classifier with no additional features, Neutral & Sexism, Neutral & Racism, Racism & Sexism and Neutral, Racism & Sexism.. Explanation: The details of each experiment, including the resulting size of each embedding can be found in Table TABREF10 , with the latter denoted `input dimension' in the table.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1801.04433",
    "question": "what results did their system obtain?",
    "gold_answer": "",
    "answer_a": "Best authors' system achieved 0.9320 F1 score.. Explanation: The results are shown in Table TABREF24 , along with the reported results from state of the art approaches proposed by other researchers in the field.",
    "answer_b": "How did they evaluate it?\n< Explanation: Table 1 shows the results of our system on the publicly available corpus of 16k tweets. The results show that our system outperforms the state-of-the-art algorithms in terms of accuracy, precision, recall, and F1 score. The results are also comparable to the state-of-the-art algorithms in terms of the number of correct predictions. The state-of-the-art algorithms are the ones that use a combination of the user's gender and race/ethnicity information. The state-of-the-art algorithms also use a combination of the user's gender and race/ethnicity information with the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat results did their system obtain?\n\nGround Truth Answer:\n\n\nAnswer A:\nBest authors' system achieved 0.9320 F1 score.. Explanation: The results are shown in Table TABREF24 , along with the reported results from state of the art approaches proposed by other researchers in the field.\n\nAnswer B:\nHow did they evaluate it?\n< Explanation: Table 1 shows the results of our system on the publicly available corpus of 16k tweets. The results show that our system outperforms the state-of-the-art algorithms in terms of accuracy, precision, recall, and F1 score. The results are also comparable to the state-of-the-art algorithms in terms of the number of correct predictions. The state-of-the-art algorithms are the ones that use a combination of the user's gender and race/ethnicity information. The state-of-the-art algorithms also use a combination of the user's gender and race/ethnicity information with the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1801.04433",
    "question": "what are the existing approaches?",
    "gold_answer": "",
    "answer_a": "BIBREF12 , in which character n-grams and gender information were used as features. Explanation: As can be seen in Table TABREF24 , the work by BIBREF12 , in which character n-grams and gender information were used as features, obtained the quite low F-score of INLINEFORM0 . Later work by the same author BIBREF5 investigated the impact of the experience of the annotator in the performance, but still obtaining a lower F-score than ours. Furthermore, while the first part of the two step classification BIBREF16 performs quite well (reported an F-score of 0.9520), it falls short in detecting the particular class the abusive text belongs to.",
    "answer_b": "What are the results of the proposed approach?\n< Explanation: The proposed approach is an ensemble of Recurrent Neural Network (RNN) classifiers, and it incorporates various features associated with user-related information, such as the users' tendency towards racism or sexism. These data are fed as input to the above classifiers along with the word frequency vectors derived from the textual content. Our approach has been evaluated on a publicly available corpus of 16k tweets, and the results demonstrate its effectiveness in comparison to existing state of the art solutions. More specifically, our scheme can successfully distinguish racism and sexism messages from normal text, and achieve higher classification quality than",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the existing approaches?\n\nGround Truth Answer:\n\n\nAnswer A:\nBIBREF12 , in which character n-grams and gender information were used as features. Explanation: As can be seen in Table TABREF24 , the work by BIBREF12 , in which character n-grams and gender information were used as features, obtained the quite low F-score of INLINEFORM0 . Later work by the same author BIBREF5 investigated the impact of the experience of the annotator in the performance, but still obtaining a lower F-score than ours. Furthermore, while the first part of the two step classification BIBREF16 performs quite well (reported an F-score of 0.9520), it falls short in detecting the particular class the abusive text belongs to.\n\nAnswer B:\nWhat are the results of the proposed approach?\n< Explanation: The proposed approach is an ensemble of Recurrent Neural Network (RNN) classifiers, and it incorporates various features associated with user-related information, such as the users' tendency towards racism or sexism. These data are fed as input to the above classifiers along with the word frequency vectors derived from the textual content. Our approach has been evaluated on a publicly available corpus of 16k tweets, and the results demonstrate its effectiveness in comparison to existing state of the art solutions. More specifically, our scheme can successfully distinguish racism and sexism messages from normal text, and achieve higher classification quality than\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.07888",
    "question": "Which dataset do they use?",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n< Explanation: We use the ASR dataset from the University of California, Berkeley (UCB) ASR project (UCB ASR) BIBREF1 . This dataset contains 1000 utterances, each of which is labeled with a single intent. The dataset is divided into 1000 utterances, each of which is labeled with a single intent. The utterances are labeled in a random order. The intent labels are in the form of a 10-letter string. The intent labels are not in the form of a 10-letter string. The",
    "answer_b": "500 rescored intent annotations found in the lattices in cancellations and refunds domain. Explanation: To evaluate the effectiveness of the proposed algorithm, we have sampled a dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dataset do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n< Explanation: We use the ASR dataset from the University of California, Berkeley (UCB) ASR project (UCB ASR) BIBREF1 . This dataset contains 1000 utterances, each of which is labeled with a single intent. The dataset is divided into 1000 utterances, each of which is labeled with a single intent. The utterances are labeled in a random order. The intent labels are in the form of a 10-letter string. The intent labels are not in the form of a 10-letter string. The\n\nAnswer B:\n500 rescored intent annotations found in the lattices in cancellations and refunds domain. Explanation: To evaluate the effectiveness of the proposed algorithm, we have sampled a dataset of 500 rescored intent annotations found in the lattices in cancellations and refunds domain.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.07888",
    "question": "How do they use extracted intent to rescore?",
    "gold_answer": "",
    "answer_a": "What is the baseline?\n< Explanation: We use the FST index to rescore the original transcript. We rescore the transcript using the most likely transcript hypothesis (best path) and the most likely intent hypothesis (best path). We rescore the transcript using the most likely intent hypothesis (best path) and the most likely intent hypothesis (best path). We rescore the transcript using the most likely intent hypothesis (best path) and the most likely intent hypothesis (best path). We rescore the transcript using the most likely intent hypothesis (best path) and the most likely intent hypothesis (best path). We rescore the transcript using the",
    "answer_b": "providing a library of intent examples. Explanation: Our approach allows performing fast ASR domain adaptation by providing a library of intent examples used for lattice rescoring.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they use extracted intent to rescore?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the baseline?\n< Explanation: We use the FST index to rescore the original transcript. We rescore the transcript using the most likely transcript hypothesis (best path) and the most likely intent hypothesis (best path). We rescore the transcript using the most likely intent hypothesis (best path) and the most likely intent hypothesis (best path). We rescore the transcript using the most likely intent hypothesis (best path) and the most likely intent hypothesis (best path). We rescore the transcript using the most likely intent hypothesis (best path) and the most likely intent hypothesis (best path). We rescore the transcript using the\n\nAnswer B:\nproviding a library of intent examples. Explanation: Our approach allows performing fast ASR domain adaptation by providing a library of intent examples used for lattice rescoring.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.03131",
    "question": "How is the model evaluated against the original recursive training algorithm?",
    "gold_answer": "",
    "answer_a": "The ability of the training algorithm to find parameters minimizing the Morfessor cost is evaluated by using the trained model to segment the training data, and loading the resulting segmentation as if it was a Morfessor Baseline model. We observe both unweighted prior and likelihood, and their $\\alpha $-weighted sum.\n\nThe closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary $F_{1}$-score BIBREF21.. Explanation: The ability of the training algorithm to find parameters minimizing the Morfessor cost is evaluated by using the trained model to segment the training data, and loading the resulting segmentation as if it was a Morfessor Baseline model. We observe both unweighted prior and likelihood, and their $\\alpha $-weighted sum.\n\nThe closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary $F_{1}$-score BIBREF21.",
    "answer_b": "What is the morphological segmentation accuracy of the new model?\n<blockquote>\n<font size=\"2\">Figure 1: Morphological segmentation accuracy of the original and new models. The horizontal axis shows the number of words in the training set, and the vertical axis shows the percentage of correctly segmented words. The horizontal line shows the original model, and the vertical line shows the new model. The new model is able to find better solutions to the optimization problem defined by the Morfessor Baseline model than its original recursive training algorithm. The new model is also able to find better solutions to the optimization problem defined by the original model. The",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is the model evaluated against the original recursive training algorithm?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe ability of the training algorithm to find parameters minimizing the Morfessor cost is evaluated by using the trained model to segment the training data, and loading the resulting segmentation as if it was a Morfessor Baseline model. We observe both unweighted prior and likelihood, and their $\\alpha $-weighted sum.\n\nThe closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary $F_{1}$-score BIBREF21.. Explanation: The ability of the training algorithm to find parameters minimizing the Morfessor cost is evaluated by using the trained model to segment the training data, and loading the resulting segmentation as if it was a Morfessor Baseline model. We observe both unweighted prior and likelihood, and their $\\alpha $-weighted sum.\n\nThe closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary $F_{1}$-score BIBREF21.\n\nAnswer B:\nWhat is the morphological segmentation accuracy of the new model?\n<blockquote>\n<font size=\"2\">Figure 1: Morphological segmentation accuracy of the original and new models. The horizontal axis shows the number of words in the training set, and the vertical axis shows the percentage of correctly segmented words. The horizontal line shows the original model, and the vertical line shows the new model. The new model is able to find better solutions to the optimization problem defined by the Morfessor Baseline model than its original recursive training algorithm. The new model is also able to find better solutions to the optimization problem defined by the original model. The\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.03131",
    "question": "What is the improvement in performance compared to the linguistic gold standard?",
    "gold_answer": "",
    "answer_a": "Proposed approach is best in:\n- Recall English: +3.47 (70.84 compared to next best 67.37)\n- Precision Finnish: +6.16 (68.18 compared to 62.02)\n- Recall NorthSami: +1.44 (62.84 compared to 61.40). Explanation: Table contains the error analysis for English, Finnish and North S\u00e1mi.",
    "answer_b": "What is the original training algorithm?\n< Explanation: We compare the performance of the new algorithm with the original training algorithm. The original training algorithm is based on the recursive training algorithm. The new algorithm is based on the Expectation Maximization algorithm. The original training algorithm is based on the lexicon pruning algorithm. The new algorithm is based on the lexicon pruning algorithm. The original training algorithm is based on the lexicon pruning algorithm. The new algorithm is based on the lexicon pruning algorithm. The original training algorithm is based on the lexicon pruning algorithm. The new algorithm is based on the lexicon pruning algorithm. The original training algorithm",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the improvement in performance compared to the linguistic gold standard?\n\nGround Truth Answer:\n\n\nAnswer A:\nProposed approach is best in:\n- Recall English: +3.47 (70.84 compared to next best 67.37)\n- Precision Finnish: +6.16 (68.18 compared to 62.02)\n- Recall NorthSami: +1.44 (62.84 compared to 61.40). Explanation: Table contains the error analysis for English, Finnish and North S\u00e1mi.\n\nAnswer B:\nWhat is the original training algorithm?\n< Explanation: We compare the performance of the new algorithm with the original training algorithm. The original training algorithm is based on the recursive training algorithm. The new algorithm is based on the Expectation Maximization algorithm. The original training algorithm is based on the lexicon pruning algorithm. The new algorithm is based on the lexicon pruning algorithm. The original training algorithm is based on the lexicon pruning algorithm. The new algorithm is based on the lexicon pruning algorithm. The original training algorithm is based on the lexicon pruning algorithm. The new algorithm is based on the lexicon pruning algorithm. The original training algorithm\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1903.01411",
    "question": "Which metrics do they use to evaluate results?",
    "gold_answer": "",
    "answer_a": "What are they?\n<sep><br/ >Answer: We use precision, recall, F1 score, and the number of correct translations. We also use the number of correct translations as a metric to evaluate the quality of the translations. We also use the number of correct translations as a metric to evaluate the quality of the translations. We also use the number of correct translations as a metric to evaluate the quality of the translations. We also use the number of correct translations as a metric to evaluate the quality of the translations. We also use the number of correct translations as a metric to evaluate the quality of the translations. We also use the",
    "answer_b": "BLEU BIBREF35 , METEOR BIBREF36 and chrF BIBREF37 metrics. Explanation: The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU BIBREF35 , METEOR BIBREF36 and chrF BIBREF37 metrics.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich metrics do they use to evaluate results?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are they?\n<sep><br/ >Answer: We use precision, recall, F1 score, and the number of correct translations. We also use the number of correct translations as a metric to evaluate the quality of the translations. We also use the number of correct translations as a metric to evaluate the quality of the translations. We also use the number of correct translations as a metric to evaluate the quality of the translations. We also use the number of correct translations as a metric to evaluate the quality of the translations. We also use the number of correct translations as a metric to evaluate the quality of the translations. We also use the\n\nAnswer B:\nBLEU BIBREF35 , METEOR BIBREF36 and chrF BIBREF37 metrics. Explanation: The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU BIBREF35 , METEOR BIBREF36 and chrF BIBREF37 metrics.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.03090",
    "question": "Which baselines did they compare to?",
    "gold_answer": "",
    "answer_a": "We also finetune (FT) DialoGPT and GPT2 on Interview without speaker information. Explanation: We also finetune (FT) DialoGPT and GPT2 on Interview without speaker information as strong speaker-agnostic baselines for host response generation.",
    "answer_b": "What was their performance?\n< Explanation: We compare our model to the following baselines: (1) a baseline that uses only the speaker role annotations (SRA) to generate responses (SRA+); (2) a baseline that uses only the transcript (TR) to generate responses (TR+); (3) a baseline that uses only the transcript (TR) to generate responses (TR+); (4) a baseline that uses only the transcript (TR) to generate responses (TR+); (5) a baseline that uses only the transcript (TR) to generate responses (TR+); (6) a baseline that",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich baselines did they compare to?\n\nGround Truth Answer:\n\n\nAnswer A:\nWe also finetune (FT) DialoGPT and GPT2 on Interview without speaker information. Explanation: We also finetune (FT) DialoGPT and GPT2 on Interview without speaker information as strong speaker-agnostic baselines for host response generation.\n\nAnswer B:\nWhat was their performance?\n< Explanation: We compare our model to the following baselines: (1) a baseline that uses only the speaker role annotations (SRA) to generate responses (SRA+); (2) a baseline that uses only the transcript (TR) to generate responses (TR+); (3) a baseline that uses only the transcript (TR) to generate responses (TR+); (4) a baseline that uses only the transcript (TR) to generate responses (TR+); (5) a baseline that uses only the transcript (TR) to generate responses (TR+); (6) a baseline that\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.03090",
    "question": "What dialog tasks was it experimented on?",
    "gold_answer": "",
    "answer_a": "How did they evaluate performance?\n< Explanation: We evaluated our model on two dialog tasks: (1) a simple \"yes/no\" dialog task, and (2) a \"yes\" to \"no\" dialog task. We used the following evaluation metrics: (1) F1 score, (2) Recall, and (3) Precision. We also evaluated our model on a \"yes\" to \"no\" dialog task using the following metrics: (1) F1 score, (2) Recall, and (3) Precision. We also evaluated our model on a \"yes\" to \"no\" dialog task using the following",
    "answer_b": "role modeling in media dialog. Explanation: In particular, we explore the tasks of role modeling in media dialog and role change detection on Interview and find that leveraging role information can enable more nuanced, on-topic and natural dialog generation, as well as improve role change classification performance.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat dialog tasks was it experimented on?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow did they evaluate performance?\n< Explanation: We evaluated our model on two dialog tasks: (1) a simple \"yes/no\" dialog task, and (2) a \"yes\" to \"no\" dialog task. We used the following evaluation metrics: (1) F1 score, (2) Recall, and (3) Precision. We also evaluated our model on a \"yes\" to \"no\" dialog task using the following metrics: (1) F1 score, (2) Recall, and (3) Precision. We also evaluated our model on a \"yes\" to \"no\" dialog task using the following\n\nAnswer B:\nrole modeling in media dialog. Explanation: In particular, we explore the tasks of role modeling in media dialog and role change detection on Interview and find that leveraging role information can enable more nuanced, on-topic and natural dialog generation, as well as improve role change classification performance.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.03090",
    "question": "How was annotation done?",
    "gold_answer": "",
    "answer_a": "What was the size of the dataset?\n< Explanation: We collected 105,000 conversations from 10 news outlets, each containing 1000 turns. Each turn was annotated with a speaker role, a question, and a response. We used a combination of human annotators and a computer-based annotation system to annotate the conversations. The human annotators were trained on a large corpus of news transcripts, and the computer-based annotation system was trained on a subset of the conversations. The computer-based annotation system was trained to label the turns with the appropriate speaker role, question, and response. The human annotators",
    "answer_b": "annotations for each turn. Explanation: Our dataset is an order of magnitude larger than existing high-quality natural dialog datasets and contains speaker role annotations for each turn, facilitating the development of conversational agents and assistive systems for settings involving specific speaker roles, such as doctor-patient interviews or hosted talk shows.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow was annotation done?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat was the size of the dataset?\n< Explanation: We collected 105,000 conversations from 10 news outlets, each containing 1000 turns. Each turn was annotated with a speaker role, a question, and a response. We used a combination of human annotators and a computer-based annotation system to annotate the conversations. The human annotators were trained on a large corpus of news transcripts, and the computer-based annotation system was trained on a subset of the conversations. The computer-based annotation system was trained to label the turns with the appropriate speaker role, question, and response. The human annotators\n\nAnswer B:\nannotations for each turn. Explanation: Our dataset is an order of magnitude larger than existing high-quality natural dialog datasets and contains speaker role annotations for each turn, facilitating the development of conversational agents and assistive systems for settings involving specific speaker roles, such as doctor-patient interviews or hosted talk shows.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.03090",
    "question": "Which news outlets did they focus on?",
    "gold_answer": "",
    "answer_a": "natural dialog. Explanation: In this paper, we investigate how appropriate such data is for modeling natural dialog, and introduce Interview, a new high-quality large-scale open-domain conversational dataset grounded in interview settings with annotations for specific speaker roles.",
    "answer_b": "What are the language models used?\n< Explanation: We focus on the New York Times (NYT) and the BBC News (BBC) news outlets. We use the following language models: BERT (Luen and Manning, 2018) for the NYT and BBC, and RoBERTa (Liu et al., 2019) for the BBC. We use the same model architecture for both datasets. We train the models on the same training data, and use the same evaluation metrics. We use the same evaluation data for both datasets. We use the same evaluation data for both datasets. We use the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich news outlets did they focus on?\n\nGround Truth Answer:\n\n\nAnswer A:\nnatural dialog. Explanation: In this paper, we investigate how appropriate such data is for modeling natural dialog, and introduce Interview, a new high-quality large-scale open-domain conversational dataset grounded in interview settings with annotations for specific speaker roles.\n\nAnswer B:\nWhat are the language models used?\n< Explanation: We focus on the New York Times (NYT) and the BBC News (BBC) news outlets. We use the following language models: BERT (Luen and Manning, 2018) for the NYT and BBC, and RoBERTa (Liu et al., 2019) for the BBC. We use the same model architecture for both datasets. We train the models on the same training data, and use the same evaluation metrics. We use the same evaluation data for both datasets. We use the same evaluation data for both datasets. We use the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.03090",
    "question": "Which baselines did they compare to?",
    "gold_answer": "",
    "answer_a": "We also finetune (FT) DialoGPT and GPT2 on Interview without speaker information. Explanation: We also finetune (FT) DialoGPT and GPT2 on Interview without speaker information as strong speaker-agnostic baselines for host response generation.",
    "answer_b": "What was their performance?\n< Explanation: We compare our model to the following baselines: (1) a baseline that uses only the speaker role annotations (SRA) to generate responses (SRA+); (2) a baseline that uses only the transcript (TR) to generate responses (TR+); (3) a baseline that uses only the transcript (TR) to generate responses (TR+); (4) a baseline that uses only the transcript (TR) to generate responses (TR+); (5) a baseline that uses only the transcript (TR) to generate responses (TR+); (6) a baseline that",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich baselines did they compare to?\n\nGround Truth Answer:\n\n\nAnswer A:\nWe also finetune (FT) DialoGPT and GPT2 on Interview without speaker information. Explanation: We also finetune (FT) DialoGPT and GPT2 on Interview without speaker information as strong speaker-agnostic baselines for host response generation.\n\nAnswer B:\nWhat was their performance?\n< Explanation: We compare our model to the following baselines: (1) a baseline that uses only the speaker role annotations (SRA) to generate responses (SRA+); (2) a baseline that uses only the transcript (TR) to generate responses (TR+); (3) a baseline that uses only the transcript (TR) to generate responses (TR+); (4) a baseline that uses only the transcript (TR) to generate responses (TR+); (5) a baseline that uses only the transcript (TR) to generate responses (TR+); (6) a baseline that\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.03090",
    "question": "Which dialog tasks did they experiment on?",
    "gold_answer": "",
    "answer_a": "role modeling in media dialog and role change detection on Interview. Explanation: In particular, we explore the tasks of role modeling in media dialog and role change detection on Interview and find that leveraging role information can enable more nuanced, on-topic and natural dialog generation, as well as improve role change classification performance.",
    "answer_b": "What baselines did they use?\n< Explanation: We evaluate our model on two dialog tasks: (1) a simple question-answer task, and (2) a more complex dialogue task. We use the following datasets for the two tasks: (1) a simple question-answer task, and (2) a more complex dialogue task. We use the following datasets for the two tasks: (1) a simple question-answer task, and (2) a more complex dialogue task. We use the following datasets for the two tasks: (1) a simple question-answer task, and (2) a more complex dialogue task. We use the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dialog tasks did they experiment on?\n\nGround Truth Answer:\n\n\nAnswer A:\nrole modeling in media dialog and role change detection on Interview. Explanation: In particular, we explore the tasks of role modeling in media dialog and role change detection on Interview and find that leveraging role information can enable more nuanced, on-topic and natural dialog generation, as well as improve role change classification performance.\n\nAnswer B:\nWhat baselines did they use?\n< Explanation: We evaluate our model on two dialog tasks: (1) a simple question-answer task, and (2) a more complex dialogue task. We use the following datasets for the two tasks: (1) a simple question-answer task, and (2) a more complex dialogue task. We use the following datasets for the two tasks: (1) a simple question-answer task, and (2) a more complex dialogue task. We use the following datasets for the two tasks: (1) a simple question-answer task, and (2) a more complex dialogue task. We use the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.03090",
    "question": "Which news sources do the transcripts come from?",
    "gold_answer": "",
    "answer_a": "7 programs on National Public Radio (NPR) over 20 years. Explanation: We collect a novel dataset of 105K multi-party interview transcripts for 7 programs on National Public Radio (NPR) over 20 years (1999\u20132019), total of 10k hours.",
    "answer_b": "How many conversations are in the dataset?\n< Explanation: The dataset is collected from news interviews conducted by the University of California, Berkeley, and Stanford University. The interviews are conducted by a panel of experts in various fields, and are recorded in a variety of settings, including in the living room, at a conference, and in a hospital. The transcripts are transcribed and edited to remove any extraneous information. The transcripts are then manually annotated for speaker roles, and for the number of turns spoken by each speaker. The dataset contains 105,000 conversations, with an average of 10 turns per conversation. The",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich news sources do the transcripts come from?\n\nGround Truth Answer:\n\n\nAnswer A:\n7 programs on National Public Radio (NPR) over 20 years. Explanation: We collect a novel dataset of 105K multi-party interview transcripts for 7 programs on National Public Radio (NPR) over 20 years (1999\u20132019), total of 10k hours.\n\nAnswer B:\nHow many conversations are in the dataset?\n< Explanation: The dataset is collected from news interviews conducted by the University of California, Berkeley, and Stanford University. The interviews are conducted by a panel of experts in various fields, and are recorded in a variety of settings, including in the living room, at a conference, and in a hospital. The transcripts are transcribed and edited to remove any extraneous information. The transcripts are then manually annotated for speaker roles, and for the number of turns spoken by each speaker. The dataset contains 105,000 conversations, with an average of 10 turns per conversation. The\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.06365",
    "question": "Which real world datasets do they experiment on?",
    "gold_answer": "",
    "answer_a": "Reuters. Explanation: In the experiments, three regular text datasets and three short text datasets were used:\n\nReuters is widely used corpus extracted from the Reuters-21578 dataset where documents without any labels are removed.",
    "answer_b": "What are the results?\n< Explanation: We evaluate our model on three real world datasets: the Wikipedia corpus (Wikipedia), the Stanford Sentiment Treebank (SST) and the Stanford CoreNLP corpus (CoreNLP). We also compare our model with other state-of-the-art topic models, namely Latent Dirichlet Allocation (LDA) and Latent Dirichlet Allocation with Topic Switching (LDA-SST). We also compare our model with other topic models, namely Latent Dirichlet Allocation with Topic Switching (LDA-TS) and Latent Dirichlet Allocation with Topic Switching and",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich real world datasets do they experiment on?\n\nGround Truth Answer:\n\n\nAnswer A:\nReuters. Explanation: In the experiments, three regular text datasets and three short text datasets were used:\n\nReuters is widely used corpus extracted from the Reuters-21578 dataset where documents without any labels are removed.\n\nAnswer B:\nWhat are the results?\n< Explanation: We evaluate our model on three real world datasets: the Wikipedia corpus (Wikipedia), the Stanford Sentiment Treebank (SST) and the Stanford CoreNLP corpus (CoreNLP). We also compare our model with other state-of-the-art topic models, namely Latent Dirichlet Allocation (LDA) and Latent Dirichlet Allocation with Topic Switching (LDA-SST). We also compare our model with other topic models, namely Latent Dirichlet Allocation with Topic Switching (LDA-TS) and Latent Dirichlet Allocation with Topic Switching and\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.06365",
    "question": "Which other models that incorporate meta information do they compare against?",
    "gold_answer": "",
    "answer_a": "LLDA. Explanation: LLDA, Labelled LDA BIBREF11 and PLLDA, Partially Labelled LDA BIBREF9 : two models that make use of multiple document labels.",
    "answer_b": "What is the performance of their models?\n< Explanation: We compare our model with the following models: (1) LDA (2) LDA with word embeddings (3) LDA with word embeddings and meta information (4) LDA with word embeddings and meta information and (5) LDA with word embeddings and meta information and (6) LDA with word embeddings and meta information and (7) LDA with word embeddings and meta information and (8) LDA with word embeddings and meta information and (9) LDA with word embeddings and meta information and (10) LDA with word embeddings and meta information",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich other models that incorporate meta information do they compare against?\n\nGround Truth Answer:\n\n\nAnswer A:\nLLDA. Explanation: LLDA, Labelled LDA BIBREF11 and PLLDA, Partially Labelled LDA BIBREF9 : two models that make use of multiple document labels.\n\nAnswer B:\nWhat is the performance of their models?\n< Explanation: We compare our model with the following models: (1) LDA (2) LDA with word embeddings (3) LDA with word embeddings and meta information (4) LDA with word embeddings and meta information and (5) LDA with word embeddings and meta information and (6) LDA with word embeddings and meta information and (7) LDA with word embeddings and meta information and (8) LDA with word embeddings and meta information and (9) LDA with word embeddings and meta information and (10) LDA with word embeddings and meta information\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.06365",
    "question": "How do they measure topic quality?",
    "gold_answer": "",
    "answer_a": "What is the difference between the two datasets?\n< Explanation: We use the following metrics to measure the quality of the topics: (1) the average perplexity of the topics, (2) the average number of topics per document, and (3) the average number of topics per word. The perplexity of a topic is defined as the average perplexity of the topics in the document. The number of topics per document is defined as the average number of topics per document. The number of topics per word is defined as the average number of topics per word. The perplexity of a word is defined as the average perplexity of the",
    "answer_b": "Normalised Pointwise Mutual Information. Explanation: We further evaluate the semantic coherence of the words in a topic learnt by LDA, PTM, DMR, LF-LDA, WF-LDA, GPU-DMM and MetaLDA. Here we use the Normalised Pointwise Mutual Information (NPMI) BIBREF31 , BIBREF32 to calculate topic coherence score for topic INLINEFORM0 with top INLINEFORM1 words: INLINEFORM2 , where INLINEFORM3 is the probability of word INLINEFORM4 , and INLINEFORM5 is the joint probability of words INLINEFORM6 and INLINEFORM7 that co-occur together within a sliding window.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they measure topic quality?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the two datasets?\n< Explanation: We use the following metrics to measure the quality of the topics: (1) the average perplexity of the topics, (2) the average number of topics per document, and (3) the average number of topics per word. The perplexity of a topic is defined as the average perplexity of the topics in the document. The number of topics per document is defined as the average number of topics per document. The number of topics per word is defined as the average number of topics per word. The perplexity of a word is defined as the average perplexity of the\n\nAnswer B:\nNormalised Pointwise Mutual Information. Explanation: We further evaluate the semantic coherence of the words in a topic learnt by LDA, PTM, DMR, LF-LDA, WF-LDA, GPU-DMM and MetaLDA. Here we use the Normalised Pointwise Mutual Information (NPMI) BIBREF31 , BIBREF32 to calculate topic coherence score for topic INLINEFORM0 with top INLINEFORM1 words: INLINEFORM2 , where INLINEFORM3 is the probability of word INLINEFORM4 , and INLINEFORM5 is the joint probability of words INLINEFORM6 and INLINEFORM7 that co-occur together within a sliding window.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.06365",
    "question": "Which data augmentation techniques do they use?",
    "gold_answer": "",
    "answer_a": "What are the results of the experiments?\n< Explanation: We use the following data augmentation techniques: (1) random word replacement, (2) random word substitution, (3) random word deletion, (4) random word insertion, (5) random word substitution and (6) random word deletion. We also use the following data augmentation techniques: (1) random word replacement, (2) random word substitution, (3) random word deletion, (4) random word substitution and (5) random word deletion. We also use the following data augmentation techniques: (1) random word replacement, (2) random word substitution,",
    "answer_b": "augmented with a set of Beta random variables. Explanation: Gamma ratio 1 in Eq. ( SECREF17 ) can be augmented with a set of Beta random variables INLINEFORM5 as: +rCl+x* (d,)(d, + md,)Gamma ratio 1 qd qdd,-1 (1-qd)md,-1 where for each document INLINEFORM6 , INLINEFORM7 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich data augmentation techniques do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results of the experiments?\n< Explanation: We use the following data augmentation techniques: (1) random word replacement, (2) random word substitution, (3) random word deletion, (4) random word insertion, (5) random word substitution and (6) random word deletion. We also use the following data augmentation techniques: (1) random word replacement, (2) random word substitution, (3) random word deletion, (4) random word substitution and (5) random word deletion. We also use the following data augmentation techniques: (1) random word replacement, (2) random word substitution,\n\nAnswer B:\naugmented with a set of Beta random variables. Explanation: Gamma ratio 1 in Eq. ( SECREF17 ) can be augmented with a set of Beta random variables INLINEFORM5 as: +rCl+x* (d,)(d, + md,)Gamma ratio 1 qd qdd,-1 (1-qd)md,-1 where for each document INLINEFORM6 , INLINEFORM7 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.10619",
    "question": "What type of annotation is performed?",
    "gold_answer": "",
    "answer_a": "How is it collected?\n< Explanation: The annotation process is based on a human-in-the-loop supervised learning framework that integrates crowdsourcing contributions and expertise on the local community and employment environment. The annotation process is based on a human-in-the-loop supervised learning framework that integrates crowdsourcing contributions and expertise on the local community and employment environment. The annotation process is based on a human-in-the-loop supervised learning framework that integrates crowdsourcing contributions and expertise on the local community and employment environment. The annotation process is based on a human-in-the-loop supervised learning framework that integrates crowdsourcing contributions and expertise on the local community and employment environment. The annotation process is",
    "answer_b": "human annotation and automatic predictions over multiple rounds to label automatically data as job-related or not job-related. Explanation: We developed and improved an effective, humans-in-the-loop active learning framework that uses human annotation and automatic predictions over multiple rounds to label automatically data as job-related or not job-related.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of annotation is performed?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it collected?\n< Explanation: The annotation process is based on a human-in-the-loop supervised learning framework that integrates crowdsourcing contributions and expertise on the local community and employment environment. The annotation process is based on a human-in-the-loop supervised learning framework that integrates crowdsourcing contributions and expertise on the local community and employment environment. The annotation process is based on a human-in-the-loop supervised learning framework that integrates crowdsourcing contributions and expertise on the local community and employment environment. The annotation process is based on a human-in-the-loop supervised learning framework that integrates crowdsourcing contributions and expertise on the local community and employment environment. The annotation process is\n\nAnswer B:\nhuman annotation and automatic predictions over multiple rounds to label automatically data as job-related or not job-related. Explanation: We developed and improved an effective, humans-in-the-loop active learning framework that uses human annotation and automatic predictions over multiple rounds to label automatically data as job-related or not job-related.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.10619",
    "question": "How are the tweets selected?",
    "gold_answer": "",
    "answer_a": "What are the criteria for selection?\n<sep><xml>\n<description>Our corpus is composed of 10,000 tweets collected from 10,000 Twitter users in the United States. The tweets are collected from 10,000 Twitter users in the United States, and are collected from 10,000 Twitter users in the United States. The tweets are collected from 10,000 Twitter users in the United States, and are collected from 10,000 Twitter users in the United States. The tweets are collected from 10,0",
    "answer_b": "They collected tweets from US and then applied some filtering rules based on Lexicons. Explanation: Using the DataSift Firehose, we collected historical tweets from public accounts with geographical coordinates located in a 15-counties region surrounding a medium sized US city from July 2013 to June 2014.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are the tweets selected?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the criteria for selection?\n<sep><xml>\n<description>Our corpus is composed of 10,000 tweets collected from 10,000 Twitter users in the United States. The tweets are collected from 10,000 Twitter users in the United States, and are collected from 10,000 Twitter users in the United States. The tweets are collected from 10,000 Twitter users in the United States, and are collected from 10,000 Twitter users in the United States. The tweets are collected from 10,0\n\nAnswer B:\nThey collected tweets from US and then applied some filtering rules based on Lexicons. Explanation: Using the DataSift Firehose, we collected historical tweets from public accounts with geographical coordinates located in a 15-counties region surrounding a medium sized US city from July 2013 to June 2014.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1612.09535",
    "question": "what dataset was used?",
    "gold_answer": "",
    "answer_a": "CoNLL_X bosque data, News data by Lusa agency, Sports news data. Explanation: We have implemented our method using the following R packages: tm BIBREF17 , cwhmisc BIBREF18 , memoise BIBREF19 , openNLP BIBREF20 , Hmisc BIBREF21 . The OpenNLP POS Tagger uses a probability model to predict the correct POS tag and, for Portuguese language, it was trained on CoNLL_X bosque data.",
    "answer_b": "Explanation: The dataset used in this paper is the Portuguese version of the Corpus of Texts for the Analysis of Social Media (COSMOS) dataset. The dataset was created by the authors of the paper \u201cA new approach to named entity recognition in Portuguese\u201d (2016) and is available at http://www.cosmos.pt/ . The dataset contains 100,000 documents, each of which is 1000 words long. The documents are written in Portuguese and contain a total of 100,000 entities. The dataset is divided into two parts: the training",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat dataset was used?\n\nGround Truth Answer:\n\n\nAnswer A:\nCoNLL_X bosque data, News data by Lusa agency, Sports news data. Explanation: We have implemented our method using the following R packages: tm BIBREF17 , cwhmisc BIBREF18 , memoise BIBREF19 , openNLP BIBREF20 , Hmisc BIBREF21 . The OpenNLP POS Tagger uses a probability model to predict the correct POS tag and, for Portuguese language, it was trained on CoNLL_X bosque data.\n\nAnswer B:\nExplanation: The dataset used in this paper is the Portuguese version of the Corpus of Texts for the Analysis of Social Media (COSMOS) dataset. The dataset was created by the authors of the paper \u201cA new approach to named entity recognition in Portuguese\u201d (2016) and is available at http://www.cosmos.pt/ . The dataset contains 100,000 documents, each of which is 1000 words long. The documents are written in Portuguese and contain a total of 100,000 entities. The dataset is divided into two parts: the training\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1612.09535",
    "question": "by how much did their model improve over current alternatives?",
    "gold_answer": "",
    "answer_a": "On average, it had better Recall by 0.481 in case of news dataset and by 0.372 in case of sports news dataset. \nOn average, it had better Precision by 0.086 in case of news dataset and by 0.37 in case of sports news dataset. \nOn average, it had better F1 by 0.381 in case of news dataset and by 0.616 in case of sports news dataset.. Explanation: To compute the INLINEFORM0 , INLINEFORM1 and INLINEFORM2 measures presented in Table TABREF40 , we used Equations EQREF30 , EQREF31 and EQREF32 with a difference in the weight given to the partial identifications.",
    "answer_b": "What is the difference between their model and the current alternatives?\n<blockquote>Our model outperforms the current alternatives in terms of $recall$ and $F_1$ measures. The current alternatives are able to extract only 10% of the entities, while our model is able to extract 90% of the entities. </blockquote> The current alternatives are able to extract only 10% of the entities, while our model is able to extract 90% of the entities. The difference between their model and the current alternatives is that the current alternatives are able to extract only 10% of the entities",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nby how much did their model improve over current alternatives?\n\nGround Truth Answer:\n\n\nAnswer A:\nOn average, it had better Recall by 0.481 in case of news dataset and by 0.372 in case of sports news dataset. \nOn average, it had better Precision by 0.086 in case of news dataset and by 0.37 in case of sports news dataset. \nOn average, it had better F1 by 0.381 in case of news dataset and by 0.616 in case of sports news dataset.. Explanation: To compute the INLINEFORM0 , INLINEFORM1 and INLINEFORM2 measures presented in Table TABREF40 , we used Equations EQREF30 , EQREF31 and EQREF32 with a difference in the weight given to the partial identifications.\n\nAnswer B:\nWhat is the difference between their model and the current alternatives?\n<blockquote>Our model outperforms the current alternatives in terms of $recall$ and $F_1$ measures. The current alternatives are able to extract only 10% of the entities, while our model is able to extract 90% of the entities. </blockquote> The current alternatives are able to extract only 10% of the entities, while our model is able to extract 90% of the entities. The difference between their model and the current alternatives is that the current alternatives are able to extract only 10% of the entities\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.02001",
    "question": "What is the state-of-the-art?",
    "gold_answer": "",
    "answer_a": "How does their method compare to theirs?\n< Explanation: The state-of-the-art is the one proposed by the authors of the original paper. They use a simple supervised learning approach, where they train a classifier on the labeled data. The classifier is trained on the tweets of the trolls, and then used to predict the political role of the trolls. The classifier is trained on the tweets of the trolls, and then used to predict the political role of the trolls. The classifier is trained on the tweets of the trolls, and then used to predict the political role of the trolls. The classifier is trained on the tweets of the trolls, and then",
    "answer_b": "BIBREF2, who predict the roles of the Russian trolls on Twitter by leveraging social theory and Actor-Network Theory approaches. Explanation: The only closely relevant work is that of BIBREF2, who predict the roles of the Russian trolls on Twitter by leveraging social theory and Actor-Network Theory approaches.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the state-of-the-art?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow does their method compare to theirs?\n< Explanation: The state-of-the-art is the one proposed by the authors of the original paper. They use a simple supervised learning approach, where they train a classifier on the labeled data. The classifier is trained on the tweets of the trolls, and then used to predict the political role of the trolls. The classifier is trained on the tweets of the trolls, and then used to predict the political role of the trolls. The classifier is trained on the tweets of the trolls, and then used to predict the political role of the trolls. The classifier is trained on the tweets of the trolls, and then\n\nAnswer B:\nBIBREF2, who predict the roles of the Russian trolls on Twitter by leveraging social theory and Actor-Network Theory approaches. Explanation: The only closely relevant work is that of BIBREF2, who predict the roles of the Russian trolls on Twitter by leveraging social theory and Actor-Network Theory approaches.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.02001",
    "question": "How large is the dataset?",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n< Explanation: The dataset consists of 10,000 tweets from the Russian Internet Research Agency (IRA) trolls, which were collected from the Twitter API. The dataset is divided into two parts: the first 5,000 tweets are used for training, and the remaining 5,000 tweets are used for testing. The tweets are labeled as left, right, or news feed, and the labels are available for 1,000 trolls. The remaining 4,000 tweets are used for testing. The dataset is available at https://github",
    "answer_b": "2973371 tweets by 2848 Twitter users. Explanation: Our main dataset contains 2973371 tweets by 2848 Twitter users, which the US House Intelligence Committee has linked to the Russian Internet Research Agency (IRA).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow large is the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n< Explanation: The dataset consists of 10,000 tweets from the Russian Internet Research Agency (IRA) trolls, which were collected from the Twitter API. The dataset is divided into two parts: the first 5,000 tweets are used for training, and the remaining 5,000 tweets are used for testing. The tweets are labeled as left, right, or news feed, and the labels are available for 1,000 trolls. The remaining 4,000 tweets are used for testing. The dataset is available at https://github\n\nAnswer B:\n2973371 tweets by 2848 Twitter users. Explanation: Our main dataset contains 2973371 tweets by 2848 Twitter users, which the US House Intelligence Committee has linked to the Russian Internet Research Agency (IRA).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.02001",
    "question": "How are labels for trolls obtained?",
    "gold_answer": "",
    "answer_a": "What are the available labels?\n< Explanation: We use the following labels for trolls: (i) left, (ii) news feed, (iii) right. We also use the following labels for news outlets mentioned by trolls: (i) news outlet, (ii) news outlet, (iii) news outlet, (iv) news outlet, (v) news outlet, (vi) news outlet, (vii) news outlet, (viii) news outlet, (ix) news outlet, (x) news outlet, (xi) news outlet, (xii) news outlet, (xiii) news outlet,",
    "answer_b": "the ground truth labels for the troll users are available. Explanation: We consider two possible scenarios. The first, prototypical ML scenario is supervised learning, where we want to learn a function from users to categories {left, right, news feed}, and the ground truth labels for the troll users are available. This scenario has been considered previously in the literature by BIBREF2.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are labels for trolls obtained?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the available labels?\n< Explanation: We use the following labels for trolls: (i) left, (ii) news feed, (iii) right. We also use the following labels for news outlets mentioned by trolls: (i) news outlet, (ii) news outlet, (iii) news outlet, (iv) news outlet, (v) news outlet, (vi) news outlet, (vii) news outlet, (viii) news outlet, (ix) news outlet, (x) news outlet, (xi) news outlet, (xii) news outlet, (xiii) news outlet,\n\nAnswer B:\nthe ground truth labels for the troll users are available. Explanation: We consider two possible scenarios. The first, prototypical ML scenario is supervised learning, where we want to learn a function from users to categories {left, right, news feed}, and the ground truth labels for the troll users are available. This scenario has been considered previously in the literature by BIBREF2.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.01547",
    "question": "Which datasets did they use to train the model?",
    "gold_answer": "",
    "answer_a": "CNN. Explanation: The first two datasets BIBREF1 were constructed from a large number of news articles from the CNN and Daily Mail websites.",
    "answer_b": "What are the sizes of the datasets?\n<sep>CNN: 10,000 words, 1000 questions, 1000 answers. Daily Mail: 10,000 words, 1000 questions, 1000 answers. Children's Book Test: 10,000 words, 1000 questions, 1000 answers. Explanation: We use the CNN dataset to train our model. It consists of 10,000 words, 1000 questions, and 1000 answers",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich datasets did they use to train the model?\n\nGround Truth Answer:\n\n\nAnswer A:\nCNN. Explanation: The first two datasets BIBREF1 were constructed from a large number of news articles from the CNN and Daily Mail websites.\n\nAnswer B:\nWhat are the sizes of the datasets?\n<sep>CNN: 10,000 words, 1000 questions, 1000 answers. Daily Mail: 10,000 words, 1000 questions, 1000 answers. Children's Book Test: 10,000 words, 1000 questions, 1000 answers. Explanation: We use the CNN dataset to train our model. It consists of 10,000 words, 1000 questions, and 1000 answers\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.01547",
    "question": "What is the performance of their model?",
    "gold_answer": "",
    "answer_a": "What is the state of the art?\n<sep>Answer: The model achieves 91.5% accuracy on the CNN dataset and 91.6% accuracy on the Daily Mail dataset. The state of the art is 91.6% on the Children's Book Test dataset. Explanation: The model achieves 91.5% accuracy on the CNN dataset and 91.6% accuracy on the Daily Mail dataset. The state of the art is 91.6% on the Children's Book Test dataset. The model is trained on 10000 documents and tested on 1",
    "answer_b": "CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5%. Explanation: On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5%.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the performance of their model?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the state of the art?\n<sep>Answer: The model achieves 91.5% accuracy on the CNN dataset and 91.6% accuracy on the Daily Mail dataset. The state of the art is 91.6% on the Children's Book Test dataset. Explanation: The model achieves 91.5% accuracy on the CNN dataset and 91.6% accuracy on the Daily Mail dataset. The state of the art is 91.6% on the Children's Book Test dataset. The model is trained on 10000 documents and tested on 1\n\nAnswer B:\nCNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5%. Explanation: On the CNN dataset our single model with best validation accuracy achieves a test accuracy of 69.5%.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.01547",
    "question": "What baseline do they compare against?",
    "gold_answer": "",
    "answer_a": "Attentive and Impatient Readers. Explanation: All of these architectures use an attention mechanism that allows them to highlight places in the document that might be relevant to answering the question. We will now briefly describe these architectures and compare them to our approach.",
    "answer_b": "Explanation: We compare our model with the following baselines: (1) a simple word-level model that uses the word embeddings of the document as input to a fully connected layer, (2) a simple word-level model that uses the word embeddings of the document as input to a fully connected layer, (3) a simple word-level model that uses the word embeddings of the document as input to a fully connected layer, (4) a simple word-level model that uses the word embeddings of the document as input to a fully connected layer, (5) a simple word-level model that uses the word embeddings of the document as input to a",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat baseline do they compare against?\n\nGround Truth Answer:\n\n\nAnswer A:\nAttentive and Impatient Readers. Explanation: All of these architectures use an attention mechanism that allows them to highlight places in the document that might be relevant to answering the question. We will now briefly describe these architectures and compare them to our approach.\n\nAnswer B:\nExplanation: We compare our model with the following baselines: (1) a simple word-level model that uses the word embeddings of the document as input to a fully connected layer, (2) a simple word-level model that uses the word embeddings of the document as input to a fully connected layer, (3) a simple word-level model that uses the word embeddings of the document as input to a fully connected layer, (4) a simple word-level model that uses the word embeddings of the document as input to a fully connected layer, (5) a simple word-level model that uses the word embeddings of the document as input to a\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.01547",
    "question": "What datasets is the model evaluated on?",
    "gold_answer": "",
    "answer_a": "CNN. Explanation: The first two datasets BIBREF1 were constructed from a large number of news articles from the CNN and Daily Mail websites. The main body of each article forms a context, while the cloze-style question is formed from one of short highlight sentences, appearing at the top of each article page. Specifically, the question is created by replacing a named entity from the summary sentence (e.g. \u201cProducer X will not press charges against Jeremy Clarkson, his lawyer says.\u201d).",
    "answer_b": "Explanation: We evaluate our model on the following datasets: CNN and Daily Mail news data, Children's Book Test, and the Stanford Question Answering Dataset (SQuAD). We also evaluate our model on the Stanford Sentiment Treebank (SST) and the Stanford Sentiment Treebank with Question Answering (SSTQA). We also evaluate our model on the Stanford Sentiment Treebank with Question Answering (SSTQA) and the Stanford Sentiment Treebank with Question Answering and Answering (SSTQA+QA). We also evaluate our model on the Stanford Sentiment Treebank with Question Answering and Answering",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets is the model evaluated on?\n\nGround Truth Answer:\n\n\nAnswer A:\nCNN. Explanation: The first two datasets BIBREF1 were constructed from a large number of news articles from the CNN and Daily Mail websites. The main body of each article forms a context, while the cloze-style question is formed from one of short highlight sentences, appearing at the top of each article page. Specifically, the question is created by replacing a named entity from the summary sentence (e.g. \u201cProducer X will not press charges against Jeremy Clarkson, his lawyer says.\u201d).\n\nAnswer B:\nExplanation: We evaluate our model on the following datasets: CNN and Daily Mail news data, Children's Book Test, and the Stanford Question Answering Dataset (SQuAD). We also evaluate our model on the Stanford Sentiment Treebank (SST) and the Stanford Sentiment Treebank with Question Answering (SSTQA). We also evaluate our model on the Stanford Sentiment Treebank with Question Answering (SSTQA) and the Stanford Sentiment Treebank with Question Answering and Answering (SSTQA+QA). We also evaluate our model on the Stanford Sentiment Treebank with Question Answering and Answering\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.00239",
    "question": "What are the performances of LSTMs and humans on the task?",
    "gold_answer": "",
    "answer_a": "mean AUC of 0.56 for the LTSM and of 0.58 for humans. Explanation: Performances are rather modest, with a mean AUC of 0.56 for the LTSM and of 0.58 for humans, compared to the chance score of 0.5 for the unigram and bigram models.",
    "answer_b": "How do they compare?\n<blockquote>Table 1: Results of the German grammaticality task. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the performances of LSTMs and humans on the task?\n\nGround Truth Answer:\n\n\nAnswer A:\nmean AUC of 0.56 for the LTSM and of 0.58 for humans. Explanation: Performances are rather modest, with a mean AUC of 0.56 for the LTSM and of 0.58 for humans, compared to the chance score of 0.5 for the unigram and bigram models.\n\nAnswer B:\nHow do they compare?\n<blockquote>Table 1: Results of the German grammaticality task. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences. The results are shown as a percentage of correctly classified sentences\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.00809",
    "question": "What social media platforms are represented?",
    "gold_answer": "",
    "answer_a": "How many users are represented?\n< Explanation: The dataset is taken from Twitter, which is the most popular social media platform in the world. The dataset is taken from the Twitter API, which provides access to the Twitter API v2.0. The dataset is taken from the Twitter API v2.0, which provides access to the Twitter API v2.0. The dataset is taken from the Twitter API v2.0, which provides access to the Twitter API v2.0. The dataset is taken from the Twitter API v2.0, which provides access to the Twitter API v2.0. The dataset is taken",
    "answer_b": "Twitter. Explanation: How well does language data represent both regional population densities and the social characteristics of regional populations? To answer this question, we collect and analyze two large global-scale datasets: web-crawled data from the Common Crawl (16.65 billion words) and social media data from Twitter (4.14 billion words). This paper evaluates demographic-type information that is derived from these datasets, information which traditionally has been collected using survey-instruments as part of a census.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat social media platforms are represented?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow many users are represented?\n< Explanation: The dataset is taken from Twitter, which is the most popular social media platform in the world. The dataset is taken from the Twitter API, which provides access to the Twitter API v2.0. The dataset is taken from the Twitter API v2.0, which provides access to the Twitter API v2.0. The dataset is taken from the Twitter API v2.0, which provides access to the Twitter API v2.0. The dataset is taken from the Twitter API v2.0, which provides access to the Twitter API v2.0. The dataset is taken\n\nAnswer B:\nTwitter. Explanation: How well does language data represent both regional population densities and the social characteristics of regional populations? To answer this question, we collect and analyze two large global-scale datasets: web-crawled data from the Common Crawl (16.65 billion words) and social media data from Twitter (4.14 billion words). This paper evaluates demographic-type information that is derived from these datasets, information which traditionally has been collected using survey-instruments as part of a census.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.00809",
    "question": "Which websites were used in the web crawl?",
    "gold_answer": "",
    "answer_a": "Which websites were used in the social media crawl?\n< Explanation: The web-crawled dataset was obtained from the web-crawled dataset of the 2010 Census of India (Census 2010) and the 2011 Census of India (Census 2011). The social media dataset was obtained from the Twitter dataset of the 2010 Census of India (Census 2010) and the 2011 Census of India (Census 2011). The datasets were obtained from the Census Bureau and the Twitter.org websites,",
    "answer_b": "81.5 billion web pages covered in Common Crawl dataset. Explanation: Data comes from two sources of digital texts: web pages from the Common Crawl and social media from Twitter.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich websites were used in the web crawl?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhich websites were used in the social media crawl?\n< Explanation: The web-crawled dataset was obtained from the web-crawled dataset of the 2010 Census of India (Census 2010) and the 2011 Census of India (Census 2011). The social media dataset was obtained from the Twitter dataset of the 2010 Census of India (Census 2010) and the 2011 Census of India (Census 2011). The datasets were obtained from the Census Bureau and the Twitter.org websites,\n\nAnswer B:\n81.5 billion web pages covered in Common Crawl dataset. Explanation: Data comes from two sources of digital texts: web pages from the Common Crawl and social media from Twitter.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.00809",
    "question": "What countries and languages are represented in the datasets?",
    "gold_answer": "",
    "answer_a": "How are the languages represented?\n< Explanation: The datasets are taken from the following sources: (i) Twitter, (ii) Wikipedia, (iii) Wikipedia articles, (iv) Wikipedia articles with a Wikipedia article link, (v) Wikipedia articles with a Wikipedia article link and a Wikipedia article link, (vi) Wikipedia articles with a Wikipedia article link and a Wikipedia article link and a Wikipedia article link, (vii) Wikipedia articles with a Wikipedia article link and a Wikipedia article link and a Wikipedia article link and a Wikipedia article link, (viii) Wikipedia articles with a Wikipedia article link and a Wikipedia article link and a Wikipedia",
    "answer_b": "English, Spanish, Russian, Serbo-Croatian, Mandarin, German, French, Slovenian, Portuguese, Finnish, Bulgarian, Arabic, Indonesian, Latvian, Estonian, Slovak, Azerbaijani, Romanina, Icelandic, Italian, among others.. Explanation: FLOAT SELECTED: Table 3: Most Common Languages by Frequency Rank and Percent of Corpora",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat countries and languages are represented in the datasets?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are the languages represented?\n< Explanation: The datasets are taken from the following sources: (i) Twitter, (ii) Wikipedia, (iii) Wikipedia articles, (iv) Wikipedia articles with a Wikipedia article link, (v) Wikipedia articles with a Wikipedia article link and a Wikipedia article link, (vi) Wikipedia articles with a Wikipedia article link and a Wikipedia article link and a Wikipedia article link, (vii) Wikipedia articles with a Wikipedia article link and a Wikipedia article link and a Wikipedia article link and a Wikipedia article link, (viii) Wikipedia articles with a Wikipedia article link and a Wikipedia article link and a Wikipedia\n\nAnswer B:\nEnglish, Spanish, Russian, Serbo-Croatian, Mandarin, German, French, Slovenian, Portuguese, Finnish, Bulgarian, Arabic, Indonesian, Latvian, Estonian, Slovak, Azerbaijani, Romanina, Icelandic, Italian, among others.. Explanation: FLOAT SELECTED: Table 3: Most Common Languages by Frequency Rank and Percent of Corpora\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1903.10318",
    "question": "What other evaluation metrics did they use other than ROUGE-L??",
    "gold_answer": "",
    "answer_a": "To answer this question, please do the following tasks: (1) list the evaluation metrics used in this paper. (2) explain how the evaluation metrics are computed. (3) explain how the evaluation metrics are compared. (4) explain how the evaluation metrics are used to evaluate the performance of the BERTSUM system. (5) explain how the evaluation metrics are used to evaluate the performance of the BERTSUM system. (6) explain how the evaluation metrics are used to evaluate the performance of the BERTSUM system. (7) explain how the evaluation metrics are used to evaluate the performance of the BERTSUM system",
    "answer_b": "they also use ROUGE-1 and ROUGE-2. Explanation: FLOAT SELECTED: Table 1: Test set results on the CNN/DailyMail dataset using ROUGE F1. Results with \u2217 mark are taken from the corresponding papers.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat other evaluation metrics did they use other than ROUGE-L??\n\nGround Truth Answer:\n\n\nAnswer A:\nTo answer this question, please do the following tasks: (1) list the evaluation metrics used in this paper. (2) explain how the evaluation metrics are computed. (3) explain how the evaluation metrics are compared. (4) explain how the evaluation metrics are used to evaluate the performance of the BERTSUM system. (5) explain how the evaluation metrics are used to evaluate the performance of the BERTSUM system. (6) explain how the evaluation metrics are used to evaluate the performance of the BERTSUM system. (7) explain how the evaluation metrics are used to evaluate the performance of the BERTSUM system\n\nAnswer B:\nthey also use ROUGE-1 and ROUGE-2. Explanation: FLOAT SELECTED: Table 1: Test set results on the CNN/DailyMail dataset using ROUGE F1. Results with \u2217 mark are taken from the corresponding papers.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1903.10318",
    "question": "How do they use BERT to encode the whole text?",
    "gold_answer": "",
    "answer_a": "What is the difference between BERT and BERTSUM?\nFor the first part, we use BERT to encode the whole text. We use the BERT model to encode the whole text, and then we use the BERT model to encode the summary. The summary is the result of the BERT model. The summary is the result of the BERT model. The summary is the result of the BERT model. The summary is the result of the BERT model. The summary is the result of the BERT model. The summary is the result of the BERT model. The summary is the result of the BERT model",
    "answer_b": "insert a [CLS] token before each sentence and a [SEP] token after each sentence. Explanation: As illustrated in Figure 1, we insert a [CLS] token before each sentence and a [SEP] token after each sentence. In vanilla BERT, The [CLS] is used as a symbol to aggregate features from one sentence or a pair of sentences.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they use BERT to encode the whole text?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between BERT and BERTSUM?\nFor the first part, we use BERT to encode the whole text. We use the BERT model to encode the whole text, and then we use the BERT model to encode the summary. The summary is the result of the BERT model. The summary is the result of the BERT model. The summary is the result of the BERT model. The summary is the result of the BERT model. The summary is the result of the BERT model. The summary is the result of the BERT model. The summary is the result of the BERT model\n\nAnswer B:\ninsert a [CLS] token before each sentence and a [SEP] token after each sentence. Explanation: As illustrated in Figure 1, we insert a [CLS] token before each sentence and a [SEP] token after each sentence. In vanilla BERT, The [CLS] is used as a symbol to aggregate features from one sentence or a pair of sentences.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1903.10318",
    "question": "What is the ROUGE-L score of baseline method?",
    "gold_answer": "",
    "answer_a": "37.17 for the baseline model using a non-pretrained Transformer. Explanation: FLOAT SELECTED: Table 1: Test set results on the CNN/DailyMail dataset using ROUGE F1. Results with \u2217 mark are taken from the corresponding papers.",
    "answer_b": "What is the ROUGE-L score of BERTSUM?\n< Explanation: Table 1 shows the results of the baseline method and BERTSUM. The baseline method is the best-performing system on the CNN/Dailymail dataset. The BERTSUM model achieves 1.65 on ROUGE-L. The results are comparable to the previous best-performing system by 1.65 on ROUGE-L. The BERTSUM model is also the best-performing system on the BERT dataset. The results are comparable to the previous best-performing system by 1.65 on ROUGE-L. The B",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the ROUGE-L score of baseline method?\n\nGround Truth Answer:\n\n\nAnswer A:\n37.17 for the baseline model using a non-pretrained Transformer. Explanation: FLOAT SELECTED: Table 1: Test set results on the CNN/DailyMail dataset using ROUGE F1. Results with \u2217 mark are taken from the corresponding papers.\n\nAnswer B:\nWhat is the ROUGE-L score of BERTSUM?\n< Explanation: Table 1 shows the results of the baseline method and BERTSUM. The baseline method is the best-performing system on the CNN/Dailymail dataset. The BERTSUM model achieves 1.65 on ROUGE-L. The results are comparable to the previous best-performing system by 1.65 on ROUGE-L. The BERTSUM model is also the best-performing system on the BERT dataset. The results are comparable to the previous best-performing system by 1.65 on ROUGE-L. The B\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1903.10318",
    "question": "Which is the baseline method?",
    "gold_answer": "",
    "answer_a": "What is the performance of BERTSUM?\nFor the baseline, we use the BERT model from the official BERT repository BIBREF10. We use the same hyperparameters as BIBREF10. We also use the same dataset as BIBREF11. We use the same hyperparameters as BIBREF11. We use the same dataset as BIBREF12. We use the same hyperparameters as BIBREF12. We use the same hyperparameters as BIBREF13. We use the same hyperparameters as BIBREF14. We use the same hyperparameters as",
    "answer_b": "non-pretrained Transformer baseline. Explanation: For comparison, we implement a non-pretrained Transformer baseline which uses the same architecture as BERT, but with smaller parameters.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich is the baseline method?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of BERTSUM?\nFor the baseline, we use the BERT model from the official BERT repository BIBREF10. We use the same hyperparameters as BIBREF10. We also use the same dataset as BIBREF11. We use the same hyperparameters as BIBREF11. We use the same dataset as BIBREF12. We use the same hyperparameters as BIBREF12. We use the same hyperparameters as BIBREF13. We use the same hyperparameters as BIBREF14. We use the same hyperparameters as\n\nAnswer B:\nnon-pretrained Transformer baseline. Explanation: For comparison, we implement a non-pretrained Transformer baseline which uses the same architecture as BERT, but with smaller parameters.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1706.02427",
    "question": "What loss function is used?",
    "gold_answer": "",
    "answer_a": "What is the performance of the model?\n< Explanation: We use the following loss function: $$\\mathcal{L} = \\sum_{i=1}^{N} \\mathcal{L}_{i}$$ where $\\mathcal{L}_{i}$ is the loss for the $i$th query. The loss for the $i$th query is defined as: $$\\mathcal{L}_{i} = \\frac{1}{N} \\sum_{j=1}^{N} \\mathcal{L}_{ij}$$ where $\\mathcal{L}_{ij}$ is the loss for the",
    "answer_b": "negative log-likelihood. Explanation: We use negative log-likelihood as the loss function.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat loss function is used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of the model?\n< Explanation: We use the following loss function: $$\\mathcal{L} = \\sum_{i=1}^{N} \\mathcal{L}_{i}$$ where $\\mathcal{L}_{i}$ is the loss for the $i$th query. The loss for the $i$th query is defined as: $$\\mathcal{L}_{i} = \\frac{1}{N} \\sum_{j=1}^{N} \\mathcal{L}_{ij}$$ where $\\mathcal{L}_{ij}$ is the loss for the\n\nAnswer B:\nnegative log-likelihood. Explanation: We use negative log-likelihood as the loss function.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1706.02427",
    "question": "Are all the tables in the dataset from the same website?",
    "gold_answer": "",
    "answer_a": "If not, what is the percentage of tables from different websites?\n< Explanation: We find that the tables in the dataset are from different websites. For example, the table from the website \u201cwww.100110.com\u201d is from the website \u201cwww.100110.com\u201d. The table from the website \u201cwww.100110.com\u201d is from the website \u201cwww.100110.com\u201d. The table from the website \u201cwww.100110.com\u201d is from the website \u201cwww.100110.com\u201d. The table from",
    "answer_b": "No, they come from the top ranked web pages relevant to a query and from Wikipedia. Explanation: We use search logs from a commercial search engine to get a list of queries that could be potentially answered by web tables. Each query in query logs is paired with a list of web pages, ordered by the number of user clicks for the query. We select the tables occurred in the top ranked web page, and ask annotators to label whether a table is relevant to a query or not.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nAre all the tables in the dataset from the same website?\n\nGround Truth Answer:\n\n\nAnswer A:\nIf not, what is the percentage of tables from different websites?\n< Explanation: We find that the tables in the dataset are from different websites. For example, the table from the website \u201cwww.100110.com\u201d is from the website \u201cwww.100110.com\u201d. The table from the website \u201cwww.100110.com\u201d is from the website \u201cwww.100110.com\u201d. The table from the website \u201cwww.100110.com\u201d is from the website \u201cwww.100110.com\u201d. The table from\n\nAnswer B:\nNo, they come from the top ranked web pages relevant to a query and from Wikipedia. Explanation: We use search logs from a commercial search engine to get a list of queries that could be potentially answered by web tables. Each query in query logs is paired with a list of web pages, ordered by the number of user clicks for the query. We select the tables occurred in the top ranked web page, and ask annotators to label whether a table is relevant to a query or not.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.02747",
    "question": "What datasets are used for experiments?",
    "gold_answer": "",
    "answer_a": "the AliMe and Quora dataset. Explanation: We conduct experiments on the AliMe and Quora dataset for the query-bag matching based information-seeking conversation.",
    "answer_b": "What are the evaluation metrics?\n<sep><font size=\"2\">Datasets</font>:\nWe use the following datasets for experiments: (1) Amazon, (2) Baidu, (3) Baidu, (4) Baidu, (5) Baidu, (6) Baidu, (7) Baidu, (8) Baidu, (9) Baidu, (10) Baidu, (11) Baidu, (12) Baidu, (13) Baidu, (14) Baidu, (15) Baidu, (16) B",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets are used for experiments?\n\nGround Truth Answer:\n\n\nAnswer A:\nthe AliMe and Quora dataset. Explanation: We conduct experiments on the AliMe and Quora dataset for the query-bag matching based information-seeking conversation.\n\nAnswer B:\nWhat are the evaluation metrics?\n<sep><font size=\"2\">Datasets</font>:\nWe use the following datasets for experiments: (1) Amazon, (2) Baidu, (3) Baidu, (4) Baidu, (5) Baidu, (6) Baidu, (7) Baidu, (8) Baidu, (9) Baidu, (10) Baidu, (11) Baidu, (12) Baidu, (13) Baidu, (14) Baidu, (15) Baidu, (16) B\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.02747",
    "question": "Which natural language(s) is/are studied?",
    "gold_answer": "",
    "answer_a": "What are the evaluation metrics?\n<sep><solution> We study the following three languages: English, Chinese and Japanese. We use the following evaluation metrics: Recall, Precision, F1-score and Jaccard similarity. The recall is the ratio of the number of correct answers to the number of all possible answers. The precision is the ratio of the number of correct answers to the number of correct answers in the bag. The F1-score is the harmonic mean of the precision and recall. The Jaccard similarity is the ratio of the number of common words in the query and bag to the number of words in the query and bag,",
    "answer_b": "Chinese and English. Explanation: FLOAT SELECTED: Table 2: Some words and their corresponding weights (e in Equation 4) in mutual coveragemodule. The average weight across the whole vocabulary is also presented here.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich natural language(s) is/are studied?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the evaluation metrics?\n<sep><solution> We study the following three languages: English, Chinese and Japanese. We use the following evaluation metrics: Recall, Precision, F1-score and Jaccard similarity. The recall is the ratio of the number of correct answers to the number of all possible answers. The precision is the ratio of the number of correct answers to the number of correct answers in the bag. The F1-score is the harmonic mean of the precision and recall. The Jaccard similarity is the ratio of the number of common words in the query and bag to the number of words in the query and bag,\n\nAnswer B:\nChinese and English. Explanation: FLOAT SELECTED: Table 2: Some words and their corresponding weights (e in Equation 4) in mutual coveragemodule. The average weight across the whole vocabulary is also presented here.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.02747",
    "question": "What datasets are used in experiments?",
    "gold_answer": "",
    "answer_a": "the AliMe and Quora dataset. Explanation: We conduct experiments on the AliMe and Quora dataset for the query-bag matching based information-seeking conversation.",
    "answer_b": "What are the evaluation metrics?\n<sep><font size=\"2\">Datasets</font>:\nWe use the following datasets for experiments: (1) Amazon, (2) Baidu, (3) Baidu, (4) Baidu, (5) Baidu, (6) Baidu, (7) Baidu, (8) Baidu, (9) Baidu, (10) Baidu, (11) Baidu, (12) Baidu, (13) Baidu, (14) Baidu, (15) Baidu, (16) B",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets are used in experiments?\n\nGround Truth Answer:\n\n\nAnswer A:\nthe AliMe and Quora dataset. Explanation: We conduct experiments on the AliMe and Quora dataset for the query-bag matching based information-seeking conversation.\n\nAnswer B:\nWhat are the evaluation metrics?\n<sep><font size=\"2\">Datasets</font>:\nWe use the following datasets for experiments: (1) Amazon, (2) Baidu, (3) Baidu, (4) Baidu, (5) Baidu, (6) Baidu, (7) Baidu, (8) Baidu, (9) Baidu, (10) Baidu, (11) Baidu, (12) Baidu, (13) Baidu, (14) Baidu, (15) Baidu, (16) B\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.10461",
    "question": "What is the performance for the three languages tested?",
    "gold_answer": "",
    "answer_a": "Best authors achieved (different models) in terms of F1 score is:\nGerman - 0.6446\nItalian - 0.6999\nDutch - 0.6057. Explanation: Table TABREF12 shows the performance of our cross-lingual models in German, Italian and Dutch.",
    "answer_b": "How does it compare to the fine-tuned parser?\n<blockquote>Table 1: Performance of the fine-tuned parser on the three languages tested. The results are averaged over 10 runs. The results are shown as a percentage of the best result on the test set. The results are shown as a percentage of the best result on the test set. The results are shown as a percentage of the best result on the test set. The results are shown as a percentage of the best result on the test set. The results are shown as a percentage of the best result on the test set. The results are shown as a percentage",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the performance for the three languages tested?\n\nGround Truth Answer:\n\n\nAnswer A:\nBest authors achieved (different models) in terms of F1 score is:\nGerman - 0.6446\nItalian - 0.6999\nDutch - 0.6057. Explanation: Table TABREF12 shows the performance of our cross-lingual models in German, Italian and Dutch.\n\nAnswer B:\nHow does it compare to the fine-tuned parser?\n<blockquote>Table 1: Performance of the fine-tuned parser on the three languages tested. The results are averaged over 10 runs. The results are shown as a percentage of the best result on the test set. The results are shown as a percentage of the best result on the test set. The results are shown as a percentage of the best result on the test set. The results are shown as a percentage of the best result on the test set. The results are shown as a percentage of the best result on the test set. The results are shown as a percentage\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.10461",
    "question": "How big is the Parallel Meaning Bank?",
    "gold_answer": "",
    "answer_a": "4405 English sentences, 1173 German sentences, 633 Italian sentences and 583 Dutch sentences. Explanation: We use the PMB v.2.1.0 for the experiments. The dataset consists of 4405 English sentences, 1173 German sentences, 633 Italian sentences and 583 Dutch sentences. We divide the English sentences into 3072 training sentences, 663 development and 670 testing sentences. We consider all the sentences in other languages as test set.",
    "answer_b": "What is the size of the dataset?\n<sep>100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big is the Parallel Meaning Bank?\n\nGround Truth Answer:\n\n\nAnswer A:\n4405 English sentences, 1173 German sentences, 633 Italian sentences and 583 Dutch sentences. Explanation: We use the PMB v.2.1.0 for the experiments. The dataset consists of 4405 English sentences, 1173 German sentences, 633 Italian sentences and 583 Dutch sentences. We divide the English sentences into 3072 training sentences, 663 development and 670 testing sentences. We consider all the sentences in other languages as test set.\n\nAnswer B:\nWhat is the size of the dataset?\n<sep>100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.10461",
    "question": "What is the source of the crosslingual word embeddings?",
    "gold_answer": "",
    "answer_a": "How are they obtained?\n<sep> The Universal Dependencies (UD) corpus is a large-scale corpus of English-Italian-Italiano (Italian) and English-German-German (German) word embeddings. The UD corpus is available at http://www.cs.cornell.edu/~gabriel/ud/. The UD corpus is a collection of 10,000,000 words, each with a 1000-word vocabulary. The corpus is available in three formats: a 1000-word vocabulary, a 1000-word vocabulary with 1000000",
    "answer_b": "MUSE BIBREF17. Explanation: Multilingual word embeddings. We use the MUSE BIBREF17 pre-trained multilingual word embeddings and keep them fixed during training.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the source of the crosslingual word embeddings?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they obtained?\n<sep> The Universal Dependencies (UD) corpus is a large-scale corpus of English-Italian-Italiano (Italian) and English-German-German (German) word embeddings. The UD corpus is available at http://www.cs.cornell.edu/~gabriel/ud/. The UD corpus is a collection of 10,000,000 words, each with a 1000-word vocabulary. The corpus is available in three formats: a 1000-word vocabulary, a 1000-word vocabulary with 1000000\n\nAnswer B:\nMUSE BIBREF17. Explanation: Multilingual word embeddings. We use the MUSE BIBREF17 pre-trained multilingual word embeddings and keep them fixed during training.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1612.05202",
    "question": "What language pairs are considered?",
    "gold_answer": "",
    "answer_a": "English-French, English-Italian, English-Spanish, English-German.. Explanation: We perform experiments on 3-class polarity classification in tweets, and report results on four different languages: French, Italian, Spanish and German. Existing English sentiment lexicons are translated to the target languages through the proposed approach, given gs trained on the respective Wikipedia of each language.",
    "answer_b": "How are the embeddings aligned across languages?\n< Explanation: We consider four languages: English, Spanish, French, and German. We use the English-English, English-French, English-German, and Spanish-English word embeddings from Word2Vec BIBREF10 . We align the embeddings across languages by computing the cosine similarity between the embeddings of the two languages. We use the cosine similarity to align the embeddings of the four languages. We use the cosine similarity to align the embeddings of the four languages. We use the cosine similarity to align the embeddings of the four languages. We use the cosine similarity to align the embeddings of the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat language pairs are considered?\n\nGround Truth Answer:\n\n\nAnswer A:\nEnglish-French, English-Italian, English-Spanish, English-German.. Explanation: We perform experiments on 3-class polarity classification in tweets, and report results on four different languages: French, Italian, Spanish and German. Existing English sentiment lexicons are translated to the target languages through the proposed approach, given gs trained on the respective Wikipedia of each language.\n\nAnswer B:\nHow are the embeddings aligned across languages?\n< Explanation: We consider four languages: English, Spanish, French, and German. We use the English-English, English-French, English-German, and Spanish-English word embeddings from Word2Vec BIBREF10 . We align the embeddings across languages by computing the cosine similarity between the embeddings of the two languages. We use the cosine similarity to align the embeddings of the four languages. We use the cosine similarity to align the embeddings of the four languages. We use the cosine similarity to align the embeddings of the four languages. We use the cosine similarity to align the embeddings of the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12237",
    "question": "How many abstractive summarizations exist for each dialogue?",
    "gold_answer": "",
    "answer_a": "Each dialogue contains only one reference summary.. Explanation: Each dialogue contains only one reference summary.",
    "answer_b": "What is the average length of the summaries?\n< Explanation: We collected 10,000 abstractive dialogue summaries for each of the 1000 dialogues. The average length of the summaries is 10.5 words. The average length of the summaries is 10.5 words. The average length of the summaries is 10.5 words. The average length of the summaries is 10.5 words. The average length of the summaries is 10.5 words. The average length of the summaries is 10.5 words. The average length of the summaries",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many abstractive summarizations exist for each dialogue?\n\nGround Truth Answer:\n\n\nAnswer A:\nEach dialogue contains only one reference summary.. Explanation: Each dialogue contains only one reference summary.\n\nAnswer B:\nWhat is the average length of the summaries?\n< Explanation: We collected 10,000 abstractive dialogue summaries for each of the 1000 dialogues. The average length of the summaries is 10.5 words. The average length of the summaries is 10.5 words. The average length of the summaries is 10.5 words. The average length of the summaries is 10.5 words. The average length of the summaries is 10.5 words. The average length of the summaries is 10.5 words. The average length of the summaries\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12237",
    "question": "How is human evaluators' judgement measured, what was the criteria?",
    "gold_answer": "",
    "answer_a": "We asked two linguists to mark the quality of every summary on the scale of $-1$, 0, 1, where $-1$ means that a summarization is poor, extracts irrelevant information or does not make sense at all, 1 \u2013 it is understandable and gives a brief overview of the text, and 0 stands for a summarization that extracts only a part of relevant information, or makes some mistakes in the produced summary.. Explanation: We asked two linguists to mark the quality of every summary on the scale of $-1$, 0, 1, where $-1$ means that a summarization is poor, extracts irrelevant information or does not make sense at all, 1 \u2013 it is understandable and gives a brief overview of the text, and 0 stands for a summarization that extracts only a part of relevant information, or makes some mistakes in the produced summary.",
    "answer_b": "What is the difference between the human evaluators' judgement and the model-generated summaries?\n< Explanation: We use the human evaluators' judgement to measure the quality of the summaries. We use the following criteria: (1) the summary should be a summary of the dialogue, (2) the summary should be a summary of the dialogue in the same order as the dialogue, (3) the summary should be a summary of the dialogue in the same order as the dialogue, (4) the summary should be a summary of the dialogue in the same order as the dialogue, (5) the summary should be a summary of the dialogue in",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is human evaluators' judgement measured, what was the criteria?\n\nGround Truth Answer:\n\n\nAnswer A:\nWe asked two linguists to mark the quality of every summary on the scale of $-1$, 0, 1, where $-1$ means that a summarization is poor, extracts irrelevant information or does not make sense at all, 1 \u2013 it is understandable and gives a brief overview of the text, and 0 stands for a summarization that extracts only a part of relevant information, or makes some mistakes in the produced summary.. Explanation: We asked two linguists to mark the quality of every summary on the scale of $-1$, 0, 1, where $-1$ means that a summarization is poor, extracts irrelevant information or does not make sense at all, 1 \u2013 it is understandable and gives a brief overview of the text, and 0 stands for a summarization that extracts only a part of relevant information, or makes some mistakes in the produced summary.\n\nAnswer B:\nWhat is the difference between the human evaluators' judgement and the model-generated summaries?\n< Explanation: We use the human evaluators' judgement to measure the quality of the summaries. We use the following criteria: (1) the summary should be a summary of the dialogue, (2) the summary should be a summary of the dialogue in the same order as the dialogue, (3) the summary should be a summary of the dialogue in the same order as the dialogue, (4) the summary should be a summary of the dialogue in the same order as the dialogue, (5) the summary should be a summary of the dialogue in\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12237",
    "question": "What models have been evaluated?",
    "gold_answer": "",
    "answer_a": "What are the results?\n< Explanation: We evaluate the performance of 10 state-of-the-art abstractive summarization models on the SAMSum Corpus. We compare the performance of the models on the news corpus and the abstractive dialogue corpus. We also compare the performance of the models on the news corpus and the abstractive dialogue corpus. We find that the models on the abstractive dialogue corpus outperform the models on the news corpus. We also find that the models on the abstractive dialogue corpus outperform the models on the news corpus. We also find that the models on the abstractive dialogue corpus out",
    "answer_b": "MIDDLE-n, LONGEST-n, LONGER-THAN-n and MOST-ACTIVE-PERSON are the baselines, and experiments also carried out on Pointer generator networks, Transformers, Fast Abs RL, Fast Abs RL Enhanced, LightConv and DynamicConv. Explanation: The baseline commonly used in the news summarization task is Lead-3 BIBREF4, which takes three leading sentences of the document as the summary. The underlying assumption is that the beginning of the article contains the most significant information. Inspired by the Lead-n model, we propose a few different simple models:\n\nMIDDLE-n, which takes n utterances from the middle of the dialogue,\n\nLONGEST-n, treating only n longest utterances in order of length as a summary,\n\nLONGER-THAN-n, taking only utterances longer than n characters in order of length (if there is no such long utterance in the dialogue, takes the longest one),\n\nMOST-ACTIVE-PERSON, which treats all utterances of the most active person in the dialogue as a summary.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat models have been evaluated?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n< Explanation: We evaluate the performance of 10 state-of-the-art abstractive summarization models on the SAMSum Corpus. We compare the performance of the models on the news corpus and the abstractive dialogue corpus. We also compare the performance of the models on the news corpus and the abstractive dialogue corpus. We find that the models on the abstractive dialogue corpus outperform the models on the news corpus. We also find that the models on the abstractive dialogue corpus outperform the models on the news corpus. We also find that the models on the abstractive dialogue corpus out\n\nAnswer B:\nMIDDLE-n, LONGEST-n, LONGER-THAN-n and MOST-ACTIVE-PERSON are the baselines, and experiments also carried out on Pointer generator networks, Transformers, Fast Abs RL, Fast Abs RL Enhanced, LightConv and DynamicConv. Explanation: The baseline commonly used in the news summarization task is Lead-3 BIBREF4, which takes three leading sentences of the document as the summary. The underlying assumption is that the beginning of the article contains the most significant information. Inspired by the Lead-n model, we propose a few different simple models:\n\nMIDDLE-n, which takes n utterances from the middle of the dialogue,\n\nLONGEST-n, treating only n longest utterances in order of length as a summary,\n\nLONGER-THAN-n, taking only utterances longer than n characters in order of length (if there is no such long utterance in the dialogue, takes the longest one),\n\nMOST-ACTIVE-PERSON, which treats all utterances of the most active person in the dialogue as a summary.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.12237",
    "question": "How big is SAMSum Corpus?",
    "gold_answer": "",
    "answer_a": "16369 conversations. Explanation: For this purpose we have created the SAMSum Corpus which contains over 16k chat dialogues with manually annotated summaries.",
    "answer_b": "What are the size of the corpus?\n< Explanation: The SAMSum Corpus contains 10,000 abstractive dialogue summaries of 1000 news articles. The corpus is manually annotated with abstractive summarizations of the 1000 news articles. The corpus contains 10,000 abstractive dialogue summaries of 1000 news articles. The corpus is manually annotated with abstractive summarizations of the 1000 news articles. The corpus contains 10,000 abstractive dialogue summaries of 1000 news articles. The",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big is SAMSum Corpus?\n\nGround Truth Answer:\n\n\nAnswer A:\n16369 conversations. Explanation: For this purpose we have created the SAMSum Corpus which contains over 16k chat dialogues with manually annotated summaries.\n\nAnswer B:\nWhat are the size of the corpus?\n< Explanation: The SAMSum Corpus contains 10,000 abstractive dialogue summaries of 1000 news articles. The corpus is manually annotated with abstractive summarizations of the 1000 news articles. The corpus contains 10,000 abstractive dialogue summaries of 1000 news articles. The corpus is manually annotated with abstractive summarizations of the 1000 news articles. The corpus contains 10,000 abstractive dialogue summaries of 1000 news articles. The\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.07873",
    "question": "Are all generated examples semantics-preserving perturbations to the original text?",
    "gold_answer": "",
    "answer_a": "If not, what are the differences between the generated examples and the original text?\n< Explanation: We first show that the generated examples are semantics-preserving perturbations to the original text. We then show that the generated examples are not necessarily semantics-preserving perturbations to the original text. We show that the generated examples are not necessarily semantics-preserving perturbations to the original text. We show that the generated examples are not necessarily semantics-preserving perturbations to the original text. We show that the generated examples are not necessarily semantics-preserving perturbations to the original text. We show that the generated examples are not necessarily",
    "answer_b": "While the models aim to generate examples which preserve the semantics of the text with minimal perturbations, the Random model randomly replaces a character, which may not preserve the semantics.. Explanation: While SCPNs and DeepWordBug primary rely only on paraphrases and character transformations respectively to fool the classifier, our model uses a hybrid word-character encoder-decoder approach to introduce both paraphrases and character-level perturbations as a part of our attack strategy.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nAre all generated examples semantics-preserving perturbations to the original text?\n\nGround Truth Answer:\n\n\nAnswer A:\nIf not, what are the differences between the generated examples and the original text?\n< Explanation: We first show that the generated examples are semantics-preserving perturbations to the original text. We then show that the generated examples are not necessarily semantics-preserving perturbations to the original text. We show that the generated examples are not necessarily semantics-preserving perturbations to the original text. We show that the generated examples are not necessarily semantics-preserving perturbations to the original text. We show that the generated examples are not necessarily semantics-preserving perturbations to the original text. We show that the generated examples are not necessarily\n\nAnswer B:\nWhile the models aim to generate examples which preserve the semantics of the text with minimal perturbations, the Random model randomly replaces a character, which may not preserve the semantics.. Explanation: While SCPNs and DeepWordBug primary rely only on paraphrases and character transformations respectively to fool the classifier, our model uses a hybrid word-character encoder-decoder approach to introduce both paraphrases and character-level perturbations as a part of our attack strategy.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.07873",
    "question": "What is success rate of fooling tested models in experiments?",
    "gold_answer": "",
    "answer_a": "Authors best attacking model resulted in dip in the accuracy of CNN-Word (IMDB) by 79.43% and CNN-Char (AG's News) model by 72.16%. Explanation: Table TABREF40 demonstrates the capability of our model.",
    "answer_b": "How is success rate measured?\n< Explanation: We evaluate our method on two datasets: IMDB sentiment classification task and AG's news corpus news categorization task. We use the following metrics to evaluate the robustness of the model: F1 score, Precision, Recall, and Accuracy. We use the following metrics to evaluate the robustness of the model: F1 score, Precision, Recall, and Accuracy. We use the following metrics to evaluate the robustness of the model: F1 score, Precision, Recall, and Accuracy. We use the following metrics to evaluate the robustness of the model: F1 score, Precision, Recall",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is success rate of fooling tested models in experiments?\n\nGround Truth Answer:\n\n\nAnswer A:\nAuthors best attacking model resulted in dip in the accuracy of CNN-Word (IMDB) by 79.43% and CNN-Char (AG's News) model by 72.16%. Explanation: Table TABREF40 demonstrates the capability of our model.\n\nAnswer B:\nHow is success rate measured?\n< Explanation: We evaluate our method on two datasets: IMDB sentiment classification task and AG's news corpus news categorization task. We use the following metrics to evaluate the robustness of the model: F1 score, Precision, Recall, and Accuracy. We use the following metrics to evaluate the robustness of the model: F1 score, Precision, Recall, and Accuracy. We use the following metrics to evaluate the robustness of the model: F1 score, Precision, Recall, and Accuracy. We use the following metrics to evaluate the robustness of the model: F1 score, Precision, Recall\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.07873",
    "question": "What models are able to be fooled for AG's news corpus news categorization task by this approach?",
    "gold_answer": "",
    "answer_a": "character-based convolutional model (CNN-Char). Explanation: We perform our experiments on AG's news corpus with a character-based convolutional model (CNN-Char) BIBREF12.",
    "answer_b": "How do they do it?\n< Explanation: We first train a deep neural network (DNN) on the IMDB sentiment classification task. We then train a DNN on the AG's news corpus news categorization task. We use the same training data and hyperparameters for both tasks. We train the DNN on the IMDB dataset and the AG's news dataset. We use the same hyperparameters for both datasets. We train the DNN on the IMDB dataset and the AG's news dataset. We use the same hyperparameters for both datasets. We train the DNN on the IMDB dataset and the AG's news dataset",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat models are able to be fooled for AG's news corpus news categorization task by this approach?\n\nGround Truth Answer:\n\n\nAnswer A:\ncharacter-based convolutional model (CNN-Char). Explanation: We perform our experiments on AG's news corpus with a character-based convolutional model (CNN-Char) BIBREF12.\n\nAnswer B:\nHow do they do it?\n< Explanation: We first train a deep neural network (DNN) on the IMDB sentiment classification task. We then train a DNN on the AG's news corpus news categorization task. We use the same training data and hyperparameters for both tasks. We train the DNN on the IMDB dataset and the AG's news dataset. We use the same hyperparameters for both datasets. We train the DNN on the IMDB dataset and the AG's news dataset. We use the same hyperparameters for both datasets. We train the DNN on the IMDB dataset and the AG's news dataset\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.07873",
    "question": "What models are able to be fooled for IMDB sentiment classification task by this approach?",
    "gold_answer": "",
    "answer_a": "A word-based convolutional neural network (CNN-Word). Explanation: Sentiment classification: We trained a word-based convolutional model (CNN-Word) BIBREF11 on IMDB sentiment dataset . The dataset contains 50k movie reviews in total which are labeled as positive or negative. The trained model achieves a test accuracy of 89.95% which is relatively close to the state-of-the-art results on this dataset.",
    "answer_b": "What is the success rate of the adversarial examples generated?\n<blockquote>\n<font size=\"4\">We find that the adversarial examples generated are semantics-preserving perturbations to the original text. We show that the model is fooled by the generated adversarial examples with a success rate of 90.0% and 80.0% for the IMDB sentiment classification task and the AG's news corpus news categorization task, respectively. </font>\n<font size=\"4\">We find that the adversarial examples generated are semantics-preserving perturbations to the original text. We show that the model is fooled",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat models are able to be fooled for IMDB sentiment classification task by this approach?\n\nGround Truth Answer:\n\n\nAnswer A:\nA word-based convolutional neural network (CNN-Word). Explanation: Sentiment classification: We trained a word-based convolutional model (CNN-Word) BIBREF11 on IMDB sentiment dataset . The dataset contains 50k movie reviews in total which are labeled as positive or negative. The trained model achieves a test accuracy of 89.95% which is relatively close to the state-of-the-art results on this dataset.\n\nAnswer B:\nWhat is the success rate of the adversarial examples generated?\n<blockquote>\n<font size=\"4\">We find that the adversarial examples generated are semantics-preserving perturbations to the original text. We show that the model is fooled by the generated adversarial examples with a success rate of 90.0% and 80.0% for the IMDB sentiment classification task and the AG's news corpus news categorization task, respectively. </font>\n<font size=\"4\">We find that the adversarial examples generated are semantics-preserving perturbations to the original text. We show that the model is fooled\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.07873",
    "question": "How does proposed reinforcement learning based approach generate adversarial examples in black-box settings?",
    "gold_answer": "",
    "answer_a": "Training ::: Training with Reinforcement learning\nWe fine-tune our model to fool a target classifier by learning a policy that maximizes a specific discrete metric formulated based on the constraints required to generate adversarial examples. In our work, we use the self-critical approach of Rennie et al. BIBREF36 as our policy gradient training algorithm.\n\nTraining ::: Training with Reinforcement learning ::: Self-critical sequence training (SCST)\nIn SCST approach, the model learns to gather more rewards from its sampled sequences that bring higher rewards than its best greedy counterparts. First, we compute two sequences: (a) $y^{\\prime }$ sampled from the model's distribution $p(y^{\\prime }_j|y^{\\prime }_{. Explanation: Proposed Attack Strategy\nLet us consider a target model $T$ and $(x,l)$ refers to the samples from the dataset. Given an instance $x$, the goal of the adversary is to generate adversarial examples $x^{\\prime }$ such that $T(x^{\\prime }) \\ne l$, where $l$ denotes the true label i.e take one of the $K$ classes of the target classification model. The changes made to $x$ to get $x^{\\prime }$ are called perturbations. We would like to have $x^{\\prime }$ close to the original instance $x$. In a black box setting, we do not have knowledge about the internals of the target model or its training data. Previous work by Papernot et al. BIBREF14 train a separate substitute classifier such that it can mimic the decision boundaries of the target classifier. The substitute classifier is then used to craft adversarial examples. While these techniques have been applied for image classification models, such methods have not been explored extensively for text.\n\nWe implement both the substitute network training and adversarial example generation using an encoder-decoder architecture called Adversarial Examples Generator (AEG). The encoder extracts the character and word information from the input text and produces hidden representations of words considering its sequence context information. A substitute network is not implemented separately but applied using an attention mechanism to weigh the encoded hidden states based on their relevance to making predictions closer to target model outputs. The attention scores provide certain level of interpretability to the model as the regions of text that need to perturbed can be identified and visualized. The decoder uses the attention scores obtained from the substitute network, combines it with decoder state information to decide if perturbation is required at this state or not and finally emits the text unit (a text unit may refer to a word or character). Inspired by a work by Luong et al. BIBREF25, the decoder is a word and character-level recurrent network employed to generate adversarial examples. Before the substitute network is trained, we pretrain our encoder-decoder model on common misspellings and paraphrase datasets to empower the model to produce character and word perturbations in the form of misspellings or paraphrases. For training substitute network and generation of adversarial examples, we randomly draw data that is disjoint from the training data of the black-box model since we assume the adversaries have no prior knowledge about the training data or the model. Specifically, we consider attacking a target classifier by generating adversarial examples based on unseen input examples. This is done by dividing the dataset into training, validation and test using 60-30-10 ratio. The training data is used by the target model, while the unseen validation samples are used with necessary data augmentation for our AEG model. We further improve our model by using a self-critical approach to finally generate better adversarial examples. The rewards are formulated based on the following goals: (a) fool the target classifier, (b) minimize the number of perturbations and (c) preserve the semantics of the text.",
    "answer_b": "What are the success rates of adversarial examples generated by proposed method?\n< Explanation: We propose a reinforcement learning based approach to generate adversarial examples in black-box settings. We use a simple adversarial training method to generate adversarial examples. We train a neural network to classify the original text into two categories: positive and negative. We use the original text as the ground truth and the generated adversarial examples as the test set. We train the neural network using the original text as the input and the generated adversarial examples as the output. We use the generated adversarial examples to train the neural network to classify the original text into two categories",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does proposed reinforcement learning based approach generate adversarial examples in black-box settings?\n\nGround Truth Answer:\n\n\nAnswer A:\nTraining ::: Training with Reinforcement learning\nWe fine-tune our model to fool a target classifier by learning a policy that maximizes a specific discrete metric formulated based on the constraints required to generate adversarial examples. In our work, we use the self-critical approach of Rennie et al. BIBREF36 as our policy gradient training algorithm.\n\nTraining ::: Training with Reinforcement learning ::: Self-critical sequence training (SCST)\nIn SCST approach, the model learns to gather more rewards from its sampled sequences that bring higher rewards than its best greedy counterparts. First, we compute two sequences: (a) $y^{\\prime }$ sampled from the model's distribution $p(y^{\\prime }_j|y^{\\prime }_{. Explanation: Proposed Attack Strategy\nLet us consider a target model $T$ and $(x,l)$ refers to the samples from the dataset. Given an instance $x$, the goal of the adversary is to generate adversarial examples $x^{\\prime }$ such that $T(x^{\\prime }) \\ne l$, where $l$ denotes the true label i.e take one of the $K$ classes of the target classification model. The changes made to $x$ to get $x^{\\prime }$ are called perturbations. We would like to have $x^{\\prime }$ close to the original instance $x$. In a black box setting, we do not have knowledge about the internals of the target model or its training data. Previous work by Papernot et al. BIBREF14 train a separate substitute classifier such that it can mimic the decision boundaries of the target classifier. The substitute classifier is then used to craft adversarial examples. While these techniques have been applied for image classification models, such methods have not been explored extensively for text.\n\nWe implement both the substitute network training and adversarial example generation using an encoder-decoder architecture called Adversarial Examples Generator (AEG). The encoder extracts the character and word information from the input text and produces hidden representations of words considering its sequence context information. A substitute network is not implemented separately but applied using an attention mechanism to weigh the encoded hidden states based on their relevance to making predictions closer to target model outputs. The attention scores provide certain level of interpretability to the model as the regions of text that need to perturbed can be identified and visualized. The decoder uses the attention scores obtained from the substitute network, combines it with decoder state information to decide if perturbation is required at this state or not and finally emits the text unit (a text unit may refer to a word or character). Inspired by a work by Luong et al. BIBREF25, the decoder is a word and character-level recurrent network employed to generate adversarial examples. Before the substitute network is trained, we pretrain our encoder-decoder model on common misspellings and paraphrase datasets to empower the model to produce character and word perturbations in the form of misspellings or paraphrases. For training substitute network and generation of adversarial examples, we randomly draw data that is disjoint from the training data of the black-box model since we assume the adversaries have no prior knowledge about the training data or the model. Specifically, we consider attacking a target classifier by generating adversarial examples based on unseen input examples. This is done by dividing the dataset into training, validation and test using 60-30-10 ratio. The training data is used by the target model, while the unseen validation samples are used with necessary data augmentation for our AEG model. We further improve our model by using a self-critical approach to finally generate better adversarial examples. The rewards are formulated based on the following goals: (a) fool the target classifier, (b) minimize the number of perturbations and (c) preserve the semantics of the text.\n\nAnswer B:\nWhat are the success rates of adversarial examples generated by proposed method?\n< Explanation: We propose a reinforcement learning based approach to generate adversarial examples in black-box settings. We use a simple adversarial training method to generate adversarial examples. We train a neural network to classify the original text into two categories: positive and negative. We use the original text as the ground truth and the generated adversarial examples as the test set. We train the neural network using the original text as the input and the generated adversarial examples as the output. We use the generated adversarial examples to train the neural network to classify the original text into two categories\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01502",
    "question": "Which languages with different script do they look at?",
    "gold_answer": "",
    "answer_a": "How many languages are there?\n< Explanation: We use the following languages: English, French, German, Spanish, Portuguese, Italian, Arabic, Chinese, Japanese, Korean, Hindi, Urdu, Arabic, Persian, Bengali, Telugu, Tamil, Telugu, Malayalam, Thai, Indonesian, Arabic, Hebrew, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic",
    "answer_b": "Urdu. Explanation: M-Bert's ability to transfer between languages that are written in different scripts, and thus have effectively zero lexical overlap, is surprising given that it was trained on separate monolingual corpora and not with a multilingual objective. To probe deeper into how the model is able to perform this generalization, Table TABREF14 shows a sample of pos results for transfer across scripts.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich languages with different script do they look at?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow many languages are there?\n< Explanation: We use the following languages: English, French, German, Spanish, Portuguese, Italian, Arabic, Chinese, Japanese, Korean, Hindi, Urdu, Arabic, Persian, Bengali, Telugu, Tamil, Telugu, Malayalam, Thai, Indonesian, Arabic, Hebrew, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic, Arabic\n\nAnswer B:\nUrdu. Explanation: M-Bert's ability to transfer between languages that are written in different scripts, and thus have effectively zero lexical overlap, is surprising given that it was trained on separate monolingual corpora and not with a multilingual objective. To probe deeper into how the model is able to perform this generalization, Table TABREF14 shows a sample of pos results for transfer across scripts.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01502",
    "question": "What languages do they experiment with?",
    "gold_answer": "",
    "answer_a": "Dutch. Explanation: We perform ner experiments on two datasets: the publicly available CoNLL-2002 and -2003 sets, containing Dutch, Spanish, English, and German BIBREF5 , BIBREF6 ; and an in-house dataset with 16 languages, using the same CoNLL categories. Table TABREF4 shows M-Bert zero-shot performance on all language pairs in the CoNLL data.",
    "answer_b": "How many languages are tested?\n< Explanation: We tested M-BERT on 104 languages, which are listed in Table TABREF1 . We also tested it on 10 languages, which are listed in Table TABREF2 . We tested it on 10 languages, which are listed in Table TABREF3 . We tested it on 10 languages, which are listed in Table TABREF4 . We tested it on 10 languages, which are listed in Table TABREF5 . We tested it on 10 languages, which are listed in Table TABREF6 . We tested it on 10",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat languages do they experiment with?\n\nGround Truth Answer:\n\n\nAnswer A:\nDutch. Explanation: We perform ner experiments on two datasets: the publicly available CoNLL-2002 and -2003 sets, containing Dutch, Spanish, English, and German BIBREF5 , BIBREF6 ; and an in-house dataset with 16 languages, using the same CoNLL categories. Table TABREF4 shows M-Bert zero-shot performance on all language pairs in the CoNLL data.\n\nAnswer B:\nHow many languages are tested?\n< Explanation: We tested M-BERT on 104 languages, which are listed in Table TABREF1 . We also tested it on 10 languages, which are listed in Table TABREF2 . We tested it on 10 languages, which are listed in Table TABREF3 . We tested it on 10 languages, which are listed in Table TABREF4 . We tested it on 10 languages, which are listed in Table TABREF5 . We tested it on 10 languages, which are listed in Table TABREF6 . We tested it on 10\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01502",
    "question": "What language pairs are affected?",
    "gold_answer": "",
    "answer_a": "How do they affect the model?\n< Explanation: We first show that M-BERT can transfer well to languages in different scripts, and that the transfer is better between languages in the same script. We then show that the model can find translation pairs, and that the model can find translation pairs between languages in different scripts. Finally, we show that the model can transfer well to languages in different scripts, and that the transfer is better between languages in the same script.\n\nWe show that M-BERT can transfer well to languages in different scripts, and that the transfer is better between languages in the same script. We show that the model can find",
    "answer_b": "Language pairs that are typologically different. Explanation: M-Bert's ability to transfer between languages that are written in different scripts, and thus have effectively zero lexical overlap, is surprising given that it was trained on separate monolingual corpora and not with a multilingual objective.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat language pairs are affected?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they affect the model?\n< Explanation: We first show that M-BERT can transfer well to languages in different scripts, and that the transfer is better between languages in the same script. We then show that the model can find translation pairs, and that the model can find translation pairs between languages in different scripts. Finally, we show that the model can transfer well to languages in different scripts, and that the transfer is better between languages in the same script.\n\nWe show that M-BERT can transfer well to languages in different scripts, and that the transfer is better between languages in the same script. We show that the model can find\n\nAnswer B:\nLanguage pairs that are typologically different. Explanation: M-Bert's ability to transfer between languages that are written in different scripts, and thus have effectively zero lexical overlap, is surprising given that it was trained on separate monolingual corpora and not with a multilingual objective.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01502",
    "question": "What evaluation metrics are used?",
    "gold_answer": "",
    "answer_a": "How are they evaluated?\n< Explanation: We use the BLEU metric to evaluate the performance of the model. The BLEU score is defined as follows: where $p$ is the hypothesis and $q$ is the gold standard. The higher the BLEU score, the better the model\u2019s performance. We use the following BLEU scores for the evaluation: BLEU-1, BLEU-2, BLEU-3, BLEU-4, and BLEU-5. We also use the F1 score, which is defined as follows: where $p$ is the hypothesis and $q$ is the gold standard.",
    "answer_b": "ner F1 score. Explanation: Figure FIGREF9 plots ner F1 score versus entity overlap for zero-shot transfer between every language pair in an in-house dataset of 16 languages, for both M-Bert and En-Bert.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat evaluation metrics are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they evaluated?\n< Explanation: We use the BLEU metric to evaluate the performance of the model. The BLEU score is defined as follows: where $p$ is the hypothesis and $q$ is the gold standard. The higher the BLEU score, the better the model\u2019s performance. We use the following BLEU scores for the evaluation: BLEU-1, BLEU-2, BLEU-3, BLEU-4, and BLEU-5. We also use the F1 score, which is defined as follows: where $p$ is the hypothesis and $q$ is the gold standard.\n\nAnswer B:\nner F1 score. Explanation: Figure FIGREF9 plots ner F1 score versus entity overlap for zero-shot transfer between every language pair in an in-house dataset of 16 languages, for both M-Bert and En-Bert.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01502",
    "question": "What datasets did they use?",
    "gold_answer": "",
    "answer_a": "How many languages are in the dataset?\n< Explanation: We use the following datasets: (i) the English Wikipedia corpus (Wikipedia) BIBREF10 , (ii) the English Wikipedia corpus with code-switching (Wikipedia-Code) BIBREF11 , (iii) the English Wikipedia corpus with code-switching and code-switching to English (Wikipedia-Code-English) BIBREF12 , (iv) the English Wikipedia corpus with code-switching and code-switching to English and English Wikipedia corpus with code-switching to English (Wikipedia-Code-English-Wiki) BIBREF",
    "answer_b": "CoNLL-2002 and -2003. Explanation: We perform ner experiments on two datasets: the publicly available CoNLL-2002 and -2003 sets, containing Dutch, Spanish, English, and German BIBREF5 , BIBREF6 ; and an in-house dataset with 16 languages, using the same CoNLL categories.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow many languages are in the dataset?\n< Explanation: We use the following datasets: (i) the English Wikipedia corpus (Wikipedia) BIBREF10 , (ii) the English Wikipedia corpus with code-switching (Wikipedia-Code) BIBREF11 , (iii) the English Wikipedia corpus with code-switching and code-switching to English (Wikipedia-Code-English) BIBREF12 , (iv) the English Wikipedia corpus with code-switching and code-switching to English and English Wikipedia corpus with code-switching to English (Wikipedia-Code-English-Wiki) BIBREF\n\nAnswer B:\nCoNLL-2002 and -2003. Explanation: We perform ner experiments on two datasets: the publicly available CoNLL-2002 and -2003 sets, containing Dutch, Spanish, English, and German BIBREF5 , BIBREF6 ; and an in-house dataset with 16 languages, using the same CoNLL categories.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.00440",
    "question": "what are the other methods they compare to?",
    "gold_answer": "",
    "answer_a": "Explanation: The other methods are: (1) a simple Naive-Bayes classifier with 10-fold cross validation, (2) a simple Naive-Bayes classifier with 10-fold cross validation and a 10-fold cross validation on the training set, (3) a simple Naive-Bayes classifier with 10-fold cross validation and a 10-fold cross validation on the training set and (4) a simple Naive-Bayes classifier with 10-fold cross validation and a 10-fold cross validation on the training set and a 10-fold cross validation on the",
    "answer_b": "BIBREF1. Explanation: We achieve 0.548 prediction accuracy, which is not good enough BIBREF1 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the other methods they compare to?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The other methods are: (1) a simple Naive-Bayes classifier with 10-fold cross validation, (2) a simple Naive-Bayes classifier with 10-fold cross validation and a 10-fold cross validation on the training set, (3) a simple Naive-Bayes classifier with 10-fold cross validation and a 10-fold cross validation on the training set and (4) a simple Naive-Bayes classifier with 10-fold cross validation and a 10-fold cross validation on the training set and a 10-fold cross validation on the\n\nAnswer B:\nBIBREF1. Explanation: We achieve 0.548 prediction accuracy, which is not good enough BIBREF1 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.00440",
    "question": "what preprocessing method is introduced?",
    "gold_answer": "",
    "answer_a": "How is it applied?\n< Explanation: We use the following preprocessing method to label the data: we remove all the tweets that contain the word \u201cTrump\u201d and \u201cClinton\u201d and replace them with the word \u201cTrump\u201d and \u201cClinton\u201d respectively. We also remove all the tweets that contain the word \u201cHillary\u201d and replace them with the word \u201cHillary\u201d and we remove all the tweets that contain the word \u201cObama\u201d and replace them with the word \u201cObama\u201d. We also remove all the tweets that contain the word \u201cObama\u201d and replace them with the word \u201cObama\u201d and we remove all the tweets that contain the word \u201cObama",
    "answer_b": "Tweets without candidate names are removed, URLs and pictures are removed from the tweets that remain.. Explanation: We preprocess the data by: (1) removing URLs and pictures, also (2) by filtering tweets which have candidates' name. Hashtags, mentions, and retweets are not removed in order to maintain the original meaning of a tweet.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat preprocessing method is introduced?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it applied?\n< Explanation: We use the following preprocessing method to label the data: we remove all the tweets that contain the word \u201cTrump\u201d and \u201cClinton\u201d and replace them with the word \u201cTrump\u201d and \u201cClinton\u201d respectively. We also remove all the tweets that contain the word \u201cHillary\u201d and replace them with the word \u201cHillary\u201d and we remove all the tweets that contain the word \u201cObama\u201d and replace them with the word \u201cObama\u201d. We also remove all the tweets that contain the word \u201cObama\u201d and replace them with the word \u201cObama\u201d and we remove all the tweets that contain the word \u201cObama\n\nAnswer B:\nTweets without candidate names are removed, URLs and pictures are removed from the tweets that remain.. Explanation: We preprocess the data by: (1) removing URLs and pictures, also (2) by filtering tweets which have candidates' name. Hashtags, mentions, and retweets are not removed in order to maintain the original meaning of a tweet.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1702.02367",
    "question": "How well does their model perform on the recommendation task?",
    "gold_answer": "",
    "answer_a": "Their model achieves 30.0 HITS@100 on the recommendation task, more than any other baseline. Explanation: In particular, the performance for the QA task is evaluated according to HITS@1, while the performance for the Recs task is evaluated according to HITS@100.",
    "answer_b": "What are the results?\n<sep><font size=\"2\">RESULTS: </font>For the recommendation task, the model achieves the best performance on the bAbI Movie Dialog dataset, with a mean F1 score of 0.63, compared to the baseline model of 0.45. The model also achieves the best performance on the bAbI Movie Dialog dataset, with a mean F1 score of 0.63, compared to the baseline model of 0.45. The model also achieves the best performance on the bAbI Movie Dialog dataset, with a mean F1 score of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow well does their model perform on the recommendation task?\n\nGround Truth Answer:\n\n\nAnswer A:\nTheir model achieves 30.0 HITS@100 on the recommendation task, more than any other baseline. Explanation: In particular, the performance for the QA task is evaluated according to HITS@1, while the performance for the Recs task is evaluated according to HITS@100.\n\nAnswer B:\nWhat are the results?\n<sep><font size=\"2\">RESULTS: </font>For the recommendation task, the model achieves the best performance on the bAbI Movie Dialog dataset, with a mean F1 score of 0.63, compared to the baseline model of 0.45. The model also achieves the best performance on the bAbI Movie Dialog dataset, with a mean F1 score of 0.63, compared to the baseline model of 0.45. The model also achieves the best performance on the bAbI Movie Dialog dataset, with a mean F1 score of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1702.02367",
    "question": "Which knowledge base do they use to retrieve facts?",
    "gold_answer": "",
    "answer_a": "bAbI Movie Dialog dataset. Explanation: The model performance is evaluated on the QA and Recs tasks of the bAbI Movie Dialog dataset using HITS@k evaluation metric, which is equal to the number of correct answers in the top- INLINEFORM0 results.",
    "answer_b": "What is the size of the knowledge base?\n<sep>Answer: The knowledge base is built using the BIBREF10 knowledge base. The BIBREF10 knowledge base contains 10,000 facts, 100,000 entities and 100,000 relations. The knowledge base is built using the BIBREF10 knowledge base. The BIBREF10 knowledge base contains 10,000 facts, 100,000 entities and 100,000 relations. Explanation: The BIBREF10",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich knowledge base do they use to retrieve facts?\n\nGround Truth Answer:\n\n\nAnswer A:\nbAbI Movie Dialog dataset. Explanation: The model performance is evaluated on the QA and Recs tasks of the bAbI Movie Dialog dataset using HITS@k evaluation metric, which is equal to the number of correct answers in the top- INLINEFORM0 results.\n\nAnswer B:\nWhat is the size of the knowledge base?\n<sep>Answer: The knowledge base is built using the BIBREF10 knowledge base. The BIBREF10 knowledge base contains 10,000 facts, 100,000 entities and 100,000 relations. The knowledge base is built using the BIBREF10 knowledge base. The BIBREF10 knowledge base contains 10,000 facts, 100,000 entities and 100,000 relations. Explanation: The BIBREF10\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1702.02367",
    "question": "Which neural network architecture do they use?",
    "gold_answer": "",
    "answer_a": "bidirectional recurrent neural network encoder with Gated Recurrent Units (GRU). Explanation: The sequences of dense representations for INLINEFORM0 and INLINEFORM1 are encoded using a bidirectional recurrent neural network encoder with Gated Recurrent Units (GRU) as in BIBREF3 which represents each word INLINEFORM2 as the concatenation of a forward encoding INLINEFORM3 and a backward encoding INLINEFORM4 .",
    "answer_b": "What are the advantages of using this architecture?\n<sep>Answer: The model is trained using a combination of a recurrent neural network (RNN) and a convolutional neural network (CNN). The CNN is trained to extract features from the user's context and the RNN is trained to generate answers from the knowledge base. The CNN is trained using a combination of a recurrent neural network (RNN) and a convolutional neural network (CNN). The CNN is trained to extract features from the user's context and the RNN is trained to generate answers from the knowledge base. The CNN is trained using a combination of a recurrent neural network",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich neural network architecture do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nbidirectional recurrent neural network encoder with Gated Recurrent Units (GRU). Explanation: The sequences of dense representations for INLINEFORM0 and INLINEFORM1 are encoded using a bidirectional recurrent neural network encoder with Gated Recurrent Units (GRU) as in BIBREF3 which represents each word INLINEFORM2 as the concatenation of a forward encoding INLINEFORM3 and a backward encoding INLINEFORM4 .\n\nAnswer B:\nWhat are the advantages of using this architecture?\n<sep>Answer: The model is trained using a combination of a recurrent neural network (RNN) and a convolutional neural network (CNN). The CNN is trained to extract features from the user's context and the RNN is trained to generate answers from the knowledge base. The CNN is trained using a combination of a recurrent neural network (RNN) and a convolutional neural network (CNN). The CNN is trained to extract features from the user's context and the RNN is trained to generate answers from the knowledge base. The CNN is trained using a combination of a recurrent neural network\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1607.07514",
    "question": "by how much did their system outperform previous tasks?",
    "gold_answer": "",
    "answer_a": "Sentiment classification task by 0,008  F1, and semantic similarity task by 0,003 F1.. Explanation: FLOAT SELECTED: Table 2: Results of the paraphrase and semantic similarity in Twitter task.",
    "answer_b": "What languages were used?\n< Explanation: We evaluated our model on three different tasks: tweet semantic similarity, tweet sentiment categorization, and tweet classification. We used the following datasets: (1) Twitter Sentiment Classification (TSC) dataset, (2) Tweet2Vec dataset, and (3) Tweet2Vec dataset with English-language tweets. We used the following evaluation metrics: (1) cosine similarity between tweet embeddings and (2) accuracy of the model on the classification task. We also used the following evaluation metrics: (1) accuracy of the model on the semantic similarity task, (2) accuracy of the model on the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nby how much did their system outperform previous tasks?\n\nGround Truth Answer:\n\n\nAnswer A:\nSentiment classification task by 0,008  F1, and semantic similarity task by 0,003 F1.. Explanation: FLOAT SELECTED: Table 2: Results of the paraphrase and semantic similarity in Twitter task.\n\nAnswer B:\nWhat languages were used?\n< Explanation: We evaluated our model on three different tasks: tweet semantic similarity, tweet sentiment categorization, and tweet classification. We used the following datasets: (1) Twitter Sentiment Classification (TSC) dataset, (2) Tweet2Vec dataset, and (3) Tweet2Vec dataset with English-language tweets. We used the following evaluation metrics: (1) cosine similarity between tweet embeddings and (2) accuracy of the model on the classification task. We also used the following evaluation metrics: (1) accuracy of the model on the semantic similarity task, (2) accuracy of the model on the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1607.07514",
    "question": "what are the previous state of the art for sentiment categorization?",
    "gold_answer": "",
    "answer_a": "How does Tweet2Vec compare to them?\n< Explanation: The previous state of the art for sentiment categorization is the BERT model. BERT has been trained on the Twitter dataset and has been shown to be a good baseline for sentiment classification. The BERT model has been trained on 1.5 million tweets and has been shown to have a 99.5% accuracy on the Twitter dataset. The BERT model has been shown to be a good baseline for sentiment classification. The BERT model has been shown to be a good baseline for sentiment classification. The BERT model has been shown to be a good baseline for",
    "answer_b": "INESC-ID,  lsislif, unitn and  Webis.. Explanation: Table 3 shows the performance of our model compared to the top four models in the SemEval 2015 competition (note that only the F1-score is reported by SemEval for this task) and ParagraphVec.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the previous state of the art for sentiment categorization?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow does Tweet2Vec compare to them?\n< Explanation: The previous state of the art for sentiment categorization is the BERT model. BERT has been trained on the Twitter dataset and has been shown to be a good baseline for sentiment classification. The BERT model has been trained on 1.5 million tweets and has been shown to have a 99.5% accuracy on the Twitter dataset. The BERT model has been shown to be a good baseline for sentiment classification. The BERT model has been shown to be a good baseline for sentiment classification. The BERT model has been shown to be a good baseline for\n\nAnswer B:\nINESC-ID,  lsislif, unitn and  Webis.. Explanation: Table 3 shows the performance of our model compared to the top four models in the SemEval 2015 competition (note that only the F1-score is reported by SemEval for this task) and ParagraphVec.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1607.07514",
    "question": "what are the previous state of the art for tweet semantic similarity?",
    "gold_answer": "",
    "answer_a": "How does Tweet2Vec compare to them?\n< Explanation: The previous state of the art for tweet semantic similarity is the one proposed by BIBREF10. The model used in BIBREF10 is a simple CNN-LSTM model with a single hidden layer. The model was trained on 100,000 tweets, and the model was evaluated using cosine similarity. The model was trained on 100,000 tweets, and the model was evaluated using cosine similarity. The model was trained on 100,000 tweets, and the model was evaluated using cosine similarity. The",
    "answer_b": "nnfeats, ikr, linearsvm and svckernel.. Explanation: FLOAT SELECTED: Table 2: Results of the paraphrase and semantic similarity in Twitter task.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the previous state of the art for tweet semantic similarity?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow does Tweet2Vec compare to them?\n< Explanation: The previous state of the art for tweet semantic similarity is the one proposed by BIBREF10. The model used in BIBREF10 is a simple CNN-LSTM model with a single hidden layer. The model was trained on 100,000 tweets, and the model was evaluated using cosine similarity. The model was trained on 100,000 tweets, and the model was evaluated using cosine similarity. The model was trained on 100,000 tweets, and the model was evaluated using cosine similarity. The\n\nAnswer B:\nnnfeats, ikr, linearsvm and svckernel.. Explanation: FLOAT SELECTED: Table 2: Results of the paraphrase and semantic similarity in Twitter task.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.03680",
    "question": "By how much do they outperform baselines?",
    "gold_answer": "",
    "answer_a": "What are the baselines?\n< Explanation: We compare our algorithm with the following baselines: (1) a simple clustering algorithm based on the Expectation Maximization (EM) algorithm, (2) a clustering algorithm based on the Expectation Maximization (EM) algorithm with a Gaussian Mixture Model (GMM) prior, (3) a clustering algorithm based on the Expectation Maximization (EM) algorithm with a Gaussian Mixture Model (GMM) prior, (4) a clustering algorithm based on the Expectation Maximization (EM) algorithm with a Gaussian Mixture Model (GMM) prior, (5)",
    "answer_b": "On r=2 SEM-HMM Approx. is 2.2% better, on r=5 SEM-HMM is 3.9% better and on r=10 SEM-HMM is 3.9% better than the best baseline. Explanation: FLOAT SELECTED: Table 1: The average accuracy on the OMICS domains",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much do they outperform baselines?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the baselines?\n< Explanation: We compare our algorithm with the following baselines: (1) a simple clustering algorithm based on the Expectation Maximization (EM) algorithm, (2) a clustering algorithm based on the Expectation Maximization (EM) algorithm with a Gaussian Mixture Model (GMM) prior, (3) a clustering algorithm based on the Expectation Maximization (EM) algorithm with a Gaussian Mixture Model (GMM) prior, (4) a clustering algorithm based on the Expectation Maximization (EM) algorithm with a Gaussian Mixture Model (GMM) prior, (5)\n\nAnswer B:\nOn r=2 SEM-HMM Approx. is 2.2% better, on r=5 SEM-HMM is 3.9% better and on r=10 SEM-HMM is 3.9% better than the best baseline. Explanation: FLOAT SELECTED: Table 1: The average accuracy on the OMICS domains\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.03680",
    "question": "Which baselines do they use?",
    "gold_answer": "",
    "answer_a": "What are their results?\n< Explanation: We compare our algorithm with the following baselines: (1) a simple clustering algorithm based on the Expectation Maximization (EM) algorithm, (2) a clustering algorithm based on the Expectation Maximization (EM) algorithm with a Gaussian Mixture Model (GMM) prior, (3) a clustering algorithm based on the Expectation Maximization (EM) algorithm with a Gaussian Mixture Model (GMM) prior, (4) a clustering algorithm based on the Expectation Maximization (EM) algorithm with a Gaussian Mixture Model (GMM) prior, (5) a",
    "answer_b": "The \"frequency\" baseline, the \"conditional\" baseline, the \"BMM\" baseline and the \"BMM+EM\" baseline. Explanation: The evaluation task is to predict missing events from an observed sequence of events. For comparison, four baselines were also evaluated. The \u201cFrequency\u201d baseline predicts the most frequent event in the training set that is not found in the observed test sequence. The \u201cConditional\u201d baseline predicts the next event based on what most frequently follows the prior event. A third baseline, referred to as \u201cBMM,\u201d is a version of our algorithm that does not use EM for parameter estimation and instead only incrementally updates the parameters starting from the raw document counts. Further, it learns a standard HMM, that is, with no INLINEFORM0 transitions. This is very similar to the Bayesian Model Merging approach for HMMs BIBREF9 . The fourth baseline is the same as above, but uses our EM algorithm for parameter estimation without INLINEFORM1 transitions. It is referred to as \u201cBMM + EM.\u201d",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich baselines do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are their results?\n< Explanation: We compare our algorithm with the following baselines: (1) a simple clustering algorithm based on the Expectation Maximization (EM) algorithm, (2) a clustering algorithm based on the Expectation Maximization (EM) algorithm with a Gaussian Mixture Model (GMM) prior, (3) a clustering algorithm based on the Expectation Maximization (EM) algorithm with a Gaussian Mixture Model (GMM) prior, (4) a clustering algorithm based on the Expectation Maximization (EM) algorithm with a Gaussian Mixture Model (GMM) prior, (5) a\n\nAnswer B:\nThe \"frequency\" baseline, the \"conditional\" baseline, the \"BMM\" baseline and the \"BMM+EM\" baseline. Explanation: The evaluation task is to predict missing events from an observed sequence of events. For comparison, four baselines were also evaluated. The \u201cFrequency\u201d baseline predicts the most frequent event in the training set that is not found in the observed test sequence. The \u201cConditional\u201d baseline predicts the next event based on what most frequently follows the prior event. A third baseline, referred to as \u201cBMM,\u201d is a version of our algorithm that does not use EM for parameter estimation and instead only incrementally updates the parameters starting from the raw document counts. Further, it learns a standard HMM, that is, with no INLINEFORM0 transitions. This is very similar to the Bayesian Model Merging approach for HMMs BIBREF9 . The fourth baseline is the same as above, but uses our EM algorithm for parameter estimation without INLINEFORM1 transitions. It is referred to as \u201cBMM + EM.\u201d\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.03680",
    "question": "Which datasets do they evaluate on?",
    "gold_answer": "",
    "answer_a": "How do they evaluate?\n< Explanation: We evaluate our model on three datasets: (1) the 2015-2016 National English Language Test (NELT) dataset, (2) the 2016-2017 National English Language Test (NELT-2017) dataset, and (3) the 2017-2018 National English Language Test (NELT-2018) dataset. We use the 2015-2016 NELT dataset as the training set and the 2016-",
    "answer_b": "The Open Minds Indoor Common Sense (OMICS) corpus. Explanation: The Open Minds Indoor Common Sense (OMICS) corpus was developed by the Honda Research Institute and is based upon the Open Mind Common Sense project BIBREF17 . It describes 175 common household tasks with each task having 14 to 122 narratives describing, in short sentences, the necessary steps to complete it. Each narrative consists of temporally ordered, simple sentences from a single author that describe a plan to accomplish a task. Examples from the \u201cAnswer the Doorbell\u201d task can be found in Table 2. The OMICS corpus has 9044 individual narratives and its short and relatively consistent language lends itself to relatively easy event extraction.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich datasets do they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate?\n< Explanation: We evaluate our model on three datasets: (1) the 2015-2016 National English Language Test (NELT) dataset, (2) the 2016-2017 National English Language Test (NELT-2017) dataset, and (3) the 2017-2018 National English Language Test (NELT-2018) dataset. We use the 2015-2016 NELT dataset as the training set and the 2016-\n\nAnswer B:\nThe Open Minds Indoor Common Sense (OMICS) corpus. Explanation: The Open Minds Indoor Common Sense (OMICS) corpus was developed by the Honda Research Institute and is based upon the Open Mind Common Sense project BIBREF17 . It describes 175 common household tasks with each task having 14 to 122 narratives describing, in short sentences, the necessary steps to complete it. Each narrative consists of temporally ordered, simple sentences from a single author that describe a plan to accomplish a task. Examples from the \u201cAnswer the Doorbell\u201d task can be found in Table 2. The OMICS corpus has 9044 individual narratives and its short and relatively consistent language lends itself to relatively easy event extraction.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1706.00188",
    "question": "What deep learning methods do they look at?",
    "gold_answer": "",
    "answer_a": "CNN. Explanation: We investigate three neural network architectures for the task, described as follows. For each of the three methods, we initialize the word embeddings with either random embeddings or GloVe embeddings. (1) CNN: Inspired by Kim et. al BIBREF3 's work on using CNNs for sentiment classification, we leverage CNNs for hate speech detection. We use the same settings for the CNN as described in BIBREF3 . (2) LSTM: Unlike feed-forward neural networks, recurrent neural networks like LSTMs can use their internal memory to process arbitrary sequences of inputs. Hence, we use LSTMs to capture long range dependencies in tweets, which may play a role in hate speech detection. (3) FastText: FastText BIBREF4 represents a document by average of word vectors similar to the BoWV model, but allows update of word vectors through Back-propagation during training as opposed to the static word representation in the BoWV model, allowing the model to fine-tune the word representations according to the task.",
    "answer_b": "How do they evaluate them?\n< Explanation: We evaluate our methods on a benchmark dataset of 16K tweets annotated by human annotators. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat deep learning methods do they look at?\n\nGround Truth Answer:\n\n\nAnswer A:\nCNN. Explanation: We investigate three neural network architectures for the task, described as follows. For each of the three methods, we initialize the word embeddings with either random embeddings or GloVe embeddings. (1) CNN: Inspired by Kim et. al BIBREF3 's work on using CNNs for sentiment classification, we leverage CNNs for hate speech detection. We use the same settings for the CNN as described in BIBREF3 . (2) LSTM: Unlike feed-forward neural networks, recurrent neural networks like LSTMs can use their internal memory to process arbitrary sequences of inputs. Hence, we use LSTMs to capture long range dependencies in tweets, which may play a role in hate speech detection. (3) FastText: FastText BIBREF4 represents a document by average of word vectors similar to the BoWV model, but allows update of word vectors through Back-propagation during training as opposed to the static word representation in the BoWV model, allowing the model to fine-tune the word representations according to the task.\n\nAnswer B:\nHow do they evaluate them?\n< Explanation: We evaluate our methods on a benchmark dataset of 16K tweets annotated by human annotators. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset. We use the Twitter API to download the dataset\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1706.00188",
    "question": "What is their baseline?",
    "gold_answer": "",
    "answer_a": "How does it compare to the state-of-the-art?\n< Explanation: We compare our results to the state-of-the-art in the following ways: (1) the state-of-the-art baseline, (2) the baseline with a word embedding model, (3) the baseline with a word embedding model and a deep learning model, and (4) the baseline with a deep learning model and a deep learning model. We also compare our results to the state-of-the-art in the following ways: (1) the state-of-the-art baseline, (2) the baseline with a word embedding model, (3) the baseline with a word",
    "answer_b": "Char n-grams. Explanation: As baselines, we experiment with three broad representations. (1) Char n-grams: It is the state-of-the-art method BIBREF0 which uses character n-grams for hate speech detection. (2) TF-IDF: TF-IDF are typical features used for text classification. (3) BoWV: Bag of Words Vector approach uses the average of the word (GloVe) embeddings to represent a sentence. We experiment with multiple classifiers for both the TF-IDF and the BoWV approaches.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is their baseline?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow does it compare to the state-of-the-art?\n< Explanation: We compare our results to the state-of-the-art in the following ways: (1) the state-of-the-art baseline, (2) the baseline with a word embedding model, (3) the baseline with a word embedding model and a deep learning model, and (4) the baseline with a deep learning model and a deep learning model. We also compare our results to the state-of-the-art in the following ways: (1) the state-of-the-art baseline, (2) the baseline with a word embedding model, (3) the baseline with a word\n\nAnswer B:\nChar n-grams. Explanation: As baselines, we experiment with three broad representations. (1) Char n-grams: It is the state-of-the-art method BIBREF0 which uses character n-grams for hate speech detection. (2) TF-IDF: TF-IDF are typical features used for text classification. (3) BoWV: Bag of Words Vector approach uses the average of the word (GloVe) embeddings to represent a sentence. We experiment with multiple classifiers for both the TF-IDF and the BoWV approaches.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1706.00188",
    "question": "Which architectures do they experiment with?",
    "gold_answer": "",
    "answer_a": "CNN. Explanation: We investigate three neural network architectures for the task, described as follows. For each of the three methods, we initialize the word embeddings with either random embeddings or GloVe embeddings. (1) CNN: Inspired by Kim et. al BIBREF3 's work on using CNNs for sentiment classification, we leverage CNNs for hate speech detection. We use the same settings for the CNN as described in BIBREF3 . (2) LSTM: Unlike feed-forward neural networks, recurrent neural networks like LSTMs can use their internal memory to process arbitrary sequences of inputs. Hence, we use LSTMs to capture long range dependencies in tweets, which may play a role in hate speech detection. (3) FastText: FastText BIBREF4 represents a document by average of word vectors similar to the BoWV model, but allows update of word vectors through Back-propagation during training as opposed to the static word representation in the BoWV model, allowing the model to fine-tune the word representations according to the task.",
    "answer_b": "What are the results?\n< Explanation: We experiment with the following architectures: (1) word2vec, (2) word2vec with a word embedding size of 100, (3) word2vec with a word embedding size of 200, (4) word2vec with a word embedding size of 300, (5) word2vec with a word embedding size of 400, (6) word2vec with a word embedding size of 500, (7) word2vec with a word embedding size of 600, (8) word",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich architectures do they experiment with?\n\nGround Truth Answer:\n\n\nAnswer A:\nCNN. Explanation: We investigate three neural network architectures for the task, described as follows. For each of the three methods, we initialize the word embeddings with either random embeddings or GloVe embeddings. (1) CNN: Inspired by Kim et. al BIBREF3 's work on using CNNs for sentiment classification, we leverage CNNs for hate speech detection. We use the same settings for the CNN as described in BIBREF3 . (2) LSTM: Unlike feed-forward neural networks, recurrent neural networks like LSTMs can use their internal memory to process arbitrary sequences of inputs. Hence, we use LSTMs to capture long range dependencies in tweets, which may play a role in hate speech detection. (3) FastText: FastText BIBREF4 represents a document by average of word vectors similar to the BoWV model, but allows update of word vectors through Back-propagation during training as opposed to the static word representation in the BoWV model, allowing the model to fine-tune the word representations according to the task.\n\nAnswer B:\nWhat are the results?\n< Explanation: We experiment with the following architectures: (1) word2vec, (2) word2vec with a word embedding size of 100, (3) word2vec with a word embedding size of 200, (4) word2vec with a word embedding size of 300, (5) word2vec with a word embedding size of 400, (6) word2vec with a word embedding size of 500, (7) word2vec with a word embedding size of 600, (8) word\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1706.00188",
    "question": "Are pretrained embeddings used?",
    "gold_answer": "",
    "answer_a": "GloVe. Explanation: For the embedding based methods, we used the GloVe BIBREF5 pre-trained word embeddings.",
    "answer_b": "How are they used?\n< Explanation: We use a pre-trained word embedding model BIBREF11 to learn embeddings for the tweets. We use the BIBREF11 model as a baseline. We also use a pre-trained word embedding model BIBREF12 to learn embeddings for the tweets. We use the BIBREF12 model as a baseline. We use the BIBREF12 model as a baseline. We use the BIBREF12 model as a baseline. We use the BIBREF12 model as a baseline. We use the BIBREF12 model as a baseline. We",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nAre pretrained embeddings used?\n\nGround Truth Answer:\n\n\nAnswer A:\nGloVe. Explanation: For the embedding based methods, we used the GloVe BIBREF5 pre-trained word embeddings.\n\nAnswer B:\nHow are they used?\n< Explanation: We use a pre-trained word embedding model BIBREF11 to learn embeddings for the tweets. We use the BIBREF11 model as a baseline. We also use a pre-trained word embedding model BIBREF12 to learn embeddings for the tweets. We use the BIBREF12 model as a baseline. We use the BIBREF12 model as a baseline. We use the BIBREF12 model as a baseline. We use the BIBREF12 model as a baseline. We use the BIBREF12 model as a baseline. We\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1609.02075",
    "question": "What sociolinguistic variables (phonetic spellings) did they analyze? ",
    "gold_answer": "",
    "answer_a": "variation and change in the use of words characteristic from eight US cities that have non-standard spellings. Explanation: Here, we focus on the most easily observable and measurable level: variation and change in the use of individual words.",
    "answer_b": "Explanation: We use a data set of 1.5 million tweets per day from Twitter, which we call the Twitter Corpus. We use the Twitter Corpus to track language change in progress, and to test the hypothesis that social influence is a form of contagion. We use a data set of 1.5 million tweets per day from Twitter, which we call the Twitter Corpus. We use the Twitter Corpus to track language change in progress, and to test the hypothesis that social influence is a form of contagion. We use a data set of 1.5 million tweets per day from Twitter, which we call the Twitter Corpus.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat sociolinguistic variables (phonetic spellings) did they analyze? \n\nGround Truth Answer:\n\n\nAnswer A:\nvariation and change in the use of words characteristic from eight US cities that have non-standard spellings. Explanation: Here, we focus on the most easily observable and measurable level: variation and change in the use of individual words.\n\nAnswer B:\nExplanation: We use a data set of 1.5 million tweets per day from Twitter, which we call the Twitter Corpus. We use the Twitter Corpus to track language change in progress, and to test the hypothesis that social influence is a form of contagion. We use a data set of 1.5 million tweets per day from Twitter, which we call the Twitter Corpus. We use the Twitter Corpus to track language change in progress, and to test the hypothesis that social influence is a form of contagion. We use a data set of 1.5 million tweets per day from Twitter, which we call the Twitter Corpus.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.09025",
    "question": "How many domains do they create ontologies for?",
    "gold_answer": "",
    "answer_a": "4. Explanation: Figure FIGREF55 shows exhaustive hierarchical topic trees extracted from a small text sample with topics from four domains: INLINEFORM0 , INLINEFORM1 INLINEFORM2 , INLINEFORM3 , and INLINEFORM4 .",
    "answer_b": "What are the domains?\n< Explanation: We create 10 ontologies for 10 different domains. The domains are: agriculture, agriculture and natural resources, agriculture and natural resources, agriculture and natural resources, agriculture and natural resources, agriculture and natural resources, agriculture and natural resources, agriculture and natural resources, agriculture and natural resources, agriculture and natural resources. The number of topics in each ontology is 10, 10, 10, 10, 10, 10, 10, 10, 10, 10. The number of documents in each ontology",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many domains do they create ontologies for?\n\nGround Truth Answer:\n\n\nAnswer A:\n4. Explanation: Figure FIGREF55 shows exhaustive hierarchical topic trees extracted from a small text sample with topics from four domains: INLINEFORM0 , INLINEFORM1 INLINEFORM2 , INLINEFORM3 , and INLINEFORM4 .\n\nAnswer B:\nWhat are the domains?\n< Explanation: We create 10 ontologies for 10 different domains. The domains are: agriculture, agriculture and natural resources, agriculture and natural resources, agriculture and natural resources, agriculture and natural resources, agriculture and natural resources, agriculture and natural resources, agriculture and natural resources, agriculture and natural resources, agriculture and natural resources. The number of topics in each ontology is 10, 10, 10, 10, 10, 10, 10, 10, 10, 10. The number of documents in each ontology\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.09025",
    "question": "How do they measure the usefulness of obtained ontologies compared to domain expert ones?",
    "gold_answer": "",
    "answer_a": "precision. Explanation: We use KB-LDA, phrase_hLDA, and LDA+GSHL as our baseline methods, and compare ontologies extracted from hrLDA, KB-LDA, phrase_hLDA, and LDA+GSHL with DBpedia ontologies. We use precision, recall and F-measure for this ontology evaluation",
    "answer_b": "What are the evaluation metrics used?\n< Explanation: We evaluate the usefulness of the ontologies created by hrLDA by comparing them with the ontologies created by domain experts. We use the F1 score as the evaluation metric. The F1 score is defined as the harmonic mean of precision and recall, where precision is the ratio of the number of correct predictions to the total number of predictions, and recall is the ratio of the number of correct predictions to the number of actual observations. The F1 score is a widely used metric for evaluating the quality of a classification system. The higher the F1 score, the better the classification system is",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they measure the usefulness of obtained ontologies compared to domain expert ones?\n\nGround Truth Answer:\n\n\nAnswer A:\nprecision. Explanation: We use KB-LDA, phrase_hLDA, and LDA+GSHL as our baseline methods, and compare ontologies extracted from hrLDA, KB-LDA, phrase_hLDA, and LDA+GSHL with DBpedia ontologies. We use precision, recall and F-measure for this ontology evaluation\n\nAnswer B:\nWhat are the evaluation metrics used?\n< Explanation: We evaluate the usefulness of the ontologies created by hrLDA by comparing them with the ontologies created by domain experts. We use the F1 score as the evaluation metric. The F1 score is defined as the harmonic mean of precision and recall, where precision is the ratio of the number of correct predictions to the total number of predictions, and recall is the ratio of the number of correct predictions to the number of actual observations. The F1 score is a widely used metric for evaluating the quality of a classification system. The higher the F1 score, the better the classification system is\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.09025",
    "question": "How do they obtain syntax from raw documents in hrLDA?",
    "gold_answer": "",
    "answer_a": "By extracting syntactically related noun phrases and their connections using a language parser.. Explanation: Extracting relation triplets is the essential step of hrLDA, and it is also the key process for converting a hierarchical topic tree to an ontology structure. The idea is to find all syntactically related noun phrases and their connections using a language parser such as the Stanford NLP parser BIBREF24 and Ollie BIBREF25 .",
    "answer_b": "What is the source of the document structure?\n< Explanation: We first extract noun phrases from raw documents using a simple rule-based method. We then use a simple rule-based method to extract the document structure. The document structure is a tree-like structure, which is used to represent the hierarchical structure of the document. The tree structure is constructed by recursively traversing the document. The tree structure is used to represent the hierarchical structure of the document. The tree structure is constructed by recursively traversing the document. The tree structure is used to represent the hierarchical structure of the document. The tree structure is used to represent the hierarchical structure of the document.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they obtain syntax from raw documents in hrLDA?\n\nGround Truth Answer:\n\n\nAnswer A:\nBy extracting syntactically related noun phrases and their connections using a language parser.. Explanation: Extracting relation triplets is the essential step of hrLDA, and it is also the key process for converting a hierarchical topic tree to an ontology structure. The idea is to find all syntactically related noun phrases and their connections using a language parser such as the Stanford NLP parser BIBREF24 and Ollie BIBREF25 .\n\nAnswer B:\nWhat is the source of the document structure?\n< Explanation: We first extract noun phrases from raw documents using a simple rule-based method. We then use a simple rule-based method to extract the document structure. The document structure is a tree-like structure, which is used to represent the hierarchical structure of the document. The tree structure is constructed by recursively traversing the document. The tree structure is used to represent the hierarchical structure of the document. The tree structure is constructed by recursively traversing the document. The tree structure is used to represent the hierarchical structure of the document. The tree structure is used to represent the hierarchical structure of the document.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.04478",
    "question": "What datasets are available for CDSA task?",
    "gold_answer": "",
    "answer_a": "DRANZIERA benchmark dataset. Explanation: The core of this work is a sentiment classifier for different domains. We use the DRANZIERA benchmark dataset BIBREF9, which consists of Amazon reviews from 20 domains such as automatives, baby products, beauty products, etc. The detailed list can be seen in Table 1.",
    "answer_b": "What are the metrics used?\n< Explanation: We use the following datasets for CDSA task: (1) the Stanford Sentiment Treebank (SST) BIBREF1 , (2) the Stanford Sentiment Treebank (SST) CTD BIBREF2 , (3) the Stanford Sentiment Treebank (SST) CTD-1 BIBREF3 , (4) the Stanford Sentiment Treebank (SST) CTD-2 BIBREF4 , (5) the Stanford Sentiment Treebank (SST) CTD-3 BIBREF5 , (6) the Stanford",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets are available for CDSA task?\n\nGround Truth Answer:\n\n\nAnswer A:\nDRANZIERA benchmark dataset. Explanation: The core of this work is a sentiment classifier for different domains. We use the DRANZIERA benchmark dataset BIBREF9, which consists of Amazon reviews from 20 domains such as automatives, baby products, beauty products, etc. The detailed list can be seen in Table 1.\n\nAnswer B:\nWhat are the metrics used?\n< Explanation: We use the following datasets for CDSA task: (1) the Stanford Sentiment Treebank (SST) BIBREF1 , (2) the Stanford Sentiment Treebank (SST) CTD BIBREF2 , (3) the Stanford Sentiment Treebank (SST) CTD-1 BIBREF3 , (4) the Stanford Sentiment Treebank (SST) CTD-2 BIBREF4 , (5) the Stanford Sentiment Treebank (SST) CTD-3 BIBREF5 , (6) the Stanford\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.04478",
    "question": "What two novel metrics proposed?",
    "gold_answer": "",
    "answer_a": "ULM4. Explanation: We devise two different metrics out of FastText models to calculate the similarity between domain-pairs. In the first metric (ULM4), we compute the Angular Similarity between the word vectors for all the common adjectives, and for each domain pair just like Word2Vec and GloVe. Overall, similarity for a domain pair is calculated using equation (DISPLAY_FORM29). As an additional metric (ULM5), we extract sentence vectors for reviews and follow a procedure similar to Doc2Vec. SentiWordnet is used to filter out train and test data using the same threshold window of $\\pm 0.01$.",
    "answer_b": "How are they evaluated?\n< Explanation: We propose two novel metrics for evaluating domain adaptability to help source domain selection of labelled data and utilize word and sentence-based embeddings as metrics for unlabelled data. The first metric is the similarity between the source domain and the target domain. The second metric is the similarity between the source domain and the target domain and the similarity between the source domain and the K best K source domains. We call this metric the K-Source Similarity (K-SS). The K-SS metric is evaluated using the following two metrics: (1) the similarity between the source domain and the target domain, and",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat two novel metrics proposed?\n\nGround Truth Answer:\n\n\nAnswer A:\nULM4. Explanation: We devise two different metrics out of FastText models to calculate the similarity between domain-pairs. In the first metric (ULM4), we compute the Angular Similarity between the word vectors for all the common adjectives, and for each domain pair just like Word2Vec and GloVe. Overall, similarity for a domain pair is calculated using equation (DISPLAY_FORM29). As an additional metric (ULM5), we extract sentence vectors for reviews and follow a procedure similar to Doc2Vec. SentiWordnet is used to filter out train and test data using the same threshold window of $\\pm 0.01$.\n\nAnswer B:\nHow are they evaluated?\n< Explanation: We propose two novel metrics for evaluating domain adaptability to help source domain selection of labelled data and utilize word and sentence-based embeddings as metrics for unlabelled data. The first metric is the similarity between the source domain and the target domain. The second metric is the similarity between the source domain and the target domain and the similarity between the source domain and the K best K source domains. We call this metric the K-Source Similarity (K-SS). The K-SS metric is evaluated using the following two metrics: (1) the similarity between the source domain and the target domain, and\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.04478",
    "question": "What similarity metrics have been tried?",
    "gold_answer": "",
    "answer_a": "What are the results?\n< Explanation: We consider 11 similarity metrics for evaluating domain adaptability to help source domain selection of labelled data. These metrics include two novel metrics for evaluating domain adaptability to help source domain selection of labelled data and utilize word and sentence-based embeddings as metrics for unlabelled data. The metrics are: (1) cosine similarity, (2) Jaccard similarity, (3) Jaccard similarity with word embeddings, (4) Jaccard similarity with sentence embeddings, (5) cosine similarity with word embeddings, (6) cosine similarity with sentence embeddings, (7) cosine similarity with word",
    "answer_b": "LM1: Significant Words Overlap. Explanation: When target domain data is labelled, we use the following four metrics for comparing and ranking source domains for a particular target domain:\n\nSimilarity Metrics ::: Metrics: Labelled Data ::: LM1: Significant Words Overlap\nAll words in a domain are not significant for sentiment expression. For example, comfortable is significant in the `Clothing' domain but not as significant in the `Movie' domain. In this metric, we build upon existing work by sharma2018identifying and extract significant words from each domain using the $\\chi ^2$ test. This method relies on computing the statistical significance of a word based on the polarity of that word in the domain. For our experiments, we consider only the words which appear at least 10 times in the corpus and have a $\\chi ^2$ value greater than or equal to 1. The $\\chi ^2$ value is calculated as follows:\n\nWhere ${c_p}^w$ and ${c_n}^w$ are the observed counts of word $w$ in positive and negative reviews, respectively. $\\mu ^w$ is the expected count, which is kept as half of the total number of occurrences of $w$ in the corpus. We hypothesize that, if a domain-pair $(D_1,D_2)$ shares a larger number of significant words than the pair $(D_1,D_3)$, then $D_1$ is closer to $D_2$ as compared to $D_3$, since they use relatively higher number of similar words for sentiment expression. For every target domain, we compute the intersection of significant words with all other domains and rank them on the basis of intersection count. The utility of this metric is that it can also be used in a scenario where target domain data is unlabelled, but source domain data is labelled. It is due to the fact that once we obtain significant words in the source domain, we just need to search for them in the target domain to find out common significant words.\n\nSimilarity Metrics ::: Metrics: Labelled Data ::: LM2: Symmetric KL-Divergence (SKLD)\nKL Divergence can be used to compare the probabilistic distribution of polar words in two domains BIBREF10. A lower KL Divergence score indicates that the probabilistic distribution of polar words in two domains is identical. This implies that the domains are close to each other, in terms of sentiment similarity. Therefore, to rank source domains for a target domain using this metric, we inherit the concept of symmetric KL Divergence proposed by murthy2018judicious and use it to compute average Symmetric KL-Divergence of common polar words shared by a domain-pair. We label a word as `polar' for a domain if,\n\nwhere $P$ is the probability of a word appearing in a review which is labelled positive and $N$ is the probability of a word appearing in a review which is labelled negative.\n\nSKLD of a polar word for domain-pair $(D_1,D_2)$ is calculated as:\n\nwhere $P_i$ and $N_i$ are probabilities of a word appearing under positively labelled and negatively labelled reviews, respectively, in domain $i$. We then take an average of all common polar words.\n\nWe observe that, on its own, this metric performs rather poorly. Upon careful analysis of results, we concluded that the imbalance in the number of polar words being shared across domain-pairs is a reason for poor performance. To mitigate this, we compute a confidence term for a domain-pair $(D_1,D_2)$ using the Jaccard Similarity Coefficient which is calculated as follows:\n\nwhere $C$ is the number of common polar words and $W_1$ and $W_2$ are number of polar words in $D_1$ and $D_2$ respectively. The intuition behind this being that the domain-pairs having higher percentage of polar words overlap should be ranked higher compared to those having relatively higher number of polar words. For example, we prefer $(C:40,W_1 :50,W_2 :50)$ over $(C:200,W_1 :500,W_2 :500)$ even though 200 is greater than 40. To compute the final similarity value, we add the reciprocal of $J$ to the SKLD value since a larger value of $J$ will add a smaller fraction to SLKD value. For a smaller SKLD value, the domains would be relatively more similar. This is computed as follows:\n\nDomain pairs are ranked in increasing order of this similarity value. After the introduction of the confidence term, a significant improvement in the results is observed.\n\nSimilarity Metrics ::: Metrics: Labelled Data ::: LM3: Chameleon Words Similarity\nThis metric is our novel contribution for domain adaptability evaluation. It helps in detection of `Chameleon Word(s)' which change their polarity across domains BIBREF11. The motivation comes from the fact that chameleon words directly affect the CDSA accuracy. For example, poignant is positive in movie domain whereas negative in many other domains viz. Beauty, Clothing etc.\n\nFor every common polar word between two domains, $L_1 \\ Distance$ between two vectors $[P_1,N_1]$ and $[P_2,N_2]$ is calculated as;\n\nThe overall distance is an average overall common polar words. Similar to SKLD, the confidence term based on Jaccard Similarity Coefficient is used to counter the imbalance of common polar word count between domain-pairs.\n\nDomain pairs are ranked in increasing order of final value.\n\nSimilarity Metrics ::: Metrics: Labelled Data ::: LM4: Entropy Change\nEntropy is the degree of randomness. A relatively lower change in entropy, when two domains are concatenated, indicates that the two domains contain similar topics and are therefore closer to each other. This metric is also our novel contribution. Using this metric, we calculate the percentage change in the entropy when the target domain is concatenated with the source domain. We calculate the entropy as the combination of entropy for unigrams, bigrams, trigrams, and quadrigrams. We consider only polar words for unigrams. For bi, tri and quadrigrams, we give priority to polar words by using a weighted entropy function and this weighted entropy $E$ is calculated as:\n\nHere, $X$ is the set of n-grams that contain at least one polar word, $Y$ is the set of n-grams which do not contain any polar word, and $w$ is the weight. For our experiments, we keep the value of $w$ as 1 for unigrams and 5 for bi, tri, and quadrigrams.\n\nWe then say that a source domain $D_2$ is more suitable for target domain $D_1$ as compared to source domain $D_3$ if;\n\nwhere $D_2+D_1$ indicates combined data obtained by mixing $D_1$ in $D_2$ and $\\Delta E$ indicates percentage change in entropy before and after mixing of source and target domains.\n\nNote that this metric offers the advantage of asymmetricity, unlike the other three metrics for labelled data.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat similarity metrics have been tried?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n< Explanation: We consider 11 similarity metrics for evaluating domain adaptability to help source domain selection of labelled data. These metrics include two novel metrics for evaluating domain adaptability to help source domain selection of labelled data and utilize word and sentence-based embeddings as metrics for unlabelled data. The metrics are: (1) cosine similarity, (2) Jaccard similarity, (3) Jaccard similarity with word embeddings, (4) Jaccard similarity with sentence embeddings, (5) cosine similarity with word embeddings, (6) cosine similarity with sentence embeddings, (7) cosine similarity with word\n\nAnswer B:\nLM1: Significant Words Overlap. Explanation: When target domain data is labelled, we use the following four metrics for comparing and ranking source domains for a particular target domain:\n\nSimilarity Metrics ::: Metrics: Labelled Data ::: LM1: Significant Words Overlap\nAll words in a domain are not significant for sentiment expression. For example, comfortable is significant in the `Clothing' domain but not as significant in the `Movie' domain. In this metric, we build upon existing work by sharma2018identifying and extract significant words from each domain using the $\\chi ^2$ test. This method relies on computing the statistical significance of a word based on the polarity of that word in the domain. For our experiments, we consider only the words which appear at least 10 times in the corpus and have a $\\chi ^2$ value greater than or equal to 1. The $\\chi ^2$ value is calculated as follows:\n\nWhere ${c_p}^w$ and ${c_n}^w$ are the observed counts of word $w$ in positive and negative reviews, respectively. $\\mu ^w$ is the expected count, which is kept as half of the total number of occurrences of $w$ in the corpus. We hypothesize that, if a domain-pair $(D_1,D_2)$ shares a larger number of significant words than the pair $(D_1,D_3)$, then $D_1$ is closer to $D_2$ as compared to $D_3$, since they use relatively higher number of similar words for sentiment expression. For every target domain, we compute the intersection of significant words with all other domains and rank them on the basis of intersection count. The utility of this metric is that it can also be used in a scenario where target domain data is unlabelled, but source domain data is labelled. It is due to the fact that once we obtain significant words in the source domain, we just need to search for them in the target domain to find out common significant words.\n\nSimilarity Metrics ::: Metrics: Labelled Data ::: LM2: Symmetric KL-Divergence (SKLD)\nKL Divergence can be used to compare the probabilistic distribution of polar words in two domains BIBREF10. A lower KL Divergence score indicates that the probabilistic distribution of polar words in two domains is identical. This implies that the domains are close to each other, in terms of sentiment similarity. Therefore, to rank source domains for a target domain using this metric, we inherit the concept of symmetric KL Divergence proposed by murthy2018judicious and use it to compute average Symmetric KL-Divergence of common polar words shared by a domain-pair. We label a word as `polar' for a domain if,\n\nwhere $P$ is the probability of a word appearing in a review which is labelled positive and $N$ is the probability of a word appearing in a review which is labelled negative.\n\nSKLD of a polar word for domain-pair $(D_1,D_2)$ is calculated as:\n\nwhere $P_i$ and $N_i$ are probabilities of a word appearing under positively labelled and negatively labelled reviews, respectively, in domain $i$. We then take an average of all common polar words.\n\nWe observe that, on its own, this metric performs rather poorly. Upon careful analysis of results, we concluded that the imbalance in the number of polar words being shared across domain-pairs is a reason for poor performance. To mitigate this, we compute a confidence term for a domain-pair $(D_1,D_2)$ using the Jaccard Similarity Coefficient which is calculated as follows:\n\nwhere $C$ is the number of common polar words and $W_1$ and $W_2$ are number of polar words in $D_1$ and $D_2$ respectively. The intuition behind this being that the domain-pairs having higher percentage of polar words overlap should be ranked higher compared to those having relatively higher number of polar words. For example, we prefer $(C:40,W_1 :50,W_2 :50)$ over $(C:200,W_1 :500,W_2 :500)$ even though 200 is greater than 40. To compute the final similarity value, we add the reciprocal of $J$ to the SKLD value since a larger value of $J$ will add a smaller fraction to SLKD value. For a smaller SKLD value, the domains would be relatively more similar. This is computed as follows:\n\nDomain pairs are ranked in increasing order of this similarity value. After the introduction of the confidence term, a significant improvement in the results is observed.\n\nSimilarity Metrics ::: Metrics: Labelled Data ::: LM3: Chameleon Words Similarity\nThis metric is our novel contribution for domain adaptability evaluation. It helps in detection of `Chameleon Word(s)' which change their polarity across domains BIBREF11. The motivation comes from the fact that chameleon words directly affect the CDSA accuracy. For example, poignant is positive in movie domain whereas negative in many other domains viz. Beauty, Clothing etc.\n\nFor every common polar word between two domains, $L_1 \\ Distance$ between two vectors $[P_1,N_1]$ and $[P_2,N_2]$ is calculated as;\n\nThe overall distance is an average overall common polar words. Similar to SKLD, the confidence term based on Jaccard Similarity Coefficient is used to counter the imbalance of common polar word count between domain-pairs.\n\nDomain pairs are ranked in increasing order of final value.\n\nSimilarity Metrics ::: Metrics: Labelled Data ::: LM4: Entropy Change\nEntropy is the degree of randomness. A relatively lower change in entropy, when two domains are concatenated, indicates that the two domains contain similar topics and are therefore closer to each other. This metric is also our novel contribution. Using this metric, we calculate the percentage change in the entropy when the target domain is concatenated with the source domain. We calculate the entropy as the combination of entropy for unigrams, bigrams, trigrams, and quadrigrams. We consider only polar words for unigrams. For bi, tri and quadrigrams, we give priority to polar words by using a weighted entropy function and this weighted entropy $E$ is calculated as:\n\nHere, $X$ is the set of n-grams that contain at least one polar word, $Y$ is the set of n-grams which do not contain any polar word, and $w$ is the weight. For our experiments, we keep the value of $w$ as 1 for unigrams and 5 for bi, tri, and quadrigrams.\n\nWe then say that a source domain $D_2$ is more suitable for target domain $D_1$ as compared to source domain $D_3$ if;\n\nwhere $D_2+D_1$ indicates combined data obtained by mixing $D_1$ in $D_2$ and $\\Delta E$ indicates percentage change in entropy before and after mixing of source and target domains.\n\nNote that this metric offers the advantage of asymmetricity, unlike the other three metrics for labelled data.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.04478",
    "question": "What 20 domains are available for selection of source domain?",
    "gold_answer": "",
    "answer_a": "What are the similarity metrics used?\n< Explanation: We consider 20 domains (all possible pairs) from the following list: 1) English, 2) French, 3) German, 4) Spanish, 5) Italian, 6) Portuguese, 7) Arabic, 8) Chinese, 9) Japanese, 10) Korean, 11) Russian, 12) Arabic, 13) Chinese, 14) Chinese, 15) Chinese, 16) Chinese, 17) Chinese, 18) Chinese, 19",
    "answer_b": "Amazon Instant Video,  Automotive, Baby,  Beauty, Books,  Clothing Accessories,  Electronics,  Health, Home, Kitchen, Movies, Music,  Office Products, Patio, Pet Supplies, Shoes,  Software,  Sports Outdoors,  Tools Home Improvement, Toys Games, Video Games.. Explanation: We use the DRANZIERA benchmark dataset BIBREF9, which consists of Amazon reviews from 20 domains such as automatives, baby products, beauty products, etc. The detailed list can be seen in Table 1.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat 20 domains are available for selection of source domain?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the similarity metrics used?\n< Explanation: We consider 20 domains (all possible pairs) from the following list: 1) English, 2) French, 3) German, 4) Spanish, 5) Italian, 6) Portuguese, 7) Arabic, 8) Chinese, 9) Japanese, 10) Korean, 11) Russian, 12) Arabic, 13) Chinese, 14) Chinese, 15) Chinese, 16) Chinese, 17) Chinese, 18) Chinese, 19\n\nAnswer B:\nAmazon Instant Video,  Automotive, Baby,  Beauty, Books,  Clothing Accessories,  Electronics,  Health, Home, Kitchen, Movies, Music,  Office Products, Patio, Pet Supplies, Shoes,  Software,  Sports Outdoors,  Tools Home Improvement, Toys Games, Video Games.. Explanation: We use the DRANZIERA benchmark dataset BIBREF9, which consists of Amazon reviews from 20 domains such as automatives, baby products, beauty products, etc. The detailed list can be seen in Table 1.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.04558",
    "question": "why do they think sentiment features do not result in improvement?",
    "gold_answer": "",
    "answer_a": "There is no answer to this question.\nThe reason is that the sentiment lexicon features are not very informative for the task. The sentiment lexicon features are based on the sentiment of the words in the tweet. The sentiment lexicon features are not very informative for the task. The sentiment lexicon features are based on the sentiment of the words in the tweet. The sentiment lexicon features are not very informative for the task. The sentiment lexicon features are not very informative for the task. The sentiment lexicon features are not very informative for the task. The sentiment lexicon features are not very informative for the task. The sentiment lexicon",
    "answer_b": "Because sentiment features extracted the same information as other features.. Explanation: However, removing any one feature subgroup in this group (e.g., general INLINEFORM0 -grams, general clusters, general embeddings, etc.) results only in slight drop or even increase in the performance (rows b.1\u2013b.4). This indicates that the features in this group capture similar information.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhy do they think sentiment features do not result in improvement?\n\nGround Truth Answer:\n\n\nAnswer A:\nThere is no answer to this question.\nThe reason is that the sentiment lexicon features are not very informative for the task. The sentiment lexicon features are based on the sentiment of the words in the tweet. The sentiment lexicon features are not very informative for the task. The sentiment lexicon features are based on the sentiment of the words in the tweet. The sentiment lexicon features are not very informative for the task. The sentiment lexicon features are not very informative for the task. The sentiment lexicon features are not very informative for the task. The sentiment lexicon features are not very informative for the task. The sentiment lexicon\n\nAnswer B:\nBecause sentiment features extracted the same information as other features.. Explanation: However, removing any one feature subgroup in this group (e.g., general INLINEFORM0 -grams, general clusters, general embeddings, etc.) results only in slight drop or even increase in the performance (rows b.1\u2013b.4). This indicates that the features in this group capture similar information.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.04558",
    "question": "what was the size of the datasets?",
    "gold_answer": "",
    "answer_a": "< Explanation: The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter",
    "answer_b": "10822, 4845. Explanation: Two labeled datasets were provided to the participants: a training set containing 10,822 tweets and a development set containing 4,845 tweets.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat was the size of the datasets?\n\nGround Truth Answer:\n\n\nAnswer A:\n< Explanation: The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter API. The datasets were created using the Twitter\n\nAnswer B:\n10822, 4845. Explanation: Two labeled datasets were provided to the participants: a training set containing 10,822 tweets and a development set containing 4,845 tweets.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.04558",
    "question": "what were the evaluation metrics?",
    "gold_answer": "",
    "answer_a": "micro-averaged F-score of the class 1 (intake) and class 2 (possible intake). Explanation: The official evaluation metric for this task was micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1",
    "answer_b": "How were they computed?\n< Explanation: The evaluation metrics were the F1 score, the precision and the recall. The F1 score is the harmonic mean of precision and recall, and is a measure of the model's performance. The precision is the ratio of the number of correct positive predictions to the total number of positive predictions. The recall is the ratio of the number of correct positive predictions to the total number of actual observations. The precision and recall are calculated for each class separately. The precision and recall are then averaged to obtain the overall F1 score. The precision and recall are calculated for each class separately. The precision and recall are",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat were the evaluation metrics?\n\nGround Truth Answer:\n\n\nAnswer A:\nmicro-averaged F-score of the class 1 (intake) and class 2 (possible intake). Explanation: The official evaluation metric for this task was micro-averaged F-score of the class 1 (intake) and class 2 (possible intake): INLINEFORM0 INLINEFORM1\n\nAnswer B:\nHow were they computed?\n< Explanation: The evaluation metrics were the F1 score, the precision and the recall. The F1 score is the harmonic mean of precision and recall, and is a measure of the model's performance. The precision is the ratio of the number of correct positive predictions to the total number of positive predictions. The recall is the ratio of the number of correct positive predictions to the total number of actual observations. The precision and recall are calculated for each class separately. The precision and recall are then averaged to obtain the overall F1 score. The precision and recall are calculated for each class separately. The precision and recall are\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.04558",
    "question": "what were their results on both tasks?",
    "gold_answer": "",
    "answer_a": "How did they do?\n< Explanation: Table 1 shows the results of the two tasks. The first column shows the number of tweets in each class. The second column shows the number of tweets in each class that were classified as positive (1) or negative (0). The third column shows the number of tweets in each class that were classified as positive (1) or negative (0). The fourth column shows the number of tweets in each class that were classified as positive (1) or negative (0). The fifth column shows the number of tweets in each class that were classified as positive (1) or negative (0). The",
    "answer_b": "0.435 on Task1 and 0.673 on Task2.. Explanation: The results for our three official submissions are presented in Table TABREF39 (rows c.1\u2013c.3). The best results in INLINEFORM0 were obtained with submission 1 (row c.1). The results for submission 2 are the lowest, with F-measure being 3.5 percentage points lower than the result for submission 1 (row c.2).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat were their results on both tasks?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow did they do?\n< Explanation: Table 1 shows the results of the two tasks. The first column shows the number of tweets in each class. The second column shows the number of tweets in each class that were classified as positive (1) or negative (0). The third column shows the number of tweets in each class that were classified as positive (1) or negative (0). The fourth column shows the number of tweets in each class that were classified as positive (1) or negative (0). The fifth column shows the number of tweets in each class that were classified as positive (1) or negative (0). The\n\nAnswer B:\n0.435 on Task1 and 0.673 on Task2.. Explanation: The results for our three official submissions are presented in Table TABREF39 (rows c.1\u2013c.3). The best results in INLINEFORM0 were obtained with submission 1 (row c.1). The results for submission 2 are the lowest, with F-measure being 3.5 percentage points lower than the result for submission 1 (row c.2).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.04558",
    "question": "what domain-specific features did they train on?",
    "gold_answer": "",
    "answer_a": "INLINEFORM0 -grams generalized over domain terms. Explanation: Twitter-specific features: the number of tokens with all characters in upper case, the number of hashtags, presence of positive and negative emoticons, whether the last token is a positive or negative emoticon, the number of elongated words (e.g., soooo);",
    "answer_b": "< Explanation: We used a variety of domain-specific features to train our classifiers. We used the following domain-specific features:\n\nSentiment lexicon features: We used the sentiment lexicon features from the NLTK library (http://www.nltk.org/) to create a sentiment lexicon. We used the following features:\n\nSentiment lexicon features: We used the sentiment lexicon features from the NLTK library (http://www.nltk.org/) to create a sentiment lexicon. We used the following features:\n\nSentiment lexicon features: We used the sentiment lexicon features from the NLTK library (http://www.nltk.org",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat domain-specific features did they train on?\n\nGround Truth Answer:\n\n\nAnswer A:\nINLINEFORM0 -grams generalized over domain terms. Explanation: Twitter-specific features: the number of tokens with all characters in upper case, the number of hashtags, presence of positive and negative emoticons, whether the last token is a positive or negative emoticon, the number of elongated words (e.g., soooo);\n\nAnswer B:\n< Explanation: We used a variety of domain-specific features to train our classifiers. We used the following domain-specific features:\n\nSentiment lexicon features: We used the sentiment lexicon features from the NLTK library (http://www.nltk.org/) to create a sentiment lexicon. We used the following features:\n\nSentiment lexicon features: We used the sentiment lexicon features from the NLTK library (http://www.nltk.org/) to create a sentiment lexicon. We used the following features:\n\nSentiment lexicon features: We used the sentiment lexicon features from the NLTK library (http://www.nltk.org\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.04558",
    "question": "what are the sentiment features used?",
    "gold_answer": "",
    "answer_a": "How are they evaluated?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Explanation:</td>\n<tr>\n<td align=\"left\">The sentiment lexicon features are extracted from the Twitter corpus using the Twitter Sentiment Analyzer (TSA) tool. The TSA tool is a Python script that extracts sentiment from tweets using the Twitter API. The script is available at <a href=\"http://www.twitter.com/talex\" target=\"_blank\">http://www.twitter.com/talex</a>. The script is used to extract the sentiment of each tweet. The sentiment lexicon features are extracted from",
    "answer_b": "the number of tokens with INLINEFORM0. Explanation: We generated features using the sentiment scores provided in the following lexicons: Hu and Liu Lexicon BIBREF17 , Norms of Valence, Arousal, and Dominance BIBREF18 , labMT BIBREF19 , and NRC Emoticon Lexicon BIBREF20 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the sentiment features used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they evaluated?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Explanation:</td>\n<tr>\n<td align=\"left\">The sentiment lexicon features are extracted from the Twitter corpus using the Twitter Sentiment Analyzer (TSA) tool. The TSA tool is a Python script that extracts sentiment from tweets using the Twitter API. The script is available at <a href=\"http://www.twitter.com/talex\" target=\"_blank\">http://www.twitter.com/talex</a>. The script is used to extract the sentiment of each tweet. The sentiment lexicon features are extracted from\n\nAnswer B:\nthe number of tokens with INLINEFORM0. Explanation: We generated features using the sentiment scores provided in the following lexicons: Hu and Liu Lexicon BIBREF17 , Norms of Valence, Arousal, and Dominance BIBREF18 , labMT BIBREF19 , and NRC Emoticon Lexicon BIBREF20 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.04558",
    "question": "what surface-form features were used?",
    "gold_answer": "",
    "answer_a": "INLINEFORM0 -grams. Explanation: The following surface-form features were used:\n\nINLINEFORM0 -grams: word INLINEFORM1 -grams (contiguous sequences of INLINEFORM2 tokens), non-contiguous word INLINEFORM3 -grams ( INLINEFORM4 -grams with one token replaced by *), character INLINEFORM5 -grams (contiguous sequences of INLINEFORM6 characters), unigram stems obtained with the Porter stemming algorithm;\n\nGeneral-domain word embeddings:\n\ndense word representations generated with word2vec on ten million English-language tweets, summed over all tokens in the tweet,\n\nword embeddings distributed as part of ConceptNet 5.5 BIBREF15 , summed over all tokens in the tweet;\n\nGeneral-domain word clusters: presence of tokens from the word clusters generated with the Brown clustering algorithm on 56 million English-language tweets; BIBREF11\n\nNegation: presence of simple negators (e.g., not, never); negation also affects the INLINEFORM0 -gram features\u2014a term INLINEFORM1 becomes INLINEFORM2 if it occurs after a negator and before a punctuation mark;\n\nTwitter-specific features: the number of tokens with all characters in upper case, the number of hashtags, presence of positive and negative emoticons, whether the last token is a positive or negative emoticon, the number of elongated words (e.g., soooo);\n\nPunctuation: presence of exclamation and question marks, whether the last token contains an exclamation or question mark.",
    "answer_b": "Explanation: We used the following surface-form features: (1) the number of words in the tweet, (2) the number of words in the tweet and the number of words in the domain term, (3) the number of words in the tweet and the number of words in the domain term and the number of words in the domain term and the number of words in the domain term and the number of words in the domain term and the number of words in the domain term and the number of words in the domain term and the number of words in the domain term and the number of words in the domain term and the number of words",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat surface-form features were used?\n\nGround Truth Answer:\n\n\nAnswer A:\nINLINEFORM0 -grams. Explanation: The following surface-form features were used:\n\nINLINEFORM0 -grams: word INLINEFORM1 -grams (contiguous sequences of INLINEFORM2 tokens), non-contiguous word INLINEFORM3 -grams ( INLINEFORM4 -grams with one token replaced by *), character INLINEFORM5 -grams (contiguous sequences of INLINEFORM6 characters), unigram stems obtained with the Porter stemming algorithm;\n\nGeneral-domain word embeddings:\n\ndense word representations generated with word2vec on ten million English-language tweets, summed over all tokens in the tweet,\n\nword embeddings distributed as part of ConceptNet 5.5 BIBREF15 , summed over all tokens in the tweet;\n\nGeneral-domain word clusters: presence of tokens from the word clusters generated with the Brown clustering algorithm on 56 million English-language tweets; BIBREF11\n\nNegation: presence of simple negators (e.g., not, never); negation also affects the INLINEFORM0 -gram features\u2014a term INLINEFORM1 becomes INLINEFORM2 if it occurs after a negator and before a punctuation mark;\n\nTwitter-specific features: the number of tokens with all characters in upper case, the number of hashtags, presence of positive and negative emoticons, whether the last token is a positive or negative emoticon, the number of elongated words (e.g., soooo);\n\nPunctuation: presence of exclamation and question marks, whether the last token contains an exclamation or question mark.\n\nAnswer B:\nExplanation: We used the following surface-form features: (1) the number of words in the tweet, (2) the number of words in the tweet and the number of words in the domain term, (3) the number of words in the tweet and the number of words in the domain term and the number of words in the domain term and the number of words in the domain term and the number of words in the domain term and the number of words in the domain term and the number of words in the domain term and the number of words in the domain term and the number of words in the domain term and the number of words\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.03324",
    "question": "How does their BERT-based model work?",
    "gold_answer": "",
    "answer_a": "What is the architecture of the model?\n< Explanation: The model is a BERT-based model. The model consists of a pre-trained BERT model and a fine-tuned BERT model. The pre-trained BERT model is trained on the Wikipedia corpus. The fine-tuned BERT model is trained on the specific datasets. The pre-trained BERT model is trained on the Wikipedia corpus, and the fine-tuned BERT model is trained on the specific datasets. The pre-trained BERT model is trained on the Wikipedia corpus, and the fine-tuned BERT model is trained on the specific datasets. The pre-trained BERT model",
    "answer_b": "The model takes the concatenation of the query and the document as input. The query-sentence and sentence-sentence relationships are jointly modeled by the self-attention mechanism BIBREF12. The model is fine-tuned to utilize the general language representations of BERT BIBREF13.. Explanation: The model takes the concatenation of the query and the document as input. The query-sentence and sentence-sentence relationships are jointly modeled by the self-attention mechanism BIBREF12. The model is fine-tuned to utilize the general language representations of BERT BIBREF13.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does their BERT-based model work?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the architecture of the model?\n< Explanation: The model is a BERT-based model. The model consists of a pre-trained BERT model and a fine-tuned BERT model. The pre-trained BERT model is trained on the Wikipedia corpus. The fine-tuned BERT model is trained on the specific datasets. The pre-trained BERT model is trained on the Wikipedia corpus, and the fine-tuned BERT model is trained on the specific datasets. The pre-trained BERT model is trained on the Wikipedia corpus, and the fine-tuned BERT model is trained on the specific datasets. The pre-trained BERT model\n\nAnswer B:\nThe model takes the concatenation of the query and the document as input. The query-sentence and sentence-sentence relationships are jointly modeled by the self-attention mechanism BIBREF12. The model is fine-tuned to utilize the general language representations of BERT BIBREF13.. Explanation: The model takes the concatenation of the query and the document as input. The query-sentence and sentence-sentence relationships are jointly modeled by the self-attention mechanism BIBREF12. The model is fine-tuned to utilize the general language representations of BERT BIBREF13.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.03324",
    "question": "How do they use Wikipedia to automatically collect a query-focused summarization dataset?",
    "gold_answer": "",
    "answer_a": "To automatically construct query-focused summarization examples using Wikipedia, the statements' citations in Wikipedia articles as pivots to align the queries and documents. Figure FIGREF1 shows an example that is constructed by the proposed method. We first take the highlighted statement as the summary. Its supporting citation is expected to provide an adequate context to derive the statement, thus can serve as the source document. On the other hand, the section titles give a hint about which aspect of the document is the summary's focus. Therefore, we use the article title and the section titles of the statement to form the query.. Explanation: To automatically construct query-focused summarization examples using Wikipedia, the statements' citations in Wikipedia articles as pivots to align the queries and documents. Figure FIGREF1 shows an example that is constructed by the proposed method. We first take the highlighted statement as the summary. Its supporting citation is expected to provide an adequate context to derive the statement, thus can serve as the source document. On the other hand, the section titles give a hint about which aspect of the document is the summary's focus. Therefore, we use the article title and the section titles of the statement to form the query.",
    "answer_b": "What is the size of the dataset?\n< Explanation: We first collected a large dataset of query-focused summarization from Wikipedia. We used the Wikipedia article summarization dataset (Wikipedia, 2018) to collect a dataset of 280,000 examples. The dataset was collected from the Wikipedia article summarization dataset (Wikipedia, 2018), which is a collection of 280,000 articles from Wikipedia. The dataset was collected from the Wikipedia article summarization dataset (Wikipedia, 2018), which is a collection of 280,0",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they use Wikipedia to automatically collect a query-focused summarization dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nTo automatically construct query-focused summarization examples using Wikipedia, the statements' citations in Wikipedia articles as pivots to align the queries and documents. Figure FIGREF1 shows an example that is constructed by the proposed method. We first take the highlighted statement as the summary. Its supporting citation is expected to provide an adequate context to derive the statement, thus can serve as the source document. On the other hand, the section titles give a hint about which aspect of the document is the summary's focus. Therefore, we use the article title and the section titles of the statement to form the query.. Explanation: To automatically construct query-focused summarization examples using Wikipedia, the statements' citations in Wikipedia articles as pivots to align the queries and documents. Figure FIGREF1 shows an example that is constructed by the proposed method. We first take the highlighted statement as the summary. Its supporting citation is expected to provide an adequate context to derive the statement, thus can serve as the source document. On the other hand, the section titles give a hint about which aspect of the document is the summary's focus. Therefore, we use the article title and the section titles of the statement to form the query.\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: We first collected a large dataset of query-focused summarization from Wikipedia. We used the Wikipedia article summarization dataset (Wikipedia, 2018) to collect a dataset of 280,000 examples. The dataset was collected from the Wikipedia article summarization dataset (Wikipedia, 2018), which is a collection of 280,000 articles from Wikipedia. The dataset was collected from the Wikipedia article summarization dataset (Wikipedia, 2018), which is a collection of 280,0\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.03177",
    "question": "How is GPU-based self-critical Reinforcement Learing model designed?",
    "gold_answer": "",
    "answer_a": "What is the performance of the model?\n< Explanation: We designed a self-critical Reinforcement Learning model to evaluate the performance of the hierarchical NSE model. The model is designed to be self-critical, i.e., it is designed to learn from its own performance. The model is designed to be self-critical, i.e., it is designed to learn from its own performance. The model is designed to be self-critical, i.e., it is designed to learn from its own performance. The model is designed to be self-critical, i.e., it is designed to learn from its own performance. The model is designed",
    "answer_b": "We used the self-critical model of BIBREF13 proposed for image captioning. Explanation: We used the self-critical model of BIBREF13 proposed for image captioning. In self-critical sequence training, the REINFORCE algorithm BIBREF20 is used by modifying its baseline as the greedy output of the current model. At each time-step $t$, the model predicts two words: $\\hat{y}_{t}$ sampled from $p(\\hat{y}_{t} | \\hat{y}_{1}, \\hat{y}_{2}, ..., \\hat{y}_{t-1}, x)$, the baseline output that is greedily generated by considering the most probable word from the vocabulary and $\\tilde{y}_{t}$ sampled from the $p(\\tilde{y}_{t} | \\tilde{y}_{1}, \\tilde{y}_{2}, ..., \\tilde{y}_{t-1}, x)$. This model is trained using the following loss function:\n\nUsing the above training objective, the model learns to generate samples with high probability and thereby increasing $r(\\tilde{y})$ above $r(\\hat{y})$. Additionally, we have used enthttps://stackoverflow.com/questions/19053077/looping-over-data-and-creating-individual-figuresropy regularization.\n\nWhere, $p(\\tilde{y}_{t})=p(\\tilde{y}_{t} | \\tilde{y}_{1}, \\tilde{y}_{2}, ..., \\tilde{y}_{t-1}, x)$ is the sampling probability and $V$ is the size of the vocabulary. It is similar to the exploration-exploitation trade-off. $\\alpha $ is the regularization coefficient that explicitly controls this trade-off: a higher $\\alpha $ corresponds to more exploration, and a lower $\\alpha $ corresponds to more exploitation. We have found that all TensorFlow based open-source implementations of self-critic models use a function (tf.py_func) that runs only on CPU and it is very slow. To the best of our knowledge, ours is the first GPU based implementation.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is GPU-based self-critical Reinforcement Learing model designed?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of the model?\n< Explanation: We designed a self-critical Reinforcement Learning model to evaluate the performance of the hierarchical NSE model. The model is designed to be self-critical, i.e., it is designed to learn from its own performance. The model is designed to be self-critical, i.e., it is designed to learn from its own performance. The model is designed to be self-critical, i.e., it is designed to learn from its own performance. The model is designed to be self-critical, i.e., it is designed to learn from its own performance. The model is designed\n\nAnswer B:\nWe used the self-critical model of BIBREF13 proposed for image captioning. Explanation: We used the self-critical model of BIBREF13 proposed for image captioning. In self-critical sequence training, the REINFORCE algorithm BIBREF20 is used by modifying its baseline as the greedy output of the current model. At each time-step $t$, the model predicts two words: $\\hat{y}_{t}$ sampled from $p(\\hat{y}_{t} | \\hat{y}_{1}, \\hat{y}_{2}, ..., \\hat{y}_{t-1}, x)$, the baseline output that is greedily generated by considering the most probable word from the vocabulary and $\\tilde{y}_{t}$ sampled from the $p(\\tilde{y}_{t} | \\tilde{y}_{1}, \\tilde{y}_{2}, ..., \\tilde{y}_{t-1}, x)$. This model is trained using the following loss function:\n\nUsing the above training objective, the model learns to generate samples with high probability and thereby increasing $r(\\tilde{y})$ above $r(\\hat{y})$. Additionally, we have used enthttps://stackoverflow.com/questions/19053077/looping-over-data-and-creating-individual-figuresropy regularization.\n\nWhere, $p(\\tilde{y}_{t})=p(\\tilde{y}_{t} | \\tilde{y}_{1}, \\tilde{y}_{2}, ..., \\tilde{y}_{t-1}, x)$ is the sampling probability and $V$ is the size of the vocabulary. It is similar to the exploration-exploitation trade-off. $\\alpha $ is the regularization coefficient that explicitly controls this trade-off: a higher $\\alpha $ corresponds to more exploration, and a lower $\\alpha $ corresponds to more exploitation. We have found that all TensorFlow based open-source implementations of self-critic models use a function (tf.py_func) that runs only on CPU and it is very slow. To the best of our knowledge, ours is the first GPU based implementation.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.03177",
    "question": "What are previoius similar models authors are referring to?",
    "gold_answer": "",
    "answer_a": "Abstractive and extractive models from Nallapati et al., 2016, Pointer generator models with and without coverage from See et al., 2017, and Reinforcement Learning models from Paulus et al., 2018, and Celikyilmaz et al., 2018.. Explanation: FLOAT SELECTED: Table 1: ROUGE F1 scores on the test set. Our hierarchical (Hier-NSE) model outperform previous hierarchical and pointer-generator models. Hier-NSE-factor is the factored model and Hier-NSE-SC is the self-critic model.",
    "answer_b": "What is their performance?\n< Explanation: The previous models are based on the following approaches: (1) LSTMs (LSTM) (Liu et al., 2014) (2) Hierarchical LSTMs (H-LSTM) (Liu et al., 2015) (3) Hierarchical attention (HAT) (Liu et al., 2016) (4) Hierarchical attention with hard attention (HAT-H) (Liu et al., 2017) (5) Hierarchical attention with soft attention (HAT-S) (",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are previoius similar models authors are referring to?\n\nGround Truth Answer:\n\n\nAnswer A:\nAbstractive and extractive models from Nallapati et al., 2016, Pointer generator models with and without coverage from See et al., 2017, and Reinforcement Learning models from Paulus et al., 2018, and Celikyilmaz et al., 2018.. Explanation: FLOAT SELECTED: Table 1: ROUGE F1 scores on the test set. Our hierarchical (Hier-NSE) model outperform previous hierarchical and pointer-generator models. Hier-NSE-factor is the factored model and Hier-NSE-SC is the self-critic model.\n\nAnswer B:\nWhat is their performance?\n< Explanation: The previous models are based on the following approaches: (1) LSTMs (LSTM) (Liu et al., 2014) (2) Hierarchical LSTMs (H-LSTM) (Liu et al., 2015) (3) Hierarchical attention (HAT) (Liu et al., 2016) (4) Hierarchical attention with hard attention (HAT-H) (Liu et al., 2017) (5) Hierarchical attention with soft attention (HAT-S) (\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.03177",
    "question": "What was previous state of the art on factored dataset?",
    "gold_answer": "",
    "answer_a": "ROUGE-1 41.69\nROUGE-2 19.47\nROUGE-L 37.92. Explanation: Table TABREF34 shows the results in comparison to the previous methods. Our hierarchical model outperforms BIBREF7 (HIER) by 5 ROUGE points. Our factored model achieves the new state-of-the-art (SoTA) result, outperforming BIBREF4 by almost 4 ROUGE points.",
    "answer_b": "What was the performance of the model?\n< Explanation: The first state-of-the-art model on factored dataset was proposed by BIBREF10. The model was trained on the factored dataset using a standard LSTM model. The model was trained on 10000 sentences and 800 tokens per sentence. The model was trained using 10000 sentences and 800 tokens per sentence. The model was trained using 10000 sentences and 800 tokens per sentence. The model was trained using 10000 sentences and 800 tokens per",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat was previous state of the art on factored dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nROUGE-1 41.69\nROUGE-2 19.47\nROUGE-L 37.92. Explanation: Table TABREF34 shows the results in comparison to the previous methods. Our hierarchical model outperforms BIBREF7 (HIER) by 5 ROUGE points. Our factored model achieves the new state-of-the-art (SoTA) result, outperforming BIBREF4 by almost 4 ROUGE points.\n\nAnswer B:\nWhat was the performance of the model?\n< Explanation: The first state-of-the-art model on factored dataset was proposed by BIBREF10. The model was trained on the factored dataset using a standard LSTM model. The model was trained on 10000 sentences and 800 tokens per sentence. The model was trained using 10000 sentences and 800 tokens per sentence. The model was trained using 10000 sentences and 800 tokens per sentence. The model was trained using 10000 sentences and 800 tokens per\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02143",
    "question": "How much did the model outperform",
    "gold_answer": "",
    "answer_a": "the absolute improvement of $4.02$ and $3.18$ points compared to NQG and Max-out Pointer model, respectively, in terms of BLEU-4 metric. Explanation: The proposed MultiHop-QG model achieves the absolute improvement of $4.02$ and $3.18$ points compared to NQG and Max-out Pointer model, respectively, in terms of BLEU-4 metric.",
    "answer_b": "the single-hop neural question generation models on automatic evaluation metrics such as BLEU, METEOR, and ROUGE?\nFloat: 0.000000\nExplanation: We compare our model with the single-hop neural QG model (SNNQG) and the multi-hop neural QG model (MNNQG) on the multi-hop QA dataset. The results are shown in Table 1. Our model outperforms the single-hop neural QG model by 0.000000 BLEU, 0.000000 METEOR, and 0.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much did the model outperform\n\nGround Truth Answer:\n\n\nAnswer A:\nthe absolute improvement of $4.02$ and $3.18$ points compared to NQG and Max-out Pointer model, respectively, in terms of BLEU-4 metric. Explanation: The proposed MultiHop-QG model achieves the absolute improvement of $4.02$ and $3.18$ points compared to NQG and Max-out Pointer model, respectively, in terms of BLEU-4 metric.\n\nAnswer B:\nthe single-hop neural question generation models on automatic evaluation metrics such as BLEU, METEOR, and ROUGE?\nFloat: 0.000000\nExplanation: We compare our model with the single-hop neural QG model (SNNQG) and the multi-hop neural QG model (MNNQG) on the multi-hop QA dataset. The results are shown in Table 1. Our model outperforms the single-hop neural QG model by 0.000000 BLEU, 0.000000 METEOR, and 0.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02143",
    "question": "What language is in the dataset?",
    "gold_answer": "",
    "answer_a": "What are the evaluation metrics?\n<sep>English. Explanation: We use the HotPotQA dataset [1] to evaluate our model. The dataset contains 10,000 questions and 10,000 answers. The questions are generated from a single-hot vector of 1000 features, and the answers are generated from a single-hot vector of 1000 features. The dataset is divided into training, validation, and test sets. The training set contains 70% of the data, the validation set contains 20% of the data, and the test set contains 1",
    "answer_b": "English",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat language is in the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the evaluation metrics?\n<sep>English. Explanation: We use the HotPotQA dataset [1] to evaluate our model. The dataset contains 10,000 questions and 10,000 answers. The questions are generated from a single-hot vector of 1000 features, and the answers are generated from a single-hot vector of 1000 features. The dataset is divided into training, validation, and test sets. The training set contains 70% of the data, the validation set contains 20% of the data, and the test set contains 1\n\nAnswer B:\nEnglish\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02143",
    "question": "How big is the HotPotQA dataset?",
    "gold_answer": "",
    "answer_a": "What are the evaluation metrics used?\n<sep>HotPotQA dataset: HotPotQA is a dataset of 1000 questions and 1000 answers. The questions are generated by a random forest model trained on the 1000 answers. The answers are generated by a random forest model trained on the 1000 questions. The dataset is divided into 1000 training and 1000 test sets. The training set is used to train the random forest model, and the test set is used to evaluate the model. The dataset is publicly available at https://github.com/",
    "answer_b": "over 113k Wikipedia-based question-answer pairs. Explanation: This dataset consists of over 113k Wikipedia-based question-answer pairs, with each question requiring multi-step reasoning across multiple supporting documents to infer the answer.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big is the HotPotQA dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the evaluation metrics used?\n<sep>HotPotQA dataset: HotPotQA is a dataset of 1000 questions and 1000 answers. The questions are generated by a random forest model trained on the 1000 answers. The answers are generated by a random forest model trained on the 1000 questions. The dataset is divided into 1000 training and 1000 test sets. The training set is used to train the random forest model, and the test set is used to evaluate the model. The dataset is publicly available at https://github.com/\n\nAnswer B:\nover 113k Wikipedia-based question-answer pairs. Explanation: This dataset consists of over 113k Wikipedia-based question-answer pairs, with each question requiring multi-step reasoning across multiple supporting documents to infer the answer.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.01339",
    "question": "What parts of their multitask model are shared?",
    "gold_answer": "",
    "answer_a": "What parts are unique?\n< Explanation: We show that the parsing paradigm is a useful tool to learn the syntactic abstraction. We show that adding a parsing paradigm as an auxiliary loss consistently improves the performance on the other paradigm. We show that the parsing paradigm is a useful tool to learn the syntactic abstraction. We show that the parsing paradigm is a useful tool to learn the syntactic abstraction. We show that the parsing paradigm is a useful tool to learn the syntactic abstraction. We show that the parsing paradigm is a useful tool to learn the syntactic abstraction. We show that the parsing paradigm is a useful tool to learn the syntactic",
    "answer_b": "stacked bilstms. Explanation: In particular, we will use a hard-sharing architecture: the sentence is first processed by stacked bilstms shared across all tasks, with a task-dependent feed-forward network on the top of it, to compute each task's outputs.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat parts of their multitask model are shared?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat parts are unique?\n< Explanation: We show that the parsing paradigm is a useful tool to learn the syntactic abstraction. We show that adding a parsing paradigm as an auxiliary loss consistently improves the performance on the other paradigm. We show that the parsing paradigm is a useful tool to learn the syntactic abstraction. We show that the parsing paradigm is a useful tool to learn the syntactic abstraction. We show that the parsing paradigm is a useful tool to learn the syntactic abstraction. We show that the parsing paradigm is a useful tool to learn the syntactic abstraction. We show that the parsing paradigm is a useful tool to learn the syntactic\n\nAnswer B:\nstacked bilstms. Explanation: In particular, we will use a hard-sharing architecture: the sentence is first processed by stacked bilstms shared across all tasks, with a task-dependent feed-forward network on the top of it, to compute each task's outputs.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.01339",
    "question": "Which dataset do they use?",
    "gold_answer": "",
    "answer_a": "English Penn Treebank. Explanation: For the evaluation on English language we use the English Penn Treebank BIBREF40 , transformed into Stanford dependencies BIBREF41 with the predicted PoS tags as in BIBREF32 .",
    "answer_b": "How do they evaluate the performance?\n< Explanation: We use the following datasets: (1) the Penn Treebank (PTB) BIBREF10, (2) the Penn Treebank with Part-of-Speech (PTB-POS) BIBREF11, (3) the Penn Treebank with Dependency (PTB-Dep) BIBREF12, (4) the Penn Treebank with Dependency and Part-of-Speech (PTB-Dep-Pos) BIBREF13, (5) the Penn Treebank with Part-of-Speech and Dependency (PTB-Pos-Dep)",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dataset do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nEnglish Penn Treebank. Explanation: For the evaluation on English language we use the English Penn Treebank BIBREF40 , transformed into Stanford dependencies BIBREF41 with the predicted PoS tags as in BIBREF32 .\n\nAnswer B:\nHow do they evaluate the performance?\n< Explanation: We use the following datasets: (1) the Penn Treebank (PTB) BIBREF10, (2) the Penn Treebank with Part-of-Speech (PTB-POS) BIBREF11, (3) the Penn Treebank with Dependency (PTB-Dep) BIBREF12, (4) the Penn Treebank with Dependency and Part-of-Speech (PTB-Dep-Pos) BIBREF13, (5) the Penn Treebank with Part-of-Speech and Dependency (PTB-Pos-Dep)\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1704.04451",
    "question": "How long is the training dataset?",
    "gold_answer": "",
    "answer_a": "3,492 documents. Explanation: We run experiments on the English portion of CoNLL 2012 data BIBREF15 which consists of 3,492 documents in various domains and formats.",
    "answer_b": "What is the size of the training dataset?\n< Explanation: The training dataset consists of 10000 utterances, each of which is a pair of a coreference and a coreference target. The coreference target is a pair of coreference words, and the coreference is a pair of coreference words. The training dataset is split into 10000 utterances, each of which is a pair of a coreference and a coreference target. The coreference target is a pair of coreference words, and the coreference is a pair of coreference words. The training dataset is split into",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long is the training dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\n3,492 documents. Explanation: We run experiments on the English portion of CoNLL 2012 data BIBREF15 which consists of 3,492 documents in various domains and formats.\n\nAnswer B:\nWhat is the size of the training dataset?\n< Explanation: The training dataset consists of 10000 utterances, each of which is a pair of a coreference and a coreference target. The coreference target is a pair of coreference words, and the coreference is a pair of coreference words. The training dataset is split into 10000 utterances, each of which is a pair of a coreference and a coreference target. The coreference target is a pair of coreference words, and the coreference is a pair of coreference words. The training dataset is split into\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1704.04451",
    "question": "What dataset do they use?",
    "gold_answer": "",
    "answer_a": "Explanation: We use the coreference dataset from the Stanford Coreference Project (SCP) [1] to train our coreference system. The dataset consists of 10,000 coreference pairs, each consisting of a coreference pair and a coreference label. The coreference pairs are generated by randomly selecting a pair of sentences from the corpus and then generating a coreference pair by adding a coreference label to the first sentence. The coreference labels are generated by randomly selecting a label from a set of 100 possible labels. The coreference pairs are then used to train a coreference system, which is",
    "answer_b": "CoNLL 2012. Explanation: We run experiments on the English portion of CoNLL 2012 data BIBREF15 which consists of 3,492 documents in various domains and formats.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat dataset do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We use the coreference dataset from the Stanford Coreference Project (SCP) [1] to train our coreference system. The dataset consists of 10,000 coreference pairs, each consisting of a coreference pair and a coreference label. The coreference pairs are generated by randomly selecting a pair of sentences from the corpus and then generating a coreference pair by adding a coreference label to the first sentence. The coreference labels are generated by randomly selecting a label from a set of 100 possible labels. The coreference pairs are then used to train a coreference system, which is\n\nAnswer B:\nCoNLL 2012. Explanation: We run experiments on the English portion of CoNLL 2012 data BIBREF15 which consists of 3,492 documents in various domains and formats.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1604.02201",
    "question": "What high-resource language pair is the parent model trained on?",
    "gold_answer": "",
    "answer_a": "French-English parent model. Explanation: Our main experiments translating Hausa, Turkish, Uzbek and Urdu into English with the help of a French-English parent model are presented in Section \"Experiments\" .",
    "answer_b": "How is the child model trained?\n< Explanation: The parent model is trained on the high-resource language pair (English to Spanish) and the child model is trained on the low-resource language pair (Spanish to English). The child model is trained using the parent model as a reference. The child model is trained using the parent model as a reference. The child model is trained using the parent model as a reference. The child model is trained using the parent model as a reference. The child model is trained using the parent model as a reference. The child model is trained using the parent model as a reference. The child model is trained using the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat high-resource language pair is the parent model trained on?\n\nGround Truth Answer:\n\n\nAnswer A:\nFrench-English parent model. Explanation: Our main experiments translating Hausa, Turkish, Uzbek and Urdu into English with the help of a French-English parent model are presented in Section \"Experiments\" .\n\nAnswer B:\nHow is the child model trained?\n< Explanation: The parent model is trained on the high-resource language pair (English to Spanish) and the child model is trained on the low-resource language pair (Spanish to English). The child model is trained using the parent model as a reference. The child model is trained using the parent model as a reference. The child model is trained using the parent model as a reference. The child model is trained using the parent model as a reference. The child model is trained using the parent model as a reference. The child model is trained using the parent model as a reference. The child model is trained using the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1604.02201",
    "question": "How did they constrain training using the parameters?",
    "gold_answer": "",
    "answer_a": "What is the role of the parameters?\n< Explanation: We use the parameters of the parent model to constrain the child model during training. Specifically, we use the parameters of the parent model to initialize the child model, and then we use the parameters of the child model to constrain the child model during training. This is done by setting the parameters of the child model to zero, and then training the child model to maximize the loss function. We use the parameters of the parent model to initialize the child model, and then we use the parameters of the child model to constrain the child model during training. This is done by setting the parameters of the",
    "answer_b": "first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model).. Explanation: Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model, letting the rest be fine-tuned by the child model.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow did they constrain training using the parameters?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the role of the parameters?\n< Explanation: We use the parameters of the parent model to constrain the child model during training. Specifically, we use the parameters of the parent model to initialize the child model, and then we use the parameters of the child model to constrain the child model during training. This is done by setting the parameters of the child model to zero, and then training the child model to maximize the loss function. We use the parameters of the parent model to initialize the child model, and then we use the parameters of the child model to constrain the child model during training. This is done by setting the parameters of the\n\nAnswer B:\nfirst train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model).. Explanation: Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model, letting the rest be fine-tuned by the child model.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.11053",
    "question": "What are their evaluation metrics?",
    "gold_answer": "",
    "answer_a": "average F1-score. Explanation: The average F1-scores for the end-to-end query generation task are reported in Table TABREF35 .",
    "answer_b": "What are their performance on the two benchmark datasets?\nFormal query generation aims to generate correct executable queries for question answering over knowledge bases (KBs), given entity and relation linking results. Current approaches build universal paraphrasing or ranking models for the whole questions, which are likely to fail in generating queries for complex, long-tail questions. In this paper, we propose SubQG, a new query generation approach based on frequent query substructures, which helps rank the existing (but nonsignificant) query structures or build new query structures. Our experiments on two benchmark datasets show that our approach significantly outperforms the existing ones, especially for",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are their evaluation metrics?\n\nGround Truth Answer:\n\n\nAnswer A:\naverage F1-score. Explanation: The average F1-scores for the end-to-end query generation task are reported in Table TABREF35 .\n\nAnswer B:\nWhat are their performance on the two benchmark datasets?\nFormal query generation aims to generate correct executable queries for question answering over knowledge bases (KBs), given entity and relation linking results. Current approaches build universal paraphrasing or ranking models for the whole questions, which are likely to fail in generating queries for complex, long-tail questions. In this paper, we propose SubQG, a new query generation approach based on frequent query substructures, which helps rank the existing (but nonsignificant) query structures or build new query structures. Our experiments on two benchmark datasets show that our approach significantly outperforms the existing ones, especially for\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.11053",
    "question": "What knowledge base do they rely on?",
    "gold_answer": "",
    "answer_a": "What is the size of the KB?\nFormal query generation aims to generate correct executable queries for question answering over knowledge bases (KBs), given entity and relation linking results. Current approaches build universal paraphrasing or ranking models for the whole questions, which are likely to fail in generating queries for complex, long-tail questions. In this paper, we propose SubQG, a new query generation approach based on frequent query substructures, which helps rank the existing (but nonsignificant) query structures or build new query structures. Our experiments on two benchmark datasets show that our approach significantly outperforms the existing ones, especially for complex questions",
    "answer_b": "DBpedia (2016-04). Explanation: We employed the same datasets as BIBREF3 ( BIBREF3 ) and BIBREF4 ( BIBREF4 ): (i) the large-scale complex question answering dataset (LC-QuAD) BIBREF8 , containing 3,253 questions with non-empty results on DBpedia (2016-04), and (ii) the fifth edition of question answering over linked data (QALD-5) dataset BIBREF9 , containing 311 questions with non-empty results on DBpedia (2015-10).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat knowledge base do they rely on?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the KB?\nFormal query generation aims to generate correct executable queries for question answering over knowledge bases (KBs), given entity and relation linking results. Current approaches build universal paraphrasing or ranking models for the whole questions, which are likely to fail in generating queries for complex, long-tail questions. In this paper, we propose SubQG, a new query generation approach based on frequent query substructures, which helps rank the existing (but nonsignificant) query structures or build new query structures. Our experiments on two benchmark datasets show that our approach significantly outperforms the existing ones, especially for complex questions\n\nAnswer B:\nDBpedia (2016-04). Explanation: We employed the same datasets as BIBREF3 ( BIBREF3 ) and BIBREF4 ( BIBREF4 ): (i) the large-scale complex question answering dataset (LC-QuAD) BIBREF8 , containing 3,253 questions with non-empty results on DBpedia (2016-04), and (ii) the fifth edition of question answering over linked data (QALD-5) dataset BIBREF9 , containing 311 questions with non-empty results on DBpedia (2015-10).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.11053",
    "question": "How do they recover from noisy entity linking?",
    "gold_answer": "",
    "answer_a": "What is the best way to handle noisy entity linking results?\nFormal query generation aims to generate correct executable queries for question answering over knowledge bases (KBs), given entity and relation linking results. Current approaches build universal paraphrasing or ranking models for the whole questions, which are likely to fail in generating queries for complex, long-tail questions. In this paper, we propose SubQG, a new query generation approach based on frequent query substructures, which helps rank the existing (but nonsignificant) query structures or build new query structures. Our experiments on two benchmark datasets show that our approach significantly outperforms the existing ones,",
    "answer_b": "by filtering errors in noisy entity linking by the empty query check or domain/range check in query structure ranking. Explanation: We simulated the real KBQA environment by considering noisy entity/relation linking results. We firstly mixed the correct linking result for each mention with the top-5 candidates generated from EARL BIBREF6 , which is a joint entity/relation linking system with state-of-the-art performance on LC-QuAD. The result is shown in the second row of Table TABREF42 . Although the precision for first output declined 11.4%, in 85% cases we still can generate correct answer in top-5. This is because SubQG ranked query structures first and considered linking results in the last step. Many error linking results can be filtered out by the empty query check or domain/range check.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they recover from noisy entity linking?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the best way to handle noisy entity linking results?\nFormal query generation aims to generate correct executable queries for question answering over knowledge bases (KBs), given entity and relation linking results. Current approaches build universal paraphrasing or ranking models for the whole questions, which are likely to fail in generating queries for complex, long-tail questions. In this paper, we propose SubQG, a new query generation approach based on frequent query substructures, which helps rank the existing (but nonsignificant) query structures or build new query structures. Our experiments on two benchmark datasets show that our approach significantly outperforms the existing ones,\n\nAnswer B:\nby filtering errors in noisy entity linking by the empty query check or domain/range check in query structure ranking. Explanation: We simulated the real KBQA environment by considering noisy entity/relation linking results. We firstly mixed the correct linking result for each mention with the top-5 candidates generated from EARL BIBREF6 , which is a joint entity/relation linking system with state-of-the-art performance on LC-QuAD. The result is shown in the second row of Table TABREF42 . Although the precision for first output declined 11.4%, in 85% cases we still can generate correct answer in top-5. This is because SubQG ranked query structures first and considered linking results in the last step. Many error linking results can be filtered out by the empty query check or domain/range check.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.11053",
    "question": "What datasets do they evaluate on?",
    "gold_answer": "",
    "answer_a": "What are the results?\nFormal query generation aims to generate correct executable queries for question answering over knowledge bases (KBs), given entity and relation linking results. Current approaches build universal paraphrasing or ranking models for the whole questions, which are likely to fail in generating queries for complex, long-tail questions. In this paper, we propose SubQG, a new query generation approach based on frequent query substructures, which helps rank the existing (but nonsignificant) query structures or build new query structures. Our experiments on two benchmark datasets show that our approach significantly outperforms the existing ones, especially for complex questions. Also,",
    "answer_b": "LC-QuAD. Explanation: We employed the same datasets as BIBREF3 ( BIBREF3 ) and BIBREF4 ( BIBREF4 ): (i) the large-scale complex question answering dataset (LC-QuAD) BIBREF8 , containing 3,253 questions with non-empty results on DBpedia (2016-04), and (ii) the fifth edition of question answering over linked data (QALD-5) dataset BIBREF9 , containing 311 questions with non-empty results on DBpedia (2015-10).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets do they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\nFormal query generation aims to generate correct executable queries for question answering over knowledge bases (KBs), given entity and relation linking results. Current approaches build universal paraphrasing or ranking models for the whole questions, which are likely to fail in generating queries for complex, long-tail questions. In this paper, we propose SubQG, a new query generation approach based on frequent query substructures, which helps rank the existing (but nonsignificant) query structures or build new query structures. Our experiments on two benchmark datasets show that our approach significantly outperforms the existing ones, especially for complex questions. Also,\n\nAnswer B:\nLC-QuAD. Explanation: We employed the same datasets as BIBREF3 ( BIBREF3 ) and BIBREF4 ( BIBREF4 ): (i) the large-scale complex question answering dataset (LC-QuAD) BIBREF8 , containing 3,253 questions with non-empty results on DBpedia (2016-04), and (ii) the fifth edition of question answering over linked data (QALD-5) dataset BIBREF9 , containing 311 questions with non-empty results on DBpedia (2015-10).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.06571",
    "question": "How much were the gains they obtained?",
    "gold_answer": "",
    "answer_a": "What were the gains on the test set?\n< Explanation: We report the results of the experiments on the Skip-gram test set. We report the average accuracy of the model on the test set, and the average accuracy of the model on the test set after the addition of the word structure information. The results are shown in Table 1. The results show that the word structure information is very effective in improving the performance of the Skip-gram model. The average accuracy of the model on the test set after the addition of the word structure information is 0.98, which is 1.2 times higher than the original test",
    "answer_b": "between 21-57% in several morpho-syntactic questions. Explanation: FLOAT SELECTED: Table 2. Results on original test set questions. The values in brackets are based on questions without any OOVs.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much were the gains they obtained?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat were the gains on the test set?\n< Explanation: We report the results of the experiments on the Skip-gram test set. We report the average accuracy of the model on the test set, and the average accuracy of the model on the test set after the addition of the word structure information. The results are shown in Table 1. The results show that the word structure information is very effective in improving the performance of the Skip-gram model. The average accuracy of the model on the test set after the addition of the word structure information is 0.98, which is 1.2 times higher than the original test\n\nAnswer B:\nbetween 21-57% in several morpho-syntactic questions. Explanation: FLOAT SELECTED: Table 2. Results on original test set questions. The values in brackets are based on questions without any OOVs.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.00424",
    "question": "What is the extractive technique used for summarization?",
    "gold_answer": "",
    "answer_a": "Answer with content missing: (baseline list) TextRank, KLSum, Lead-1, Lead-K and Random-K. Explanation: We chose to look at the following common baselines:",
    "answer_b": "How is it evaluated?\n< Explanation: We use a simple rule-based approach to extract the summary. We first split the text into sentences and then extract the longest sentence as the summary. We use the following rule: if the sentence is longer than 100 words, we extract the sentence and add it to the summary. We repeat this process until the sentence is shorter than 100 words. We then use the summary as the input to a summarization model. We evaluate the quality of the summaries using the BLEU metric. We also evaluate the quality of the summaries using the F1 metric. We find that the BLE",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the extractive technique used for summarization?\n\nGround Truth Answer:\n\n\nAnswer A:\nAnswer with content missing: (baseline list) TextRank, KLSum, Lead-1, Lead-K and Random-K. Explanation: We chose to look at the following common baselines:\n\nAnswer B:\nHow is it evaluated?\n< Explanation: We use a simple rule-based approach to extract the summary. We first split the text into sentences and then extract the longest sentence as the summary. We use the following rule: if the sentence is longer than 100 words, we extract the sentence and add it to the summary. We repeat this process until the sentence is shorter than 100 words. We then use the summary as the input to a summarization model. We evaluate the quality of the summaries using the BLEU metric. We also evaluate the quality of the summaries using the F1 metric. We find that the BLE\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.00424",
    "question": "How big is the dataset?",
    "gold_answer": "",
    "answer_a": "446. Explanation: The dataset we propose contains 446 sets of parallel text.",
    "answer_b": "What is the size of the dataset?\n< Explanation: We collected 1000 legal text snippets from the web. We selected 1000 legal text snippets from the web, which are from the following sources: (1) Wikipedia, (2) Stack Overflow, (3) Reddit, (4) Quora, (5) Stack Exchange, (6) GitHub, (7) Stack Overflow, (8) Stack Exchange, (9) Quora, (10) Stack Exchange, (11) Quora, (12) Stack Exchange, (13) Quora, (14",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big is the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\n446. Explanation: The dataset we propose contains 446 sets of parallel text.\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: We collected 1000 legal text snippets from the web. We selected 1000 legal text snippets from the web, which are from the following sources: (1) Wikipedia, (2) Stack Overflow, (3) Reddit, (4) Quora, (5) Stack Exchange, (6) GitHub, (7) Stack Overflow, (8) Stack Exchange, (9) Quora, (10) Stack Exchange, (11) Quora, (12) Stack Exchange, (13) Quora, (14\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.06053",
    "question": "By how much they outperform the baseline?",
    "gold_answer": "",
    "answer_a": "18.08 percent points on F-score. Explanation: We observe that i) the best word boundary detection (F-score) is obtained with MBN features, an informative prior and the SVAE model; this confirms the results of table TABREF23 and shows that better AUD leads to better word segmentation ii) word segmentation from AUD graph Lattices is slightly better than from flat sequences of AUD symbols (1-best); iii) our results outperform a pure speech based baseline based on segmental DTW BIBREF22 (F-score of 19.3% on the exact same corpus).",
    "answer_b": "What is the baseline?\n< Explanation: We compare our approach with the Segmental-DTW baseline and with the best results of the previous work on the same corpus. The Segmental-DTW baseline is the best performing approach on the same corpus and achieves 99.5% accuracy. The best results of the previous work are 99.4% and 99.3% for the English and the Spanish corpora respectively. Our approach achieves 99.6% accuracy on the English corpus and 99.7% on the Spanish corpus. The best results of the previous work are 99.4",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much they outperform the baseline?\n\nGround Truth Answer:\n\n\nAnswer A:\n18.08 percent points on F-score. Explanation: We observe that i) the best word boundary detection (F-score) is obtained with MBN features, an informative prior and the SVAE model; this confirms the results of table TABREF23 and shows that better AUD leads to better word segmentation ii) word segmentation from AUD graph Lattices is slightly better than from flat sequences of AUD symbols (1-best); iii) our results outperform a pure speech based baseline based on segmental DTW BIBREF22 (F-score of 19.3% on the exact same corpus).\n\nAnswer B:\nWhat is the baseline?\n< Explanation: We compare our approach with the Segmental-DTW baseline and with the best results of the previous work on the same corpus. The Segmental-DTW baseline is the best performing approach on the same corpus and achieves 99.5% accuracy. The best results of the previous work are 99.4% and 99.3% for the English and the Spanish corpora respectively. Our approach achieves 99.6% accuracy on the English corpus and 99.7% on the Spanish corpus. The best results of the previous work are 99.4\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.06053",
    "question": "How long are the datasets?",
    "gold_answer": "",
    "answer_a": "What are the languages?\nYes, we use the same dataset as in the previous work. We use the same dataset for both the acoustic unit discovery and word segmentation tasks. The dataset consists of 1000 utterances of the low-resource language of the Chinese language. The dataset is available at https://github.com/zhengyuanliu/ACU-Det-WordSeg. The dataset is divided into 1000 utterances of 10000 words each. The utterances are randomly selected from the corpus. The dataset is divided into 1000 utterances of 10",
    "answer_b": "5130. Explanation: The corpus is multilingual (5130 Mboshi speech utterances aligned to French text) and contains linguists' transcriptions in Mboshi (in the form of a non-standard graphemic form close to the language phonology).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long are the datasets?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the languages?\nYes, we use the same dataset as in the previous work. We use the same dataset for both the acoustic unit discovery and word segmentation tasks. The dataset consists of 1000 utterances of the low-resource language of the Chinese language. The dataset is available at https://github.com/zhengyuanliu/ACU-Det-WordSeg. The dataset is divided into 1000 utterances of 10000 words each. The utterances are randomly selected from the corpus. The dataset is divided into 1000 utterances of 10\n\nAnswer B:\n5130. Explanation: The corpus is multilingual (5130 Mboshi speech utterances aligned to French text) and contains linguists' transcriptions in Mboshi (in the form of a non-standard graphemic form close to the language phonology).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.06053",
    "question": "What bayesian model is trained?",
    "gold_answer": "",
    "answer_a": "Structured Variational AutoEncoder (SVAE) AUD. Explanation: The AUD model described in BIBREF0 , BIBREF1 is a non-parametric Bayesian Hidden Markov Model (HMM).",
    "answer_b": "What is the prior?\nFor the acoustic unit discovery, we use a mixture of Dirichlet process (DP) priors. The DP is a mixture of Dirichlet distributions, where the parameters of the mixture are drawn from a Dirichlet distribution. The DP is a mixture of Dirichlet distributions, where the parameters of the mixture are drawn from a Dirichlet distribution. The DP is a mixture of Dirichlet distributions, where the parameters of the mixture are drawn from a Dirichlet distribution. The DP is a mixture of Dirichlet distributions, where the parameters of the mixture are drawn from a Dirichlet",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat bayesian model is trained?\n\nGround Truth Answer:\n\n\nAnswer A:\nStructured Variational AutoEncoder (SVAE) AUD. Explanation: The AUD model described in BIBREF0 , BIBREF1 is a non-parametric Bayesian Hidden Markov Model (HMM).\n\nAnswer B:\nWhat is the prior?\nFor the acoustic unit discovery, we use a mixture of Dirichlet process (DP) priors. The DP is a mixture of Dirichlet distributions, where the parameters of the mixture are drawn from a Dirichlet distribution. The DP is a mixture of Dirichlet distributions, where the parameters of the mixture are drawn from a Dirichlet distribution. The DP is a mixture of Dirichlet distributions, where the parameters of the mixture are drawn from a Dirichlet distribution. The DP is a mixture of Dirichlet distributions, where the parameters of the mixture are drawn from a Dirichlet\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.06053",
    "question": "What low resource languages are considered?",
    "gold_answer": "",
    "answer_a": "Explanation: We consider three low-resource languages: Arabic, Chinese and Japanese. We use the same acoustic unit discovery model as in our previous work on English. We also use the same segmentation model as in our previous work on Chinese. We use the same acoustic unit discovery model as in our previous work on English. We also use the same segmentation model as in our previous work on Chinese. We use the same acoustic unit discovery model as in our previous work on Japanese. We also use the same segmentation model as in our previous work on Japanese. We use the same acoustic unit discovery model as in our previous work on Arabic. We also use the",
    "answer_b": "Mboshi. Explanation: Mboshi (Bantu C25) is a typical Bantu language spoken in Congo-Brazzaville.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat low resource languages are considered?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We consider three low-resource languages: Arabic, Chinese and Japanese. We use the same acoustic unit discovery model as in our previous work on English. We also use the same segmentation model as in our previous work on Chinese. We use the same acoustic unit discovery model as in our previous work on English. We also use the same segmentation model as in our previous work on Chinese. We use the same acoustic unit discovery model as in our previous work on Japanese. We also use the same segmentation model as in our previous work on Japanese. We use the same acoustic unit discovery model as in our previous work on Arabic. We also use the\n\nAnswer B:\nMboshi. Explanation: Mboshi (Bantu C25) is a typical Bantu language spoken in Congo-Brazzaville.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.00871",
    "question": "How is cluster purity measured?",
    "gold_answer": "",
    "answer_a": "V-measure. Explanation: For each debiased embedding we then project these words into 2D space with tSNE BIBREF15, compute clusters with k-means, and calculate the clusters' V-measure BIBREF16.",
    "answer_b": "What is the state-of-the-art?\n< Explanation: We define cluster purity as the proportion of words in a cluster that are of the same gender. We use the following formula to calculate cluster purity: $\\frac{N_{\\text{male}} + N_{\\text{female}}}{N_{\\text{male}} + N_{\\text{female}} + N_{\\text{other}}}$, where $N_{\\text{male}}$ is the number of male words in the cluster, $N_{\\text{female}}$ is the number of female words in the cluster, and $N_{\\",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is cluster purity measured?\n\nGround Truth Answer:\n\n\nAnswer A:\nV-measure. Explanation: For each debiased embedding we then project these words into 2D space with tSNE BIBREF15, compute clusters with k-means, and calculate the clusters' V-measure BIBREF16.\n\nAnswer B:\nWhat is the state-of-the-art?\n< Explanation: We define cluster purity as the proportion of words in a cluster that are of the same gender. We use the following formula to calculate cluster purity: $\\frac{N_{\\text{male}} + N_{\\text{female}}}{N_{\\text{male}} + N_{\\text{female}} + N_{\\text{other}}}$, where $N_{\\text{male}}$ is the number of male words in the cluster, $N_{\\text{female}}$ is the number of female words in the cluster, and $N_{\\\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.00871",
    "question": "What was the previous state of the art for bias mitigation?",
    "gold_answer": "",
    "answer_a": "What is the difference between the two methods?\n< Explanation: The state-of-the-art for gender bias mitigation is the CDA method of BIBREF10, which is based on the projection of the embedding onto a subspace of words that are not gendered. This subspace is then used to generate a new corpus, which is then augmented with a small number of words that are gendered. The corpus is then compared to the original corpus to determine the amount of bias. The CDA method is shown to be effective in reducing direct gender bias, but not in mitigating indirect gender bias. The CDA method is also shown",
    "answer_b": "WED. Explanation: We have replicated two state-of-the-art bias mitigation techniques, WED and CDA, on two large corpora, Wikipedia and the English Gigaword.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat was the previous state of the art for bias mitigation?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the two methods?\n< Explanation: The state-of-the-art for gender bias mitigation is the CDA method of BIBREF10, which is based on the projection of the embedding onto a subspace of words that are not gendered. This subspace is then used to generate a new corpus, which is then augmented with a small number of words that are gendered. The corpus is then compared to the original corpus to determine the amount of bias. The CDA method is shown to be effective in reducing direct gender bias, but not in mitigating indirect gender bias. The CDA method is also shown\n\nAnswer B:\nWED. Explanation: We have replicated two state-of-the-art bias mitigation techniques, WED and CDA, on two large corpora, Wikipedia and the English Gigaword.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.00871",
    "question": "How are names paired in the Names Intervention?",
    "gold_answer": "",
    "answer_a": "What is the effect of the Names Intervention on the number of words being treated?\n< Explanation: The Names Intervention is a novel name-pairing technique that is used to increase the number of words being treated. It is based on the idea that names are often more gender-neutral than the words they are paired with. The technique is applied to the Wikipedia corpus, and the number of words being treated is increased by 10% (from 100,000 to 110,000). The number of words being treated is then compared to the number of words being debiased, and the difference",
    "answer_b": "name frequency. Explanation: We fixedly associate pairs of names for swapping, thus expanding BIBREF5's short list of gender pairs vastly. Clearly both name frequency and the degree of gender-specificity are relevant to this bipartite matching.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are names paired in the Names Intervention?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the effect of the Names Intervention on the number of words being treated?\n< Explanation: The Names Intervention is a novel name-pairing technique that is used to increase the number of words being treated. It is based on the idea that names are often more gender-neutral than the words they are paired with. The technique is applied to the Wikipedia corpus, and the number of words being treated is increased by 10% (from 100,000 to 110,000). The number of words being treated is then compared to the number of words being debiased, and the difference\n\nAnswer B:\nname frequency. Explanation: We fixedly associate pairs of names for swapping, thus expanding BIBREF5's short list of gender pairs vastly. Clearly both name frequency and the degree of gender-specificity are relevant to this bipartite matching.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.00871",
    "question": "Which tasks quantify embedding quality?",
    "gold_answer": "",
    "answer_a": "How do they measure it?\n< Explanation: We use the following tasks to measure embedding quality:\n\n1. Word Embedding Quality (WQ) : We use the Word2Vec embedding quality metric to measure the quality of the embeddings. We use the following formula to calculate WQ:\n\nWQ = 1 - (1 - W) / (1 - W) + (1 - W) / (1 - W) + (1 - W) / (1 - W) + (1 - W) / (1 - W) + (1 - W) / (1 - W) + (1 - W)",
    "answer_b": "word similarity. Explanation: Additionally, although one could expect that the debiased embeddings might suffer performance losses in computational linguistic tasks, our embeddings remain useful for at least two such tasks, word similarity and sentiment classification BIBREF6.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich tasks quantify embedding quality?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they measure it?\n< Explanation: We use the following tasks to measure embedding quality:\n\n1. Word Embedding Quality (WQ) : We use the Word2Vec embedding quality metric to measure the quality of the embeddings. We use the following formula to calculate WQ:\n\nWQ = 1 - (1 - W) / (1 - W) + (1 - W) / (1 - W) + (1 - W) / (1 - W) + (1 - W) / (1 - W) + (1 - W) / (1 - W) + (1 - W)\n\nAnswer B:\nword similarity. Explanation: Additionally, although one could expect that the debiased embeddings might suffer performance losses in computational linguistic tasks, our embeddings remain useful for at least two such tasks, word similarity and sentiment classification BIBREF6.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.00871",
    "question": "What empirical comparison methods are used?",
    "gold_answer": "",
    "answer_a": "What are the results?\n<blockquote>Table 1: Comparison of methods on the English Gigaword corpus. The first column shows the number of words in the corpus, the second column shows the number of words in the corpus that are treated as gender-biased, the third column shows the number of words in the corpus that are treated as gender-neutral, and the fourth column shows the number of words in the corpus that are treated as gender-neutral. The fifth column shows the number of words in the corpus that are treated as gender-neutral, the sixth column shows the number of words in the corpus that are treated as gender-neutral,",
    "answer_b": "test the degree to which the spaces are successful at mitigating direct and indirect bias, as well as the degree to which they can still be used in two NLP tasks standardly performed with embeddings, word similarity and sentiment classification. Explanation: In our experiments, we test the degree to which the spaces are successful at mitigating direct and indirect bias, as well as the degree to which they can still be used in two NLP tasks standardly performed with embeddings, word similarity and sentiment classification.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat empirical comparison methods are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n<blockquote>Table 1: Comparison of methods on the English Gigaword corpus. The first column shows the number of words in the corpus, the second column shows the number of words in the corpus that are treated as gender-biased, the third column shows the number of words in the corpus that are treated as gender-neutral, and the fourth column shows the number of words in the corpus that are treated as gender-neutral. The fifth column shows the number of words in the corpus that are treated as gender-neutral, the sixth column shows the number of words in the corpus that are treated as gender-neutral,\n\nAnswer B:\ntest the degree to which the spaces are successful at mitigating direct and indirect bias, as well as the degree to which they can still be used in two NLP tasks standardly performed with embeddings, word similarity and sentiment classification. Explanation: In our experiments, we test the degree to which the spaces are successful at mitigating direct and indirect bias, as well as the degree to which they can still be used in two NLP tasks standardly performed with embeddings, word similarity and sentiment classification.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.03010",
    "question": "How do they define their tokens (words, word-piece)?",
    "gold_answer": "",
    "answer_a": "What is the difference between the two?\n< Explanation: The tokens are defined as follows: (1) word: a sequence of characters, e.g., \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"",
    "answer_b": "a word or a word-piece. Explanation: However, instead of using a random mask as in SpecAugment, our approach masks out the whole patch of the features corresponding to an output token during training, e.g., a word or a word-piece.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they define their tokens (words, word-piece)?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the two?\n< Explanation: The tokens are defined as follows: (1) word: a sequence of characters, e.g., \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"the\", \"\n\nAnswer B:\na word or a word-piece. Explanation: However, instead of using a random mask as in SpecAugment, our approach masks out the whole patch of the features corresponding to an output token during training, e.g., a word or a word-piece.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.03010",
    "question": "By how much do they outperform existing state-of-the-art model on end-to-end Speech recognition?s ",
    "gold_answer": "",
    "answer_a": "?\n< Explanation: We compare our model with the state-of-the-art model in Librispeech 960h and TedLium2 data sets. The results are shown in Table 1. Our model achieves the state-of-the-art performance on Librispeech 960h and TedLium2 data sets, which are 1.03 and 1.05 BLEU respectively. Our model also achieves the state-of-the-art performance on Librispeech 960h data set, which is 1.04 BLEU. The results are comparable to the state-of-the-art model in",
    "answer_b": "relative 4.5$\\%$ gain. Explanation: Semantic mask is complementary to specagumentation, which enables better S2S language modeling training in an E2E model, resulting in a relative 4.5$\\%$ gain.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much do they outperform existing state-of-the-art model on end-to-end Speech recognition?s \n\nGround Truth Answer:\n\n\nAnswer A:\n?\n< Explanation: We compare our model with the state-of-the-art model in Librispeech 960h and TedLium2 data sets. The results are shown in Table 1. Our model achieves the state-of-the-art performance on Librispeech 960h and TedLium2 data sets, which are 1.03 and 1.05 BLEU respectively. Our model also achieves the state-of-the-art performance on Librispeech 960h data set, which is 1.04 BLEU. The results are comparable to the state-of-the-art model in\n\nAnswer B:\nrelative 4.5$\\%$ gain. Explanation: Semantic mask is complementary to specagumentation, which enables better S2S language modeling training in an E2E model, resulting in a relative 4.5$\\%$ gain.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.09960",
    "question": "what were the evaluation metrics?",
    "gold_answer": "",
    "answer_a": "BLEU. Explanation: . We use case-insensitive 4-gram BLEU score as the automatic metric BIBREF21 for translation quality evaluation.",
    "answer_b": "How were they evaluated?\n< Explanation: We evaluate the performance of our proposed methods by using BLEU and METEOR. The BLEU score is used to evaluate the quality of the translations, while the METEOR score is used to evaluate the fluency of the translations. The BLEU score is defined as follows: BLEU = (B + E + F) / N, where B is the number of best matches, E is the number of best matches, and F is the number of best matches. The BLEU score is a metric that is widely used to evaluate the quality of translations. The METEOR score is",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat were the evaluation metrics?\n\nGround Truth Answer:\n\n\nAnswer A:\nBLEU. Explanation: . We use case-insensitive 4-gram BLEU score as the automatic metric BIBREF21 for translation quality evaluation.\n\nAnswer B:\nHow were they evaluated?\n< Explanation: We evaluate the performance of our proposed methods by using BLEU and METEOR. The BLEU score is used to evaluate the quality of the translations, while the METEOR score is used to evaluate the fluency of the translations. The BLEU score is defined as follows: BLEU = (B + E + F) / N, where B is the number of best matches, E is the number of best matches, and F is the number of best matches. The BLEU score is a metric that is widely used to evaluate the quality of translations. The METEOR score is\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.09960",
    "question": "what language pairs are explored?",
    "gold_answer": "",
    "answer_a": "Explanation: We evaluate our method on 100 pairs of English and Chinese. The English-English pairs are from the English Wikipedia corpus, and the Chinese-Chinese pairs are from the Chinese Wikipedia corpus. The English-English pairs are selected from the 10000 English Wikipedia sentences and the Chinese-Chinese pairs are selected from the 10000 Chinese Wikipedia sentences. The total number of pairs is 20000. The English-English pairs are used to train the model, and the Chinese-Chinese pairs are used to evaluate the model. The English-English pairs are used to train the",
    "answer_b": "Chinese-English. Explanation: Our empirical experiments on Chinese-English translation and English-Japanese translation tasks show the efficacy of our methods.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat language pairs are explored?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We evaluate our method on 100 pairs of English and Chinese. The English-English pairs are from the English Wikipedia corpus, and the Chinese-Chinese pairs are from the Chinese Wikipedia corpus. The English-English pairs are selected from the 10000 English Wikipedia sentences and the Chinese-Chinese pairs are selected from the 10000 Chinese Wikipedia sentences. The total number of pairs is 20000. The English-English pairs are used to train the model, and the Chinese-Chinese pairs are used to evaluate the model. The English-English pairs are used to train the\n\nAnswer B:\nChinese-English. Explanation: Our empirical experiments on Chinese-English translation and English-Japanese translation tasks show the efficacy of our methods.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.09960",
    "question": "what datasets did they use?",
    "gold_answer": "",
    "answer_a": "Explanation: We use the following datasets: (1) the English to Chinese (E2C) dataset, which is a large-scale dataset for NMT, and (2) the Chinese to English (C2E) dataset, which is a small-scale dataset for NMT. The E2C dataset is collected from the Chinese Wikipedia and the C2E dataset is collected from the Chinese Wikipedia. The E2C dataset contains 10,000 sentences and the C2E dataset contains 1,000 sentences. The E2C dataset is divided into 1000 training sentences and 1",
    "answer_b": "NIST 2003 (MT03). Explanation: In CH-EN translation, we test the proposed methods with two data sets: 1) small data set, which includes 0.63M sentence pairs; 2) large-scale data set, which contains about 2.1M sentence pairs. NIST 2003 (MT03) dataset is used for validation. NIST2004-2006 (MT04-06) and NIST 2008 (MT08) datasets are used for testing. In EN-JA translation, we use KFTT dataset, which includes 0.44M sentence pairs for training, 1166 sentence pairs for validation and 1160 sentence pairs for testing.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat datasets did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We use the following datasets: (1) the English to Chinese (E2C) dataset, which is a large-scale dataset for NMT, and (2) the Chinese to English (C2E) dataset, which is a small-scale dataset for NMT. The E2C dataset is collected from the Chinese Wikipedia and the C2E dataset is collected from the Chinese Wikipedia. The E2C dataset contains 10,000 sentences and the C2E dataset contains 1,000 sentences. The E2C dataset is divided into 1000 training sentences and 1\n\nAnswer B:\nNIST 2003 (MT03). Explanation: In CH-EN translation, we test the proposed methods with two data sets: 1) small data set, which includes 0.63M sentence pairs; 2) large-scale data set, which contains about 2.1M sentence pairs. NIST 2003 (MT03) dataset is used for validation. NIST2004-2006 (MT04-06) and NIST 2008 (MT08) datasets are used for testing. In EN-JA translation, we use KFTT dataset, which includes 0.44M sentence pairs for training, 1166 sentence pairs for validation and 1160 sentence pairs for testing.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.09960",
    "question": "which attention based nmt method did they compare with?",
    "gold_answer": "",
    "answer_a": "Explanation: We compare our method with the attention based NMT (ATN) [1] and the attention based NMT with the attention based NMT (ATN-ATN) [2] in the experiments. The ATN-ATN method is a combination of the ATN method and the attention based NMT method. The ATN-ATN method is a combination of the ATN method and the attention based NMT method. The ATN-ATN method is a combination of the ATN method and the attention based NMT method. The ATN-ATN method is a combination of the ATN",
    "answer_b": "attention-based NMT system BIBREF23 , BIBREF24. Explanation: Baseline: It is the baseline attention-based NMT system BIBREF23 , BIBREF24 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhich attention based nmt method did they compare with?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We compare our method with the attention based NMT (ATN) [1] and the attention based NMT with the attention based NMT (ATN-ATN) [2] in the experiments. The ATN-ATN method is a combination of the ATN method and the attention based NMT method. The ATN-ATN method is a combination of the ATN method and the attention based NMT method. The ATN-ATN method is a combination of the ATN method and the attention based NMT method. The ATN-ATN method is a combination of the ATN\n\nAnswer B:\nattention-based NMT system BIBREF23 , BIBREF24. Explanation: Baseline: It is the baseline attention-based NMT system BIBREF23 , BIBREF24 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.09960",
    "question": "by how much did their system improve?",
    "gold_answer": "",
    "answer_a": "What were the results?\n< Explanation: Table 1 shows the results of our proposed methods. The results show that our methods outperform the state-of-the-art attention-based NMT by 0.1 BLEU and 0.2 F1. The results of our methods are also better than the state-of-the-art attention-based NMT by 0.1 BLEU and 0.2 F1. The results of our methods are also better than the state-of-the-art attention-based NMT by 0.1 BLEU and 0.2 F1. The results of our methods are also better than the state",
    "answer_b": "The average improvement for CH-EN is 3.99 BLEU points, for EN-JA it is 3.59 BLEU points.. Explanation: The average improvement for CH-EN and EN-JA translation is up to 3.99 BLEU points (32.71 vs. 28.72) and 3.59 BLEU (25.99 vs. 22.40) points, respectively.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nby how much did their system improve?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat were the results?\n< Explanation: Table 1 shows the results of our proposed methods. The results show that our methods outperform the state-of-the-art attention-based NMT by 0.1 BLEU and 0.2 F1. The results of our methods are also better than the state-of-the-art attention-based NMT by 0.1 BLEU and 0.2 F1. The results of our methods are also better than the state-of-the-art attention-based NMT by 0.1 BLEU and 0.2 F1. The results of our methods are also better than the state\n\nAnswer B:\nThe average improvement for CH-EN is 3.99 BLEU points, for EN-JA it is 3.59 BLEU points.. Explanation: The average improvement for CH-EN and EN-JA translation is up to 3.99 BLEU points (32.71 vs. 28.72) and 3.59 BLEU (25.99 vs. 22.40) points, respectively.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.00937",
    "question": "What were the baseline methods?",
    "gold_answer": "",
    "answer_a": "DSSM. Explanation: Finally, in Table TABREF30 , we compare the results of our model to four baselines: DSSM BIBREF12 , Match Pyramid BIBREF18 , ARC-II BIBREF15 , and our model with frozen, randomly initialized embeddings.",
    "answer_b": "What were the results?\n< Explanation: We compare our method with the following baseline methods: (1) a) a simple ranking model that ranks all products in the catalog, b) b) a simple ranking model that ranks all products in the catalog, c) c) a simple ranking model that ranks all products in the catalog, d) d) a simple ranking model that ranks all products in the catalog, e) e) a simple ranking model that ranks all products in the catalog, f) f) a simple ranking model that ranks all products in the catalog, g) g) a simple ranking model that ranks all products in",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat were the baseline methods?\n\nGround Truth Answer:\n\n\nAnswer A:\nDSSM. Explanation: Finally, in Table TABREF30 , we compare the results of our model to four baselines: DSSM BIBREF12 , Match Pyramid BIBREF18 , ARC-II BIBREF15 , and our model with frozen, randomly initialized embeddings.\n\nAnswer B:\nWhat were the results?\n< Explanation: We compare our method with the following baseline methods: (1) a) a simple ranking model that ranks all products in the catalog, b) b) a simple ranking model that ranks all products in the catalog, c) c) a simple ranking model that ranks all products in the catalog, d) d) a simple ranking model that ranks all products in the catalog, e) e) a simple ranking model that ranks all products in the catalog, f) f) a simple ranking model that ranks all products in the catalog, g) g) a simple ranking model that ranks all products in\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.00937",
    "question": "What dataset is used for training?",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n< Explanation: We use the product search dataset from Amazon, which is a subset of the full Amazon product search dataset. The dataset contains 100,000 products and 100,000 queries. The dataset is split into training and test sets. The training set contains 70,000 queries and 70,000 products, while the test set contains 30,000 queries and 30,000 products. The dataset is available at https://www.kaggle.com/c/amazon-product-search.",
    "answer_b": "a self-collected dataset of 11 months of search logs as query-product pairs. Explanation: We use 11 months of search logs as training data and 1 month as evaluation. We sample 54 billion query-product training pairs. We preprocess these sampled pairs to 650 million rows by grouping the training data by query-product pairs over the entire time period and using the aggregated counts as weights for the pairs. We also decrease the training time by 3X by preprocessing the training data into tokens and using mmap to store the tokens. More details on our best practices for reducing training time can be found in Section SECREF6 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat dataset is used for training?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n< Explanation: We use the product search dataset from Amazon, which is a subset of the full Amazon product search dataset. The dataset contains 100,000 products and 100,000 queries. The dataset is split into training and test sets. The training set contains 70,000 queries and 70,000 products, while the test set contains 30,000 queries and 30,000 products. The dataset is available at https://www.kaggle.com/c/amazon-product-search.\n\nAnswer B:\na self-collected dataset of 11 months of search logs as query-product pairs. Explanation: We use 11 months of search logs as training data and 1 month as evaluation. We sample 54 billion query-product training pairs. We preprocess these sampled pairs to 650 million rows by grouping the training data by query-product pairs over the entire time period and using the aggregated counts as weights for the pairs. We also decrease the training time by 3X by preprocessing the training data into tokens and using mmap to store the tokens. More details on our best practices for reducing training time can be found in Section SECREF6 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1811.01183",
    "question": "What is the source of their data?",
    "gold_answer": "",
    "answer_a": "a curated database of high-quality in vivo rodent uterotrophic bioassay data. Explanation: To facilitate the development of novel in vitro methods and assess the adherence to existing study guidelines, a curated database of high-quality in vivo rodent uterotrophic bioassay data extracted from research publications has recently been developed and published BIBREF1 .",
    "answer_b": "How is it collected?\n< Explanation: We collected data from the PubMed Central (PMC) database, which contains over 100 million articles. We downloaded the text of all PMC articles published between 2006 and 2015, and extracted the study descriptors from the text using a combination of a rule-based approach and a machine learning approach. We used a rule-based approach to identify the study descriptors in the text, and a machine learning approach to train a binary classifier to identify publications that met the criteria. We used a rule-based approach to identify the study descriptors in the text, and a machine learning approach to",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the source of their data?\n\nGround Truth Answer:\n\n\nAnswer A:\na curated database of high-quality in vivo rodent uterotrophic bioassay data. Explanation: To facilitate the development of novel in vitro methods and assess the adherence to existing study guidelines, a curated database of high-quality in vivo rodent uterotrophic bioassay data extracted from research publications has recently been developed and published BIBREF1 .\n\nAnswer B:\nHow is it collected?\n< Explanation: We collected data from the PubMed Central (PMC) database, which contains over 100 million articles. We downloaded the text of all PMC articles published between 2006 and 2015, and extracted the study descriptors from the text using a combination of a rule-based approach and a machine learning approach. We used a rule-based approach to identify the study descriptors in the text, and a machine learning approach to train a binary classifier to identify publications that met the criteria. We used a rule-based approach to identify the study descriptors in the text, and a machine learning approach to\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1811.01183",
    "question": "What is their binary classifier?",
    "gold_answer": "",
    "answer_a": "How is it trained?\n< Explanation: The binary classifier is trained on the candidate sentences. The training set consists of 1000 sentences, each consisting of a single sentence from the text and a binary label indicating whether the sentence contains the study descriptor. The classifier is trained using the training set and then tested on the test set. The classifier is trained on the candidate sentences and then tested on the test set. The classifier is trained on the candidate sentences and then tested on the test set. The classifier is trained on the candidate sentences and then tested on the test set. The classifier is trained on the candidate sentences and then tested on",
    "answer_b": "Bernoulli Na\u00efve Bayes classifier. Explanation: All models used a Bernoulli Na\u00efve Bayes classifier (scikit-learn implementation which used a uniform class prior) trained on binary occurrence matrices created using 1-3-grams extracted from the publications, with n-grams appearing in only one document removed.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is their binary classifier?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it trained?\n< Explanation: The binary classifier is trained on the candidate sentences. The training set consists of 1000 sentences, each consisting of a single sentence from the text and a binary label indicating whether the sentence contains the study descriptor. The classifier is trained using the training set and then tested on the test set. The classifier is trained on the candidate sentences and then tested on the test set. The classifier is trained on the candidate sentences and then tested on the test set. The classifier is trained on the candidate sentences and then tested on the test set. The classifier is trained on the candidate sentences and then tested on\n\nAnswer B:\nBernoulli Na\u00efve Bayes classifier. Explanation: All models used a Bernoulli Na\u00efve Bayes classifier (scikit-learn implementation which used a uniform class prior) trained on binary occurrence matrices created using 1-3-grams extracted from the publications, with n-grams appearing in only one document removed.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1811.01183",
    "question": "How long is their dataset?",
    "gold_answer": "",
    "answer_a": "What are the criteria?\nDataset: We use the 2016 PubMed Central (PMC) database to collect data on 1,000 publications. We use the PubMed API to retrieve the full text of each publication, and then use a pre-trained word embedding model to extract the study descriptors. We use the PubMed API to retrieve the full text of each publication, and then use a pre-trained word embedding model to extract the study descriptors. We use the PubMed API to retrieve the full text of each publication, and then use a pre-trained word embedding model to extract the study descriptors. We use the PubMed API to retrieve the",
    "answer_b": "670. Explanation: The version of the database which contains both GL and non-GL studies consists of 670 publications (spanning the years 1938 through 2014) with results from 2,615 uterotrophic bioassays.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long is their dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the criteria?\nDataset: We use the 2016 PubMed Central (PMC) database to collect data on 1,000 publications. We use the PubMed API to retrieve the full text of each publication, and then use a pre-trained word embedding model to extract the study descriptors. We use the PubMed API to retrieve the full text of each publication, and then use a pre-trained word embedding model to extract the study descriptors. We use the PubMed API to retrieve the full text of each publication, and then use a pre-trained word embedding model to extract the study descriptors. We use the PubMed API to retrieve the\n\nAnswer B:\n670. Explanation: The version of the database which contains both GL and non-GL studies consists of 670 publications (spanning the years 1938 through 2014) with results from 2,615 uterotrophic bioassays.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1811.01183",
    "question": "What is a study descriptor?",
    "gold_answer": "",
    "answer_a": "How is it defined?\n<sep><br>Study descriptor: A study descriptor is a word or phrase describing the study, such as species, route of administration, or dosing regimen. For example, a study descriptor for a study of a drug for a patient with a specific disease might be \u201cdrug for patient with disease\u201d. A study descriptor for a study of a drug for a patient with a specific disease might be \u201cdrug for patient with disease\u201d. A study descriptor for a study of a drug for a patient with a specific disease might be \u201cdrug for patient with disease\u201d. A study descriptor for a study of a drug for a patient",
    "answer_b": "Study descriptor is a set of structured data elements extracted from a publication text that contains specific expert knowledge pertaining to domain topics.. Explanation: QUESTION (5 / 5): WHAT IS A STUDY DESCRIPTOR?",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is a study descriptor?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it defined?\n<sep><br>Study descriptor: A study descriptor is a word or phrase describing the study, such as species, route of administration, or dosing regimen. For example, a study descriptor for a study of a drug for a patient with a specific disease might be \u201cdrug for patient with disease\u201d. A study descriptor for a study of a drug for a patient with a specific disease might be \u201cdrug for patient with disease\u201d. A study descriptor for a study of a drug for a patient with a specific disease might be \u201cdrug for patient with disease\u201d. A study descriptor for a study of a drug for a patient\n\nAnswer B:\nStudy descriptor is a set of structured data elements extracted from a publication text that contains specific expert knowledge pertaining to domain topics.. Explanation: QUESTION (5 / 5): WHAT IS A STUDY DESCRIPTOR?\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02401",
    "question": "How are experiments designed to measure impact on performance by different choices?",
    "gold_answer": "",
    "answer_a": "What are the criteria used to evaluate the performance?\n<sep><br>Experiments are designed to measure the impact of different choices of optimizers and learning rate policies on the performance of the neural machine translation model. The experiments are conducted on the BERT model with the following settings: (1) the learning rate is set to 0.001, (2) the learning rate is set to 0.0001, (3) the learning rate is set to 0.00001, (4) the learning rate is set to 0.000001, (",
    "answer_b": "CLR is selected by the range test. Explanation: The learning rate boundary of the CLR is selected by the range test (shown in Figure FIGREF7). The base and maximal learning rates adopted in this study are presented in Table TABREF13. Shrink strategy is applied when examining the effects of CLR in training NMT. The optimizers (Adam and SGD) are assigned with two options: 1) without shrink (as \u201cnshrink\"); 2) with shrink at a rate of 0.5 (\u201cyshrink\"), which means the maximal learning rate for each cycle is reduced at a decay rate of 0.5.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are experiments designed to measure impact on performance by different choices?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the criteria used to evaluate the performance?\n<sep><br>Experiments are designed to measure the impact of different choices of optimizers and learning rate policies on the performance of the neural machine translation model. The experiments are conducted on the BERT model with the following settings: (1) the learning rate is set to 0.001, (2) the learning rate is set to 0.0001, (3) the learning rate is set to 0.00001, (4) the learning rate is set to 0.000001, (\n\nAnswer B:\nCLR is selected by the range test. Explanation: The learning rate boundary of the CLR is selected by the range test (shown in Figure FIGREF7). The base and maximal learning rates adopted in this study are presented in Table TABREF13. Shrink strategy is applied when examining the effects of CLR in training NMT. The optimizers (Adam and SGD) are assigned with two options: 1) without shrink (as \u201cnshrink\"); 2) with shrink at a rate of 0.5 (\u201cyshrink\"), which means the maximal learning rate for each cycle is reduced at a decay rate of 0.5.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.02401",
    "question": "What impact on performance is shown for different choices of optimizers and learning rate policies?",
    "gold_answer": "",
    "answer_a": "The training takes fewer epochs to converge to reach a local minimum with better BLEU scores. Explanation: The training takes fewer epochs to converge to reach a local minimum with better BLEU scores (i.e., bold fonts in Table TABREF18).",
    "answer_b": "How do they compare?\n<blockquote>For the first two experiments, we use the Adam optimizer and the AdamW optimizer with a learning rate of 1e-5 and 1e-6, respectively. For the third experiment, we use the Adam optimizer with a learning rate of 1e-5 and the AdamW optimizer with a learning rate of 1e-6. For the fourth experiment, we use the Adam optimizer with a learning rate of 1e-5 and the AdamW optimizer with a learning rate of 1e-6. For the fifth experiment, we use the Adam optimizer with a learning",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat impact on performance is shown for different choices of optimizers and learning rate policies?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe training takes fewer epochs to converge to reach a local minimum with better BLEU scores. Explanation: The training takes fewer epochs to converge to reach a local minimum with better BLEU scores (i.e., bold fonts in Table TABREF18).\n\nAnswer B:\nHow do they compare?\n<blockquote>For the first two experiments, we use the Adam optimizer and the AdamW optimizer with a learning rate of 1e-5 and 1e-6, respectively. For the third experiment, we use the Adam optimizer with a learning rate of 1e-5 and the AdamW optimizer with a learning rate of 1e-6. For the fourth experiment, we use the Adam optimizer with a learning rate of 1e-5 and the AdamW optimizer with a learning rate of 1e-6. For the fifth experiment, we use the Adam optimizer with a learning\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.09387",
    "question": "What domain do the audio samples fall under?",
    "gold_answer": "",
    "answer_a": "\u201cspeech\u201d, \u201cspeak\u201d, and \u201cwoman\u201d. Explanation: We collect the set of audio samples $\\mathbb {X}_{\\text{init}}=\\lbrace \\mathbf {x}_{\\text{init}}^{i}\\rbrace _{i=1}^{N_{\\text{init}}}$, with $N_{\\text{init}}=12000$ and their corresponding metadata (e.g. tags that indicate their content, and a short textual description), from the online platform Freesound BIBREF8. $\\mathbf {x}_{\\text{init}}$ was obtained by randomly sampling audio files from Freesound fulfilling the following criteria: lossless file type, audio quality at least 44.1 kHz and 16-bit, duration $10\\text{ s}\\le d({\\mathbf {x}_{\\text{init}}^{i}})\\le 300$ s (where $d(\\mathbf {x})$ is the duration of $\\mathbf {x}$), a textual description which first sentence does not have spelling errors according to US and UK English dictionaries (as an indication of the correctness of the metadata, e.g. tags), and not having tags that indicate music, sound effects, or speech. As tags indicating speech files we consider those like \u201cspeech\u201d, \u201cspeak\u201d, and \u201cwoman\u201d.",
    "answer_b": "How are they selected?\n< Explanation: The dataset consists of 4981 audio samples of 15 to 30 seconds duration and 24 905 captions of eight to 20 words length, and is divided into 1000 samples and 3980 captions. The dataset is built with focus on audio content and caption diversity, and the splits of the data are not hampering the training or evaluation of methods. All sounds are from the Freesound platform, and captions are crowdsourced using Amazon Mechanical Turk and annotators from English speaking countries. Unique words, named entities,",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat domain do the audio samples fall under?\n\nGround Truth Answer:\n\n\nAnswer A:\n\u201cspeech\u201d, \u201cspeak\u201d, and \u201cwoman\u201d. Explanation: We collect the set of audio samples $\\mathbb {X}_{\\text{init}}=\\lbrace \\mathbf {x}_{\\text{init}}^{i}\\rbrace _{i=1}^{N_{\\text{init}}}$, with $N_{\\text{init}}=12000$ and their corresponding metadata (e.g. tags that indicate their content, and a short textual description), from the online platform Freesound BIBREF8. $\\mathbf {x}_{\\text{init}}$ was obtained by randomly sampling audio files from Freesound fulfilling the following criteria: lossless file type, audio quality at least 44.1 kHz and 16-bit, duration $10\\text{ s}\\le d({\\mathbf {x}_{\\text{init}}^{i}})\\le 300$ s (where $d(\\mathbf {x})$ is the duration of $\\mathbf {x}$), a textual description which first sentence does not have spelling errors according to US and UK English dictionaries (as an indication of the correctness of the metadata, e.g. tags), and not having tags that indicate music, sound effects, or speech. As tags indicating speech files we consider those like \u201cspeech\u201d, \u201cspeak\u201d, and \u201cwoman\u201d.\n\nAnswer B:\nHow are they selected?\n< Explanation: The dataset consists of 4981 audio samples of 15 to 30 seconds duration and 24 905 captions of eight to 20 words length, and is divided into 1000 samples and 3980 captions. The dataset is built with focus on audio content and caption diversity, and the splits of the data are not hampering the training or evaluation of methods. All sounds are from the Freesound platform, and captions are crowdsourced using Amazon Mechanical Turk and annotators from English speaking countries. Unique words, named entities,\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.09387",
    "question": "How did they evaluate the quality of annotations?",
    "gold_answer": "",
    "answer_a": "They manually check the captions and employ extra annotators to further revise the annotations.. Explanation: We manually sanitize further $\\mathbb {C}_{\\text{sam}}^{z}$, e.g. by replacing \u201cit's\u201d with \u201cit is\u201d or \u201cits\u201d, making consistent hyphenation and compound words (e.g. \u201cnonstop\u201d, \u201cnon-stop\u201d, and \u201cnon stop\u201d), removing words or rephrasing captions pertaining to the content of speech (e.g. \u201cFrench\u201d, \u201cforeign\u201d), and removing/replacing named entities (e.g. \u201cWindex\u201d).\n\nFinally, we observe that some captions include transcription of speech. To remove it, we employ extra annotators (not from AMT) which had access only at the captions. We instruct the annotators to remove the transcribed speech and rephrase the caption. If the result is less than eight words, we check the bottom $N_{\\text{cp}}$ captions for that audio sample. If they include a caption that has been rated with at least 3 by all the annotators for both accuracy and fluency, and does not contain transcribed speech, we use that caption. Otherwise, we remove completely the audio sample.",
    "answer_b": "What metrics were used?\n< Explanation: We used the following metrics to evaluate the quality of annotations: (1) F1 score, (2) precision, (3) recall, (4) F-score, (5) accuracy, and (6) the number of errors. We also used the following metrics to evaluate the quality of the captions: (1) the number of words, (2) the number of named entities, (3) the number of speech transcription errors, (4) the number of errors in the caption, and (5) the number of errors in the caption and the number of errors in the text",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow did they evaluate the quality of annotations?\n\nGround Truth Answer:\n\n\nAnswer A:\nThey manually check the captions and employ extra annotators to further revise the annotations.. Explanation: We manually sanitize further $\\mathbb {C}_{\\text{sam}}^{z}$, e.g. by replacing \u201cit's\u201d with \u201cit is\u201d or \u201cits\u201d, making consistent hyphenation and compound words (e.g. \u201cnonstop\u201d, \u201cnon-stop\u201d, and \u201cnon stop\u201d), removing words or rephrasing captions pertaining to the content of speech (e.g. \u201cFrench\u201d, \u201cforeign\u201d), and removing/replacing named entities (e.g. \u201cWindex\u201d).\n\nFinally, we observe that some captions include transcription of speech. To remove it, we employ extra annotators (not from AMT) which had access only at the captions. We instruct the annotators to remove the transcribed speech and rephrase the caption. If the result is less than eight words, we check the bottom $N_{\\text{cp}}$ captions for that audio sample. If they include a caption that has been rated with at least 3 by all the annotators for both accuracy and fluency, and does not contain transcribed speech, we use that caption. Otherwise, we remove completely the audio sample.\n\nAnswer B:\nWhat metrics were used?\n< Explanation: We used the following metrics to evaluate the quality of annotations: (1) F1 score, (2) precision, (3) recall, (4) F-score, (5) accuracy, and (6) the number of errors. We also used the following metrics to evaluate the quality of the captions: (1) the number of words, (2) the number of named entities, (3) the number of speech transcription errors, (4) the number of errors in the caption, and (5) the number of errors in the caption and the number of errors in the text\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.09387",
    "question": "What is their baseline method?",
    "gold_answer": "",
    "answer_a": "How does it work?\n< Explanation: We propose a novel method to provide initial results for the task of audio captioning. We use a combination of a deep learning model and a pre-trained language model to generate captions from audio samples. We train a deep learning model to generate captions from audio samples, and then use the generated captions to train a pre-trained language model to generate captions from the pre-trained language model. We train the language model using a large corpus of English text, and then use the language model to generate captions from the pre-trained language model. We train the language model using a large corpus of English text, and then use",
    "answer_b": "previously utilized method for audio captioning BIBREF3 which is based on an encoder-decoder scheme with attention. Explanation: In order to provide an example of how to employ Clotho and some initial (baseline) results, we use a previously utilized method for audio captioning BIBREF3 which is based on an encoder-decoder scheme with attention.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is their baseline method?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow does it work?\n< Explanation: We propose a novel method to provide initial results for the task of audio captioning. We use a combination of a deep learning model and a pre-trained language model to generate captions from audio samples. We train a deep learning model to generate captions from audio samples, and then use the generated captions to train a pre-trained language model to generate captions from the pre-trained language model. We train the language model using a large corpus of English text, and then use the language model to generate captions from the pre-trained language model. We train the language model using a large corpus of English text, and then use\n\nAnswer B:\npreviously utilized method for audio captioning BIBREF3 which is based on an encoder-decoder scheme with attention. Explanation: In order to provide an example of how to employ Clotho and some initial (baseline) results, we use a previously utilized method for audio captioning BIBREF3 which is based on an encoder-decoder scheme with attention.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.03695",
    "question": "What is the size of the dataset?",
    "gold_answer": "",
    "answer_a": "How is it divided?\n<blockquote> The dataset is comprised by images along with their respective textual captions. The dataset is divided into two parts: the training set and the test set. The training set contains 10,000 images and 10,000 captions, while the test set contains 1,000 images and 1,000 captions. The training set is used to train the multimodal model, while the test set is used to evaluate the performance of the model. </blockquote>\n<blockquote> The dataset is comprised by images along with their respective textual captions. The dataset is",
    "answer_b": "829 instances. Explanation: In total, the dataset comprises 829 instances, each instance containing a pair of images and their description, as well as a similarity value that ranges from 0 to 5.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the size of the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it divided?\n<blockquote> The dataset is comprised by images along with their respective textual captions. The dataset is divided into two parts: the training set and the test set. The training set contains 10,000 images and 10,000 captions, while the test set contains 1,000 images and 1,000 captions. The training set is used to train the multimodal model, while the test set is used to evaluate the performance of the model. </blockquote>\n<blockquote> The dataset is comprised by images along with their respective textual captions. The dataset is\n\nAnswer B:\n829 instances. Explanation: In total, the dataset comprises 829 instances, each instance containing a pair of images and their description, as well as a similarity value that ranges from 0 to 5.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.03695",
    "question": "What is the source of the images and textual captions?",
    "gold_answer": "",
    "answer_a": "How are they annotated?\n<blockquote>\n<font size=\"2\">The dataset is comprised by images along with their respective textual captions. The images are taken from the following sources: </font>\n<font size=\"3\">1) The dataset is comprised by images along with their respective textual captions. The images are taken from the following sources: </font>\n<font size=\"3\">1) The dataset is comprised by images along with their respective textual captions. The images are taken from the following sources: </font>\n<font size=\"3\">1) The dataset is comprised by images along with their respective textual captions. The images",
    "answer_b": "Image Descriptions dataset, which is a subset of 8k-picture of Flickr. Explanation: The instances are derived from the following datasets:\n\nSubset 2014 This subset is derived from the Image Descriptions dataset which is a subset of the PASCAL VOC-2008 dataset BIBREF16 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the source of the images and textual captions?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they annotated?\n<blockquote>\n<font size=\"2\">The dataset is comprised by images along with their respective textual captions. The images are taken from the following sources: </font>\n<font size=\"3\">1) The dataset is comprised by images along with their respective textual captions. The images are taken from the following sources: </font>\n<font size=\"3\">1) The dataset is comprised by images along with their respective textual captions. The images are taken from the following sources: </font>\n<font size=\"3\">1) The dataset is comprised by images along with their respective textual captions. The images\n\nAnswer B:\nImage Descriptions dataset, which is a subset of 8k-picture of Flickr. Explanation: The instances are derived from the following datasets:\n\nSubset 2014 This subset is derived from the Image Descriptions dataset which is a subset of the PASCAL VOC-2008 dataset BIBREF16 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.08902",
    "question": "what evaluation metrics did they use?",
    "gold_answer": "",
    "answer_a": "Precision, recall and F1 score.. Explanation: FLOAT SELECTED: Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.",
    "answer_b": "Explanation: We use the F1 score as the evaluation metric for the NER task. The F1 score is a measure of the performance of a classifier that is the sum of the precision and the recall divided by the sum of the precision and the recall. The precision is the ratio of the number of correctly predicted entities to the total number of entities predicted by the model. The recall is the ratio of the number of correctly predicted entities to the total number of entities in the dataset. The F1 score is the harmonic mean of precision and recall. The higher the F1 score, the better the model\u2019s performance. The precision and recall",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat evaluation metrics did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nPrecision, recall and F1 score.. Explanation: FLOAT SELECTED: Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.\n\nAnswer B:\nExplanation: We use the F1 score as the evaluation metric for the NER task. The F1 score is a measure of the performance of a classifier that is the sum of the precision and the recall divided by the sum of the precision and the recall. The precision is the ratio of the number of correctly predicted entities to the total number of entities predicted by the model. The recall is the ratio of the number of correctly predicted entities to the total number of entities in the dataset. The F1 score is the harmonic mean of precision and recall. The higher the F1 score, the better the model\u2019s performance. The precision and recall\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.08902",
    "question": "what was the baseline?",
    "gold_answer": "",
    "answer_a": "What is the performance of the baseline?\n< Explanation: The baseline is the pre-trained BERT model. The baseline is trained on the MSRA-2006 dataset. The baseline is trained on the MSRA-2006 dataset. The baseline is trained on the MSRA-2006 dataset. The baseline is trained on the MSRA-2006 dataset. The baseline is trained on the MSRA-2006 dataset. The baseline is trained on the MSRA-2006 dataset. The baseline is trained on the MSRA-2006 dataset. The baseline",
    "answer_b": "BiGRU+CRF. Explanation: The BiGRU+CRF model was used as the baseline model.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat was the baseline?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of the baseline?\n< Explanation: The baseline is the pre-trained BERT model. The baseline is trained on the MSRA-2006 dataset. The baseline is trained on the MSRA-2006 dataset. The baseline is trained on the MSRA-2006 dataset. The baseline is trained on the MSRA-2006 dataset. The baseline is trained on the MSRA-2006 dataset. The baseline is trained on the MSRA-2006 dataset. The baseline is trained on the MSRA-2006 dataset. The baseline\n\nAnswer B:\nBiGRU+CRF. Explanation: The BiGRU+CRF model was used as the baseline model.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.08902",
    "question": "what were roberta's results?",
    "gold_answer": "",
    "answer_a": "the RoBERTa model achieves the highest F1 value of 94.17. Explanation: Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.",
    "answer_b": "Explanation: The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat were roberta's results?\n\nGround Truth Answer:\n\n\nAnswer A:\nthe RoBERTa model achieves the highest F1 value of 94.17. Explanation: Table I shows that the baseline model has already achieved an F1 value of 90.32. However, using the pre-training models can significantly increase F1 values by 1 to 2 percentage points except for ERNIE-tiny model. Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.\n\nAnswer B:\nExplanation: The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1. The results of the pre-training models are shown in Table 1.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.08902",
    "question": "which was the worst performing model?",
    "gold_answer": "",
    "answer_a": "ERNIE-tiny. Explanation: Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.",
    "answer_b": "What was the reason?\n< Explanation: Table 1 shows the results of the four models on the MSRA-2006 dataset. The results show that the best model is RoBERTa, which achieved 99.63% F1-score on the MSRA-2006 dataset. The second best model is BERT, which achieved 99.53% F1-score. The third best model is ERNIE, which achieved 99.43% F1-score. The fourth best model is ERNIE2.0-tiny, which achieved 99.3",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhich was the worst performing model?\n\nGround Truth Answer:\n\n\nAnswer A:\nERNIE-tiny. Explanation: Among the pre-training models, the RoBERTa model achieves the highest F1 value of 94.17, while the value of ERNIE-tiny is relatively low, even 4 percentage points lower than the baseline model.\n\nAnswer B:\nWhat was the reason?\n< Explanation: Table 1 shows the results of the four models on the MSRA-2006 dataset. The results show that the best model is RoBERTa, which achieved 99.63% F1-score on the MSRA-2006 dataset. The second best model is BERT, which achieved 99.53% F1-score. The third best model is ERNIE, which achieved 99.43% F1-score. The fourth best model is ERNIE2.0-tiny, which achieved 99.3\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.04815",
    "question": "How long is their sentiment analysis dataset?",
    "gold_answer": "",
    "answer_a": "Three datasets had total of 14.5k samples.. Explanation: Statistics of these datasets are shown in Table TABREF15.",
    "answer_b": "What is the size of the dataset?\n< Explanation: We use the dataset of 10000 reviews from 10000 different domains. The dataset is divided into training and testing sets. The training set is used to fine-tune BERT and the testing set is used to evaluate the performance of the fine-tuned BERT model. The dataset is available at https://github.com/zhengyuanliu1991/aspect-based-sentiment-analysis. The dataset contains 10000 reviews from 10000 different domains. The dataset is divided into training and",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long is their sentiment analysis dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nThree datasets had total of 14.5k samples.. Explanation: Statistics of these datasets are shown in Table TABREF15.\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: We use the dataset of 10000 reviews from 10000 different domains. The dataset is divided into training and testing sets. The training set is used to fine-tune BERT and the testing set is used to evaluate the performance of the fine-tuned BERT model. The dataset is available at https://github.com/zhengyuanliu1991/aspect-based-sentiment-analysis. The dataset contains 10000 reviews from 10000 different domains. The dataset is divided into training and\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.04815",
    "question": "What NLI dataset was used?",
    "gold_answer": "",
    "answer_a": "What is the performance of the proposed approach?\n< Explanation: We use the NLI dataset BIBREF11. The dataset consists of 10000 pairs of sentences, where each pair consists of a positive and a negative sentence. The positive sentence is the target sentence and the negative sentence is the sentence that is supposed to be negated. The dataset is divided into training, validation and test sets. The training set consists of 7000 pairs, the validation set consists of 2000 pairs and the test set consists of 2000 pairs. The dataset is available at https://github",
    "answer_b": "Stanford Natural Language Inference BIBREF7. Explanation: The Stanford Natural Language Inference BIBREF7 dataset contains 570k human annotated hypothesis/premise pairs. This is the most widely used entailment dataset for natural language inference.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat NLI dataset was used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of the proposed approach?\n< Explanation: We use the NLI dataset BIBREF11. The dataset consists of 10000 pairs of sentences, where each pair consists of a positive and a negative sentence. The positive sentence is the target sentence and the negative sentence is the sentence that is supposed to be negated. The dataset is divided into training, validation and test sets. The training set consists of 7000 pairs, the validation set consists of 2000 pairs and the test set consists of 2000 pairs. The dataset is available at https://github\n\nAnswer B:\nStanford Natural Language Inference BIBREF7. Explanation: The Stanford Natural Language Inference BIBREF7 dataset contains 570k human annotated hypothesis/premise pairs. This is the most widely used entailment dataset for natural language inference.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.04815",
    "question": "What aspects are considered?",
    "gold_answer": "",
    "answer_a": "dot-product attention module to dynamically combine all intermediates. Explanation: We use a dot-product attention module to dynamically combine all intermediates:\n\nwhere $W_h^T$ and $\\mathbf {q}$ are learnable weights.",
    "answer_b": "Explanation: We consider the following aspects: (1) sentiment polarity, (2) sentiment polarity of the aspect, (3) sentiment polarity of the aspect and the aspect itself, (4) sentiment polarity of the aspect and the aspect itself and the aspect itself, (5) sentiment polarity of the aspect and the aspect itself and the aspect itself and the aspect itself, (6) sentiment polarity of the aspect and the aspect itself and the aspect itself and the aspect itself and the aspect itself, (7) sentiment polarity of the aspect and the aspect itself and the aspect itself and the aspect itself and the aspect itself and the aspect itself,",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat aspects are considered?\n\nGround Truth Answer:\n\n\nAnswer A:\ndot-product attention module to dynamically combine all intermediates. Explanation: We use a dot-product attention module to dynamically combine all intermediates:\n\nwhere $W_h^T$ and $\\mathbf {q}$ are learnable weights.\n\nAnswer B:\nExplanation: We consider the following aspects: (1) sentiment polarity, (2) sentiment polarity of the aspect, (3) sentiment polarity of the aspect and the aspect itself, (4) sentiment polarity of the aspect and the aspect itself and the aspect itself, (5) sentiment polarity of the aspect and the aspect itself and the aspect itself and the aspect itself, (6) sentiment polarity of the aspect and the aspect itself and the aspect itself and the aspect itself and the aspect itself, (7) sentiment polarity of the aspect and the aspect itself and the aspect itself and the aspect itself and the aspect itself and the aspect itself,\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.04815",
    "question": "What layer gave the better results?",
    "gold_answer": "",
    "answer_a": "How much better?\n< Explanation: Table 1 shows the results of the proposed approach on the fine-tuning of BERT. The results show that the proposed approach outperforms the baseline approach by 0.13 BLEU and 0.12 F1. The results of the proposed approach on the natural language inference task are also shown in Table 2. The results show that the proposed approach outperforms the baseline approach by 0.05 BLEU and 0.05 F1. The results of the proposed approach on the natural language inference task are also shown in Table 3. The results",
    "answer_b": "12. Explanation: FLOAT SELECTED: Figure 2: Visualization of BERT and BERT-LSTM on Twitter dataset with the last six intermediates layers of BERT at the end of the 1st and 6th epoch. Among the PCA results, (a) and (b) illustrate that BERT-LSTM converges faster than BERT after just one epoch, while (c) and (d) demonstrate that BERT-LSTM cluster each class of data more dense and discriminative than BERT after the model nearly converges.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat layer gave the better results?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow much better?\n< Explanation: Table 1 shows the results of the proposed approach on the fine-tuning of BERT. The results show that the proposed approach outperforms the baseline approach by 0.13 BLEU and 0.12 F1. The results of the proposed approach on the natural language inference task are also shown in Table 2. The results show that the proposed approach outperforms the baseline approach by 0.05 BLEU and 0.05 F1. The results of the proposed approach on the natural language inference task are also shown in Table 3. The results\n\nAnswer B:\n12. Explanation: FLOAT SELECTED: Figure 2: Visualization of BERT and BERT-LSTM on Twitter dataset with the last six intermediates layers of BERT at the end of the 1st and 6th epoch. Among the PCA results, (a) and (b) illustrate that BERT-LSTM converges faster than BERT after just one epoch, while (c) and (d) demonstrate that BERT-LSTM cluster each class of data more dense and discriminative than BERT after the model nearly converges.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.12450",
    "question": "How many annotators were used for sentiment labeling?",
    "gold_answer": "",
    "answer_a": "What is the average accuracy of the model?\n< Explanation: We used 10 annotators to label the tweets. The average accuracy of the model was 85.5% (out of 100). The average accuracy of the model was 85.5% (out of 100). The average accuracy of the model was 85.5% (out of 100). The average accuracy of the model was 85.5% (out of 100). The average accuracy of the model was 85.5% (out of 100",
    "answer_b": "Each labelled Data point was verified by at least one other person after initial labelling.. Explanation: Three people who are indigenes or lived in the South South part of Nigeria, where Nigerian Pidgin is a prevalent method of communication were briefed on the fundamentals of word sentiments. Each labelled Data point was verified by at least one other person after initial labelling.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many annotators were used for sentiment labeling?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the average accuracy of the model?\n< Explanation: We used 10 annotators to label the tweets. The average accuracy of the model was 85.5% (out of 100). The average accuracy of the model was 85.5% (out of 100). The average accuracy of the model was 85.5% (out of 100). The average accuracy of the model was 85.5% (out of 100). The average accuracy of the model was 85.5% (out of 100\n\nAnswer B:\nEach labelled Data point was verified by at least one other person after initial labelling.. Explanation: Three people who are indigenes or lived in the South South part of Nigeria, where Nigerian Pidgin is a prevalent method of communication were briefed on the fundamentals of word sentiments. Each labelled Data point was verified by at least one other person after initial labelling.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.12450",
    "question": "How is data collected?",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n<sep>Data collection: We collected 1000 tweets from Twitter using the Twitter API. We used the Twitter API to collect tweets from the following Twitter accounts: @NigerianPidgin, @NigerianPidginTwitter, @NigerianPidginTwitter, @NigerianPidginTwitter, @NigerianPidginTwitter, @NigerianPidginTwitter, @NigerianPidginTwitter, @NigerianPidginTwitter, @NigerianPidginTwitter, @NigerianP",
    "answer_b": "original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner). Explanation: This study uses the original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner) to calculate the compound sentiment scores for about 14,000 Nigerian Pidgin tweets.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is data collected?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n<sep>Data collection: We collected 1000 tweets from Twitter using the Twitter API. We used the Twitter API to collect tweets from the following Twitter accounts: @NigerianPidgin, @NigerianPidginTwitter, @NigerianPidginTwitter, @NigerianPidginTwitter, @NigerianPidginTwitter, @NigerianPidginTwitter, @NigerianPidginTwitter, @NigerianPidginTwitter, @NigerianPidginTwitter, @NigerianP\n\nAnswer B:\noriginal and updated VADER (Valence Aware Dictionary and Sentiment Reasoner). Explanation: This study uses the original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner) to calculate the compound sentiment scores for about 14,000 Nigerian Pidgin tweets.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.12450",
    "question": "What full English language based sentiment analysis models are tried?",
    "gold_answer": "",
    "answer_a": "How do they compare to the Nigerian Pidgin sentiment analysis?\n< Explanation: We use the following full English language based sentiment analysis models: (1) Bag of Words (BoW) model: This model is a simple model that uses a bag of words to represent the sentiment of a tweet. The sentiment of a tweet is represented by a vector of 0s and 1s. The sentiment of a tweet is represented by a vector of 0s and 1s. The sentiment of a tweet is represented by a vector of 0s and 1s. The sentiment of a tweet is represented by a vector of",
    "answer_b": "the original VADER English lexicon.. Explanation: This study uses the original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner) to calculate the compound sentiment scores for about 14,000 Nigerian Pidgin tweets.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat full English language based sentiment analysis models are tried?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they compare to the Nigerian Pidgin sentiment analysis?\n< Explanation: We use the following full English language based sentiment analysis models: (1) Bag of Words (BoW) model: This model is a simple model that uses a bag of words to represent the sentiment of a tweet. The sentiment of a tweet is represented by a vector of 0s and 1s. The sentiment of a tweet is represented by a vector of 0s and 1s. The sentiment of a tweet is represented by a vector of 0s and 1s. The sentiment of a tweet is represented by a vector of\n\nAnswer B:\nthe original VADER English lexicon.. Explanation: This study uses the original and updated VADER (Valence Aware Dictionary and Sentiment Reasoner) to calculate the compound sentiment scores for about 14,000 Nigerian Pidgin tweets.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.09439",
    "question": "How do they bootstrap with contextual information?",
    "gold_answer": "",
    "answer_a": "pre-trained word embeddings need to be tuned with local context during our experiments. Explanation: Using different word embedding or end to end models where word representation learned from local context create different results in emotion detection. We noted that pre-trained word embeddings need to be tuned with local context during our experiments or it causes the model to not converge.",
    "answer_b": "What is the multigenre corpus?\n< Explanation: We use the following steps to bootstrap the results: (1) we use the multigenre corpus to train the model with the contextual information, (2) we use the model to predict the emotion of the next utterance, and (3) we use the model to predict the emotion of the next utterance, and so on. We use the multigenre corpus to train the model with the contextual information, which is the most relevant information for the task. We use the model to predict the emotion of the next utterance, and we use the model to predict the emotion",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they bootstrap with contextual information?\n\nGround Truth Answer:\n\n\nAnswer A:\npre-trained word embeddings need to be tuned with local context during our experiments. Explanation: Using different word embedding or end to end models where word representation learned from local context create different results in emotion detection. We noted that pre-trained word embeddings need to be tuned with local context during our experiments or it causes the model to not converge.\n\nAnswer B:\nWhat is the multigenre corpus?\n< Explanation: We use the following steps to bootstrap the results: (1) we use the multigenre corpus to train the model with the contextual information, (2) we use the model to predict the emotion of the next utterance, and (3) we use the model to predict the emotion of the next utterance, and so on. We use the multigenre corpus to train the model with the contextual information, which is the most relevant information for the task. We use the model to predict the emotion of the next utterance, and we use the model to predict the emotion\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.09439",
    "question": "Which word embeddings do they utilize for the EmoContext task?",
    "gold_answer": "",
    "answer_a": "ELMo. Explanation: Using different word embedding or end to end models where word representation learned from local context create different results in emotion detection. We noted that pre-trained word embeddings need to be tuned with local context during our experiments or it causes the model to not converge. We experimented with different word embedding methods such as word2vec, GloVe BIBREF7 , fasttext BIBREF8 , and ELMo. Among these methods fasttext and ELMo create better results.",
    "answer_b": "What are the results?\n< Explanation: We use the word embeddings from the BERT model BIBREF10 . We use the following word embeddings: BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , B",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich word embeddings do they utilize for the EmoContext task?\n\nGround Truth Answer:\n\n\nAnswer A:\nELMo. Explanation: Using different word embedding or end to end models where word representation learned from local context create different results in emotion detection. We noted that pre-trained word embeddings need to be tuned with local context during our experiments or it causes the model to not converge. We experimented with different word embedding methods such as word2vec, GloVe BIBREF7 , fasttext BIBREF8 , and ELMo. Among these methods fasttext and ELMo create better results.\n\nAnswer B:\nWhat are the results?\n< Explanation: We use the word embeddings from the BERT model BIBREF10 . We use the following word embeddings: BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , B\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1704.03279",
    "question": "What were the performance results of their network?",
    "gold_answer": "",
    "answer_a": "For the test set a BLEU score of 25.7 on Ja-En and 20.7 (2014 test set), 23.1 (2015 test set), and 26.1 (2016 test set) on En-De. Explanation: FLOAT SELECTED: Table 5: Our best models on Ja-En.",
    "answer_b": "How did they compare to the single system?\n< Explanation: We compare our system with the single system in terms of BLEU and TER. We also compare our system with the single system in terms of BLEU and TER. We also compare our system with the single system in terms of BLEU and TER. We also compare our system with the single system in terms of BLEU and TER. We also compare our system with the single system in terms of BLEU and TER. We also compare our system with the single system in terms of BLEU and TER. We also compare our system with the single system in terms of BLEU and",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat were the performance results of their network?\n\nGround Truth Answer:\n\n\nAnswer A:\nFor the test set a BLEU score of 25.7 on Ja-En and 20.7 (2014 test set), 23.1 (2015 test set), and 26.1 (2016 test set) on En-De. Explanation: FLOAT SELECTED: Table 5: Our best models on Ja-En.\n\nAnswer B:\nHow did they compare to the single system?\n< Explanation: We compare our system with the single system in terms of BLEU and TER. We also compare our system with the single system in terms of BLEU and TER. We also compare our system with the single system in terms of BLEU and TER. We also compare our system with the single system in terms of BLEU and TER. We also compare our system with the single system in terms of BLEU and TER. We also compare our system with the single system in terms of BLEU and TER. We also compare our system with the single system in terms of BLEU and\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1704.03279",
    "question": "What were the baselines?",
    "gold_answer": "",
    "answer_a": "How were they evaluated?\n< Explanation: We compare our system with the following baselines:\n\n1. Single NMT system: This is the baseline system which is trained with a single neural net. We use the same architecture as the one used in the original NMT paper [1] and train the network with the same parameters. We use the same encoder and decoder architecture as the one used in the original NMT paper. We use the same learning rate schedule and the same number of training epochs. We use the same training data and the same evaluation data. We use the same evaluation metric. We use the same evaluation metric as the one",
    "answer_b": "simple ensembling method (prediction averaging). Explanation: We choose a widely used, simple ensembling method (prediction averaging) as our baseline.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat were the baselines?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow were they evaluated?\n< Explanation: We compare our system with the following baselines:\n\n1. Single NMT system: This is the baseline system which is trained with a single neural net. We use the same architecture as the one used in the original NMT paper [1] and train the network with the same parameters. We use the same encoder and decoder architecture as the one used in the original NMT paper. We use the same learning rate schedule and the same number of training epochs. We use the same training data and the same evaluation data. We use the same evaluation metric. We use the same evaluation metric as the one\n\nAnswer B:\nsimple ensembling method (prediction averaging). Explanation: We choose a widely used, simple ensembling method (prediction averaging) as our baseline.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1704.03279",
    "question": "What dataset is used?",
    "gold_answer": "",
    "answer_a": "Explanation: We use the English-English dataset from the NMT-2017 competition. The dataset consists of 10000 pairs of sentences from English and Japanese. The sentences are randomly shuffled and the length of each sentence is 1000 words. The dataset is divided into training and validation sets. The training set consists of 7000 pairs and the validation set consists of 3000 pairs. The training set is used to train the NMT system and the validation set is used to evaluate the system. The dataset is available at https://github.com/zhengyuanli",
    "answer_b": "Japanese-English (Ja-En) ASPEC data set BIBREF26. Explanation: Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat dataset is used?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We use the English-English dataset from the NMT-2017 competition. The dataset consists of 10000 pairs of sentences from English and Japanese. The sentences are randomly shuffled and the length of each sentence is 1000 words. The dataset is divided into training and validation sets. The training set consists of 7000 pairs and the validation set consists of 3000 pairs. The training set is used to train the NMT system and the validation set is used to evaluate the system. The dataset is available at https://github.com/zhengyuanli\n\nAnswer B:\nJapanese-English (Ja-En) ASPEC data set BIBREF26. Explanation: Our primary corpus is the Japanese-English (Ja-En) ASPEC data set BIBREF26 . We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. sys-neubig-wat15. We report cased BLEU scores calculated with Moses' multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.05389",
    "question": "How do they preprocess Tweets?",
    "gold_answer": "",
    "answer_a": "They removed retweets, URLs, emoticons, mentions of other users, hashtags; lowercased the text and removed the punctuation.. Explanation: As our central question here deals with language semantics of individuals, re-tweets do not bring any additional information to our study, thus we removed them by default. We also removed any expressions considered to be semantically meaningless like URLs, emoticons, mentions of other users (denoted by the @ symbol) and hashtags (denoted by the # symbol) to simplify later post-processing. In addition, as a last step of textual pre-processing, we downcased and stripped the punctuation from the text of every tweet.",
    "answer_b": "What are the features they use?\n< Explanation: We first preprocess the Twitter data by removing non-English characters, converting all words to lowercase, and removing stop words. We then use a bag of words model to extract the semantic features of each tweet. We use the bag of words model to extract the semantic features of each tweet. We then use the bag of words model to extract the semantic features of each tweet. We then use the bag of words model to extract the semantic features of each tweet. We then use the bag of words model to extract the semantic features of each tweet. We then use the bag of words model to extract",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they preprocess Tweets?\n\nGround Truth Answer:\n\n\nAnswer A:\nThey removed retweets, URLs, emoticons, mentions of other users, hashtags; lowercased the text and removed the punctuation.. Explanation: As our central question here deals with language semantics of individuals, re-tweets do not bring any additional information to our study, thus we removed them by default. We also removed any expressions considered to be semantically meaningless like URLs, emoticons, mentions of other users (denoted by the @ symbol) and hashtags (denoted by the # symbol) to simplify later post-processing. In addition, as a last step of textual pre-processing, we downcased and stripped the punctuation from the text of every tweet.\n\nAnswer B:\nWhat are the features they use?\n< Explanation: We first preprocess the Twitter data by removing non-English characters, converting all words to lowercase, and removing stop words. We then use a bag of words model to extract the semantic features of each tweet. We use the bag of words model to extract the semantic features of each tweet. We then use the bag of words model to extract the semantic features of each tweet. We then use the bag of words model to extract the semantic features of each tweet. We then use the bag of words model to extract the semantic features of each tweet. We then use the bag of words model to extract\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.05389",
    "question": "What kind of inference model do they build to estimate socioeconomic status?",
    "gold_answer": "",
    "answer_a": "XGBoost. Explanation: In order to assess the degree to which linguistic features can be used for discriminating users by their socioeconomic class, we trained with these feature sets different learning algorithms. Namely, we used the XGBoost algorithm BIBREF43 , an implementation of the gradient-boosted decision trees for this task.",
    "answer_b": "How do they compare to earlier results?\n<blockquote>\n<font size=\"2\">We propose three different data collection and combination methods to first estimate and, in turn, infer the socioeconomic status of French Twitter users from their online semantics. Our methods are based on open census data, crawled professional profiles, and remotely sensed, expert annotated information on living environment. Our inference models reach similar performance of earlier results with the advantage of relying on broadly available datasets and of providing a generalizable framework to estimate socioeconomic status of large numbers of Twitter users. These results may contribute to the scientific discussion on social stratification and inequalities, and may fuel several applications.</",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat kind of inference model do they build to estimate socioeconomic status?\n\nGround Truth Answer:\n\n\nAnswer A:\nXGBoost. Explanation: In order to assess the degree to which linguistic features can be used for discriminating users by their socioeconomic class, we trained with these feature sets different learning algorithms. Namely, we used the XGBoost algorithm BIBREF43 , an implementation of the gradient-boosted decision trees for this task.\n\nAnswer B:\nHow do they compare to earlier results?\n<blockquote>\n<font size=\"2\">We propose three different data collection and combination methods to first estimate and, in turn, infer the socioeconomic status of French Twitter users from their online semantics. Our methods are based on open census data, crawled professional profiles, and remotely sensed, expert annotated information on living environment. Our inference models reach similar performance of earlier results with the advantage of relying on broadly available datasets and of providing a generalizable framework to estimate socioeconomic status of large numbers of Twitter users. These results may contribute to the scientific discussion on social stratification and inequalities, and may fuel several applications.</\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.05389",
    "question": "How much data do they gather in total?",
    "gold_answer": "",
    "answer_a": "90,369,215 tweets written in French, posted by 1.3 Million users. Explanation: Specifically, we recorded 90,369,215 tweets written in French, posted by 1.3 Million users in the timezones GMT and GMT+1 over one year (between August 2014 to July 2015) BIBREF23 .",
    "answer_b": "What are the sources of data?\n< Explanation: We collected data from 10,000 users of Twitter in France, using the Twitter API. We used the following criteria to select users: (i) users with at least 1000 tweets; (ii) users with at least 1000 tweets in the last 3 months; (iii) users with at least 1000 tweets in the last 6 months; (iv) users with at least 1000 tweets in the last 12 months; (v) users with at least 1000",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much data do they gather in total?\n\nGround Truth Answer:\n\n\nAnswer A:\n90,369,215 tweets written in French, posted by 1.3 Million users. Explanation: Specifically, we recorded 90,369,215 tweets written in French, posted by 1.3 Million users in the timezones GMT and GMT+1 over one year (between August 2014 to July 2015) BIBREF23 .\n\nAnswer B:\nWhat are the sources of data?\n< Explanation: We collected data from 10,000 users of Twitter in France, using the Twitter API. We used the following criteria to select users: (i) users with at least 1000 tweets; (ii) users with at least 1000 tweets in the last 3 months; (iii) users with at least 1000 tweets in the last 6 months; (iv) users with at least 1000 tweets in the last 12 months; (v) users with at least 1000\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.05389",
    "question": "What inference models are used?",
    "gold_answer": "",
    "answer_a": "XGBoost. Explanation: Namely, we used the XGBoost algorithm BIBREF43 , an implementation of the gradient-boosted decision trees for this task.",
    "answer_b": "How are they evaluated?\n< Explanation: We propose three different inference models to infer the socioeconomic status of French Twitter users from their online semantics. The first model is a simple linear regression model, the second one is a logistic regression model, and the third one is a Bayesian model. We compare the performance of these models with the results of earlier studies on the same data. We also compare the performance of the models with the results of a new study on the same data. The models are evaluated using the mean absolute error (MAE) and the mean squared error (MSE). The results show that the Bayesian model outperforms the logistic",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat inference models are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nXGBoost. Explanation: Namely, we used the XGBoost algorithm BIBREF43 , an implementation of the gradient-boosted decision trees for this task.\n\nAnswer B:\nHow are they evaluated?\n< Explanation: We propose three different inference models to infer the socioeconomic status of French Twitter users from their online semantics. The first model is a simple linear regression model, the second one is a logistic regression model, and the third one is a Bayesian model. We compare the performance of these models with the results of earlier studies on the same data. We also compare the performance of the models with the results of a new study on the same data. The models are evaluated using the mean absolute error (MAE) and the mean squared error (MSE). The results show that the Bayesian model outperforms the logistic\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.05389",
    "question": "What baseline model is used?",
    "gold_answer": "",
    "answer_a": "AdaBoost. Explanation: To evaluate the performance of this method we benchmarked it against more standard ensemble learning algorithms such as AdaBoost and Random Forest.",
    "answer_b": "How is it evaluated?\n< Explanation: We use a baseline model based on the following three steps: (i) we use the census data to estimate the socioeconomic status of each user, (ii) we use the census data to estimate the socioeconomic status of each user in the neighborhood, and (iii) we use the census data to estimate the socioeconomic status of each user in the neighborhood, and so on. We use the census data to estimate the socioeconomic status of each user in the neighborhood, and so on. We use the census data to estimate the socioeconomic status of each user in the neighborhood, and so on. We use the census",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat baseline model is used?\n\nGround Truth Answer:\n\n\nAnswer A:\nAdaBoost. Explanation: To evaluate the performance of this method we benchmarked it against more standard ensemble learning algorithms such as AdaBoost and Random Forest.\n\nAnswer B:\nHow is it evaluated?\n< Explanation: We use a baseline model based on the following three steps: (i) we use the census data to estimate the socioeconomic status of each user, (ii) we use the census data to estimate the socioeconomic status of each user in the neighborhood, and (iii) we use the census data to estimate the socioeconomic status of each user in the neighborhood, and so on. We use the census data to estimate the socioeconomic status of each user in the neighborhood, and so on. We use the census data to estimate the socioeconomic status of each user in the neighborhood, and so on. We use the census\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.05389",
    "question": "How is the remotely sensed data annotated?",
    "gold_answer": "",
    "answer_a": "What is the annotation process?\n< Explanation: We use the OpenCensus API to retrieve the census data from the French National Institute for Statistics and Economic Studies (INSEE). We then use the OpenCensus API to retrieve the census data from the French National Institute for Health and Medical Research (INSERM). We use the OpenCensus API to retrieve the census data from the French National Institute for Health and Medical Research (INSERM). We use the OpenCensus API to retrieve the census data from the French National Institute for Health and Medical Research (INSERM). We use the OpenCensus API to retrieve the census data from the",
    "answer_b": "The SES score was assigned by architects based on the satellite and Street View images of users' homes.. Explanation: We randomly selected a sample of INLINEFORM1 locations and involved architects to assign a SES score (from 1 to 9) to a sample set of selected locations based on the satellite and Street View around it (both samples had 333 overlapping locations).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is the remotely sensed data annotated?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the annotation process?\n< Explanation: We use the OpenCensus API to retrieve the census data from the French National Institute for Statistics and Economic Studies (INSEE). We then use the OpenCensus API to retrieve the census data from the French National Institute for Health and Medical Research (INSERM). We use the OpenCensus API to retrieve the census data from the French National Institute for Health and Medical Research (INSERM). We use the OpenCensus API to retrieve the census data from the French National Institute for Health and Medical Research (INSERM). We use the OpenCensus API to retrieve the census data from the\n\nAnswer B:\nThe SES score was assigned by architects based on the satellite and Street View images of users' homes.. Explanation: We randomly selected a sample of INLINEFORM1 locations and involved architects to assign a SES score (from 1 to 9) to a sample set of selected locations based on the satellite and Street View around it (both samples had 333 overlapping locations).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.05389",
    "question": "Where are the professional profiles crawled from?",
    "gold_answer": "",
    "answer_a": "LinkedIn. Explanation: Using these pointers we collected professional profile descriptions from LinkedIn by relying on an automatic crawler mainly used in Search Engine Optimization (SEO) tasks BIBREF30 .",
    "answer_b": "How are they annotated?\n< Explanation: We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhere are the professional profiles crawled from?\n\nGround Truth Answer:\n\n\nAnswer A:\nLinkedIn. Explanation: Using these pointers we collected professional profile descriptions from LinkedIn by relying on an automatic crawler mainly used in Search Engine Optimization (SEO) tasks BIBREF30 .\n\nAnswer B:\nHow are they annotated?\n< Explanation: We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the Twitter API to crawl professional profiles of French users. We use the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.10290",
    "question": "How much additional data do they manage to generate from translations?",
    "gold_answer": "",
    "answer_a": "What is the quality of the additional data?\n<sep><table> <tr> <td> Explanation: We use the following metrics to evaluate the quality of the additional data: (1) the number of correct relations, (2) the number of correct relations in the correct language, and (3) the number of correct relations in the correct language and the number of correct relations in the target language. We also report the average number of correct relations in the target language. The number of correct relations is calculated by comparing the predicted relations with the gold relations. The number of correct relations in the target language is calculated by comparing the predicted",
    "answer_b": "45680. Explanation: FLOAT SELECTED: Figure 1: The pipeline of proposed method. \u201cSMT\u201d and \u201cDRP\u201d denote statistical machine translation and discourse relation parser respectively.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much additional data do they manage to generate from translations?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the quality of the additional data?\n<sep><table> <tr> <td> Explanation: We use the following metrics to evaluate the quality of the additional data: (1) the number of correct relations, (2) the number of correct relations in the correct language, and (3) the number of correct relations in the correct language and the number of correct relations in the target language. We also report the average number of correct relations in the target language. The number of correct relations is calculated by comparing the predicted relations with the gold relations. The number of correct relations in the target language is calculated by comparing the predicted\n\nAnswer B:\n45680. Explanation: FLOAT SELECTED: Figure 1: The pipeline of proposed method. \u201cSMT\u201d and \u201cDRP\u201d denote statistical machine translation and discourse relation parser respectively.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.10290",
    "question": "How many languages do they at most attempt to use to generate discourse relation labelled data?",
    "gold_answer": "",
    "answer_a": "4. Explanation: FLOAT SELECTED: Figure 2: Numbers of implicit discourse relation instances from different agreements of explicit instances in three back-translations. En-Fr denotes instances that are implicit in English but explicit in back-translation of French, same for En-De and En-Cz. The overlap means they share the same relational arguments. The numbers under \u201cTwo-Votes\u201d and \u201cThree-Votes\u201d are the numbers of discourse relation agreement / disagreement between explicits in back-translations of two or three languages.",
    "answer_b": "What is the average number of instances per language?\n<sep>Answer: We use 10 languages: English, French, German, Spanish, Italian, Portuguese, Dutch, Greek, Hebrew, and Arabic. We use 1000 instances per language. We use 1000 instances per language. We use 1000 instances per language. We use 1000 instances per language. We use 1000 instances per language. We use 1000 instances per language. We use 1000 instances per language. We use 1000 instances",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many languages do they at most attempt to use to generate discourse relation labelled data?\n\nGround Truth Answer:\n\n\nAnswer A:\n4. Explanation: FLOAT SELECTED: Figure 2: Numbers of implicit discourse relation instances from different agreements of explicit instances in three back-translations. En-Fr denotes instances that are implicit in English but explicit in back-translation of French, same for En-De and En-Cz. The overlap means they share the same relational arguments. The numbers under \u201cTwo-Votes\u201d and \u201cThree-Votes\u201d are the numbers of discourse relation agreement / disagreement between explicits in back-translations of two or three languages.\n\nAnswer B:\nWhat is the average number of instances per language?\n<sep>Answer: We use 10 languages: English, French, German, Spanish, Italian, Portuguese, Dutch, Greek, Hebrew, and Arabic. We use 1000 instances per language. We use 1000 instances per language. We use 1000 instances per language. We use 1000 instances per language. We use 1000 instances per language. We use 1000 instances per language. We use 1000 instances per language. We use 1000 instances\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1612.04118",
    "question": "by how much did the system improve?",
    "gold_answer": "",
    "answer_a": "What were the benchmarks used?\n< Explanation: We evaluated the performance of the proposed architecture on the BIBREF10 benchmark for information extraction from text. The BIBREF10 benchmark consists of 1000 sentences from the Wikipedia article on the subject of the 2010 US presidential election. The sentences were extracted using a constraint-based system and then fed into a neural network. The neural network was trained using a measure of consistency of extracted data with existing databases as a form of noisy supervision. The neural network was trained using a corpus of 1000 sentences from the Wikipedia article on the subject of the",
    "answer_b": "By more than 90%. Explanation: In a production setting, the neural architecture presented here reduced the number of false positive extractions in financial information extraction application by INLINEFORM0 relative to a mature system developed over the course of several years.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nby how much did the system improve?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat were the benchmarks used?\n< Explanation: We evaluated the performance of the proposed architecture on the BIBREF10 benchmark for information extraction from text. The BIBREF10 benchmark consists of 1000 sentences from the Wikipedia article on the subject of the 2010 US presidential election. The sentences were extracted using a constraint-based system and then fed into a neural network. The neural network was trained using a measure of consistency of extracted data with existing databases as a form of noisy supervision. The neural network was trained using a corpus of 1000 sentences from the Wikipedia article on the subject of the\n\nAnswer B:\nBy more than 90%. Explanation: In a production setting, the neural architecture presented here reduced the number of false positive extractions in financial information extraction application by INLINEFORM0 relative to a mature system developed over the course of several years.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1612.04118",
    "question": "what existing databases were used?",
    "gold_answer": "",
    "answer_a": "Explanation: The system was evaluated on the following databases: (1) the 2009-2010 Bloomberg Financial Language Corpus (BFLC) BIBREF1 , (2) the 2009-2010 Bloomberg Financial Language Corpus with additional data BIBREF2 , (3) the 2009-2010 Bloomberg Financial Language Corpus with additional data BIBREF3 , (4) the 2009-2010 Bloomberg Financial Language Corpus with additional data BIBREF4 , (5) the 2009-2",
    "answer_b": "database containing historical time series data. Explanation: In our application, this reference was a database containing historical time series data, which enabled us to check how well the extracted numerical data fit into time series in the database.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat existing databases were used?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The system was evaluated on the following databases: (1) the 2009-2010 Bloomberg Financial Language Corpus (BFLC) BIBREF1 , (2) the 2009-2010 Bloomberg Financial Language Corpus with additional data BIBREF2 , (3) the 2009-2010 Bloomberg Financial Language Corpus with additional data BIBREF3 , (4) the 2009-2010 Bloomberg Financial Language Corpus with additional data BIBREF4 , (5) the 2009-2\n\nAnswer B:\ndatabase containing historical time series data. Explanation: In our application, this reference was a database containing historical time series data, which enabled us to check how well the extracted numerical data fit into time series in the database.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1612.04118",
    "question": "what existing parser is used?",
    "gold_answer": "",
    "answer_a": "candidate-generating parser. Explanation: We present an information extraction architecture that augments a candidate-generating parser with a deep neural network.",
    "answer_b": "Explanation: The existing parser is a constraint-based parser that uses a set of rules to extract information from text. The parser is trained on a large corpus of text and uses the rules to extract information from the text. The parser is trained using a set of rules that are based on the rules of a language. The rules are based on the grammar of the language and are used to extract information from the text. The parser is trained using a set of rules that are based on the grammar of the language and are used to extract information from the text. The parser is trained using a set of rules that are based on the grammar of the language",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat existing parser is used?\n\nGround Truth Answer:\n\n\nAnswer A:\ncandidate-generating parser. Explanation: We present an information extraction architecture that augments a candidate-generating parser with a deep neural network.\n\nAnswer B:\nExplanation: The existing parser is a constraint-based parser that uses a set of rules to extract information from text. The parser is trained on a large corpus of text and uses the rules to extract information from the text. The parser is trained using a set of rules that are based on the rules of a language. The rules are based on the grammar of the language and are used to extract information from the text. The parser is trained using a set of rules that are based on the grammar of the language and are used to extract information from the text. The parser is trained using a set of rules that are based on the grammar of the language\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.01155",
    "question": "How do they combine the socioeconomic maps with Twitter data? ",
    "gold_answer": "",
    "answer_a": "Match geolocation data for Twitter users with patches from INSEE socioeconomic maps.. Explanation: To overcome this limitation we combined our Twitter data with the socioeconomic maps of INSEE by assigning each geolocated Twitter user to a patch closest to their estimated home location (within 1 km). This way we obtained for all $110,369$ geolocated users their dynamical linguistic data, their egocentric social network as well as a set of SES indicators.",
    "answer_b": "What is the size of the dataset?\nThey combine the largest French Twitter corpus to date with detailed socioeconomic maps obtained from national census in France. The dataset is composed of 1.5 million tweets from 2015 to 2017. The dataset is available at https://github.com/jeanfrancois-roux/2015-2017-twitter-socioeconomic-map. The dataset is divided into two parts: the first part contains 1.5 million tweets from 2015 to 2017, while the second part contains 1.5 million tweets",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they combine the socioeconomic maps with Twitter data? \n\nGround Truth Answer:\n\n\nAnswer A:\nMatch geolocation data for Twitter users with patches from INSEE socioeconomic maps.. Explanation: To overcome this limitation we combined our Twitter data with the socioeconomic maps of INSEE by assigning each geolocated Twitter user to a patch closest to their estimated home location (within 1 km). This way we obtained for all $110,369$ geolocated users their dynamical linguistic data, their egocentric social network as well as a set of SES indicators.\n\nAnswer B:\nWhat is the size of the dataset?\nThey combine the largest French Twitter corpus to date with detailed socioeconomic maps obtained from national census in France. The dataset is composed of 1.5 million tweets from 2015 to 2017. The dataset is available at https://github.com/jeanfrancois-roux/2015-2017-twitter-socioeconomic-map. The dataset is divided into two parts: the first part contains 1.5 million tweets from 2015 to 2017, while the second part contains 1.5 million tweets\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.01155",
    "question": "Does the fact that people are active during the day time define their SEC?",
    "gold_answer": "",
    "answer_a": "No, but the authors identified a correlation.. Explanation: It turned out that people active during the day have higher average income (warmer colors in Fig. 4 ) than people active during the night (colder colors in Fig. 4 ). Thus the variability of standard language patterns was largely explained by the changing overall composition of active Twitter users during different times of day and the positive correlation between socioeconomic status and the usage of higher linguistic standards (that we have seen earlier). This explanation was supported by the high coefficients (summarized in Table 3 ), which were indicative of strong and significant correlations between the temporal variability of average linguistic variables and average income of the active population on Twitter.",
    "answer_b": "If so, how?\n<sep>Yes, people are active during the day time. Explanation: We found that people of higher socioeconomic status are more likely to be active during the day time, while those of lower socioeconomic status are more likely to be active at night. This is consistent with the hypothesis that people of higher socioeconomic status are more likely to be educated and therefore more likely to be active during the day time. We also found that people of higher socioeconomic status are more likely to be active in the morning and evening, while those of lower socioeconomic status are more likely to be active in the afternoon. This is consistent with the hypothesis that",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nDoes the fact that people are active during the day time define their SEC?\n\nGround Truth Answer:\n\n\nAnswer A:\nNo, but the authors identified a correlation.. Explanation: It turned out that people active during the day have higher average income (warmer colors in Fig. 4 ) than people active during the night (colder colors in Fig. 4 ). Thus the variability of standard language patterns was largely explained by the changing overall composition of active Twitter users during different times of day and the positive correlation between socioeconomic status and the usage of higher linguistic standards (that we have seen earlier). This explanation was supported by the high coefficients (summarized in Table 3 ), which were indicative of strong and significant correlations between the temporal variability of average linguistic variables and average income of the active population on Twitter.\n\nAnswer B:\nIf so, how?\n<sep>Yes, people are active during the day time. Explanation: We found that people of higher socioeconomic status are more likely to be active during the day time, while those of lower socioeconomic status are more likely to be active at night. This is consistent with the hypothesis that people of higher socioeconomic status are more likely to be educated and therefore more likely to be active during the day time. We also found that people of higher socioeconomic status are more likely to be active in the morning and evening, while those of lower socioeconomic status are more likely to be active in the afternoon. This is consistent with the hypothesis that\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.01155",
    "question": "How did they define standard language?",
    "gold_answer": "",
    "answer_a": "What were the linguistic variables measured?\n< Explanation: We define standard language as the language that is used by the majority of the population. We use the following definition: a language is considered standard if it is used by more than 50% of the population. We use this definition to define standard language in the following way: we define standard language as the language that is used by the majority of the population. We use this definition to define standard language in the following way: we define standard language as the language that is used by the majority of the population. We use this definition to define standard language in the following way: we define",
    "answer_b": "Use of both French negative particles and spelling out plural ending on adjectives and nouns. Explanation: The basic form of negation in French includes two negative particles: ne (no) before the verb and another particle after the verb that conveys more accurate meaning: pas (not), jamais (never), personne (no one), rien (nothing), etc. Due to this double construction, the first part of the negation (ne) is optional in spoken French, but it is obligatory in standard writing.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow did they define standard language?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat were the linguistic variables measured?\n< Explanation: We define standard language as the language that is used by the majority of the population. We use the following definition: a language is considered standard if it is used by more than 50% of the population. We use this definition to define standard language in the following way: we define standard language as the language that is used by the majority of the population. We use this definition to define standard language in the following way: we define standard language as the language that is used by the majority of the population. We use this definition to define standard language in the following way: we define\n\nAnswer B:\nUse of both French negative particles and spelling out plural ending on adjectives and nouns. Explanation: The basic form of negation in French includes two negative particles: ne (no) before the verb and another particle after the verb that conveys more accurate meaning: pas (not), jamais (never), personne (no one), rien (nothing), etc. Due to this double construction, the first part of the negation (ne) is optional in spoken French, but it is obligatory in standard writing.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.01155",
    "question": "How do they operationalize socioeconomic status from twitter user data?",
    "gold_answer": "",
    "answer_a": "What are the sources of data?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Figure 1: Socioeconomic status (SES) is measured by the number of tweets per day in the last 30 days. The SES is calculated by dividing the number of tweets per day by the number of days in the last 30 days. The SES is then normalized to the range [0, 1] and the number of tweets per day is then normalized to the range [0, 1000]. The SES is then normalized to the range [0, 1",
    "answer_b": "we combined our Twitter data with the socioeconomic maps of INSEE by assigning each geolocated Twitter user to a patch closest to their estimated home location. Explanation: To obtain a unique representative location of each user, we extracted the sequence of all declared locations from their geolocated tweets. Using this set of locations we selected the most frequent to be the representative one, and we took it as a proxy for the user's home location.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they operationalize socioeconomic status from twitter user data?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the sources of data?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Figure 1: Socioeconomic status (SES) is measured by the number of tweets per day in the last 30 days. The SES is calculated by dividing the number of tweets per day by the number of days in the last 30 days. The SES is then normalized to the range [0, 1] and the number of tweets per day is then normalized to the range [0, 1000]. The SES is then normalized to the range [0, 1\n\nAnswer B:\nwe combined our Twitter data with the socioeconomic maps of INSEE by assigning each geolocated Twitter user to a patch closest to their estimated home location. Explanation: To obtain a unique representative location of each user, we extracted the sequence of all declared locations from their geolocated tweets. Using this set of locations we selected the most frequent to be the representative one, and we took it as a proxy for the user's home location.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.06648",
    "question": "What dimensions do the considered embeddings have?",
    "gold_answer": "",
    "answer_a": "Answer with content missing: (Models sections) 100, 200 and 400. Explanation: We hypothesise that breaking this linearity, and allowing a more local fit to the training data will undermine the global structure that the analogy predictions exploit.",
    "answer_b": "How are they computed?\n< Explanation: We consider two embeddings: the word embedding and the word2vec embedding. The word embedding is computed using the word2vec algorithm, which is a fast and simple way to compute word embeddings. The word2vec algorithm is described in detail in Section S1. The word embedding is computed using the word2vec algorithm, which is a fast and simple way to compute word embeddings. The word embedding is computed using the word2vec algorithm, which is a fast and simple way to compute word embeddings. The word embedding is computed using the word2vec algorithm, which is a fast and simple way",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat dimensions do the considered embeddings have?\n\nGround Truth Answer:\n\n\nAnswer A:\nAnswer with content missing: (Models sections) 100, 200 and 400. Explanation: We hypothesise that breaking this linearity, and allowing a more local fit to the training data will undermine the global structure that the analogy predictions exploit.\n\nAnswer B:\nHow are they computed?\n< Explanation: We consider two embeddings: the word embedding and the word2vec embedding. The word embedding is computed using the word2vec algorithm, which is a fast and simple way to compute word embeddings. The word2vec algorithm is described in detail in Section S1. The word embedding is computed using the word2vec algorithm, which is a fast and simple way to compute word embeddings. The word embedding is computed using the word2vec algorithm, which is a fast and simple way to compute word embeddings. The word embedding is computed using the word2vec algorithm, which is a fast and simple way\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.06648",
    "question": "How are global structures considered?",
    "gold_answer": "",
    "answer_a": "global structure in the learned embeddings is related to a linearity in the training objective. Explanation: Here, we consider how this global structure in the learned embeddings is related to a linearity in the training objective.",
    "answer_b": "What are the models that capture them?\n< Explanation: We consider two models: the Decomposable Attention Model (DAM) and word2vec. The DAM is a model that uses a sequence of attention vectors to compute a global representation of a word. The word2vec model is a simple word embedding model that uses a bag of words to represent a word. We use the following notation: $\\mathbf{w}_i$ is the $i$th word in the vocabulary, $\\mathbf{w}_i^T$ is the transpose of $\\mathbf{w}_i$, $\\mathbf{w}_i^",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are global structures considered?\n\nGround Truth Answer:\n\n\nAnswer A:\nglobal structure in the learned embeddings is related to a linearity in the training objective. Explanation: Here, we consider how this global structure in the learned embeddings is related to a linearity in the training objective.\n\nAnswer B:\nWhat are the models that capture them?\n< Explanation: We consider two models: the Decomposable Attention Model (DAM) and word2vec. The DAM is a model that uses a sequence of attention vectors to compute a global representation of a word. The word2vec model is a simple word embedding model that uses a bag of words to represent a word. We use the following notation: $\\mathbf{w}_i$ is the $i$th word in the vocabulary, $\\mathbf{w}_i^T$ is the transpose of $\\mathbf{w}_i$, $\\mathbf{w}_i^\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.06510",
    "question": "Which translation model do they employ?",
    "gold_answer": "",
    "answer_a": "What is the performance of the models?\nYes, we use the following models: (1) a simple BPE model, (2) a simple BPE+orthographic syllable model, (3) a simple BPE+orthographic syllable+word model, (4) a simple BPE+orthographic syllable+word+word model, (5) a simple BPE+orthographic syllable+word+word+word model, (6) a simple BPE+orthographic syllable+word+word+word+word model, (7) a simple BPE+orthographic syllable+",
    "answer_b": "BPE level. Explanation: We train subword level phrase-based SMT models between related languages. Along with BPE level, we also train PBSMT models at morpheme and OS levels for comparison.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich translation model do they employ?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of the models?\nYes, we use the following models: (1) a simple BPE model, (2) a simple BPE+orthographic syllable model, (3) a simple BPE+orthographic syllable+word model, (4) a simple BPE+orthographic syllable+word+word model, (5) a simple BPE+orthographic syllable+word+word+word model, (6) a simple BPE+orthographic syllable+word+word+word+word model, (7) a simple BPE+orthographic syllable+\n\nAnswer B:\nBPE level. Explanation: We train subword level phrase-based SMT models between related languages. Along with BPE level, we also train PBSMT models at morpheme and OS levels for comparison.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.06510",
    "question": "Which datasets do they experiment on?",
    "gold_answer": "",
    "answer_a": "How do they evaluate the performance of their models?\n< Explanation: We use the following datasets: (i) the English to French translation dataset from the Corpus of Contemporary American English (CoCA-2) BIBREF10 , (ii) the English to German translation dataset from the Corpus of Contemporary American English (CoCA-2) BIBREF11 , (iii) the English to Spanish translation dataset from the Corpus of Contemporary American English (CoCA-2) BIBREF12 , (iv) the English to Turkish translation dataset from the Corpus of Contemporary American English (CoCA-2) BIBREF",
    "answer_b": "Indian Language Corpora Initiative (ILCI) corpus. Explanation: The Indo-Aryan and Dravidian language parallel corpora are obtained from the multilingual Indian Language Corpora Initiative (ILCI) corpus BIBREF25 . Parallel corpora for other pairs were obtained from the OpenSubtitles2016 section of the OPUS corpus collection BIBREF26 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich datasets do they experiment on?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate the performance of their models?\n< Explanation: We use the following datasets: (i) the English to French translation dataset from the Corpus of Contemporary American English (CoCA-2) BIBREF10 , (ii) the English to German translation dataset from the Corpus of Contemporary American English (CoCA-2) BIBREF11 , (iii) the English to Spanish translation dataset from the Corpus of Contemporary American English (CoCA-2) BIBREF12 , (iv) the English to Turkish translation dataset from the Corpus of Contemporary American English (CoCA-2) BIBREF\n\nAnswer B:\nIndian Language Corpora Initiative (ILCI) corpus. Explanation: The Indo-Aryan and Dravidian language parallel corpora are obtained from the multilingual Indian Language Corpora Initiative (ILCI) corpus BIBREF25 . Parallel corpora for other pairs were obtained from the OpenSubtitles2016 section of the OPUS corpus collection BIBREF26 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.06510",
    "question": "Which other units of text do they experiment with (apart from BPE and ortographic syllables)?",
    "gold_answer": "",
    "answer_a": "character. Explanation: We trained translation systems over the following basic units: character, morpheme, word, orthographic syllable and BPE unit.",
    "answer_b": "What are their results?\nYes, we also experimented with other units of text. We compared the performance of BPE with the following units: orthographic syllables, orthographic morphemes, orthographic words, orthographic phrases, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich other units of text do they experiment with (apart from BPE and ortographic syllables)?\n\nGround Truth Answer:\n\n\nAnswer A:\ncharacter. Explanation: We trained translation systems over the following basic units: character, morpheme, word, orthographic syllable and BPE unit.\n\nAnswer B:\nWhat are their results?\nYes, we also experimented with other units of text. We compared the performance of BPE with the following units: orthographic syllables, orthographic morphemes, orthographic words, orthographic phrases, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and words, orthographic phrases and\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.06510",
    "question": "How many steps of BPE do they experiment with?",
    "gold_answer": "",
    "answer_a": "from 1000 to 4000. Explanation: We experimented with number of merge operations ranging from 1000 to 4000 and the translation results for these are shown in Table TABREF25 .",
    "answer_b": "What are the languages they compare?\n< Explanation: We compare BPE with orthographic syllables, which are currently the best performing basic units for this translation task. We use the same 1000 word training set and 1000 word test set, and we use the same 1000 word validation set. We use the same 1000 word validation set for both training and testing. We use the same 1000 word test set for both training and testing. We use the same 1000 word validation set for both training and testing. We use the same",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many steps of BPE do they experiment with?\n\nGround Truth Answer:\n\n\nAnswer A:\nfrom 1000 to 4000. Explanation: We experimented with number of merge operations ranging from 1000 to 4000 and the translation results for these are shown in Table TABREF25 .\n\nAnswer B:\nWhat are the languages they compare?\n< Explanation: We compare BPE with orthographic syllables, which are currently the best performing basic units for this translation task. We use the same 1000 word training set and 1000 word test set, and we use the same 1000 word validation set. We use the same 1000 word validation set for both training and testing. We use the same 1000 word test set for both training and testing. We use the same 1000 word validation set for both training and testing. We use the same\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.01160",
    "question": "What nuances between fake news and satire were discovered?",
    "gold_answer": "",
    "answer_a": "What linguistic features were used?\n< Explanation: We first investigate the differences between fake news and satire stories. We find that fake news stories are more likely to contain a political message, and that satire stories are more likely to be humorous. We then investigate the linguistic features that could be used to distinguish between fake news and satire. We find that the semantic representation of the text is a strong predictor of the sentiment of the text. We also find that the presence of a political message is a strong predictor of the sentiment of the text. Finally, we find that the presence of humor is a strong predictor of the sentiment of the text. We then",
    "answer_b": "semantic and linguistic differences between. Explanation: Evaluation of our methods pointed to the existence of semantic and linguistic differences between fake news and satire.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat nuances between fake news and satire were discovered?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat linguistic features were used?\n< Explanation: We first investigate the differences between fake news and satire stories. We find that fake news stories are more likely to contain a political message, and that satire stories are more likely to be humorous. We then investigate the linguistic features that could be used to distinguish between fake news and satire. We find that the semantic representation of the text is a strong predictor of the sentiment of the text. We also find that the presence of a political message is a strong predictor of the sentiment of the text. Finally, we find that the presence of humor is a strong predictor of the sentiment of the text. We then\n\nAnswer B:\nsemantic and linguistic differences between. Explanation: Evaluation of our methods pointed to the existence of semantic and linguistic differences between fake news and satire.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.01160",
    "question": "What empirical evaluation was used?",
    "gold_answer": "",
    "answer_a": "coherence metrics. Explanation: With regard to research question RQ2 on the understanding of semantic and linguistic nuances between fake news and satire - a key advantage of studying the coherence metrics is explainability. While the pre-trained model of BERT gives the best result, it is not easily interpretable. The coherence metrics allow us to study the differences between fake news and satire in a straightforward manner.",
    "answer_b": "What are the results?\n<blockquote>Table 1: Results of the empirical evaluation. The first column shows the number of correct predictions, the second column shows the number of correct predictions for the fake news category, and the third column shows the number of correct predictions for the satire category. The fourth column shows the number of correct predictions for the fake news category and the fifth column shows the number of correct predictions for the satire category. The sixth column shows the number of correct predictions for the fake news category and the seventh column shows the number of correct predictions for the satire category. The eighth column shows the number of correct predictions for the fake",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat empirical evaluation was used?\n\nGround Truth Answer:\n\n\nAnswer A:\ncoherence metrics. Explanation: With regard to research question RQ2 on the understanding of semantic and linguistic nuances between fake news and satire - a key advantage of studying the coherence metrics is explainability. While the pre-trained model of BERT gives the best result, it is not easily interpretable. The coherence metrics allow us to study the differences between fake news and satire in a straightforward manner.\n\nAnswer B:\nWhat are the results?\n<blockquote>Table 1: Results of the empirical evaluation. The first column shows the number of correct predictions, the second column shows the number of correct predictions for the fake news category, and the third column shows the number of correct predictions for the satire category. The fourth column shows the number of correct predictions for the fake news category and the fifth column shows the number of correct predictions for the satire category. The sixth column shows the number of correct predictions for the fake news category and the seventh column shows the number of correct predictions for the satire category. The eighth column shows the number of correct predictions for the fake\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.01160",
    "question": "What is the baseline?",
    "gold_answer": "",
    "answer_a": "Naive Bayes Multinomial algorithm. Explanation: A model using the Naive Bayes Multinomial algorithm is proposed in their paper which serves as the baseline in our experiments.",
    "answer_b": "What is the baseline?\nOutput: The baseline is a simple classifier that uses the following features: (1) the presence of a political or social message, (2) the presence of a political or social message, (3) the presence of a political or social message, (4) the presence of a political or social message, (5) the presence of a political or social message, (6) the presence of a political or social message, (7) the presence of a political or social message, (8) the presence of a political or social message, (9) the presence of a political or social message, (",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the baseline?\n\nGround Truth Answer:\n\n\nAnswer A:\nNaive Bayes Multinomial algorithm. Explanation: A model using the Naive Bayes Multinomial algorithm is proposed in their paper which serves as the baseline in our experiments.\n\nAnswer B:\nWhat is the baseline?\nOutput: The baseline is a simple classifier that uses the following features: (1) the presence of a political or social message, (2) the presence of a political or social message, (3) the presence of a political or social message, (4) the presence of a political or social message, (5) the presence of a political or social message, (6) the presence of a political or social message, (7) the presence of a political or social message, (8) the presence of a political or social message, (9) the presence of a political or social message, (\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.01160",
    "question": "Which linguistic features are used?",
    "gold_answer": "",
    "answer_a": "First person singular pronoun incidence\nSentence length, number of words, \nEstimates of hypernymy for nouns \n...\nAgentless passive voice density,\nAverage word frequency for content words ,\nAdverb incidence\n\n.... Explanation: FLOAT SELECTED: Table 1: Significant components of our logistic regression model using the Coh-Metrix features. Variables are also separated by their association with either satire or fake news. Bold: the remaining features following the step-wise backward elimination. Note: *** p < 0.001, ** p < 0.01, * p < 0.05.",
    "answer_b": "What are the results?\n<sep><br/ >Answer: We consider the following linguistic features: (1) the number of words in the text, (2) the number of sentences, (3) the number of words per sentence, (4) the number of words per sentence in the headline, (5) the number of words per sentence in the body, (6) the number of words per sentence in the headline and body, (7) the number of words per sentence in the headline and body in the headline, (8) the number of words per sentence in the headline and body in the headline and body,",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich linguistic features are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nFirst person singular pronoun incidence\nSentence length, number of words, \nEstimates of hypernymy for nouns \n...\nAgentless passive voice density,\nAverage word frequency for content words ,\nAdverb incidence\n\n.... Explanation: FLOAT SELECTED: Table 1: Significant components of our logistic regression model using the Coh-Metrix features. Variables are also separated by their association with either satire or fake news. Bold: the remaining features following the step-wise backward elimination. Note: *** p < 0.001, ** p < 0.01, * p < 0.05.\n\nAnswer B:\nWhat are the results?\n<sep><br/ >Answer: We consider the following linguistic features: (1) the number of words in the text, (2) the number of sentences, (3) the number of words per sentence, (4) the number of words per sentence in the headline, (5) the number of words per sentence in the body, (6) the number of words per sentence in the headline and body, (7) the number of words per sentence in the headline and body in the headline, (8) the number of words per sentence in the headline and body in the headline and body,\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.01160",
    "question": "What contextual language model is used?",
    "gold_answer": "",
    "answer_a": "BERT. Explanation: To study the semantic nuances between fake news and satire, we use BERT BIBREF8, which stands for Bidirectional Encoder Representations from Transformers, and represents a state-of-the-art contextual language model.",
    "answer_b": "How is it trained?\n< Explanation: We use a contextual language model (CLM) to train our model. The CLM is a deep neural network that is trained on a large corpus of news articles. The CLM is trained on a corpus of news articles, and the model is trained to predict the sentiment of the article. The CLM is trained using a large corpus of news articles, and the model is trained to predict the sentiment of the article. The CLM is trained using a large corpus of news articles, and the model is trained to predict the sentiment of the article. The CLM is trained using a large corpus",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat contextual language model is used?\n\nGround Truth Answer:\n\n\nAnswer A:\nBERT. Explanation: To study the semantic nuances between fake news and satire, we use BERT BIBREF8, which stands for Bidirectional Encoder Representations from Transformers, and represents a state-of-the-art contextual language model.\n\nAnswer B:\nHow is it trained?\n< Explanation: We use a contextual language model (CLM) to train our model. The CLM is a deep neural network that is trained on a large corpus of news articles. The CLM is trained on a corpus of news articles, and the model is trained to predict the sentiment of the article. The CLM is trained using a large corpus of news articles, and the model is trained to predict the sentiment of the article. The CLM is trained using a large corpus of news articles, and the model is trained to predict the sentiment of the article. The CLM is trained using a large corpus\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1602.07776",
    "question": "what state of the art models do they compare to?",
    "gold_answer": "",
    "answer_a": "Vinyals et al (2015) for English parsing, Wang et al (2015) for Chinese parsing, and LSTM LM for Language modeling both in English and Chinese. Explanation: FLOAT SELECTED: Table 2: Parsing results on PTB \u00a723 (D=discriminative, G=generative, S=semisupervised). ? indicates the (Vinyals et al., 2015) result with trained only on the WSJ corpus without ensembling.",
    "answer_b": "Explanation: We compare our models to the following models:\n\n* BIBREF10: a state-of-the-art RNN-based model for English, which is the baseline for our experiments. It is trained on the Penn Treebank corpus and uses a simple RNN with a single hidden layer. It is trained using the same training data as our models.\n\n* BIBREF11: a state-of-the-art RNN-based model for Chinese, which is the baseline for our experiments. It is trained on the Hanzi Corpus and uses a simple RNN with a single hidden layer. It is trained using the same training data as our",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat state of the art models do they compare to?\n\nGround Truth Answer:\n\n\nAnswer A:\nVinyals et al (2015) for English parsing, Wang et al (2015) for Chinese parsing, and LSTM LM for Language modeling both in English and Chinese. Explanation: FLOAT SELECTED: Table 2: Parsing results on PTB \u00a723 (D=discriminative, G=generative, S=semisupervised). ? indicates the (Vinyals et al., 2015) result with trained only on the WSJ corpus without ensembling.\n\nAnswer B:\nExplanation: We compare our models to the following models:\n\n* BIBREF10: a state-of-the-art RNN-based model for English, which is the baseline for our experiments. It is trained on the Penn Treebank corpus and uses a simple RNN with a single hidden layer. It is trained using the same training data as our models.\n\n* BIBREF11: a state-of-the-art RNN-based model for Chinese, which is the baseline for our experiments. It is trained on the Hanzi Corpus and uses a simple RNN with a single hidden layer. It is trained using the same training data as our\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.05908",
    "question": "What is the weak supervision signal used in Baidu Baike corpus?",
    "gold_answer": "",
    "answer_a": "How is it used?\n< Explanation: We use the Baidu Baike corpus to train our model. The corpus contains 10,000,000 words, and 100,000,000 words are used for training. The training set is 100,000,000 words, and the test set is 10,000,000 words. The corpus is divided into 100,000,000 words for training and 10,000,000 words for testing.",
    "answer_b": "consider the title of each sample as a pseudo label and conduct NER pre-training. Explanation: These samples are auto-crawled so there is no actual entity label. We consider the title of each sample as a pseudo label and conduct NER pre-training using these data.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the weak supervision signal used in Baidu Baike corpus?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it used?\n< Explanation: We use the Baidu Baike corpus to train our model. The corpus contains 10,000,000 words, and 100,000,000 words are used for training. The training set is 100,000,000 words, and the test set is 10,000,000 words. The corpus is divided into 100,000,000 words for training and 10,000,000 words for testing.\n\nAnswer B:\nconsider the title of each sample as a pseudo label and conduct NER pre-training. Explanation: These samples are auto-crawled so there is no actual entity label. We consider the title of each sample as a pseudo label and conduct NER pre-training using these data.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.05908",
    "question": "How is BERT optimized for this task?",
    "gold_answer": "",
    "answer_a": "We also optimize the pre-training process of BERT by introducing a semantic-enhanced task.. Explanation: We also optimize the pre-training process of BERT by introducing a semantic-enhanced task.",
    "answer_b": "What is the semantic-enhanced task?\n< Explanation: We first introduce a semantic-enhanced task to optimize BERT. In this task, we add a new task to BERT pre-training, which is to predict the semantic meaning of the words in the sentence. We use the BERT model to predict the semantic meaning of the words in the sentence. The semantic meaning of the words is used to enhance the BERT model. The semantic-enhanced task is to predict the semantic meaning of the words in the sentence. The semantic-enhanced task is to predict the semantic meaning of the words in the sentence. The semantic-enhanced task is",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is BERT optimized for this task?\n\nGround Truth Answer:\n\n\nAnswer A:\nWe also optimize the pre-training process of BERT by introducing a semantic-enhanced task.. Explanation: We also optimize the pre-training process of BERT by introducing a semantic-enhanced task.\n\nAnswer B:\nWhat is the semantic-enhanced task?\n< Explanation: We first introduce a semantic-enhanced task to optimize BERT. In this task, we add a new task to BERT pre-training, which is to predict the semantic meaning of the words in the sentence. We use the BERT model to predict the semantic meaning of the words in the sentence. The semantic meaning of the words is used to enhance the BERT model. The semantic-enhanced task is to predict the semantic meaning of the words in the sentence. The semantic-enhanced task is to predict the semantic meaning of the words in the sentence. The semantic-enhanced task is\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.05908",
    "question": "What is a soft label?",
    "gold_answer": "",
    "answer_a": "To solve the problem that one entity belongs to multiple triplets, a multi-sigmoid layer is applied. Explanation: To effectively transmit information between entity recognition and relation extraction, soft label embedding is built on the top of CRF logits. To solve the problem that one entity belongs to multiple triplets, a multi-sigmoid layer is applied. We find that adding an auxiliary global relation prediction task also improve the performance.",
    "answer_b": "How is it used?\n< Explanation: Soft label is a label that is not a hard label. It is used to indicate the presence of a soft label. For example, in the BERT model, the soft label is the word embedding of the entity. The soft label is used to indicate the presence of the entity in the relation. The soft label is used to indicate the presence of the entity in the relation. The soft label is used to indicate the presence of the entity in the relation. The soft label is used to indicate the presence of the entity in the relation. The soft label is used to indicate the presence of the entity",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is a soft label?\n\nGround Truth Answer:\n\n\nAnswer A:\nTo solve the problem that one entity belongs to multiple triplets, a multi-sigmoid layer is applied. Explanation: To effectively transmit information between entity recognition and relation extraction, soft label embedding is built on the top of CRF logits. To solve the problem that one entity belongs to multiple triplets, a multi-sigmoid layer is applied. We find that adding an auxiliary global relation prediction task also improve the performance.\n\nAnswer B:\nHow is it used?\n< Explanation: Soft label is a label that is not a hard label. It is used to indicate the presence of a soft label. For example, in the BERT model, the soft label is the word embedding of the entity. The soft label is used to indicate the presence of the entity in the relation. The soft label is used to indicate the presence of the entity in the relation. The soft label is used to indicate the presence of the entity in the relation. The soft label is used to indicate the presence of the entity in the relation. The soft label is used to indicate the presence of the entity\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.02208",
    "question": "Which of the 12 languages showed the strongest tendency towards male defaults?",
    "gold_answer": "",
    "answer_a": "Malay. Explanation: FLOAT SELECTED: Table 11: Percentage of female, male and neutral gender pronouns obtained for each language, averaged over all occupations detailed in Table",
    "answer_b": "What is the gender distribution of the translated sentences?\n< Explanation: We show that the gender distribution of the translated sentences is significantly different from the expected distribution of female workers in the BLS data. In particular, we find that the proportion of female workers in the translated sentences is significantly higher than the proportion of female workers in the BLS data. We also show that the proportion of female workers in the translated sentences is significantly higher than the proportion of female workers in the BLS data. We show that the proportion of female workers in the translated sentences is significantly higher than the proportion of female workers in the BLS data. We show that the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich of the 12 languages showed the strongest tendency towards male defaults?\n\nGround Truth Answer:\n\n\nAnswer A:\nMalay. Explanation: FLOAT SELECTED: Table 11: Percentage of female, male and neutral gender pronouns obtained for each language, averaged over all occupations detailed in Table\n\nAnswer B:\nWhat is the gender distribution of the translated sentences?\n< Explanation: We show that the gender distribution of the translated sentences is significantly different from the expected distribution of female workers in the BLS data. In particular, we find that the proportion of female workers in the translated sentences is significantly higher than the proportion of female workers in the BLS data. We also show that the proportion of female workers in the translated sentences is significantly higher than the proportion of female workers in the BLS data. We show that the proportion of female workers in the translated sentences is significantly higher than the proportion of female workers in the BLS data. We show that the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.02208",
    "question": "How many different sentence constructions are translated in gender neutral languages?",
    "gold_answer": "",
    "answer_a": "What is the frequency of female, male and gender-neutral pronouns in the translated output?\nFor each gender neutral language, we collected 12 sentences from the BLS job positions, and translated them into English. We counted the frequency of female, male and gender-neutral pronouns in the translated output. We also counted the frequency of female, male and gender-neutral pronouns in the original sentences. The results are shown in Table 1. We found that the frequency of female, male and gender-neutral pronouns in the translated output is significantly lower than in the original sentences. For example, in the sentence \"He/She is an",
    "answer_b": "17. Explanation: FLOAT SELECTED: Table 3: Templates used to infer gender biases in the translation of job occupations and adjectives to the English language.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many different sentence constructions are translated in gender neutral languages?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the frequency of female, male and gender-neutral pronouns in the translated output?\nFor each gender neutral language, we collected 12 sentences from the BLS job positions, and translated them into English. We counted the frequency of female, male and gender-neutral pronouns in the translated output. We also counted the frequency of female, male and gender-neutral pronouns in the original sentences. The results are shown in Table 1. We found that the frequency of female, male and gender-neutral pronouns in the translated output is significantly lower than in the original sentences. For example, in the sentence \"He/She is an\n\nAnswer B:\n17. Explanation: FLOAT SELECTED: Table 3: Templates used to infer gender biases in the translation of job occupations and adjectives to the English language.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.09283",
    "question": "What are the evaluation metrics used?",
    "gold_answer": "",
    "answer_a": "How are they evaluated?\n< Explanation: We use the following metrics to evaluate the performance of the proposed method: Recall, Precision, F1-score, and ROC-AUC. Recall is the proportion of correct answers to the total number of questions. Precision is the proportion of correct answers to the total number of questions that are actually correct. F1-score is the harmonic mean of precision and recall. ROC-AUC is the area under the receiver operating characteristic curve. The higher the better. The evaluation results are shown in Table 1. The results show that the proposed method can significantly improve the performance of the SLA modeling task in low-resource",
    "answer_b": "ROC curve (AUC) BIBREF32. Explanation: SLA modeling is actually the word level classification task, so we use area under the ROC curve (AUC) BIBREF32 and $F_{1}$ score BIBREF33 as evaluation metric.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the evaluation metrics used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they evaluated?\n< Explanation: We use the following metrics to evaluate the performance of the proposed method: Recall, Precision, F1-score, and ROC-AUC. Recall is the proportion of correct answers to the total number of questions. Precision is the proportion of correct answers to the total number of questions that are actually correct. F1-score is the harmonic mean of precision and recall. ROC-AUC is the area under the receiver operating characteristic curve. The higher the better. The evaluation results are shown in Table 1. The results show that the proposed method can significantly improve the performance of the SLA modeling task in low-resource\n\nAnswer B:\nROC curve (AUC) BIBREF32. Explanation: SLA modeling is actually the word level classification task, so we use area under the ROC curve (AUC) BIBREF32 and $F_{1}$ score BIBREF33 as evaluation metric.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.09283",
    "question": "What are the baselines?",
    "gold_answer": "",
    "answer_a": "What are the results?\n<blockquote>Baseline 1: The baseline model is the same as the baseline model in the previous section. The only difference is that the model is trained on the training data of the second language. The model is trained on the training data of the second language and the test data of the second language. The model is trained on the training data of the second language and the test data of the second language. The model is trained on the training data of the second language and the test data of the second language. The model is trained on the training data of the second language and the test data of the second language",
    "answer_b": "LR Here. Explanation: We compare our method with the following state-of-the-art baselines:\n\nLR Here, we use the official baseline provided by Duolingo BIBREF29. It is a simple logistic regression using all the meta information and context information provided by datasets.\n\nGBDT Here, we use NYU's method BIBREF7, which is the best method among all tree ensemble methods. It uses an ensemble of GBDTs with existing features of dataset and manually constructed features based on psychological theories.\n\nRNN Here, we use singsound's method BIBREF30, which is the best method among all sequence modeling methods. It uses an RNN architecture which has four types of encoders, representing different types of features: token context, linguistic information, user data, and exercise format.\n\nours-MTL It is our encoder-decoder model without multi-task learning. Thus, we will separately train a model for each language-learning dataset.\n\nIn the experiments, the embedding size is set to 150 and the hidden size is also set to 150. Dropout BIBREF31 regularization is applied, where the dropout rate is set to 0.5. We use the Adam optimization algorithm with a learning rate of 0.001.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the baselines?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n<blockquote>Baseline 1: The baseline model is the same as the baseline model in the previous section. The only difference is that the model is trained on the training data of the second language. The model is trained on the training data of the second language and the test data of the second language. The model is trained on the training data of the second language and the test data of the second language. The model is trained on the training data of the second language and the test data of the second language. The model is trained on the training data of the second language and the test data of the second language\n\nAnswer B:\nLR Here. Explanation: We compare our method with the following state-of-the-art baselines:\n\nLR Here, we use the official baseline provided by Duolingo BIBREF29. It is a simple logistic regression using all the meta information and context information provided by datasets.\n\nGBDT Here, we use NYU's method BIBREF7, which is the best method among all tree ensemble methods. It uses an ensemble of GBDTs with existing features of dataset and manually constructed features based on psychological theories.\n\nRNN Here, we use singsound's method BIBREF30, which is the best method among all sequence modeling methods. It uses an RNN architecture which has four types of encoders, representing different types of features: token context, linguistic information, user data, and exercise format.\n\nours-MTL It is our encoder-decoder model without multi-task learning. Thus, we will separately train a model for each language-learning dataset.\n\nIn the experiments, the embedding size is set to 150 and the hidden size is also set to 150. Dropout BIBREF31 regularization is applied, where the dropout rate is set to 0.5. We use the Adam optimization algorithm with a learning rate of 0.001.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.09283",
    "question": "Which language learning datasets are used?",
    "gold_answer": "",
    "answer_a": "Duolingo SLA modeling shared datasets. Explanation: We conduct experiments on Duolingo SLA modeling shared datasets, which have three datasets and are collected from English students who can speak Spanish (en_es), Spanish students who can speak English (es_en), and French students who can speak English (fr_en) BIBREF29.",
    "answer_b": "What are the latent common patterns?\n< Explanation: We use the following three datasets to train our model: (1) the English to Japanese dataset (ENJ), (2) the English to Chinese dataset (ENC), and (3) the Chinese to English dataset (CEN). The ENJ dataset is a corpus of 10000 sentences, the ENC dataset is a corpus of 10000 sentences, and the CEN dataset is a corpus of 10000 sentences. The ENJ dataset is a corpus of 10000 sentences, the ENC dataset is",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich language learning datasets are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nDuolingo SLA modeling shared datasets. Explanation: We conduct experiments on Duolingo SLA modeling shared datasets, which have three datasets and are collected from English students who can speak Spanish (en_es), Spanish students who can speak English (es_en), and French students who can speak English (fr_en) BIBREF29.\n\nAnswer B:\nWhat are the latent common patterns?\n< Explanation: We use the following three datasets to train our model: (1) the English to Japanese dataset (ENJ), (2) the English to Chinese dataset (ENC), and (3) the Chinese to English dataset (CEN). The ENJ dataset is a corpus of 10000 sentences, the ENC dataset is a corpus of 10000 sentences, and the CEN dataset is a corpus of 10000 sentences. The ENJ dataset is a corpus of 10000 sentences, the ENC dataset is\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.03052",
    "question": "What does it mean for sentences to be \"lexically overlapping\"?",
    "gold_answer": "",
    "answer_a": "What is the difference between \"lexically overlapping\" and \"semantically overlapping\"?\nYou are an AI assistant that helps people find information. Please use the context of the sentences to fill the missing portions of the first table. Each table has three rows and three columns. The first row is the question, the second row is the answer, and the third row is the explanation. The first column is the sentence, the second column is the sentence type, and the third column is the sentence number. The first sentence is the explanation sentence, the second sentence is the explanation sentence type, and the third sentence is the explanation sentence number. The",
    "answer_b": "They share words.. Explanation: To increase their utility for knowledge and inference analyses, we require that each sentence in an explanation be explicitly lexically connected (i.e. share words) with either the question, answer, or other sentences in the explanation.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat does it mean for sentences to be \"lexically overlapping\"?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between \"lexically overlapping\" and \"semantically overlapping\"?\nYou are an AI assistant that helps people find information. Please use the context of the sentences to fill the missing portions of the first table. Each table has three rows and three columns. The first row is the question, the second row is the answer, and the third row is the explanation. The first column is the sentence, the second column is the sentence type, and the third column is the sentence number. The first sentence is the explanation sentence, the second sentence is the explanation sentence type, and the third sentence is the explanation sentence number. The\n\nAnswer B:\nThey share words.. Explanation: To increase their utility for knowledge and inference analyses, we require that each sentence in an explanation be explicitly lexically connected (i.e. share words) with either the question, answer, or other sentences in the explanation.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.03052",
    "question": "How many tables are in the tablestore?",
    "gold_answer": "",
    "answer_a": "What is the size of the tablestore?\n<sep>Answer: 1,680. Explanation: The tablestore contains 1,680 tables, each of which is a set of 100 sentences. The sentences are ordered by the question, and the sentences in each table are ordered by the sentence number. The sentences in each table are ordered by the sentence number, and the sentences in each table are ordered by the question. The sentences in each table are ordered by the question, and the sentences in each table are ordered by the sentence number. The sentences in each table are ordered by the sentence number",
    "answer_b": "62. Explanation: Our tablestore includes 62 such tables, each centered around a particular relation such as taxonomy, meronymy, causality, changes, actions, requirements, or affordances, and a number of tables specified around specific properties, such as average lifespans of living things, the magnetic properties of materials, or the nominal durations of certain processes (like the Earth orbiting the Sun).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many tables are in the tablestore?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the tablestore?\n<sep>Answer: 1,680. Explanation: The tablestore contains 1,680 tables, each of which is a set of 100 sentences. The sentences are ordered by the question, and the sentences in each table are ordered by the sentence number. The sentences in each table are ordered by the sentence number, and the sentences in each table are ordered by the question. The sentences in each table are ordered by the question, and the sentences in each table are ordered by the sentence number. The sentences in each table are ordered by the sentence number\n\nAnswer B:\n62. Explanation: Our tablestore includes 62 such tables, each centered around a particular relation such as taxonomy, meronymy, causality, changes, actions, requirements, or affordances, and a number of tables specified around specific properties, such as average lifespans of living things, the magnetic properties of materials, or the nominal durations of certain processes (like the Earth orbiting the Sun).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.08899",
    "question": "what dataset is used?",
    "gold_answer": "",
    "answer_a": "Essays collected from students from American Institutes for Research tests, Synthetic responses from Reddit and Teen Line. Explanation: The American Institutes for Research tests up to 1.8 million students a day during peak testing periods.",
    "answer_b": "Explanation: We use the responses from the 2017-2018 school year of the National Assessment of Educational Progress (NAEP) to train our neural network models. The dataset contains responses from 10,000 students in 10,000 schools. The responses are from the 2017-2018 school year, which is the last year for which the dataset was collected. The dataset is available at https://www.census.gov/data/developers/NAEP/NAEP2017-2018/NAEP2017-",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat dataset is used?\n\nGround Truth Answer:\n\n\nAnswer A:\nEssays collected from students from American Institutes for Research tests, Synthetic responses from Reddit and Teen Line. Explanation: The American Institutes for Research tests up to 1.8 million students a day during peak testing periods.\n\nAnswer B:\nExplanation: We use the responses from the 2017-2018 school year of the National Assessment of Educational Progress (NAEP) to train our neural network models. The dataset contains responses from 10,000 students in 10,000 schools. The responses are from the 2017-2018 school year, which is the last year for which the dataset was collected. The dataset is available at https://www.census.gov/data/developers/NAEP/NAEP2017-2018/NAEP2017-\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.08899",
    "question": "what neural network models are used?",
    "gold_answer": "",
    "answer_a": "GRU and LSTM models with a combination of the following characteristics: bidirectional vs normal, attention vs no attention, stacked vs flat.. Explanation: FLOAT SELECTED: Table 5. The effect of each of the attributes we endowed our networks.",
    "answer_b": "Explanation: We use a combination of a recurrent neural network (RNN) and a convolutional neural network (CNN) to build our model. The CNN is used to extract features from the text, and the RNN is used to generate a classification score for each feature. The CNN is trained to learn features that are useful for classification, and the RNN is trained to generate a classification score for each feature. The CNN is trained on a large corpus of typical responses, and the RNN is trained on a small corpus of anomalous responses. The CNN is trained to learn features that are useful for classification, and the RNN is",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat neural network models are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nGRU and LSTM models with a combination of the following characteristics: bidirectional vs normal, attention vs no attention, stacked vs flat.. Explanation: FLOAT SELECTED: Table 5. The effect of each of the attributes we endowed our networks.\n\nAnswer B:\nExplanation: We use a combination of a recurrent neural network (RNN) and a convolutional neural network (CNN) to build our model. The CNN is used to extract features from the text, and the RNN is used to generate a classification score for each feature. The CNN is trained to learn features that are useful for classification, and the RNN is trained to generate a classification score for each feature. The CNN is trained on a large corpus of typical responses, and the RNN is trained on a small corpus of anomalous responses. The CNN is trained to learn features that are useful for classification, and the RNN is\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.08899",
    "question": "What baseline model is used?",
    "gold_answer": "",
    "answer_a": "What is the baseline model?\n< Explanation: We use a baseline model that is trained on a large corpus of typical responses to the same questions. The baseline model is trained on a corpus of typical responses to the same questions. The baseline model is trained on a corpus of typical responses to the same questions. The baseline model is trained on a corpus of typical responses to the same questions. The baseline model is trained on a corpus of typical responses to the same questions. The baseline model is trained on a corpus of typical responses to the same questions. The baseline model is trained on a corpus of typical responses to the same questions. The baseline model",
    "answer_b": "Logistic regression with TF-IDF with latent semantic analysis representations. Explanation: Our baseline model is logistic regression applied to a TF-IDF model with latent semantic analysis used to reduce the representations of words to three hundred dimensions.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat baseline model is used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the baseline model?\n< Explanation: We use a baseline model that is trained on a large corpus of typical responses to the same questions. The baseline model is trained on a corpus of typical responses to the same questions. The baseline model is trained on a corpus of typical responses to the same questions. The baseline model is trained on a corpus of typical responses to the same questions. The baseline model is trained on a corpus of typical responses to the same questions. The baseline model is trained on a corpus of typical responses to the same questions. The baseline model is trained on a corpus of typical responses to the same questions. The baseline model\n\nAnswer B:\nLogistic regression with TF-IDF with latent semantic analysis representations. Explanation: Our baseline model is logistic regression applied to a TF-IDF model with latent semantic analysis used to reduce the representations of words to three hundred dimensions.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.08899",
    "question": "What type of neural network models are used?",
    "gold_answer": "",
    "answer_a": "Recurrent neural network. Explanation: The second type of recurrent unit we consider is the LSTM, which appeared in the literature before the GRU and contains more parameters BIBREF18 .",
    "answer_b": "What are their strengths and weaknesses?\n<sep><font color=\"#1da9ef\">Answer: We use a deep convolutional neural network (CNN) with a fully connected layer to process the text. The CNN is trained on a large corpus of typical responses to the questions. The CNN is then used to process the text of the responses that are flagged for review. The CNN is trained to identify the most salient features of the text and to flag responses that are likely to be anomalous. The CNN is trained to process the text of the responses that are flagged for review. The CNN is trained to flag responses that are likely to",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of neural network models are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nRecurrent neural network. Explanation: The second type of recurrent unit we consider is the LSTM, which appeared in the literature before the GRU and contains more parameters BIBREF18 .\n\nAnswer B:\nWhat are their strengths and weaknesses?\n<sep><font color=\"#1da9ef\">Answer: We use a deep convolutional neural network (CNN) with a fully connected layer to process the text. The CNN is trained on a large corpus of typical responses to the questions. The CNN is then used to process the text of the responses that are flagged for review. The CNN is trained to identify the most salient features of the text and to flag responses that are likely to be anomalous. The CNN is trained to process the text of the responses that are flagged for review. The CNN is trained to flag responses that are likely to\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.08899",
    "question": "How is validity identified and what metric is used to quantify it?",
    "gold_answer": "",
    "answer_a": "approximation of the probability that a held-out alert is flagged if a fixed percentage of a typical population were to be flagged as potential alerts. Explanation: The metric we use to evaluate the efficacy of our model is an approximation of the probability that a held-out alert is flagged if a fixed percentage of a typical population were to be flagged as potential alerts.",
    "answer_b": "What is the threshold for validity?\n<sep><br></br>We use the following metrics to quantify the validity of the responses: (1) the number of times the response is flagged by a human reader, (2) the number of times the response is flagged by a human reader and is not in fact an anomaly, (3) the number of times the response is flagged by a human reader and is in fact an anomaly, and (4) the number of times the response is flagged by a human reader and is not in fact an anomaly, and (5) the number of times the response is flagged by a human reader",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is validity identified and what metric is used to quantify it?\n\nGround Truth Answer:\n\n\nAnswer A:\napproximation of the probability that a held-out alert is flagged if a fixed percentage of a typical population were to be flagged as potential alerts. Explanation: The metric we use to evaluate the efficacy of our model is an approximation of the probability that a held-out alert is flagged if a fixed percentage of a typical population were to be flagged as potential alerts.\n\nAnswer B:\nWhat is the threshold for validity?\n<sep><br></br>We use the following metrics to quantify the validity of the responses: (1) the number of times the response is flagged by a human reader, (2) the number of times the response is flagged by a human reader and is not in fact an anomaly, (3) the number of times the response is flagged by a human reader and is in fact an anomaly, and (4) the number of times the response is flagged by a human reader and is not in fact an anomaly, and (5) the number of times the response is flagged by a human reader\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.08899",
    "question": "How is severity identified and what metric is used to quantify it?",
    "gold_answer": "",
    "answer_a": "What is the threshold for severity?\n<sep><br></br>Severity is identified by the neural network model as a probability of the response being a self-harm response. The threshold for severity is set to 0.5. The neural network model is trained to predict the probability of a response being a self-harm response. The probability is then compared to the threshold to determine if the response is a self-harm response. The neural network model is trained on a large dataset of typical responses and a small dataset of self-harm responses. The neural network model is trained to predict the probability of a response being a self-harm",
    "answer_b": "Severity is manually identified by a team of reviewers.. Explanation: In our classification of alerts, with respect to how they are identified by the team of reviewers, we have two tiers of alerts, Tier A and Tier B. Tier A consists of true responses that are alarming and require urgent attention while Tier B consists of responses that are concerning in nature but require further review.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is severity identified and what metric is used to quantify it?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the threshold for severity?\n<sep><br></br>Severity is identified by the neural network model as a probability of the response being a self-harm response. The threshold for severity is set to 0.5. The neural network model is trained to predict the probability of a response being a self-harm response. The probability is then compared to the threshold to determine if the response is a self-harm response. The neural network model is trained on a large dataset of typical responses and a small dataset of self-harm responses. The neural network model is trained to predict the probability of a response being a self-harm\n\nAnswer B:\nSeverity is manually identified by a team of reviewers.. Explanation: In our classification of alerts, with respect to how they are identified by the team of reviewers, we have two tiers of alerts, Tier A and Tier B. Tier A consists of true responses that are alarming and require urgent attention while Tier B consists of responses that are concerning in nature but require further review.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.08899",
    "question": "How is urgency identified and what metric is used to quantify it?",
    "gold_answer": "",
    "answer_a": "What is the severity of the responses?\n<sep><br></br>Answer: The neural network models are trained to identify the urgency of the responses. The neural network models are trained to identify the urgency of the responses. The neural network models are trained to identify the urgency of the responses. The neural network models are trained to identify the urgency of the responses. The neural network models are trained to identify the urgency of the responses. The neural network models are trained to identify the urgency of the responses. The neural network models are trained to identify the urgency of the responses. The neural network models are trained to identify the urgency of the responses",
    "answer_b": "Urgency is manually identified by a team of reviewers.. Explanation: In our classification of alerts, with respect to how they are identified by the team of reviewers, we have two tiers of alerts, Tier A and Tier B. Tier A consists of true responses that are alarming and require urgent attention while Tier B consists of responses that are concerning in nature but require further review.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is urgency identified and what metric is used to quantify it?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the severity of the responses?\n<sep><br></br>Answer: The neural network models are trained to identify the urgency of the responses. The neural network models are trained to identify the urgency of the responses. The neural network models are trained to identify the urgency of the responses. The neural network models are trained to identify the urgency of the responses. The neural network models are trained to identify the urgency of the responses. The neural network models are trained to identify the urgency of the responses. The neural network models are trained to identify the urgency of the responses. The neural network models are trained to identify the urgency of the responses\n\nAnswer B:\nUrgency is manually identified by a team of reviewers.. Explanation: In our classification of alerts, with respect to how they are identified by the team of reviewers, we have two tiers of alerts, Tier A and Tier B. Tier A consists of true responses that are alarming and require urgent attention while Tier B consists of responses that are concerning in nature but require further review.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1711.11118",
    "question": "How many different semi-structured templates are represented in the data?",
    "gold_answer": "",
    "answer_a": "7.6 million. Explanation: FLOAT SELECTED: Table 1: MAE dataset statistics.",
    "answer_b": "What are the types of attributes represented?\n< Explanation: We provide a dataset containing mixed-media data for over 2 million product items along with 7 million attribute-value pairs describing the items which can be used to train attribute extractors in a weakly supervised manner. We provide a variety of baselines which demonstrate the relative effectiveness of the individual modes of information towards solving the task, as well as study human performance. The dataset is available at https://github.com/abhishek1010/attribute-extraction. The dataset contains 100000 product items, each with 100000 attributes",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many different semi-structured templates are represented in the data?\n\nGround Truth Answer:\n\n\nAnswer A:\n7.6 million. Explanation: FLOAT SELECTED: Table 1: MAE dataset statistics.\n\nAnswer B:\nWhat are the types of attributes represented?\n< Explanation: We provide a dataset containing mixed-media data for over 2 million product items along with 7 million attribute-value pairs describing the items which can be used to train attribute extractors in a weakly supervised manner. We provide a variety of baselines which demonstrate the relative effectiveness of the individual modes of information towards solving the task, as well as study human performance. The dataset is available at https://github.com/abhishek1010/attribute-extraction. The dataset contains 100000 product items, each with 100000 attributes\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.09753",
    "question": "What are the baseline models?",
    "gold_answer": "",
    "answer_a": "What are the baselines?\n< Explanation: We compare our method to the following baselines: (1) a baseline that uses the human ratings as the only reward, (2) a baseline that uses the human ratings as the only reward, (3) a baseline that uses the human ratings as the only reward, (4) a baseline that uses the human ratings as the only reward, (5) a baseline that uses the human ratings as the only reward, (6) a baseline that uses the human ratings as the only reward, (7) a baseline that uses the human ratings as the only reward, (8) a",
    "answer_b": "MLE model. Explanation: We first train an MLE model as our baseline, trained on the Conceptual Captions training split alone. We referred to this model as Baseline. For a baseline approach that utilizes (some of) the Caption-Quality data, we merge positively-rated captions from the Caption-Quality training split with the Conceptual Captions examples and finetune the baseline model. We call this model Baseline$+(t)$, where $t \\in [0,1]$ is the rating threshold for the included positive captions. We train models for two variants, $t\\in \\lbrace 0.5, 0.7\\rbrace $, which results in $\\sim $72K and $\\sim $51K additional (pseudo-)ground-truth captions, respectively. Note that the Baseline$+(t)$ approaches attempt to make use of the same additional dataset as our two reinforced models, OnPG and OffPG, but they need to exclude below-threshold captions due to the constraints in MLE.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the baseline models?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the baselines?\n< Explanation: We compare our method to the following baselines: (1) a baseline that uses the human ratings as the only reward, (2) a baseline that uses the human ratings as the only reward, (3) a baseline that uses the human ratings as the only reward, (4) a baseline that uses the human ratings as the only reward, (5) a baseline that uses the human ratings as the only reward, (6) a baseline that uses the human ratings as the only reward, (7) a baseline that uses the human ratings as the only reward, (8) a\n\nAnswer B:\nMLE model. Explanation: We first train an MLE model as our baseline, trained on the Conceptual Captions training split alone. We referred to this model as Baseline. For a baseline approach that utilizes (some of) the Caption-Quality data, we merge positively-rated captions from the Caption-Quality training split with the Conceptual Captions examples and finetune the baseline model. We call this model Baseline$+(t)$, where $t \\in [0,1]$ is the rating threshold for the included positive captions. We train models for two variants, $t\\in \\lbrace 0.5, 0.7\\rbrace $, which results in $\\sim $72K and $\\sim $51K additional (pseudo-)ground-truth captions, respectively. Note that the Baseline$+(t)$ approaches attempt to make use of the same additional dataset as our two reinforced models, OnPG and OffPG, but they need to exclude below-threshold captions due to the constraints in MLE.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.09753",
    "question": "What image caption datasets were used in this work?",
    "gold_answer": "",
    "answer_a": "Conceptual Captions. Explanation: In the experiments, we use Conceptual Captions BIBREF0, a large-scale captioning dataset that consists of images crawled from the Internet, with captions derived from corresponding Alt-text labels on the webpages. The training and validation splits have approximately 3.3M and 16K samples, respectively.",
    "answer_b": "What are the sizes of the datasets?\n< Explanation: We use the following datasets: (1) the ImageNet captioning dataset (IMC) BIBREF10, which is a publicly available dataset of 1.5 million images with 1000 captions, and (2) the ImageNet captioning dataset (IMC) BIBREF11, which is a publicly available dataset of 1.5 million images with 1000 captions, and (3) the ImageNet captioning dataset (IMC) BIBREF12, which is a publicly available dataset of 1.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat image caption datasets were used in this work?\n\nGround Truth Answer:\n\n\nAnswer A:\nConceptual Captions. Explanation: In the experiments, we use Conceptual Captions BIBREF0, a large-scale captioning dataset that consists of images crawled from the Internet, with captions derived from corresponding Alt-text labels on the webpages. The training and validation splits have approximately 3.3M and 16K samples, respectively.\n\nAnswer B:\nWhat are the sizes of the datasets?\n< Explanation: We use the following datasets: (1) the ImageNet captioning dataset (IMC) BIBREF10, which is a publicly available dataset of 1.5 million images with 1000 captions, and (2) the ImageNet captioning dataset (IMC) BIBREF11, which is a publicly available dataset of 1.5 million images with 1000 captions, and (3) the ImageNet captioning dataset (IMC) BIBREF12, which is a publicly available dataset of 1.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.09753",
    "question": "How long does it take to train the model on the mentioned dataset? ",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n< Explanation: We train the model on the caption ratings dataset with 1000 images, and the human ratings dataset with 1000 images. The training dataset is split into 1000 images for training and 1000 images for validation. The training and validation sets are used to train the model. The model is trained for 10 epochs with a batch size of 16. The model is trained using Adam optimizer with a learning rate of 0.0001. The model is trained using a batch size of 16.",
    "answer_b": "3M iterations with the batch size of 4,096. Explanation: We train Baseline using the Adam optimizer BIBREF34 on the training split of the Conceptual dataset for 3M iterations with the batch size of 4,096 and the learning rate of $3.2\\times 10^{-5}$. The learning rate is warmed up for 20 epochs and exponentially decayed by a factor of 0.95 every 25 epochs. Baseline$+(t)$ are obtained by fine-tuning Baseline on the merged dataset for 1M iterations, with the learning rate of $3.2\\times 10^{-7}$ and the same decaying factor. For OnPG, because its memory footprint is increased significantly due to the additional parameters for the rating estimator, we reduce the batch size for training this model by a 0.25 factor; the value of $b$ in Eq. (DISPLAY_FORM12) is set to the moving average of the rating estimates. During OffPG training, for each batch, we sample half of the examples from the Conceptual dataset and the other half from Caption-Quality dataset; $b$ is set to the average of the ratings in the dataset.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long does it take to train the model on the mentioned dataset? \n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n< Explanation: We train the model on the caption ratings dataset with 1000 images, and the human ratings dataset with 1000 images. The training dataset is split into 1000 images for training and 1000 images for validation. The training and validation sets are used to train the model. The model is trained for 10 epochs with a batch size of 16. The model is trained using Adam optimizer with a learning rate of 0.0001. The model is trained using a batch size of 16.\n\nAnswer B:\n3M iterations with the batch size of 4,096. Explanation: We train Baseline using the Adam optimizer BIBREF34 on the training split of the Conceptual dataset for 3M iterations with the batch size of 4,096 and the learning rate of $3.2\\times 10^{-5}$. The learning rate is warmed up for 20 epochs and exponentially decayed by a factor of 0.95 every 25 epochs. Baseline$+(t)$ are obtained by fine-tuning Baseline on the merged dataset for 1M iterations, with the learning rate of $3.2\\times 10^{-7}$ and the same decaying factor. For OnPG, because its memory footprint is increased significantly due to the additional parameters for the rating estimator, we reduce the batch size for training this model by a 0.25 factor; the value of $b$ in Eq. (DISPLAY_FORM12) is set to the moving average of the rating estimates. During OffPG training, for each batch, we sample half of the examples from the Conceptual dataset and the other half from Caption-Quality dataset; $b$ is set to the average of the ratings in the dataset.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.09753",
    "question": "How big is the human ratings dataset?",
    "gold_answer": "",
    "answer_a": "1K images sampled from the Open Images Dataset. Explanation: To evaluate our models, we run human evaluation studies on the T2 test dataset used in the CVPR 2019 Conceptual Captions Challenge. The dataset contains 1K images sampled from the Open Images Dataset BIBREF29. Note that the images in the Caption-Quality dataset are also sampled from the Open Images Dataset, but using a disjoint split. So there is no overlap between the caption ratings dataset $\\mathcal {D}_\\mathrm {CR}$ we use for training, and the T2 test set we use for evaluations.",
    "answer_b": "What is the size of the dataset?\n< Explanation: We use the dataset of 1000 human ratings of 1000 images, which is a subset of the dataset of 10000 ratings of 10000 images. The ratings are in the range of 1 to 5, with 1 being the most positive and 5 being the most negative. The ratings are collected from the Kaggle dataset of 10000 images. The ratings are collected from the Kaggle dataset of 10000 images. The ratings are collected from the Kaggle dataset",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big is the human ratings dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\n1K images sampled from the Open Images Dataset. Explanation: To evaluate our models, we run human evaluation studies on the T2 test dataset used in the CVPR 2019 Conceptual Captions Challenge. The dataset contains 1K images sampled from the Open Images Dataset BIBREF29. Note that the images in the Caption-Quality dataset are also sampled from the Open Images Dataset, but using a disjoint split. So there is no overlap between the caption ratings dataset $\\mathcal {D}_\\mathrm {CR}$ we use for training, and the T2 test set we use for evaluations.\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: We use the dataset of 1000 human ratings of 1000 images, which is a subset of the dataset of 10000 ratings of 10000 images. The ratings are in the range of 1 to 5, with 1 being the most positive and 5 being the most negative. The ratings are collected from the Kaggle dataset of 10000 images. The ratings are collected from the Kaggle dataset of 10000 images. The ratings are collected from the Kaggle dataset\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.05700",
    "question": "What existing techniques do the authors compare against?",
    "gold_answer": "",
    "answer_a": "What are the results?\n<blockquote>Table 1: Comparison of existing techniques with MERF. The results are shown in terms of accuracy, time, and run time. The results are shown in terms of accuracy, time, and run time. The results are shown in terms of accuracy, time, and run time. The results are shown in terms of accuracy, time, and run time. The results are shown in terms of accuracy, time, and run time. The results are shown in terms of accuracy, time, and run time. The results are shown in terms of accuracy, time, and run time. The results are",
    "answer_b": "ANGE, ATEEMA, GENTREE, and NUMNORM. Explanation: Table TABREF35 shows that MERF has a clear advantage over task specific techniques in the effort required to develop the application at a reasonable cost in terms of accuracy and run time.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat existing techniques do the authors compare against?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n<blockquote>Table 1: Comparison of existing techniques with MERF. The results are shown in terms of accuracy, time, and run time. The results are shown in terms of accuracy, time, and run time. The results are shown in terms of accuracy, time, and run time. The results are shown in terms of accuracy, time, and run time. The results are shown in terms of accuracy, time, and run time. The results are shown in terms of accuracy, time, and run time. The results are shown in terms of accuracy, time, and run time. The results are\n\nAnswer B:\nANGE, ATEEMA, GENTREE, and NUMNORM. Explanation: Table TABREF35 shows that MERF has a clear advantage over task specific techniques in the effort required to develop the application at a reasonable cost in terms of accuracy and run time.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.05758",
    "question": "How big is the resulting SESAME dataset?",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n< Explanation: The dataset is composed of 100,000 labeled sentences, with 100,000 words per sentence. The dataset is composed of 100,000 labeled sentences, with 100,000 words per sentence. The dataset is composed of 100,000 labeled sentences, with 100,000 words per sentence. The dataset is composed of 100,000 labeled sentences, with 100,000 words per sentence. The",
    "answer_b": "3,650,909 sentences. Explanation: SESAME consists of 3,650,909 sentences, with lengths (in terms of number of tokens) following the distribution shown in Figure FIGREF42. A breakdown of relevant statistics, such as the mean and standard deviation of sentences' lengths, is given in Table TABREF43.\n\nPreprocessing ::: SESAME ::: Tokens\nSESAME consists of 87,769,158 tokens in total. The count and proportion of each entity tag (not a named entity, organization, person, location) is given in TABREF45.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big is the resulting SESAME dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n< Explanation: The dataset is composed of 100,000 labeled sentences, with 100,000 words per sentence. The dataset is composed of 100,000 labeled sentences, with 100,000 words per sentence. The dataset is composed of 100,000 labeled sentences, with 100,000 words per sentence. The dataset is composed of 100,000 labeled sentences, with 100,000 words per sentence. The\n\nAnswer B:\n3,650,909 sentences. Explanation: SESAME consists of 3,650,909 sentences, with lengths (in terms of number of tokens) following the distribution shown in Figure FIGREF42. A breakdown of relevant statistics, such as the mean and standard deviation of sentences' lengths, is given in Table TABREF43.\n\nPreprocessing ::: SESAME ::: Tokens\nSESAME consists of 87,769,158 tokens in total. The count and proportion of each entity tag (not a named entity, organization, person, location) is given in TABREF45.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1903.00384",
    "question": "What measures of quality do they use for a Q&A platform?",
    "gold_answer": "",
    "answer_a": "What are the results of the experiments?\n<sep><EXPERIMENT: Q&A Platform Quality Evaluation</EXPERIMENT> The proposed method is evaluated on the Zhihu Live dataset. The evaluation is conducted on three aspects: (1) the accuracy of the answers, (2) the accuracy of the questions, and (3) the accuracy of the answers and questions. The accuracy of the answers is evaluated by the number of correct answers and the accuracy of the questions is evaluated by the number of correct questions. The accuracy of the answers and questions are evaluated by the number of correct answers and the number of correct questions, respectively.",
    "answer_b": "Rating scores given by users. Explanation: The rating scores are within a range of INLINEFORM0 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat measures of quality do they use for a Q&A platform?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results of the experiments?\n<sep><EXPERIMENT: Q&A Platform Quality Evaluation</EXPERIMENT> The proposed method is evaluated on the Zhihu Live dataset. The evaluation is conducted on three aspects: (1) the accuracy of the answers, (2) the accuracy of the questions, and (3) the accuracy of the answers and questions. The accuracy of the answers is evaluated by the number of correct answers and the accuracy of the questions is evaluated by the number of correct questions. The accuracy of the answers and questions are evaluated by the number of correct answers and the number of correct questions, respectively.\n\nAnswer B:\nRating scores given by users. Explanation: The rating scores are within a range of INLINEFORM0 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.00129",
    "question": "How many layers of recurrent neural networks do they use for encoding the global context?",
    "gold_answer": "",
    "answer_a": "What is the size of the input to the recurrent neural network?\n< Explanation: The third part of the model is a stack of feed-forward and recurrent neural networks. The size of the input to the recurrent neural network is the same as the size of the output of the embedding layer. The size of the output of the recurrent neural network is the same as the size of the output of the embedding layer. The size of the output of the recurrent neural network is the same as the size of the output of the convolution layer. The size of the output of the convolution layer is the same as the size of the output of the embedding layer.",
    "answer_b": "8. Explanation: After we obtain the representation of the source-target word pair by the convolution layer, we follow a similar architecture as BIBREF6 to refine the representation of the word pairs using feed-forward and recurrent networks.\n\nTwo feed-forward layers of size 400 with rectified linear units (ReLU; BIBREF9 );\n\nOne bi-directional gated recurrent unit (BiGRU; BIBREF10 ) layer with hidden size 200, where the forward and backward hidden states are concatenated and further normalized by layer normalization BIBREF11 .\n\nTwo feed-forward layers of hidden size 200 with rectified linear units;\n\nOne BiGRU layer with hidden size 100 using the same configuration of the previous BiGRU layer;\n\nTwo feed-forward layers of size 100 and 50 respectively with ReLU activation.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many layers of recurrent neural networks do they use for encoding the global context?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the input to the recurrent neural network?\n< Explanation: The third part of the model is a stack of feed-forward and recurrent neural networks. The size of the input to the recurrent neural network is the same as the size of the output of the embedding layer. The size of the output of the recurrent neural network is the same as the size of the output of the embedding layer. The size of the output of the recurrent neural network is the same as the size of the output of the convolution layer. The size of the output of the convolution layer is the same as the size of the output of the embedding layer.\n\nAnswer B:\n8. Explanation: After we obtain the representation of the source-target word pair by the convolution layer, we follow a similar architecture as BIBREF6 to refine the representation of the word pairs using feed-forward and recurrent networks.\n\nTwo feed-forward layers of size 400 with rectified linear units (ReLU; BIBREF9 );\n\nOne bi-directional gated recurrent unit (BiGRU; BIBREF10 ) layer with hidden size 200, where the forward and backward hidden states are concatenated and further normalized by layer normalization BIBREF11 .\n\nTwo feed-forward layers of hidden size 200 with rectified linear units;\n\nOne BiGRU layer with hidden size 100 using the same configuration of the previous BiGRU layer;\n\nTwo feed-forward layers of size 100 and 50 respectively with ReLU activation.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.00129",
    "question": "How did their model rank in three CMU WMT2018 tracks it didn't rank first?",
    "gold_answer": "",
    "answer_a": "Second on De-En and En-De (NMT) tasks, and third on En-De (SMT) task.. Explanation: FLOAT SELECTED: Table 3: Best performance of our model on six datasets in the WMT2018 word-level QE shared task on the leader board (updated on July 27th 2018)",
    "answer_b": "What is the ranking of their model?\n< Explanation: The model achieved the best results in the following three tracks: (1) the first track (2018-03-01) was ranked first in the English-English and English-French tracks, and second in the French-English track. (2) the second track (2018-03-02) was ranked second in the English-English and English-French tracks, and third in the French-English track. (3) the third track (2018-03-03) was ranked second in the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow did their model rank in three CMU WMT2018 tracks it didn't rank first?\n\nGround Truth Answer:\n\n\nAnswer A:\nSecond on De-En and En-De (NMT) tasks, and third on En-De (SMT) task.. Explanation: FLOAT SELECTED: Table 3: Best performance of our model on six datasets in the WMT2018 word-level QE shared task on the leader board (updated on July 27th 2018)\n\nAnswer B:\nWhat is the ranking of their model?\n< Explanation: The model achieved the best results in the following three tracks: (1) the first track (2018-03-01) was ranked first in the English-English and English-French tracks, and second in the French-English track. (2) the second track (2018-03-02) was ranked second in the English-English and English-French tracks, and third in the French-English track. (3) the third track (2018-03-03) was ranked second in the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1710.11027",
    "question": "What is the Ritter dataset?",
    "gold_answer": "",
    "answer_a": "How is it used?\n< Explanation: The Ritter dataset is a collection of 1000 tweets from Twitter, collected from 2015 to 2016. It contains 1000 tweets, each with 140 characters. The tweets are annotated with 10 named entities, each with a 10-character name. The dataset is available at http://www.cs.cornell.edu/~ritter/ . The dataset is divided into 1000 tweets, each with 140 characters. The tweets are annotated with 10 named entities, each with a",
    "answer_b": "a gold standard for NER in microblogs. Explanation: In order to check the overall performance of the proposed technique, we ran our algorithm without any further rule or apriori knowledge using a gold standard for NER in microblogs (Ritter dataset BIBREF2 ),",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the Ritter dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it used?\n< Explanation: The Ritter dataset is a collection of 1000 tweets from Twitter, collected from 2015 to 2016. It contains 1000 tweets, each with 140 characters. The tweets are annotated with 10 named entities, each with a 10-character name. The dataset is available at http://www.cs.cornell.edu/~ritter/ . The dataset is divided into 1000 tweets, each with 140 characters. The tweets are annotated with 10 named entities, each with a\n\nAnswer B:\na gold standard for NER in microblogs. Explanation: In order to check the overall performance of the proposed technique, we ran our algorithm without any further rule or apriori knowledge using a gold standard for NER in microblogs (Ritter dataset BIBREF2 ),\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1710.11027",
    "question": "What features are extracted from text?",
    "gold_answer": "",
    "answer_a": "How are they used?\n< Explanation: We use the following features to extract named entities from text: (1) word embeddings, (2) word embeddings of the image, (3) word embeddings of the image and the text, (4) word embeddings of the image and the text of the image, (5) word embeddings of the image and the text of the image and the text of the image, (6) word embeddings of the image and the text of the image and the text of the image and the text of the image, (7) word embeddings of the image and the text of the image and the text of",
    "answer_b": "word feature. Explanation: Thus, for each retrieved web page (title and excerpt of its content), we perform the classification based on the main NER classes. We extracted features using a classical sparse vectorizer (Term frequency-Inverse document frequency - TF-IDF.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat features are extracted from text?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they used?\n< Explanation: We use the following features to extract named entities from text: (1) word embeddings, (2) word embeddings of the image, (3) word embeddings of the image and the text, (4) word embeddings of the image and the text of the image, (5) word embeddings of the image and the text of the image and the text of the image, (6) word embeddings of the image and the text of the image and the text of the image and the text of the image, (7) word embeddings of the image and the text of the image and the text of\n\nAnswer B:\nword feature. Explanation: Thus, for each retrieved web page (title and excerpt of its content), we perform the classification based on the main NER classes. We extracted features using a classical sparse vectorizer (Term frequency-Inverse document frequency - TF-IDF.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1710.11027",
    "question": "What features are extracted from images?",
    "gold_answer": "",
    "answer_a": "LOC (Building, Suburb, Street, City, Country, Mountain, Highway, Forest, Coast and Map), ORG (Company Logo), PER (Human Face ).. Explanation: Thus, we query the web for a given term INLINEFORM2 and then extract the features from each image and try to detect a specific object (e.g., logos for ORG) for the top INLINEFORM3 images retrieved as source candidates. The mapping between objects and NER classes is detailed in tab:tbempirical.",
    "answer_b": "How are they used?\n< Explanation: We use the following features to extract the named entities: (1) the image features extracted from the image, (2) the text features extracted from the text, and (3) the image text features extracted from the image and text. The image features are extracted from the image using the following steps: (1) the image is resized to 224x224 pixels, (2) the image is normalized to the range [0, 1], and (3) the image is converted to grayscale. The text features are extracted from the text using the following steps: (",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat features are extracted from images?\n\nGround Truth Answer:\n\n\nAnswer A:\nLOC (Building, Suburb, Street, City, Country, Mountain, Highway, Forest, Coast and Map), ORG (Company Logo), PER (Human Face ).. Explanation: Thus, we query the web for a given term INLINEFORM2 and then extract the features from each image and try to detect a specific object (e.g., logos for ORG) for the top INLINEFORM3 images retrieved as source candidates. The mapping between objects and NER classes is detailed in tab:tbempirical.\n\nAnswer B:\nHow are they used?\n< Explanation: We use the following features to extract the named entities: (1) the image features extracted from the image, (2) the text features extracted from the text, and (3) the image text features extracted from the image and text. The image features are extracted from the image using the following steps: (1) the image is resized to 224x224 pixels, (2) the image is normalized to the range [0, 1], and (3) the image is converted to grayscale. The text features are extracted from the text using the following steps: (\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.10401",
    "question": "What are the baseline models?",
    "gold_answer": "",
    "answer_a": "What are the results?\n< Explanation: Table 1 shows the results of the proposed hybrid neural architecture on the four benchmark datasets. The proposed architecture outperforms all other methodologies and published studies, with a significant margin of 1.3% on the sarcasm dataset, 1.2% on the irony dataset, 1.1% on the metaphor dataset and 1.0% on the general dataset. The proposed architecture also outperforms all other methodologies and published studies, with a significant margin of 1.3% on the sarcasm dataset, 1.2% on the irony dataset, 1.",
    "answer_b": "ELMo. Explanation: The used methodologies were appropriately implemented using the available codes and guidelines, and include: ELMo BIBREF64, USE BIBREF79, NBSVM BIBREF93, FastText BIBREF94, XLnet base cased model (XLnet) BIBREF88, BERT BIBREF18 in two setups: BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the baseline models?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n< Explanation: Table 1 shows the results of the proposed hybrid neural architecture on the four benchmark datasets. The proposed architecture outperforms all other methodologies and published studies, with a significant margin of 1.3% on the sarcasm dataset, 1.2% on the irony dataset, 1.1% on the metaphor dataset and 1.0% on the general dataset. The proposed architecture also outperforms all other methodologies and published studies, with a significant margin of 1.3% on the sarcasm dataset, 1.2% on the irony dataset, 1.\n\nAnswer B:\nELMo. Explanation: The used methodologies were appropriately implemented using the available codes and guidelines, and include: ELMo BIBREF64, USE BIBREF79, NBSVM BIBREF93, FastText BIBREF94, XLnet base cased model (XLnet) BIBREF88, BERT BIBREF18 in two setups: BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.10401",
    "question": "How are the three different forms defined in this work?",
    "gold_answer": "",
    "answer_a": "What are the main differences between them?\n< Explanation: The three main FL forms are: (1) sarcasm, (2) irony, and (3) metaphor. The main difference between them is the meaning content of the words used in the text. The meaning of the words in the text can be either sarcastic, ironic or metaphorical. The meaning of the words in the text can be either sarcastic, ironic or metaphorical. The meaning of the words in the text can be either sarcastic, ironic or metaphorical. The meaning of the words in the text can be either sarcastic, ironic or metaphorical.",
    "answer_b": "Irony, sarcasm and metaphor are figurative language form. Irony and sarcasm are considered as a way of indirect denial.. Explanation: We may identify three common FL expression forms namely, irony, sarcasm and metaphor. In this paper, figurative expressions, and especially ironic or sarcastic ones, are considered as a way of indirect denial.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are the three different forms defined in this work?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the main differences between them?\n< Explanation: The three main FL forms are: (1) sarcasm, (2) irony, and (3) metaphor. The main difference between them is the meaning content of the words used in the text. The meaning of the words in the text can be either sarcastic, ironic or metaphorical. The meaning of the words in the text can be either sarcastic, ironic or metaphorical. The meaning of the words in the text can be either sarcastic, ironic or metaphorical. The meaning of the words in the text can be either sarcastic, ironic or metaphorical.\n\nAnswer B:\nIrony, sarcasm and metaphor are figurative language form. Irony and sarcasm are considered as a way of indirect denial.. Explanation: We may identify three common FL expression forms namely, irony, sarcasm and metaphor. In this paper, figurative expressions, and especially ironic or sarcastic ones, are considered as a way of indirect denial.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.10401",
    "question": "What datasets are used for training and testing?",
    "gold_answer": "",
    "answer_a": "How are they preprocessed?\n< Explanation: We use the following datasets for training and testing: (1) The dataset of the Twitter Sentiment Analysis Challenge (TSAC) 2017, which is a benchmark dataset for the task of sentiment analysis. The dataset consists of 100,000 tweets from the Twitter platform. The tweets are labeled as positive, negative or neutral. The dataset is available at https://www.kaggle.com/c/tweetsentimentchallenge. (2) The dataset of the Twitter Sentiment Analysis Challenge (TSAC) 2017, which is a benchmark dataset",
    "answer_b": "SemEval-2018. Explanation: For the comparison we utilized benchmark datasets that include ironic, sarcastic and metaphoric expressions. Namely, we used the dataset provided in \u201cSemantic Evaluation Workshop Task 3\u201d (SemEval-2018) that contains ironic tweets BIBREF95; Riloff\u2019s high quality sarcastic unbalanced dataset BIBREF96; a large dataset containing political comments from Reddit BIBREF97; and a SA dataset that contains tweets with various FL forms from \u201cSemEval-2015 Task 11\u201d BIBREF66.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets are used for training and testing?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they preprocessed?\n< Explanation: We use the following datasets for training and testing: (1) The dataset of the Twitter Sentiment Analysis Challenge (TSAC) 2017, which is a benchmark dataset for the task of sentiment analysis. The dataset consists of 100,000 tweets from the Twitter platform. The tweets are labeled as positive, negative or neutral. The dataset is available at https://www.kaggle.com/c/tweetsentimentchallenge. (2) The dataset of the Twitter Sentiment Analysis Challenge (TSAC) 2017, which is a benchmark dataset\n\nAnswer B:\nSemEval-2018. Explanation: For the comparison we utilized benchmark datasets that include ironic, sarcastic and metaphoric expressions. Namely, we used the dataset provided in \u201cSemantic Evaluation Workshop Task 3\u201d (SemEval-2018) that contains ironic tweets BIBREF95; Riloff\u2019s high quality sarcastic unbalanced dataset BIBREF96; a large dataset containing political comments from Reddit BIBREF97; and a SA dataset that contains tweets with various FL forms from \u201cSemEval-2015 Task 11\u201d BIBREF66.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.03854",
    "question": "What classification tasks do they experiment on?",
    "gold_answer": "",
    "answer_a": "How do they evaluate the models?\n< Explanation: We evaluate our models on 2-way, 3-way, and 5-way classification tasks. We use the following classification tasks:\n\n2-way classification: We use the 2-way classification task to evaluate our models. We use the 2-way classification task to evaluate our models. We use the 2-way classification task to evaluate our models. We use the 2-way classification task to evaluate our models. We use the 2-way classification task to evaluate our models. We use the 2-way classification task to evaluate our models. We use the 2-way classification task to",
    "answer_b": "fake news detection through text, image and text+image modes. Explanation: We evaluate our dataset through text, image, and text+image modes with a neural network architecture that integrates both the image and text data. We run experiments for several types of models, providing a comprehensive overview of classification results.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat classification tasks do they experiment on?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate the models?\n< Explanation: We evaluate our models on 2-way, 3-way, and 5-way classification tasks. We use the following classification tasks:\n\n2-way classification: We use the 2-way classification task to evaluate our models. We use the 2-way classification task to evaluate our models. We use the 2-way classification task to evaluate our models. We use the 2-way classification task to evaluate our models. We use the 2-way classification task to evaluate our models. We use the 2-way classification task to evaluate our models. We use the 2-way classification task to\n\nAnswer B:\nfake news detection through text, image and text+image modes. Explanation: We evaluate our dataset through text, image, and text+image modes with a neural network architecture that integrates both the image and text data. We run experiments for several types of models, providing a comprehensive overview of classification results.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.03854",
    "question": "What categories of fake news are in the dataset?",
    "gold_answer": "",
    "answer_a": "How many are there?\n< Explanation: The dataset contains 800,000 samples from 10 categories of fake news. The categories are: 1) Political, 2) Religious, 3) Social, 4) Entertainment, 5) Sports, 6) Business, 7) Crime, 8) Crime and Violence, 9) Health, and 10) Religion and Politics. Each sample is labeled according to 2-way, 3-way, and 5-way classification categories. The dataset is available at https://github.com/abhishek101/Fake",
    "answer_b": "Satire/Parody. Explanation: Satire/Parody: This category consists of content that spins true contemporary content with a satirical tone or information that makes it false.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat categories of fake news are in the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow many are there?\n< Explanation: The dataset contains 800,000 samples from 10 categories of fake news. The categories are: 1) Political, 2) Religious, 3) Social, 4) Entertainment, 5) Sports, 6) Business, 7) Crime, 8) Crime and Violence, 9) Health, and 10) Religion and Politics. Each sample is labeled according to 2-way, 3-way, and 5-way classification categories. The dataset is available at https://github.com/abhishek101/Fake\n\nAnswer B:\nSatire/Parody. Explanation: Satire/Parody: This category consists of content that spins true contemporary content with a satirical tone or information that makes it false.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.03699",
    "question": "How much gain in performance was obtained with user embeddings?",
    "gold_answer": "",
    "answer_a": "On test set RNN that uses user embedding has AUC of 80.53 compared to base RNN 79.24.. Explanation: User-specific information always improves our original rnn-based method (Table TABREF15 ), but the best results are obtained by adding user embeddings (uernn).",
    "answer_b": "What was the size of the gain?\n< Explanation: We observe that the performance of the RNN-based model is improved by 1.5% on average, with the best model achieving a 1.9% gain. The size of the gain is 1.5% for the model with user embeddings, 2.0% for the model with user type embeddings, 2.5% for the model with user type biases, and 3.0% for the model with user type embeddings and user type biases. The size of the gain is 1.5% for the model with user embeddings, 2",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much gain in performance was obtained with user embeddings?\n\nGround Truth Answer:\n\n\nAnswer A:\nOn test set RNN that uses user embedding has AUC of 80.53 compared to base RNN 79.24.. Explanation: User-specific information always improves our original rnn-based method (Table TABREF15 ), but the best results are obtained by adding user embeddings (uernn).\n\nAnswer B:\nWhat was the size of the gain?\n< Explanation: We observe that the performance of the RNN-based model is improved by 1.5% on average, with the best model achieving a 1.9% gain. The size of the gain is 1.5% for the model with user embeddings, 2.0% for the model with user type embeddings, 2.5% for the model with user type biases, and 3.0% for the model with user type embeddings and user type biases. The size of the gain is 1.5% for the model with user embeddings, 2\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1608.01972",
    "question": "By how much does their similarity measure outperform BM25?",
    "gold_answer": "",
    "answer_a": "How does it compare to other similarity measures?\n< Explanation: The proposed method is able to outperform BM25 by 0.01% on TREC and 0.03% on PubMed. The results are shown in Table 1. The proposed method is able to outperform BM25 by 0.01% on TREC and 0.03% on PubMed. The results are shown in Table 1. The proposed method is able to outperform BM25 by 0.01% on TREC and 0.03% on PubMed. The results are shown in",
    "answer_b": "embedding approach boosts the average precision of BM25 by 19% and 6% on TREC 2006 and 2007. Explanation: From the table, the embedding approach boosts the average precision of BM25 by 19% and 6% on TREC 2006 and 2007, respectively.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much does their similarity measure outperform BM25?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow does it compare to other similarity measures?\n< Explanation: The proposed method is able to outperform BM25 by 0.01% on TREC and 0.03% on PubMed. The results are shown in Table 1. The proposed method is able to outperform BM25 by 0.01% on TREC and 0.03% on PubMed. The results are shown in Table 1. The proposed method is able to outperform BM25 by 0.01% on TREC and 0.03% on PubMed. The results are shown in\n\nAnswer B:\nembedding approach boosts the average precision of BM25 by 19% and 6% on TREC 2006 and 2007. Explanation: From the table, the embedding approach boosts the average precision of BM25 by 19% and 6% on TREC 2006 and 2007, respectively.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1608.01972",
    "question": "How do they represent documents when using their proposed similarity measure?",
    "gold_answer": "",
    "answer_a": "What is the difference between the two methods?\n< Explanation: The proposed method is based on the similarity measure between the document and the query. The similarity measure is calculated by the word embedding method. The document is represented by a word embedding vector, and the query is represented by a query embedding vector. The similarity between the document and the query is calculated by the cosine similarity between the two vectors. The similarity between the document and the query is then used to calculate the distance between the document and the query. The distance is then used to calculate the similarity between the document and the query. The proposed method is different from the traditional method in that",
    "answer_b": "documents are represented by normalized bag-of-words (BOW) vectors. Explanation: First, following Kusner et al. Kusner2015, documents are represented by normalized bag-of-words (BOW) vectors, i.e. if a word INLINEFORM0 appears INLINEFORM1 times in a document, the weight is DISPLAYFORM0\n\nwhere INLINEFORM0 is number of words in the document. The higher the weight, the more important the word. They assume a word embedding so that each word INLINEFORM1 has an associated vector INLINEFORM2 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they represent documents when using their proposed similarity measure?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the two methods?\n< Explanation: The proposed method is based on the similarity measure between the document and the query. The similarity measure is calculated by the word embedding method. The document is represented by a word embedding vector, and the query is represented by a query embedding vector. The similarity between the document and the query is calculated by the cosine similarity between the two vectors. The similarity between the document and the query is then used to calculate the distance between the document and the query. The distance is then used to calculate the similarity between the document and the query. The proposed method is different from the traditional method in that\n\nAnswer B:\ndocuments are represented by normalized bag-of-words (BOW) vectors. Explanation: First, following Kusner et al. Kusner2015, documents are represented by normalized bag-of-words (BOW) vectors, i.e. if a word INLINEFORM0 appears INLINEFORM1 times in a document, the weight is DISPLAYFORM0\n\nwhere INLINEFORM0 is number of words in the document. The higher the weight, the more important the word. They assume a word embedding so that each word INLINEFORM1 has an associated vector INLINEFORM2 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1608.01972",
    "question": "How do they propose to combine BM25 and word embedding similarity?",
    "gold_answer": "",
    "answer_a": "They merge features of BM25 and semantic measures.. Explanation: In our study, we use learning to rank to merge two distinctive features, BM25 scores and our semantic measures.",
    "answer_b": "What is the synergy effect?\n< Explanation: We first propose a new similarity measure based on the Word Mover's Distance (WMD) between words. The WMD is a measure of the distance between two words in a document. The WMD is defined as follows: where is the document, is the document, and is the document. The WMD is a measure of the distance between two words in a document. The WMD is a measure of the distance between two words in a document. The WMD is a measure of the distance between two words in a document. The WMD is a measure of the distance between two",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they propose to combine BM25 and word embedding similarity?\n\nGround Truth Answer:\n\n\nAnswer A:\nThey merge features of BM25 and semantic measures.. Explanation: In our study, we use learning to rank to merge two distinctive features, BM25 scores and our semantic measures.\n\nAnswer B:\nWhat is the synergy effect?\n< Explanation: We first propose a new similarity measure based on the Word Mover's Distance (WMD) between words. The WMD is a measure of the distance between two words in a document. The WMD is defined as follows: where is the document, is the document, and is the document. The WMD is a measure of the distance between two words in a document. The WMD is a measure of the distance between two words in a document. The WMD is a measure of the distance between two words in a document. The WMD is a measure of the distance between two\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1705.01306",
    "question": "Which Twitter sentiment treebank is used?",
    "gold_answer": "",
    "answer_a": "How is it constructed?\n<blockquote>\n<font size=\"4\">The Twitter sentiment treebank is a collection of 100,000 tweets from the Twitter API. It is a collection of 100,000 tweets from the Twitter API. The tweets are annotated with a sentiment label (positive, negative, or neutral) and a text field. The tweets are then split into training and testing sets. The training set is used to train a sentiment classifier, and the testing set is used to evaluate the performance of the classifier. The sentiment treebank is available at <a href=\"http://sentiment",
    "answer_b": "They built their own. Explanation: Our training data for this part was created by taking a random sample from Twitter and having it manually annotated on a 5-label basis to produce fully sentiment-labeled parse-trees, much like the Stanford sentiment treebank.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich Twitter sentiment treebank is used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it constructed?\n<blockquote>\n<font size=\"4\">The Twitter sentiment treebank is a collection of 100,000 tweets from the Twitter API. It is a collection of 100,000 tweets from the Twitter API. The tweets are annotated with a sentiment label (positive, negative, or neutral) and a text field. The tweets are then split into training and testing sets. The training set is used to train a sentiment classifier, and the testing set is used to evaluate the performance of the classifier. The sentiment treebank is available at <a href=\"http://sentiment\n\nAnswer B:\nThey built their own. Explanation: Our training data for this part was created by taking a random sample from Twitter and having it manually annotated on a 5-label basis to produce fully sentiment-labeled parse-trees, much like the Stanford sentiment treebank.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1705.01306",
    "question": "Where did the system place in the other sub-tasks?",
    "gold_answer": "",
    "answer_a": "What was the ranking of the system in the other sub-tasks?\n<blockquote>Table 1: Results of the competition for the 5-label classification task. The results are reported in terms of accuracy, precision, recall and F1-score. The results are reported in terms of accuracy, precision, recall and F1-score. The results are reported in terms of accuracy, precision, recall and F1-score. The results are reported in terms of accuracy, precision, recall and F1-score. The results are reported in terms of accuracy, precision, recall and F1-score. The results are reported in terms of accuracy, precision",
    "answer_b": "which we scored lower. Explanation: Compared with classification with 2 and 3 labels, in which we scored lower, and the fact we used similar workflow for tasks A, B, C, we speculate that the relative success is due to our sentiment treebank ranking on a 5-label basis.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhere did the system place in the other sub-tasks?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat was the ranking of the system in the other sub-tasks?\n<blockquote>Table 1: Results of the competition for the 5-label classification task. The results are reported in terms of accuracy, precision, recall and F1-score. The results are reported in terms of accuracy, precision, recall and F1-score. The results are reported in terms of accuracy, precision, recall and F1-score. The results are reported in terms of accuracy, precision, recall and F1-score. The results are reported in terms of accuracy, precision, recall and F1-score. The results are reported in terms of accuracy, precision\n\nAnswer B:\nwhich we scored lower. Explanation: Compared with classification with 2 and 3 labels, in which we scored lower, and the fact we used similar workflow for tasks A, B, C, we speculate that the relative success is due to our sentiment treebank ranking on a 5-label basis.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1705.01306",
    "question": "What were the five labels to be predicted in sub-task C?",
    "gold_answer": "",
    "answer_a": "very negative, negative, neutral, positive, very positive. Explanation: The goals of these tasks are to classify tweets sentiment regarding a given entity into five classes\u2014very negative, negative, neutral, positive, very positive\u2014(task C) and estimate sentiment distribution over five classes for each entity (task E).",
    "answer_b": "Explanation: The five labels to be predicted in sub-task C are: positive, neutral, negative, neutral and negative. The system was trained on the Twitter sentiment treebank, which contains 100,000 tweets, and was tested on 100,000 tweets from the same Twitter account. The tweets were preprocessed using the Twitter API, and the sentiment was determined using the Twitter API. The system was trained using the Twitter API, and the system was tested using the Twitter API. The system was trained using the Twitter API, and the system was tested using the Twitter API. The system was trained",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat were the five labels to be predicted in sub-task C?\n\nGround Truth Answer:\n\n\nAnswer A:\nvery negative, negative, neutral, positive, very positive. Explanation: The goals of these tasks are to classify tweets sentiment regarding a given entity into five classes\u2014very negative, negative, neutral, positive, very positive\u2014(task C) and estimate sentiment distribution over five classes for each entity (task E).\n\nAnswer B:\nExplanation: The five labels to be predicted in sub-task C are: positive, neutral, negative, neutral and negative. The system was trained on the Twitter sentiment treebank, which contains 100,000 tweets, and was tested on 100,000 tweets from the same Twitter account. The tweets were preprocessed using the Twitter API, and the sentiment was determined using the Twitter API. The system was trained using the Twitter API, and the system was tested using the Twitter API. The system was trained using the Twitter API, and the system was tested using the Twitter API. The system was trained\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.13028",
    "question": "What is the previous state-of-the-art?",
    "gold_answer": "",
    "answer_a": "What is the proposed combination model?\n< Explanation: The previous state-of-the-art results of abstractive summarization are shown in Table TABREF1 . The proposed combination model outperformed the previous best-performed model by 1.33 points on ROUGE-L. The proposed combination model is composed of a saliency model that extracts a token sequence from a source text and a seq-to-seq model that takes the sequence as an additional input text. The saliency model is trained on the source text and the seq-to-seq model is trained on the sequence. The saliency model is trained on the source",
    "answer_b": "Transformer-based encoder-decoder. Explanation: Transformer-based encoder-decoder model using large unlabeled data and achieved state-of-the-art results.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the previous state-of-the-art?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the proposed combination model?\n< Explanation: The previous state-of-the-art results of abstractive summarization are shown in Table TABREF1 . The proposed combination model outperformed the previous best-performed model by 1.33 points on ROUGE-L. The proposed combination model is composed of a saliency model that extracts a token sequence from a source text and a seq-to-seq model that takes the sequence as an additional input text. The saliency model is trained on the source text and the seq-to-seq model is trained on the sequence. The saliency model is trained on the source\n\nAnswer B:\nTransformer-based encoder-decoder. Explanation: Transformer-based encoder-decoder model using large unlabeled data and achieved state-of-the-art results.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.13028",
    "question": "What is the architecture of the decoder?",
    "gold_answer": "",
    "answer_a": "self-attention module, a context-attention module, and a two-layer feed-forward network. Explanation: The decoder consists of $M$ layer decoder blocks.",
    "answer_b": "What is the architecture of the encoder?\n<sep><table cellspacing=\"0\" cellpadding=\"0\" border=\"0\" width=\"100%\" align=\"center\">\n<tr>\n<td>\nFigure 1: Architecture of the encoder and decoder. The encoder takes the source text as an input and outputs a sequence of hidden states. The decoder takes the sequence as an input and outputs a summary. The decoder uses the sequence as an additional input to the encoder. The decoder uses the sequence as an additional input to the encoder. The decoder uses the sequence as an additional input to the encoder. The decoder uses the sequence as an additional input to",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the architecture of the decoder?\n\nGround Truth Answer:\n\n\nAnswer A:\nself-attention module, a context-attention module, and a two-layer feed-forward network. Explanation: The decoder consists of $M$ layer decoder blocks.\n\nAnswer B:\nWhat is the architecture of the encoder?\n<sep><table cellspacing=\"0\" cellpadding=\"0\" border=\"0\" width=\"100%\" align=\"center\">\n<tr>\n<td>\nFigure 1: Architecture of the encoder and decoder. The encoder takes the source text as an input and outputs a sequence of hidden states. The decoder takes the sequence as an input and outputs a summary. The decoder uses the sequence as an additional input to the encoder. The decoder uses the sequence as an additional input to the encoder. The decoder uses the sequence as an additional input to the encoder. The decoder uses the sequence as an additional input to\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.13028",
    "question": "What is the architecture of the encoder?",
    "gold_answer": "",
    "answer_a": "M blocks, each consisting of self-attention module and a two-layer feed-forward network.. Explanation: The encoder consists of $M$ layer encoder blocks.",
    "answer_b": "What is the architecture of the decoder?\n<sep><table cellspacing=\"0\" cellpadding=\"0\" border=\"0\" width=\"100%\" align=\"center\">\n<tr>\n<td>\nFigure 1: Architecture of the encoder and decoder. The encoder consists of a sequence encoder and a token encoder. The decoder consists of a sequence decoder and a token decoder. The sequence decoder takes the source text as an input and outputs a sequence of tokens. The token decoder takes the sequence of tokens as an input and outputs a sequence of tokens. The source text is fed into the sequence decoder and the sequence decoder outputs a sequence of tokens. The",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the architecture of the encoder?\n\nGround Truth Answer:\n\n\nAnswer A:\nM blocks, each consisting of self-attention module and a two-layer feed-forward network.. Explanation: The encoder consists of $M$ layer encoder blocks.\n\nAnswer B:\nWhat is the architecture of the decoder?\n<sep><table cellspacing=\"0\" cellpadding=\"0\" border=\"0\" width=\"100%\" align=\"center\">\n<tr>\n<td>\nFigure 1: Architecture of the encoder and decoder. The encoder consists of a sequence encoder and a token encoder. The decoder consists of a sequence decoder and a token decoder. The sequence decoder takes the source text as an input and outputs a sequence of tokens. The token decoder takes the sequence of tokens as an input and outputs a sequence of tokens. The source text is fed into the sequence decoder and the sequence decoder outputs a sequence of tokens. The\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.13028",
    "question": "What is the architecture of the saliency model?",
    "gold_answer": "",
    "answer_a": "How is the saliency model trained?\n< Explanation: The saliency model is a CNN that extracts a token sequence from a source text. The saliency model is trained using the source text as the input and the saliency of each token is computed by the CNN. The saliency model is pre-trained on the source text of the CNN/DM dataset. The saliency model is trained using the source text as the input and the saliency of each token is computed by the CNN. The saliency model is trained using the source text as the input and the saliency of each token is",
    "answer_b": "basic saliency model consists of $M$-layer Transformer encoder blocks ($\\mathrm {Encoder}_\\mathrm {sal}$) and a single-layer feed-forward network. Explanation: A basic saliency model consists of $M$-layer Transformer encoder blocks ($\\mathrm {Encoder}_\\mathrm {sal}$) and a single-layer feed-forward network.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the architecture of the saliency model?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is the saliency model trained?\n< Explanation: The saliency model is a CNN that extracts a token sequence from a source text. The saliency model is trained using the source text as the input and the saliency of each token is computed by the CNN. The saliency model is pre-trained on the source text of the CNN/DM dataset. The saliency model is trained using the source text as the input and the saliency of each token is computed by the CNN. The saliency model is trained using the source text as the input and the saliency of each token is\n\nAnswer B:\nbasic saliency model consists of $M$-layer Transformer encoder blocks ($\\mathrm {Encoder}_\\mathrm {sal}$) and a single-layer feed-forward network. Explanation: A basic saliency model consists of $M$-layer Transformer encoder blocks ($\\mathrm {Encoder}_\\mathrm {sal}$) and a single-layer feed-forward network.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.03977",
    "question": "What are special architectures this review focuses on that are related to multimodal fusion?",
    "gold_answer": "",
    "answer_a": "Please provide references.\n<sep>1. Image-to-text caption generation. This is a special case of multimodal caption generation, where the multimodal input is the image and the multimodal output is the text. The most common approach is to use a multimodal CNN to learn a multimodal representation of the image and then use a multimodal CNN to generate the text. This approach has been shown to be effective in several tasks, including image-to-text caption generation. The most recent work on multimodal caption generation is by BIBREF10. 2. Text-to-image generation. This is a special case of multimodal image",
    "answer_b": "attention mechanism. Explanation: On multimodal fusion, special architectures, such as attention mechanism and bilinear pooling, are discussed.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are special architectures this review focuses on that are related to multimodal fusion?\n\nGround Truth Answer:\n\n\nAnswer A:\nPlease provide references.\n<sep>1. Image-to-text caption generation. This is a special case of multimodal caption generation, where the multimodal input is the image and the multimodal output is the text. The most common approach is to use a multimodal CNN to learn a multimodal representation of the image and then use a multimodal CNN to generate the text. This approach has been shown to be effective in several tasks, including image-to-text caption generation. The most recent work on multimodal caption generation is by BIBREF10. 2. Text-to-image generation. This is a special case of multimodal image\n\nAnswer B:\nattention mechanism. Explanation: On multimodal fusion, special architectures, such as attention mechanism and bilinear pooling, are discussed.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.01992",
    "question": "What other model inference optimization schemes authors explore?",
    "gold_answer": "",
    "answer_a": "shortcut connections. Explanation: ResNet BIBREF8 solves many problems in training very deep CNNs. The key ResNet innovation is the shortcut connections shown in Figure FIGREF1. Figure FIGREF1 is a typical building block of ResNet. The input to the block, $x$, will go through both the original mapping $F(x)$ (weight layers, RELU activations and batch normalization BIBREF3) and the identity shortcut connection.",
    "answer_b": "How do they compare to SNDCNN?\n< Explanation: We also explore other model inference optimization schemes to further reduce latency for production use. We use the same model architecture as SNDCNN, but we also use the following inference optimization schemes: (1) using a single GPU for inference, (2) using a single GPU for inference and using a single CPU for training, (3) using a single GPU for inference and using a single CPU for training, (4) using a single GPU for inference and using a single CPU for training and using a single GPU for training, (5) using a single GPU for inference and using",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat other model inference optimization schemes authors explore?\n\nGround Truth Answer:\n\n\nAnswer A:\nshortcut connections. Explanation: ResNet BIBREF8 solves many problems in training very deep CNNs. The key ResNet innovation is the shortcut connections shown in Figure FIGREF1. Figure FIGREF1 is a typical building block of ResNet. The input to the block, $x$, will go through both the original mapping $F(x)$ (weight layers, RELU activations and batch normalization BIBREF3) and the identity shortcut connection.\n\nAnswer B:\nHow do they compare to SNDCNN?\n< Explanation: We also explore other model inference optimization schemes to further reduce latency for production use. We use the same model architecture as SNDCNN, but we also use the following inference optimization schemes: (1) using a single GPU for inference, (2) using a single GPU for inference and using a single CPU for training, (3) using a single GPU for inference and using a single CPU for training, (4) using a single GPU for inference and using a single CPU for training and using a single GPU for training, (5) using a single GPU for inference and using\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.01992",
    "question": "On what dataset is model trained/tested?",
    "gold_answer": "",
    "answer_a": "Siri internal datasets (en_US and zh_CN). Explanation: All the data used in this paper comes from Siri internal datasets (en_US and zh_CN). All the models are trained with Blockwise Model-Update Filtering (BMUF) BIBREF18 with 32 GPUs.",
    "answer_b": "What is the dataset used for?\n< Explanation: We use the 20 Newsgroups dataset (http://yann.lecun.com/exdb/newsgroups/) for training and the 20 Newsgroups dataset (http://yann.lecun.com/exdb/newsgroups/) for testing. The dataset consists of 20,000 documents, each containing 1000 words. The training set is 80% of the total documents, and the test set is 20% of the total documents. The dataset is divided into 1000 words per document. The",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nOn what dataset is model trained/tested?\n\nGround Truth Answer:\n\n\nAnswer A:\nSiri internal datasets (en_US and zh_CN). Explanation: All the data used in this paper comes from Siri internal datasets (en_US and zh_CN). All the models are trained with Blockwise Model-Update Filtering (BMUF) BIBREF18 with 32 GPUs.\n\nAnswer B:\nWhat is the dataset used for?\n< Explanation: We use the 20 Newsgroups dataset (http://yann.lecun.com/exdb/newsgroups/) for training and the 20 Newsgroups dataset (http://yann.lecun.com/exdb/newsgroups/) for testing. The dataset consists of 20,000 documents, each containing 1000 words. The training set is 80% of the total documents, and the test set is 20% of the total documents. The dataset is divided into 1000 words per document. The\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.03728",
    "question": "By how much do they improve on domain classification?",
    "gold_answer": "",
    "answer_a": "F-1 score was improved by 1.19 percent points.. Explanation: FLOAT SELECTED: Table 1. Evaluation results on various metrics (%). pseudo, neg feed, and self dist denote using derived pseudo labels, negative feedback, and self-distillation, respectively.",
    "answer_b": "What is the performance improvement?\n<blockquote>Our results show that the proposed approach significantly improves the performance of domain classification with hypothesis reranking. The average F1 score of the proposed approach is 0.765 (\u00b10.001) compared to 0.735 (\u00b10.001) for the baseline approach. The average F1 score of the proposed approach is 0.765 (\u00b10.001) compared to 0.735 (\u00b10.001) for the baseline approach. The average F1 score of the proposed approach is",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much do they improve on domain classification?\n\nGround Truth Answer:\n\n\nAnswer A:\nF-1 score was improved by 1.19 percent points.. Explanation: FLOAT SELECTED: Table 1. Evaluation results on various metrics (%). pseudo, neg feed, and self dist denote using derived pseudo labels, negative feedback, and self-distillation, respectively.\n\nAnswer B:\nWhat is the performance improvement?\n<blockquote>Our results show that the proposed approach significantly improves the performance of domain classification with hypothesis reranking. The average F1 score of the proposed approach is 0.765 (\u00b10.001) compared to 0.735 (\u00b10.001) for the baseline approach. The average F1 score of the proposed approach is 0.765 (\u00b10.001) compared to 0.735 (\u00b10.001) for the baseline approach. The average F1 score of the proposed approach is\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.03728",
    "question": "Which dataset do they evaluate on?",
    "gold_answer": "",
    "answer_a": "10K random utterances from the user log data. Explanation: For the evaluation, we have extracted 10K random utterances from the user log data and independent annotators labeled the top three predictions of all the evaluated models for each utterance so that we can correctly compute nDCG at rank position 3.",
    "answer_b": "How do they evaluate on user utterances?\n< Explanation: We use the dataset of the Intelligent Conversational System (ICS) 1.0. The dataset consists of 1000 utterances and 1000 ground-truth domains. The utterances are randomly divided into 1000 training utterances and 1000 test utterances. The test utterances are used to evaluate the performance of the model. The ground-truth domains are randomly selected from the 1000 utterances. The test utterances are annotated with the predicted domains. The predicted domains are used to evaluate the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dataset do they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\n10K random utterances from the user log data. Explanation: For the evaluation, we have extracted 10K random utterances from the user log data and independent annotators labeled the top three predictions of all the evaluated models for each utterance so that we can correctly compute nDCG at rank position 3.\n\nAnswer B:\nHow do they evaluate on user utterances?\n< Explanation: We use the dataset of the Intelligent Conversational System (ICS) 1.0. The dataset consists of 1000 utterances and 1000 ground-truth domains. The utterances are randomly divided into 1000 training utterances and 1000 test utterances. The test utterances are used to evaluate the performance of the model. The ground-truth domains are randomly selected from the 1000 utterances. The test utterances are annotated with the predicted domains. The predicted domains are used to evaluate the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.03728",
    "question": "How do they decide by how much to decrease confidences of incorrectly predicted domains?",
    "gold_answer": "",
    "answer_a": "The confidence of the incorrectly predicted domain is decreased only when it is highest among all predictions.. Explanation: We find that it is more effective to explicitly demote the prediction confidences of the domains resulted in negative responses if they are top ranked. It is formulated as a loss function:\n\nwhere $j$ denotes the index corresponding to the negative ground-truth domain. We demote the confidences of the negative ground-truths only when they are the highest so that the influence of using the negative ground-truths is not overwhelming.",
    "answer_b": "What is the rationale behind this?\n<sep><br>We first compute the mean confidence of the predicted domains for each utterance. Then, we compute the mean confidence of the predicted domains for each utterance and subtract the mean confidence of the predicted domains for the same utterance. The rationale behind this is that the predicted domain with the highest confidence is the one that is most likely to be the correct one. The rationale behind this is that the predicted domain with the highest confidence is the one that is most likely to be the correct one. The rationale behind this is that the predicted domain with the highest confidence is the one that is most",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they decide by how much to decrease confidences of incorrectly predicted domains?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe confidence of the incorrectly predicted domain is decreased only when it is highest among all predictions.. Explanation: We find that it is more effective to explicitly demote the prediction confidences of the domains resulted in negative responses if they are top ranked. It is formulated as a loss function:\n\nwhere $j$ denotes the index corresponding to the negative ground-truth domain. We demote the confidences of the negative ground-truths only when they are the highest so that the influence of using the negative ground-truths is not overwhelming.\n\nAnswer B:\nWhat is the rationale behind this?\n<sep><br>We first compute the mean confidence of the predicted domains for each utterance. Then, we compute the mean confidence of the predicted domains for each utterance and subtract the mean confidence of the predicted domains for the same utterance. The rationale behind this is that the predicted domain with the highest confidence is the one that is most likely to be the correct one. The rationale behind this is that the predicted domain with the highest confidence is the one that is most likely to be the correct one. The rationale behind this is that the predicted domain with the highest confidence is the one that is most\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.00342",
    "question": "What potential applications are demonstrated?",
    "gold_answer": "",
    "answer_a": "What are the limitations of the dataset?\n<sep><xml>\n<object name=\"application\" type=\"application/x-www-form-urlencoded\">\n<property name=\"type\" value=\"text/plain\">The dataset is available at <a href=\"https://github.com/abhi1990/Dialog-Maker-2\">https://github.com/abhi1990/Dialog-Maker-2</a>. The dataset is available at <a href=\"https://github.com/abhi1990/Dialog-Maker-2\">https://github.com/abhi1990/Dialog-Maker-",
    "answer_b": "for a general AI that talks with human beings without making the users feel isolated. Explanation: Since the proposed approach regards the formal semantics and the task domain is not specified, we expect our study to be meaningful for a general AI that talks with human beings without making the users feel isolated.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat potential applications are demonstrated?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the limitations of the dataset?\n<sep><xml>\n<object name=\"application\" type=\"application/x-www-form-urlencoded\">\n<property name=\"type\" value=\"text/plain\">The dataset is available at <a href=\"https://github.com/abhi1990/Dialog-Maker-2\">https://github.com/abhi1990/Dialog-Maker-2</a>. The dataset is available at <a href=\"https://github.com/abhi1990/Dialog-Maker-2\">https://github.com/abhi1990/Dialog-Maker-\n\nAnswer B:\nfor a general AI that talks with human beings without making the users feel isolated. Explanation: Since the proposed approach regards the formal semantics and the task domain is not specified, we expect our study to be meaningful for a general AI that talks with human beings without making the users feel isolated.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.00342",
    "question": "What method is proposed to mitigate class imbalance in final dataset?",
    "gold_answer": "",
    "answer_a": "we annotate an existing corpus and then augment the dataset. Explanation: For the automation, we annotate an existing corpus and then augment the dataset to mitigate class imbalance, demonstrating the flexibility, practicality, and extensibility of the proposed methods.",
    "answer_b": "What is the size of the dataset?\n<sep> We propose a method for mitigating class imbalance in the final dataset. We use the majority class to train a classifier and then use the trained model to predict the minority class. The classifier is trained on the majority class and the minority class is predicted by the trained model. The classifier is trained on the majority class and the minority class is predicted by the trained model. The classifier is trained on the majority class and the minority class is predicted by the trained model. The classifier is trained on the majority class and the minority class is predicted by the trained model. The classifier is trained on the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat method is proposed to mitigate class imbalance in final dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nwe annotate an existing corpus and then augment the dataset. Explanation: For the automation, we annotate an existing corpus and then augment the dataset to mitigate class imbalance, demonstrating the flexibility, practicality, and extensibility of the proposed methods.\n\nAnswer B:\nWhat is the size of the dataset?\n<sep> We propose a method for mitigating class imbalance in the final dataset. We use the majority class to train a classifier and then use the trained model to predict the minority class. The classifier is trained on the majority class and the minority class is predicted by the trained model. The classifier is trained on the majority class and the minority class is predicted by the trained model. The classifier is trained on the majority class and the minority class is predicted by the trained model. The classifier is trained on the majority class and the minority class is predicted by the trained model. The classifier is trained on the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.01860",
    "question": "What are remaining challenges in VQA?",
    "gold_answer": "",
    "answer_a": "develop better deep learning models. Explanation: The performance of even best model is still lagging and around 60-70% only. Thus, it is still an open problem to develop better deep learning models as well as more challenging datasets for VQA.",
    "answer_b": "What are the future research directions?\n<sep><EXPLANATION> The VQA Challenge 2017 winner model was trained on the VQA-2017 dataset. The challenge was to improve the performance of the model on the VQA-2017 dataset. The challenge was won by the model trained on the VQA-2017 dataset. The challenge was won by the model trained on the VQA-2017 dataset. The challenge was won by the model trained on the VQA-2017 dataset. The challenge was won by the model trained on the VQA",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are remaining challenges in VQA?\n\nGround Truth Answer:\n\n\nAnswer A:\ndevelop better deep learning models. Explanation: The performance of even best model is still lagging and around 60-70% only. Thus, it is still an open problem to develop better deep learning models as well as more challenging datasets for VQA.\n\nAnswer B:\nWhat are the future research directions?\n<sep><EXPLANATION> The VQA Challenge 2017 winner model was trained on the VQA-2017 dataset. The challenge was to improve the performance of the model on the VQA-2017 dataset. The challenge was won by the model trained on the VQA-2017 dataset. The challenge was won by the model trained on the VQA-2017 dataset. The challenge was won by the model trained on the VQA-2017 dataset. The challenge was won by the model trained on the VQA\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.01860",
    "question": "What are the new deep learning models discussed in the paper?  ",
    "gold_answer": "",
    "answer_a": "What are the results of the vanilla VQA models?\n < Explanation: We present the following new deep learning models: (i) Stacked Attention Network (SAN) (ii) VQA Challenge 2017 winner model (VQA-2017) (iii) VQA-2017 baseline model (VQA-2017-Baseline) (iv) VQA-2017 baseline model with attention (VQA-2017-Att) (v) VQA-2017 baseline model with attention and attention-weighted attention (VQA-20",
    "answer_b": "Vanilla VQA. Explanation: The emergence of deep-learning architectures have led to the development of the VQA systems. We discuss the state-of-the-art methods with an overview in Table TABREF6.\n\nVanilla VQA BIBREF0: Considered as a benchmark for deep learning methods, the vanilla VQA model uses CNN for feature extraction and LSTM or Recurrent networks for language processing. These features are combined using element-wise operations to a common feature, which is used to classify to one of the answers as shown in Fig. FIGREF4.\n\nStacked Attention Networks BIBREF11: This model introduced the attention using the softmax output of the intermediate question feature. The attention between the features are stacked which helps the model to focus on the important portion of the image.\n\nTeney et al. Model BIBREF13: Teney et al. introduced the use of object detection on VQA models and won the VQA Challenge 2017. The model helps in narrowing down the features and apply better attention to images. The model employs the use of R-CNN architecture and showed significant performance in accuracy over other architectures.\n\nNeural-Symbolic VQA BIBREF23: Specifically made for CLEVR dataset, this model leverages the question formation and image generation strategy of CLEVR. The images are converted to structured features and the question features are converted to their original root question strategy. This feature is used to filter out the required answer.\n\nFocal Visual Text Attention (FVTA) BIBREF24: This model combines the sequence of image features generated by the network, text features of the image (or probable answers) and the question. It applies the attention based on the both text components, and finally classifies the features to answer the question. This model is better suited for the VQA in videos which has more use cases than images.\n\nPythia v1.0 BIBREF27: Pythia v1.0 is the award winning architecture for VQA Challenge 2018. The architecture is similar to Teney et al. BIBREF13 with reduced computations with element-wise multiplication, use of GloVe vectors BIBREF22, and ensemble of 30 models.\n\nDifferential Networks BIBREF19: This model uses the differences between forward propagation steps to reduce the noise and to learn the interdependency between features. Image features are extracted using Faster-RCNN BIBREF21. The differential modules BIBREF29 are used to refine the features in both text and images. GRU BIBREF30 is used for question feature extraction. Finally, it is combined with an attention module to classify the answers. The Differential Networks architecture is illustrated in Fig. FIGREF5.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the new deep learning models discussed in the paper?  \n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results of the vanilla VQA models?\n < Explanation: We present the following new deep learning models: (i) Stacked Attention Network (SAN) (ii) VQA Challenge 2017 winner model (VQA-2017) (iii) VQA-2017 baseline model (VQA-2017-Baseline) (iv) VQA-2017 baseline model with attention (VQA-2017-Att) (v) VQA-2017 baseline model with attention and attention-weighted attention (VQA-20\n\nAnswer B:\nVanilla VQA. Explanation: The emergence of deep-learning architectures have led to the development of the VQA systems. We discuss the state-of-the-art methods with an overview in Table TABREF6.\n\nVanilla VQA BIBREF0: Considered as a benchmark for deep learning methods, the vanilla VQA model uses CNN for feature extraction and LSTM or Recurrent networks for language processing. These features are combined using element-wise operations to a common feature, which is used to classify to one of the answers as shown in Fig. FIGREF4.\n\nStacked Attention Networks BIBREF11: This model introduced the attention using the softmax output of the intermediate question feature. The attention between the features are stacked which helps the model to focus on the important portion of the image.\n\nTeney et al. Model BIBREF13: Teney et al. introduced the use of object detection on VQA models and won the VQA Challenge 2017. The model helps in narrowing down the features and apply better attention to images. The model employs the use of R-CNN architecture and showed significant performance in accuracy over other architectures.\n\nNeural-Symbolic VQA BIBREF23: Specifically made for CLEVR dataset, this model leverages the question formation and image generation strategy of CLEVR. The images are converted to structured features and the question features are converted to their original root question strategy. This feature is used to filter out the required answer.\n\nFocal Visual Text Attention (FVTA) BIBREF24: This model combines the sequence of image features generated by the network, text features of the image (or probable answers) and the question. It applies the attention based on the both text components, and finally classifies the features to answer the question. This model is better suited for the VQA in videos which has more use cases than images.\n\nPythia v1.0 BIBREF27: Pythia v1.0 is the award winning architecture for VQA Challenge 2018. The architecture is similar to Teney et al. BIBREF13 with reduced computations with element-wise multiplication, use of GloVe vectors BIBREF22, and ensemble of 30 models.\n\nDifferential Networks BIBREF19: This model uses the differences between forward propagation steps to reduce the noise and to learn the interdependency between features. Image features are extracted using Faster-RCNN BIBREF21. The differential modules BIBREF29 are used to refine the features in both text and images. GRU BIBREF30 is used for question feature extraction. Finally, it is combined with an attention module to classify the answers. The Differential Networks architecture is illustrated in Fig. FIGREF5.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.01860",
    "question": "What was the architecture of the 2017 Challenge Winner model?",
    "gold_answer": "",
    "answer_a": "What was the performance of the model?\n<sep>Answer: The VQA Challenge 2017 winner model is a 12-layer VQA model with 12 convolutional layers and 12 fully connected layers. The model is trained using a 10-fold cross validation. The model is trained using the VQA Challenge 2017 dataset. The model is trained using the VQA Challenge 2017 dataset. The model is trained using the VQA Challenge 2017 dataset. The model is trained using the VQA Challenge 2017 dataset. The model is",
    "answer_b": "Region-based CNN. Explanation: Teney et al. Model BIBREF13: Teney et al. introduced the use of object detection on VQA models and won the VQA Challenge 2017. The model helps in narrowing down the features and apply better attention to images. The model employs the use of R-CNN architecture and showed significant performance in accuracy over other architectures.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat was the architecture of the 2017 Challenge Winner model?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat was the performance of the model?\n<sep>Answer: The VQA Challenge 2017 winner model is a 12-layer VQA model with 12 convolutional layers and 12 fully connected layers. The model is trained using a 10-fold cross validation. The model is trained using the VQA Challenge 2017 dataset. The model is trained using the VQA Challenge 2017 dataset. The model is trained using the VQA Challenge 2017 dataset. The model is trained using the VQA Challenge 2017 dataset. The model is\n\nAnswer B:\nRegion-based CNN. Explanation: Teney et al. Model BIBREF13: Teney et al. introduced the use of object detection on VQA models and won the VQA Challenge 2017. The model helps in narrowing down the features and apply better attention to images. The model employs the use of R-CNN architecture and showed significant performance in accuracy over other architectures.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.01860",
    "question": "What is an example of a common sense question?",
    "gold_answer": "",
    "answer_a": "How many giraffes are drinking water?. Explanation: FLOAT SELECTED: TABLE I OVERVIEW OF VQA DATASETS DESCRIBED IN THIS PAPER.",
    "answer_b": "What are the types of question-formats?\nOutput: The VQA task combines challenges for processing data with both Visual and Linguistic processing, to answer basic `common sense' questions about given images. Given an image and a question in natural language, the VQA system tries to find the correct answer to it using visual elements of the image and inference gathered from textual questions. In this survey, we cover and discuss the recent datasets released in the VQA domain dealing with various types of question-formats and enabling robustness of the machine-learning models. Next, we discuss about new deep learning models that have shown promising results over the VQA",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is an example of a common sense question?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow many giraffes are drinking water?. Explanation: FLOAT SELECTED: TABLE I OVERVIEW OF VQA DATASETS DESCRIBED IN THIS PAPER.\n\nAnswer B:\nWhat are the types of question-formats?\nOutput: The VQA task combines challenges for processing data with both Visual and Linguistic processing, to answer basic `common sense' questions about given images. Given an image and a question in natural language, the VQA system tries to find the correct answer to it using visual elements of the image and inference gathered from textual questions. In this survey, we cover and discuss the recent datasets released in the VQA domain dealing with various types of question-formats and enabling robustness of the machine-learning models. Next, we discuss about new deep learning models that have shown promising results over the VQA\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.08788",
    "question": "What pretrained language representations are used?",
    "gold_answer": "",
    "answer_a": "How are they selected?\n< Explanation: We use the pretrained language representations from the BERT model BIBREF10 . The BERT model is trained on the 100M Penn Treebank sentences and is available at this https URL . We use the pretrained language representations from the BERT model BIBREF10 . The BERT model is trained on the 100M Penn Treebank sentences and is available at this https URL . We use the pretrained language representations from the BERT model BIBREF10 . The BERT model is trained on the 100M Penn Treebank sentences and is",
    "answer_b": "GloVe. Explanation: GloVe is the method with pretrained GloVe BIBREF22 word embeddings; w/o pretrain is our method without pre-trained embeddings (random initialization).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat pretrained language representations are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they selected?\n< Explanation: We use the pretrained language representations from the BERT model BIBREF10 . The BERT model is trained on the 100M Penn Treebank sentences and is available at this https URL . We use the pretrained language representations from the BERT model BIBREF10 . The BERT model is trained on the 100M Penn Treebank sentences and is available at this https URL . We use the pretrained language representations from the BERT model BIBREF10 . The BERT model is trained on the 100M Penn Treebank sentences and is\n\nAnswer B:\nGloVe. Explanation: GloVe is the method with pretrained GloVe BIBREF22 word embeddings; w/o pretrain is our method without pre-trained embeddings (random initialization).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.08788",
    "question": "What tasks are explored?",
    "gold_answer": "",
    "answer_a": "How are the few-shot learning models evaluated?\n< Explanation: We evaluate our proposed few-shot learning model on the Sentiment Classification dataset. The dataset consists of 10000 samples, each containing 10000 words. The dataset is divided into 10000 samples, each containing 10000 words. The first 10000 samples are used for training, and the remaining 10000 samples are used for testing. The dataset is divided into 10000 samples, each containing 10000 words. The first 1",
    "answer_b": "69 tasks. Explanation: We use the multiple tasks with the multi-domain sentiment classification BIBREF19 dataset ARSC. This dataset comprises English reviews for 23 types of products on Amazon. For each product domain, there are three different binary classification tasks. These buckets then form 23 INLINEFORM0 3 = 69 tasks in total. Following BIBREF20 , we select 12 (4 INLINEFORM1 3) tasks from four domains (i.e., Books, DVDs, Electronics, and Kitchen) as the test set, with only five examples as support set for each label in the test set.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat tasks are explored?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are the few-shot learning models evaluated?\n< Explanation: We evaluate our proposed few-shot learning model on the Sentiment Classification dataset. The dataset consists of 10000 samples, each containing 10000 words. The dataset is divided into 10000 samples, each containing 10000 words. The first 10000 samples are used for training, and the remaining 10000 samples are used for testing. The dataset is divided into 10000 samples, each containing 10000 words. The first 1\n\nAnswer B:\n69 tasks. Explanation: We use the multiple tasks with the multi-domain sentiment classification BIBREF19 dataset ARSC. This dataset comprises English reviews for 23 types of products on Amazon. For each product domain, there are three different binary classification tasks. These buckets then form 23 INLINEFORM0 3 = 69 tasks in total. Following BIBREF20 , we select 12 (4 INLINEFORM1 3) tasks from four domains (i.e., Books, DVDs, Electronics, and Kitchen) as the test set, with only five examples as support set for each label in the test set.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.02155",
    "question": "How is the training time compared to the original position encoding? ",
    "gold_answer": "",
    "answer_a": "What is the difference between the two methods?\n< Explanation: We compare our method to the original position encoding and to the position-aware attention mechanism (PAAM) of Vaswani et al. (2017). We use the same training data and the same evaluation data. We use the same hyperparameters for both methods. We use the same number of training and evaluation batches. We use the same number of epochs. We use the same number of iterations per epoch. We use the same number of iterations per batch. We use the same number of iterations per epoch. We use the same number of iterations per batch. We use",
    "answer_b": "7% decrease in steps per second. Explanation: For our machine translation experiments, the result was a modest 7% decrease in steps per second, but we were able to maintain the same model and batch sizes on P100 GPUs as Vaswani et al. vaswani2017.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is the training time compared to the original position encoding? \n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the two methods?\n< Explanation: We compare our method to the original position encoding and to the position-aware attention mechanism (PAAM) of Vaswani et al. (2017). We use the same training data and the same evaluation data. We use the same hyperparameters for both methods. We use the same number of training and evaluation batches. We use the same number of epochs. We use the same number of iterations per epoch. We use the same number of iterations per batch. We use the same number of iterations per epoch. We use the same number of iterations per batch. We use\n\nAnswer B:\n7% decrease in steps per second. Explanation: For our machine translation experiments, the result was a modest 7% decrease in steps per second, but we were able to maintain the same model and batch sizes on P100 GPUs as Vaswani et al. vaswani2017.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.02155",
    "question": "Can the new position representation be generalized to other tasks?",
    "gold_answer": "",
    "answer_a": "Not sure. Explanation: Including relative position representations solely when determining compatibility between elements may be sufficient, but further work is needed to determine whether this is true for other tasks.",
    "answer_b": "If so, what are they?\n< Explanation: We first introduce a new position representation that is based on the attention mechanism. We then show that this representation can be generalized to other tasks, such as image captioning and question answering. We also show that the new representation can be used to improve the performance of the Transformer model on the WMT 2014 English-to-German and English-to-French translation tasks.\n\nWe first introduce a new position representation that is based on the attention mechanism. We denote the input sequence as $x \\in \\mathbb{R}^{n \\times d}$, where $n$",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nCan the new position representation be generalized to other tasks?\n\nGround Truth Answer:\n\n\nAnswer A:\nNot sure. Explanation: Including relative position representations solely when determining compatibility between elements may be sufficient, but further work is needed to determine whether this is true for other tasks.\n\nAnswer B:\nIf so, what are they?\n< Explanation: We first introduce a new position representation that is based on the attention mechanism. We then show that this representation can be generalized to other tasks, such as image captioning and question answering. We also show that the new representation can be used to improve the performance of the Transformer model on the WMT 2014 English-to-German and English-to-French translation tasks.\n\nWe first introduce a new position representation that is based on the attention mechanism. We denote the input sequence as $x \\in \\mathbb{R}^{n \\times d}$, where $n$\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.06745",
    "question": "which social media platforms was the data collected from?",
    "gold_answer": "",
    "answer_a": "Explanation: The data was collected from Twitter, Facebook, Instagram, and YouTube. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data",
    "answer_b": "Twitter. Explanation: Data collection is a time consuming and tedious task in terms of human resource. Two code-mixed data pairs HI-EN and BN-EN are provided for developing sentiment analysis systems. The Twitter4j API was used to collect both Bengali and Hindi code-mixed data from Twitter",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhich social media platforms was the data collected from?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The data was collected from Twitter, Facebook, Instagram, and YouTube. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data was collected from the different social media platforms. The data\n\nAnswer B:\nTwitter. Explanation: Data collection is a time consuming and tedious task in terms of human resource. Two code-mixed data pairs HI-EN and BN-EN are provided for developing sentiment analysis systems. The Twitter4j API was used to collect both Bengali and Hindi code-mixed data from Twitter\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.06745",
    "question": "how many data pairs were there for each dataset?",
    "gold_answer": "",
    "answer_a": "Explanation: The dataset consists of 1000 pairs of Hindi-English and Bengali-English. The data pairs are collected from the different social media platforms. The data pairs are collected from 10 different social media platforms. The data pairs are collected from 10 different social media platforms. The data pairs are collected from 10 different social media platforms. The data pairs are collected from 10 different social media platforms. The data pairs are collected from 10 different social media platforms. The data pairs are collected from 10 different social media platforms. The data pairs are collected from 10 different",
    "answer_b": "18461 for Hindi-English and 5538 for Bengali-English. Explanation: Data collection is a time consuming and tedious task in terms of human resource. Two code-mixed data pairs HI-EN and BN-EN are provided for developing sentiment analysis systems. The Twitter4j API was used to collect both Bengali and Hindi code-mixed data from Twitter. Initially, common Bengali and Hindi words were collected and then searched using the above API. The collected words are mostly sentiment words in Romanized format. Plenty of tweets had noisy words such as words from other languages and words in utf-8 format.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow many data pairs were there for each dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The dataset consists of 1000 pairs of Hindi-English and Bengali-English. The data pairs are collected from the different social media platforms. The data pairs are collected from 10 different social media platforms. The data pairs are collected from 10 different social media platforms. The data pairs are collected from 10 different social media platforms. The data pairs are collected from 10 different social media platforms. The data pairs are collected from 10 different social media platforms. The data pairs are collected from 10 different social media platforms. The data pairs are collected from 10 different\n\nAnswer B:\n18461 for Hindi-English and 5538 for Bengali-English. Explanation: Data collection is a time consuming and tedious task in terms of human resource. Two code-mixed data pairs HI-EN and BN-EN are provided for developing sentiment analysis systems. The Twitter4j API was used to collect both Bengali and Hindi code-mixed data from Twitter. Initially, common Bengali and Hindi words were collected and then searched using the above API. The collected words are mostly sentiment words in Romanized format. Plenty of tweets had noisy words such as words from other languages and words in utf-8 format.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.06745",
    "question": "how many systems were there?",
    "gold_answer": "",
    "answer_a": "Six. Explanation: Six teams have submitted their system details and those are described below in order of decreasing f-score.",
    "answer_b": "Explanation: The proposed system consists of two parts. The first part is a code-switching detection system which is trained on the Hindi-English and Bengali-English data pairs. The second part is a sentiment analysis system which is trained on the Hindi-English data pair. The proposed system is trained on the Hindi-English and Bengali-English data pairs. The proposed system is trained on the Hindi-English data pair. The proposed system is trained on the Hindi-English data pair. The proposed system is trained on the Hindi-English data pair. The proposed system is trained on the Hindi-English data pair. The proposed system",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow many systems were there?\n\nGround Truth Answer:\n\n\nAnswer A:\nSix. Explanation: Six teams have submitted their system details and those are described below in order of decreasing f-score.\n\nAnswer B:\nExplanation: The proposed system consists of two parts. The first part is a code-switching detection system which is trained on the Hindi-English and Bengali-English data pairs. The second part is a sentiment analysis system which is trained on the Hindi-English data pair. The proposed system is trained on the Hindi-English and Bengali-English data pairs. The proposed system is trained on the Hindi-English data pair. The proposed system is trained on the Hindi-English data pair. The proposed system is trained on the Hindi-English data pair. The proposed system is trained on the Hindi-English data pair. The proposed system\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.06745",
    "question": "what was the baseline?",
    "gold_answer": "",
    "answer_a": "Random labeling. Explanation: The baseline systems are developed by randomly assigning any of the sentiment values to each of the test instances. Then, similar evaluation techniques are applied to the baseline system and the results are presented in Table TABREF29 .",
    "answer_b": "Explanation: The baseline is the system that is used as a reference for the system to be evaluated. The baseline is the system that is used as a reference for the system to be evaluated. The baseline is the system that is used as a reference for the system to be evaluated. The baseline is the system that is used as a reference for the system to be evaluated. The baseline is the system that is used as a reference for the system to be evaluated. The baseline is the system that is used as a reference for the system to be evaluated. The baseline is the system that is used as a reference for the system to be evaluated.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat was the baseline?\n\nGround Truth Answer:\n\n\nAnswer A:\nRandom labeling. Explanation: The baseline systems are developed by randomly assigning any of the sentiment values to each of the test instances. Then, similar evaluation techniques are applied to the baseline system and the results are presented in Table TABREF29 .\n\nAnswer B:\nExplanation: The baseline is the system that is used as a reference for the system to be evaluated. The baseline is the system that is used as a reference for the system to be evaluated. The baseline is the system that is used as a reference for the system to be evaluated. The baseline is the system that is used as a reference for the system to be evaluated. The baseline is the system that is used as a reference for the system to be evaluated. The baseline is the system that is used as a reference for the system to be evaluated. The baseline is the system that is used as a reference for the system to be evaluated.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.06745",
    "question": "what metrics did they use for evaluation?",
    "gold_answer": "",
    "answer_a": "precision, recall and f-score. Explanation: The precision, recall and f-score are calculated using the sklearn package of scikit-learn BIBREF15 . The macro average f-score is used to rank the submitted systems, because it independently calculates the metric for each classes and then takes the average hence treating all classes equally.",
    "answer_b": "Explanation: The evaluation metrics used are F1 score, Precision, Recall and ROC curve. The F1 score is the harmonic mean of precision and recall. The precision is the ratio of true positive and true positive divided by the sum of true positive and false positive. The recall is the ratio of true positive and true positive divided by the sum of true positive and false negative. The ROC curve is a plot of the true positive rate against the false positive rate. The ROC curve is used to evaluate the performance of a classifier. The higher the area under the curve, the better the performance of the classifier. The ROC curve is used to evaluate",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat metrics did they use for evaluation?\n\nGround Truth Answer:\n\n\nAnswer A:\nprecision, recall and f-score. Explanation: The precision, recall and f-score are calculated using the sklearn package of scikit-learn BIBREF15 . The macro average f-score is used to rank the submitted systems, because it independently calculates the metric for each classes and then takes the average hence treating all classes equally.\n\nAnswer B:\nExplanation: The evaluation metrics used are F1 score, Precision, Recall and ROC curve. The F1 score is the harmonic mean of precision and recall. The precision is the ratio of true positive and true positive divided by the sum of true positive and false positive. The recall is the ratio of true positive and true positive divided by the sum of true positive and false negative. The ROC curve is a plot of the true positive rate against the false positive rate. The ROC curve is used to evaluate the performance of a classifier. The higher the area under the curve, the better the performance of the classifier. The ROC curve is used to evaluate\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.06745",
    "question": "what datasets did they use?",
    "gold_answer": "",
    "answer_a": "Bengali-English and Hindi-English. Explanation: Data collection is a time consuming and tedious task in terms of human resource. Two code-mixed data pairs HI-EN and BN-EN are provided for developing sentiment analysis systems. The Twitter4j API was used to collect both Bengali and Hindi code-mixed data from Twitter. Initially, common Bengali and Hindi words were collected and then searched using the above API. The collected words are mostly sentiment words in Romanized format. Plenty of tweets had noisy words such as words from other languages and words in utf-8 format. After collection of code-mixed tweets, some were rejected. There are three reasons for which a tweet was rejected.",
    "answer_b": "Explanation: The dataset used in this paper is from the Twitter platform. The dataset contains 1000 tweets from 1000 users. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat datasets did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nBengali-English and Hindi-English. Explanation: Data collection is a time consuming and tedious task in terms of human resource. Two code-mixed data pairs HI-EN and BN-EN are provided for developing sentiment analysis systems. The Twitter4j API was used to collect both Bengali and Hindi code-mixed data from Twitter. Initially, common Bengali and Hindi words were collected and then searched using the above API. The collected words are mostly sentiment words in Romanized format. Plenty of tweets had noisy words such as words from other languages and words in utf-8 format. After collection of code-mixed tweets, some were rejected. There are three reasons for which a tweet was rejected.\n\nAnswer B:\nExplanation: The dataset used in this paper is from the Twitter platform. The dataset contains 1000 tweets from 1000 users. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter platform. The tweets are collected from the Twitter\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.01248",
    "question": "What is the Semantic Web?",
    "gold_answer": "",
    "answer_a": "aims to make information available on the Web easier to process for machines and humans. Explanation: The SW aims to make information available on the Web easier to process for machines and humans.",
    "answer_b": "How is it structured?\n<blockquote>The Semantic Web is a set of technologies and protocols that enable the exchange of information between Web resources. It is a complex and dynamic system that is composed of three main components: RDF, OWL, and SPARQL. These components are used to describe the structure of Web resources, to describe the semantics of Web resources, and to describe the relationships between Web resources. </blockquote>\nAnswer: The Semantic Web is a set of technologies and protocols that enable the exchange of information between Web resources. It is a complex and dynamic system that is composed of three main components: RDF, OWL, and",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the Semantic Web?\n\nGround Truth Answer:\n\n\nAnswer A:\naims to make information available on the Web easier to process for machines and humans. Explanation: The SW aims to make information available on the Web easier to process for machines and humans.\n\nAnswer B:\nHow is it structured?\n<blockquote>The Semantic Web is a set of technologies and protocols that enable the exchange of information between Web resources. It is a complex and dynamic system that is composed of three main components: RDF, OWL, and SPARQL. These components are used to describe the structure of Web resources, to describe the semantics of Web resources, and to describe the relationships between Web resources. </blockquote>\nAnswer: The Semantic Web is a set of technologies and protocols that enable the exchange of information between Web resources. It is a complex and dynamic system that is composed of three main components: RDF, OWL, and\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.06502",
    "question": "How many tags are included in the ENE tag set?",
    "gold_answer": "",
    "answer_a": "141. Explanation: The collected dataset contained 120,333 Japanese Wikipedia articles in different areas, covering 141 out of 200 ENE labels.",
    "answer_b": "What are the best models available for ENE tag set classification?\n<sep>Answer: The ENE tag set contains 1000 tags. The best models for ENE tag set classification are the following: 1) BIBREF10, 2) BIBREF11, 3) BIBREF12, 4) BIBREF13, 5) BIBREF14, 6) BIBREF15, 7) BIBREF16, 8) BIBREF17, 9) BIBREF18, 10",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many tags are included in the ENE tag set?\n\nGround Truth Answer:\n\n\nAnswer A:\n141. Explanation: The collected dataset contained 120,333 Japanese Wikipedia articles in different areas, covering 141 out of 200 ENE labels.\n\nAnswer B:\nWhat are the best models available for ENE tag set classification?\n<sep>Answer: The ENE tag set contains 1000 tags. The best models for ENE tag set classification are the following: 1) BIBREF10, 2) BIBREF11, 3) BIBREF12, 4) BIBREF13, 5) BIBREF14, 6) BIBREF15, 7) BIBREF16, 8) BIBREF17, 9) BIBREF18, 10\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.05295",
    "question": "What are the most discriminating patterns which are analyzed?",
    "gold_answer": "",
    "answer_a": "Patterns containing the preposition \"of\" make up the vast majority of prepositional phrases for both the fact and feel classes and  patterns with the preposition \u201cfor\u201d are substantially more common in the feel class than the fact class.. Explanation: Patterns containing the preposition \"of\" make up the vast majority of prepositional phrases for both the fact and feel classes, but is more common in the fact class. In contrast, we observe that patterns with the preposition \u201cfor\u201d are substantially more common in the feel class than the fact class.",
    "answer_b": "Explanation: We find that the most discriminating patterns are those that are related to the use of the word \"however\" and the use of the word \"however\" followed by a \"however\" (see Table 1). We also find that the most discriminating patterns are those that are related to the use of the word \"however\" and the use of the word \"however\" followed by a \"however\" (see Table 1). We also find that the most discriminating patterns are those that are related to the use of the word \"however\" and the use of the word \"however\" followed by a \"however\" (",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the most discriminating patterns which are analyzed?\n\nGround Truth Answer:\n\n\nAnswer A:\nPatterns containing the preposition \"of\" make up the vast majority of prepositional phrases for both the fact and feel classes and  patterns with the preposition \u201cfor\u201d are substantially more common in the feel class than the fact class.. Explanation: Patterns containing the preposition \"of\" make up the vast majority of prepositional phrases for both the fact and feel classes, but is more common in the fact class. In contrast, we observe that patterns with the preposition \u201cfor\u201d are substantially more common in the feel class than the fact class.\n\nAnswer B:\nExplanation: We find that the most discriminating patterns are those that are related to the use of the word \"however\" and the use of the word \"however\" followed by a \"however\" (see Table 1). We also find that the most discriminating patterns are those that are related to the use of the word \"however\" and the use of the word \"however\" followed by a \"however\" (see Table 1). We also find that the most discriminating patterns are those that are related to the use of the word \"however\" and the use of the word \"however\" followed by a \"however\" (\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.05295",
    "question": "What bootstrapping methodology was used to find new patterns?",
    "gold_answer": "",
    "answer_a": "What is the size of the set of patterns?\n< Explanation: We use a bootstrapping methodology to find new patterns in the annotated set of posts. We randomly select 100 posts from the annotated set and use them to train a classifier to predict whether a post is factual or emotional. We then use the same classifier to predict the factual or emotional nature of 1000 randomly selected posts from the annotated set. We repeat this process 100 times, and use the 1000 randomly selected posts to train a classifier to predict the factual or emotional nature of 1000 posts. We",
    "answer_b": "flow diagram for the bootstrapping system is shown in Figure FIGREF10. Explanation: Since the IAC data set contains a large number of unannotated debate forum posts, we embedd AutoSlog-TS in a bootstrapping framework to learn additional patterns. The flow diagram for the bootstrapping system is shown in Figure FIGREF10 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat bootstrapping methodology was used to find new patterns?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the set of patterns?\n< Explanation: We use a bootstrapping methodology to find new patterns in the annotated set of posts. We randomly select 100 posts from the annotated set and use them to train a classifier to predict whether a post is factual or emotional. We then use the same classifier to predict the factual or emotional nature of 1000 randomly selected posts from the annotated set. We repeat this process 100 times, and use the 1000 randomly selected posts to train a classifier to predict the factual or emotional nature of 1000 posts. We\n\nAnswer B:\nflow diagram for the bootstrapping system is shown in Figure FIGREF10. Explanation: Since the IAC data set contains a large number of unannotated debate forum posts, we embedd AutoSlog-TS in a bootstrapping framework to learn additional patterns. The flow diagram for the bootstrapping system is shown in Figure FIGREF10 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.05295",
    "question": "What patterns were extracted which were correlated with emotional arguments?",
    "gold_answer": "",
    "answer_a": "Explanation: We find that the most discriminative patterns are those that are related to the use of emoticons, and that the most discriminative patterns are those that are related to the use of emoticons. We find that the most discriminative patterns are those that are related to the use of emoticons, and that the most discriminative patterns are those that are related to the use of emoticons. We find that the most discriminative patterns are those that are related to the use of emoticons, and that the most discriminative patterns are those that are related to the use of emoticons. We find that the most discriminative patterns are",
    "answer_b": "Examples of extracted patters with high probability that contain of are: MARRIAGE FOR, STANDING FOR, SAME FOR, TREATMENT FOR, DEMAND FOR, ATTENTION FOR, ADVOCATE FOR, NO EVIDENCE FOR, JUSTIFICATION FOR, EXCUSE FOR. Explanation: Table TABREF20 shows examples of learned NP Prep patterns with the preposition \"of\" in the fact class and \"for\" in the feel class",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat patterns were extracted which were correlated with emotional arguments?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We find that the most discriminative patterns are those that are related to the use of emoticons, and that the most discriminative patterns are those that are related to the use of emoticons. We find that the most discriminative patterns are those that are related to the use of emoticons, and that the most discriminative patterns are those that are related to the use of emoticons. We find that the most discriminative patterns are those that are related to the use of emoticons, and that the most discriminative patterns are those that are related to the use of emoticons. We find that the most discriminative patterns are\n\nAnswer B:\nExamples of extracted patters with high probability that contain of are: MARRIAGE FOR, STANDING FOR, SAME FOR, TREATMENT FOR, DEMAND FOR, ATTENTION FOR, ADVOCATE FOR, NO EVIDENCE FOR, JUSTIFICATION FOR, EXCUSE FOR. Explanation: Table TABREF20 shows examples of learned NP Prep patterns with the preposition \"of\" in the fact class and \"for\" in the feel class\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.05295",
    "question": "What patterns were extracted which were correlated with factual arguments?",
    "gold_answer": "",
    "answer_a": "patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases. Explanation: We observe that patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases.",
    "answer_b": "Explanation: We find that the most discriminative patterns are those that are related to factual arguments. These patterns are related to the use of the following words: \"fact\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"fact",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat patterns were extracted which were correlated with factual arguments?\n\nGround Truth Answer:\n\n\nAnswer A:\npatterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases. Explanation: We observe that patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases.\n\nAnswer B:\nExplanation: We find that the most discriminative patterns are those that are related to factual arguments. These patterns are related to the use of the following words: \"fact\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"factoid\", \"fact\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.05295",
    "question": "How were the factual and feeling forum posts annotated?",
    "gold_answer": "",
    "answer_a": "binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class.. Explanation: For our experiments, we use only the response texts and assign a binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class. We did not use the responses with scores between -1 and 1 because they had a very weak Fact/Feeling assessment, which could be attributed to responses either containing aspects of both factual and feeling expression, or neither.",
    "answer_b": "What were the features used?\n< Explanation: The annotated set of posts was created by a human annotator who used a pre-defined set of features to annotate the posts. The features used were: (1) the number of words in the post, (2) the number of sentences in the post, (3) the number of emoticons in the post, (4) the number of emoticons in the post that were emoticons for factual or emotional language, (5) the number of emoticons in the post that were emoticons for factual or emotional language, (6) the number of emoticons in the post that were",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow were the factual and feeling forum posts annotated?\n\nGround Truth Answer:\n\n\nAnswer A:\nbinary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class.. Explanation: For our experiments, we use only the response texts and assign a binary Fact or Feel label to each response: texts with score INLINEFORM0 1 are assigned to the fact class and texts with score INLINEFORM1 -1 are assigned to the feeling class. We did not use the responses with scores between -1 and 1 because they had a very weak Fact/Feeling assessment, which could be attributed to responses either containing aspects of both factual and feeling expression, or neither.\n\nAnswer B:\nWhat were the features used?\n< Explanation: The annotated set of posts was created by a human annotator who used a pre-defined set of features to annotate the posts. The features used were: (1) the number of words in the post, (2) the number of sentences in the post, (3) the number of emoticons in the post, (4) the number of emoticons in the post that were emoticons for factual or emotional language, (5) the number of emoticons in the post that were emoticons for factual or emotional language, (6) the number of emoticons in the post that were\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.05685",
    "question": "What evaluation metrics did they use?",
    "gold_answer": "",
    "answer_a": "BLEU. Explanation: Section SECREF9 describes the quantitative performance of the models by comparing BLEU scores, while a qualitative analysis is performed in Section SECREF10 by analysing translated sentences as well as attention maps.",
    "answer_b": "How were they evaluated?\n< Explanation: We used BLEU as our evaluation metric. BLEU is a widely used metric for evaluating machine translation systems. It is based on the probability of the translation and the reference translation. The higher the BLEU score, the better the translation. We used the following formula to calculate the BLEU score:\n\nwhere P is the probability of the translation, R is the probability of the reference translation, and E is the number of words in the translation. The higher the BLEU score, the better the translation. We used the following BLEU scores for each language:\n\nThe results obtained show that the models trained",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat evaluation metrics did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nBLEU. Explanation: Section SECREF9 describes the quantitative performance of the models by comparing BLEU scores, while a qualitative analysis is performed in Section SECREF10 by analysing translated sentences as well as attention maps.\n\nAnswer B:\nHow were they evaluated?\n< Explanation: We used BLEU as our evaluation metric. BLEU is a widely used metric for evaluating machine translation systems. It is based on the probability of the translation and the reference translation. The higher the BLEU score, the better the translation. We used the following formula to calculate the BLEU score:\n\nwhere P is the probability of the translation, R is the probability of the reference translation, and E is the number of words in the translation. The higher the BLEU score, the better the translation. We used the following BLEU scores for each language:\n\nThe results obtained show that the models trained\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.05685",
    "question": "What NMT techniques did they explore?",
    "gold_answer": "",
    "answer_a": "ConvS2S. Explanation: We trained translation models for two established NMT architectures for each language, namely, ConvS2S and Transformer.",
    "answer_b": "(Please provide references)\nThe authors used the following NMT techniques: (1) word-level attention, (2) attention to word and sentence, (3) attention to word and sentence, (4) attention to word and sentence, (5) attention to word and sentence, (6) attention to word and sentence, (7) attention to word and sentence, (8) attention to word and sentence, (9) attention to word and sentence, (10) attention to word and sentence, (11) attention to word and sentence, (12) attention to word and sentence, (13) attention",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat NMT techniques did they explore?\n\nGround Truth Answer:\n\n\nAnswer A:\nConvS2S. Explanation: We trained translation models for two established NMT architectures for each language, namely, ConvS2S and Transformer.\n\nAnswer B:\n(Please provide references)\nThe authors used the following NMT techniques: (1) word-level attention, (2) attention to word and sentence, (3) attention to word and sentence, (4) attention to word and sentence, (5) attention to word and sentence, (6) attention to word and sentence, (7) attention to word and sentence, (8) attention to word and sentence, (9) attention to word and sentence, (10) attention to word and sentence, (11) attention to word and sentence, (12) attention to word and sentence, (13) attention\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.05685",
    "question": "What was their best performing model?",
    "gold_answer": "",
    "answer_a": "Transformer. Explanation: In general, the Transformer model outperformed the ConvS2S model for all of the languages, sometimes achieving 10 BLEU points or more over the ConvS2S models.",
    "answer_b": "What was their best performing language?\n< Explanation: We trained a model using the following parameters: 1000 epochs, 100000 words, 1000000 words, 10000000 words, 100000000 words, 1000000000 words, 10000000000 words, 100000000000 words, 1000000000000 words, 1",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat was their best performing model?\n\nGround Truth Answer:\n\n\nAnswer A:\nTransformer. Explanation: In general, the Transformer model outperformed the ConvS2S model for all of the languages, sometimes achieving 10 BLEU points or more over the ConvS2S models.\n\nAnswer B:\nWhat was their best performing language?\n< Explanation: We trained a model using the following parameters: 1000 epochs, 100000 words, 1000000 words, 10000000 words, 100000000 words, 1000000000 words, 10000000000 words, 100000000000 words, 1000000000000 words, 1\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.05685",
    "question": "What datasets did they use?",
    "gold_answer": "",
    "answer_a": "English to Afrikaans, isiZulu, N. Sotho,\nSetswana, and Xitsonga parallel corpora from the Autshumato project. Explanation: The Autshumato project provides parallel corpora for English to Afrikaans, isiZulu, N. Sotho, Setswana, and Xitsonga.",
    "answer_b": "How did they obtain them?\n< Explanation: We used the following datasets: (1) the 2016 South African National Language Survey (SANS) dataset, (2) the 2017 South African National Language Survey (SANS) dataset, (3) the 2018 South African National Language Survey (SANS) dataset, (4) the 2019 South African National Language Survey (SANS) dataset, (5) the 2020 South African National Language Survey (SANS) dataset, (6) the 2021 South African National Language",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nEnglish to Afrikaans, isiZulu, N. Sotho,\nSetswana, and Xitsonga parallel corpora from the Autshumato project. Explanation: The Autshumato project provides parallel corpora for English to Afrikaans, isiZulu, N. Sotho, Setswana, and Xitsonga.\n\nAnswer B:\nHow did they obtain them?\n< Explanation: We used the following datasets: (1) the 2016 South African National Language Survey (SANS) dataset, (2) the 2017 South African National Language Survey (SANS) dataset, (3) the 2018 South African National Language Survey (SANS) dataset, (4) the 2019 South African National Language Survey (SANS) dataset, (5) the 2020 South African National Language Survey (SANS) dataset, (6) the 2021 South African National Language\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1811.00051",
    "question": "Which ontologies did they use?",
    "gold_answer": "",
    "answer_a": "What are the lengths of the texts?\n<blockquote>\n<font size=\"2\">The three ontologies used in this work are: </font>\n<font size=\"3\">1) The Ontology of the Semantic Web (OSW) (OSW) </font>\n<font size=\"3\">2) The Ontology of the Semantic Web (OSW) (OSW) </font>\n<font size=\"3\">3) The Ontology of the Semantic Web (OSW) (OSW) </font>\n</blockquote>\n<blockquote>\n<font size=\"2\">The three ontologies used in this work",
    "answer_b": "We experimented with three owl ontologies: (1) the Wine Ontology, which provides information about wines, wine producers etc.; (2) the Consumer Electronics Ontology, intended to help exchange information about consumer electronics products; and (3) the Disease Ontology, which describes diseases, including their symptoms, causes etc.. Explanation: We experimented with three owl ontologies: (1) the Wine Ontology, which provides information about wines, wine producers etc.; (2) the Consumer Electronics Ontology, intended to help exchange information about consumer electronics products; and (3) the Disease Ontology, which describes diseases, including their symptoms, causes etc.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich ontologies did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the lengths of the texts?\n<blockquote>\n<font size=\"2\">The three ontologies used in this work are: </font>\n<font size=\"3\">1) The Ontology of the Semantic Web (OSW) (OSW) </font>\n<font size=\"3\">2) The Ontology of the Semantic Web (OSW) (OSW) </font>\n<font size=\"3\">3) The Ontology of the Semantic Web (OSW) (OSW) </font>\n</blockquote>\n<blockquote>\n<font size=\"2\">The three ontologies used in this work\n\nAnswer B:\nWe experimented with three owl ontologies: (1) the Wine Ontology, which provides information about wines, wine producers etc.; (2) the Consumer Electronics Ontology, intended to help exchange information about consumer electronics products; and (3) the Disease Ontology, which describes diseases, including their symptoms, causes etc.. Explanation: We experimented with three owl ontologies: (1) the Wine Ontology, which provides information about wines, wine producers etc.; (2) the Consumer Electronics Ontology, intended to help exchange information about consumer electronics products; and (3) the Disease Ontology, which describes diseases, including their symptoms, causes etc.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.05415",
    "question": "how is user satisfaction estimated?",
    "gold_answer": "",
    "answer_a": "Workers chatted with our baseline dialogue agent and assigned a rating 1-5 for the quality of each of the agent's responses. Contexts with rating 1 were mapped to the negative class (dissatisfied) and ratings INLINEFORM0 mapped to the positive class (satisfied).. Explanation: The dataset for this task was collected via crowdsourcing. Workers chatted with our baseline dialogue agent and assigned a rating 1-5 for the quality of each of the agent's responses. Contexts with rating 1 were mapped to the negative class (dissatisfied) and ratings INLINEFORM0 mapped to the positive class (satisfied). Contexts with rating 2 were discarded to increase the separation between classes for a cleaner training set. Note that these numeric ratings were requested only when collecting the initial training data, not during deployment, where only natural dialogue is used.",
    "answer_b": "Is it done by the agent or by the user?\n<sep><solution>User satisfaction is estimated by the agent. The agent estimates the user satisfaction by comparing the user's response to the expected satisfaction. The agent uses a simple model to estimate the user satisfaction, which is based on the user's response to the agent's response. The agent uses a simple model to estimate the user satisfaction, which is based on the user's response to the agent's response. The agent uses a simple model to estimate the user satisfaction, which is based on the user's response to the agent's response. The agent uses a simple model to estimate the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow is user satisfaction estimated?\n\nGround Truth Answer:\n\n\nAnswer A:\nWorkers chatted with our baseline dialogue agent and assigned a rating 1-5 for the quality of each of the agent's responses. Contexts with rating 1 were mapped to the negative class (dissatisfied) and ratings INLINEFORM0 mapped to the positive class (satisfied).. Explanation: The dataset for this task was collected via crowdsourcing. Workers chatted with our baseline dialogue agent and assigned a rating 1-5 for the quality of each of the agent's responses. Contexts with rating 1 were mapped to the negative class (dissatisfied) and ratings INLINEFORM0 mapped to the positive class (satisfied). Contexts with rating 2 were discarded to increase the separation between classes for a cleaner training set. Note that these numeric ratings were requested only when collecting the initial training data, not during deployment, where only natural dialogue is used.\n\nAnswer B:\nIs it done by the agent or by the user?\n<sep><solution>User satisfaction is estimated by the agent. The agent estimates the user satisfaction by comparing the user's response to the expected satisfaction. The agent uses a simple model to estimate the user satisfaction, which is based on the user's response to the agent's response. The agent uses a simple model to estimate the user satisfaction, which is based on the user's response to the agent's response. The agent uses a simple model to estimate the user satisfaction, which is based on the user's response to the agent's response. The agent uses a simple model to estimate the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.05415",
    "question": "by how much did performance improve?",
    "gold_answer": "",
    "answer_a": "an increase of up to 9.4 accuracy points, a 31% improvement.. Explanation: FLOAT SELECTED: Table 6: The accuracy of various models and baselines on the original PERSONACHAT test set.",
    "answer_b": "What was the baseline?\n< Explanation: We compare our model to the baseline of a traditional chatbot that is trained with a single conversation per user. We find that our model significantly outperforms the baseline, with a mean F1 score of 0.735, compared to 0.655 for the baseline. The difference is significant at the 5% level. We also find that the model is significantly better at predicting the feedback that the user will receive, with a mean F1 score of 0.735, compared to 0.655 for the baseline. The difference is significant",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nby how much did performance improve?\n\nGround Truth Answer:\n\n\nAnswer A:\nan increase of up to 9.4 accuracy points, a 31% improvement.. Explanation: FLOAT SELECTED: Table 6: The accuracy of various models and baselines on the original PERSONACHAT test set.\n\nAnswer B:\nWhat was the baseline?\n< Explanation: We compare our model to the baseline of a traditional chatbot that is trained with a single conversation per user. We find that our model significantly outperforms the baseline, with a mean F1 score of 0.735, compared to 0.655 for the baseline. The difference is significant at the 5% level. We also find that the model is significantly better at predicting the feedback that the user will receive, with a mean F1 score of 0.735, compared to 0.655 for the baseline. The difference is significant\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.08969",
    "question": "What datasets do they use in the experiment?",
    "gold_answer": "",
    "answer_a": "How do they evaluate the performance?\n< Explanation: We use the following datasets: (1) the Penn Treebank (PTB) BIBREF10 , (2) the Stanford Sentiment Treebank (SST) BIBREF11 , (3) the Stanford CoreNLP (SCN) BIBREF12 , (4) the Stanford CoreNLP (SCN) BIBREF13 , (5) the Stanford CoreNLP (SCN) BIBREF14 , (6) the Stanford CoreNLP (SCN) BIBREF15 , (7) the",
    "answer_b": "Wall Street Journal(WSJ) portion of Penn Treebank (PTB). Explanation: For classification task, we test our model on 16 classification datasets, the first 14 datasets are product reviews that collected based on the dataset, constructed by BIBREF27 , contains Amazon product reviews from different domains: Books, DVDs, Electronics and Kitchen and so on.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets do they use in the experiment?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate the performance?\n< Explanation: We use the following datasets: (1) the Penn Treebank (PTB) BIBREF10 , (2) the Stanford Sentiment Treebank (SST) BIBREF11 , (3) the Stanford CoreNLP (SCN) BIBREF12 , (4) the Stanford CoreNLP (SCN) BIBREF13 , (5) the Stanford CoreNLP (SCN) BIBREF14 , (6) the Stanford CoreNLP (SCN) BIBREF15 , (7) the\n\nAnswer B:\nWall Street Journal(WSJ) portion of Penn Treebank (PTB). Explanation: For classification task, we test our model on 16 classification datasets, the first 14 datasets are product reviews that collected based on the dataset, constructed by BIBREF27 , contains Amazon product reviews from different domains: Books, DVDs, Electronics and Kitchen and so on.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.08969",
    "question": "What new tasks do they use to show the transferring ability of the shared meta-knowledge?",
    "gold_answer": "",
    "answer_a": "choosing 15 tasks to train our model with multi-task learning, then the learned Meta-LSTM are transferred to the remaining one task. Explanation: To test the transferability of our learned Meta-LSTM, we also design an experiment, in which we take turns choosing 15 tasks to train our model with multi-task learning, then the learned Meta-LSTM are transferred to the remaining one task.",
    "answer_b": "< Explanation: We use two types of tasks: text classification and sequence tagging. In text classification, we use the task-specific task classification task as the target task. In sequence tagging, we use the task-specific task tagging task as the target task. We use the task-specific task classification task as the target task and the task-specific task tagging task as the target task. We use the task-specific task classification task as the target task and the task-specific task tagging task as the target task. We use the task-specific task classification task as the target task and the task-specific task tagging task as the target task. We use the task-specific",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat new tasks do they use to show the transferring ability of the shared meta-knowledge?\n\nGround Truth Answer:\n\n\nAnswer A:\nchoosing 15 tasks to train our model with multi-task learning, then the learned Meta-LSTM are transferred to the remaining one task. Explanation: To test the transferability of our learned Meta-LSTM, we also design an experiment, in which we take turns choosing 15 tasks to train our model with multi-task learning, then the learned Meta-LSTM are transferred to the remaining one task.\n\nAnswer B:\n< Explanation: We use two types of tasks: text classification and sequence tagging. In text classification, we use the task-specific task classification task as the target task. In sequence tagging, we use the task-specific task tagging task as the target task. We use the task-specific task classification task as the target task and the task-specific task tagging task as the target task. We use the task-specific task classification task as the target task and the task-specific task tagging task as the target task. We use the task-specific task classification task as the target task and the task-specific task tagging task as the target task. We use the task-specific\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.08969",
    "question": "What kind of meta learning algorithm do they use?",
    "gold_answer": "",
    "answer_a": "a function-level sharing scheme for multi-task learning. Explanation: In this paper, inspired by recent work on dynamic parameter generation (BIBREF15 , BIBREF16, BIBREF17), we propose a function-level sharing scheme for multi-task learning, in which a shared meta-network is used to learn the meta-knowledge of semantic composition among the different tasks.",
    "answer_b": "What is the off-the-shelf knowledge they transfer?\n< Explanation: We use a meta-learning algorithm to learn the meta-knowledge of semantic composition. The meta-learning algorithm is a meta-learning algorithm that learns the meta-knowledge of semantic composition by using the meta-knowledge of the task-specific models. The meta-learning algorithm is a meta-learning algorithm that learns the meta-knowledge of semantic composition by using the meta-knowledge of the task-specific models. The meta-learning algorithm is a meta-learning algorithm that learns the meta-knowledge of semantic composition by using the meta-knowledge of the task-specific models. The meta-learning algorithm is a meta-learning",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat kind of meta learning algorithm do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\na function-level sharing scheme for multi-task learning. Explanation: In this paper, inspired by recent work on dynamic parameter generation (BIBREF15 , BIBREF16, BIBREF17), we propose a function-level sharing scheme for multi-task learning, in which a shared meta-network is used to learn the meta-knowledge of semantic composition among the different tasks.\n\nAnswer B:\nWhat is the off-the-shelf knowledge they transfer?\n< Explanation: We use a meta-learning algorithm to learn the meta-knowledge of semantic composition. The meta-learning algorithm is a meta-learning algorithm that learns the meta-knowledge of semantic composition by using the meta-knowledge of the task-specific models. The meta-learning algorithm is a meta-learning algorithm that learns the meta-knowledge of semantic composition by using the meta-knowledge of the task-specific models. The meta-learning algorithm is a meta-learning algorithm that learns the meta-knowledge of semantic composition by using the meta-knowledge of the task-specific models. The meta-learning algorithm is a meta-learning\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.07158",
    "question": "what dataset were used?",
    "gold_answer": "",
    "answer_a": "HatEval. Explanation: Experimental Setting\nWe first present the data sets used for the evaluation of the proposed approach, followed by the experimental scenario. The results are presented in Section SECREF5.\n\nExperimental Setting ::: Hate Speech Data Sets\nWe use three data sets related to the hate speech.\n\nExperimental Setting ::: Hate Speech Data Sets ::: 1 - HatEval\ndata set is taken from the SemEval task \"Multilingual detection of hate speech against immigrants and women in Twitter (hatEval)\". The competition was organized for two languages, Spanish and English; we only processed the English data set. The data set consists of 100 tweets labeled as 1 (hate speech) or 0 (not hate speech).\n\nExperimental Setting ::: Hate Speech Data Sets ::: 2 - YouToxic\ndata set is a manually labeled text toxicity data, originally containing 1000 comments crawled from YouTube videos about the Ferguson unrest in 2014. Apart from the main label describing if the comment is hate speech, there are several other labels characterizing each comment, e.g., if it is a threat, provocative, racist, sexist, etc. (not used in our study). There are 138 comments labeled as a hate speech and 862 as non-hate speech. We produced a data set of 300 comments using all 138 hate speech comments and randomly sampled 162 non-hate speech comments.\n\nExperimental Setting ::: Hate Speech Data Sets ::: 3 - OffensiveTweets\ndata set originates in a study regarding hate speech detection and the problem of offensive language BIBREF3. Our data set consists of 3000 tweets. We took 1430 tweets labeled as hate speech and randomly sampled 1670 tweets from the collection of remaining 23353 tweets.",
    "answer_b": "Explanation: We use the Hate Speech Dataset (HS) [1] to evaluate our approach. The dataset contains 10,000 hate speech texts from the Twitter platform. The dataset is divided into two parts: training and testing. The training set contains 5,000 texts, and the testing set contains 5,000 texts. The training set is used to train the model, and the testing set is used to evaluate the model. The dataset contains 10,000 hate speech texts from the Twitter platform. The dataset is divided into two parts: training and testing. The training set",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat dataset were used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHatEval. Explanation: Experimental Setting\nWe first present the data sets used for the evaluation of the proposed approach, followed by the experimental scenario. The results are presented in Section SECREF5.\n\nExperimental Setting ::: Hate Speech Data Sets\nWe use three data sets related to the hate speech.\n\nExperimental Setting ::: Hate Speech Data Sets ::: 1 - HatEval\ndata set is taken from the SemEval task \"Multilingual detection of hate speech against immigrants and women in Twitter (hatEval)\". The competition was organized for two languages, Spanish and English; we only processed the English data set. The data set consists of 100 tweets labeled as 1 (hate speech) or 0 (not hate speech).\n\nExperimental Setting ::: Hate Speech Data Sets ::: 2 - YouToxic\ndata set is a manually labeled text toxicity data, originally containing 1000 comments crawled from YouTube videos about the Ferguson unrest in 2014. Apart from the main label describing if the comment is hate speech, there are several other labels characterizing each comment, e.g., if it is a threat, provocative, racist, sexist, etc. (not used in our study). There are 138 comments labeled as a hate speech and 862 as non-hate speech. We produced a data set of 300 comments using all 138 hate speech comments and randomly sampled 162 non-hate speech comments.\n\nExperimental Setting ::: Hate Speech Data Sets ::: 3 - OffensiveTweets\ndata set originates in a study regarding hate speech detection and the problem of offensive language BIBREF3. Our data set consists of 3000 tweets. We took 1430 tweets labeled as hate speech and randomly sampled 1670 tweets from the collection of remaining 23353 tweets.\n\nAnswer B:\nExplanation: We use the Hate Speech Dataset (HS) [1] to evaluate our approach. The dataset contains 10,000 hate speech texts from the Twitter platform. The dataset is divided into two parts: training and testing. The training set contains 5,000 texts, and the testing set contains 5,000 texts. The training set is used to train the model, and the testing set is used to evaluate the model. The dataset contains 10,000 hate speech texts from the Twitter platform. The dataset is divided into two parts: training and testing. The training set\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.07158",
    "question": "what was the baseline?",
    "gold_answer": "",
    "answer_a": "logistic regression (LR). Explanation: We use logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library BIBREF28 as the baseline classification models. As a baseline RNN, the LSTM network from the Keras library was applied BIBREF29. Both LSTM and MCD LSTM networks consist of an embedding layer, LSTM layer, and a fully connected layer within the Word2Vec and ELMo embeddings. The embedding layer was not used in TF-IDF and Universal Sentence encoding.",
    "answer_b": "What is the difference between the baseline and the proposed approach?\n< Explanation: We use the following baseline: a simple bag-of-words model trained on the Hate Speech Dataset (HSD) [1] with 10000 training examples and 10000 test examples. The model is trained with 10000 epochs and 0.01 learning rate. The model is trained on the Hate Speech Dataset (HSD) with 10000 training examples and 10000 test examples. The model is trained with 10000 epochs and 0",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat was the baseline?\n\nGround Truth Answer:\n\n\nAnswer A:\nlogistic regression (LR). Explanation: We use logistic regression (LR) and Support Vector Machines (SVM) from the scikit-learn library BIBREF28 as the baseline classification models. As a baseline RNN, the LSTM network from the Keras library was applied BIBREF29. Both LSTM and MCD LSTM networks consist of an embedding layer, LSTM layer, and a fully connected layer within the Word2Vec and ELMo embeddings. The embedding layer was not used in TF-IDF and Universal Sentence encoding.\n\nAnswer B:\nWhat is the difference between the baseline and the proposed approach?\n< Explanation: We use the following baseline: a simple bag-of-words model trained on the Hate Speech Dataset (HSD) [1] with 10000 training examples and 10000 test examples. The model is trained with 10000 epochs and 0.01 learning rate. The model is trained on the Hate Speech Dataset (HSD) with 10000 training examples and 10000 test examples. The model is trained with 10000 epochs and 0\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.07158",
    "question": "what text embedding methods were used in their approach?",
    "gold_answer": "",
    "answer_a": "(Output: 2 tables)\n<table> <tbody> <table> <tr> <td> Text embedding methods </td> <td> Description </td> <td> Explanation </td> </tr> <tr> <td> Word2Vec </td> <td> Word2Vec is a probabilistic vector space model that uses a bag of words to represent a document. It is a popular choice for text classification. </td> <td> It is a probabilistic model that uses a bag of words to represent a document. It is a popular choice for text classification. </td> </tr>",
    "answer_b": "Word2Vec. Explanation: Both LSTM and MCD LSTM networks consist of an embedding layer, LSTM layer, and a fully connected layer within the Word2Vec and ELMo embeddings.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat text embedding methods were used in their approach?\n\nGround Truth Answer:\n\n\nAnswer A:\n(Output: 2 tables)\n<table> <tbody> <table> <tr> <td> Text embedding methods </td> <td> Description </td> <td> Explanation </td> </tr> <tr> <td> Word2Vec </td> <td> Word2Vec is a probabilistic vector space model that uses a bag of words to represent a document. It is a popular choice for text classification. </td> <td> It is a probabilistic model that uses a bag of words to represent a document. It is a popular choice for text classification. </td> </tr>\n\nAnswer B:\nWord2Vec. Explanation: Both LSTM and MCD LSTM networks consist of an embedding layer, LSTM layer, and a fully connected layer within the Word2Vec and ELMo embeddings.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.02892",
    "question": "What are the benchmark datasets?",
    "gold_answer": "",
    "answer_a": "How are they evaluated?\n< Explanation: We evaluate our model on five benchmark datasets for text classification: BIBREF1, BIBREF2, BIBREF3, BIBREF4, and BIBREF5. The datasets are collected from the UCI repository. The datasets are divided into training and validation sets. The training set is used to train the model, and the validation set is used to evaluate the model. The datasets are publicly available at https://www.github.com/zhengyuanliu/MTL. The datasets are divided into training and validation sets. The training set is used to train the model",
    "answer_b": "SST-1 BIBREF14. Explanation: Multi-Cardinality Movie review datasets with different average lengths and class numbers, including SST-1 BIBREF14 , SST-2 and IMDB BIBREF15 .\n\nMulti-Domain Product review datasets on different domains from Multi-Domain Sentiment Dataset BIBREF16 , including Books, DVDs, Electronics and Kitchen.\n\nMulti-Objective Classification datasets with different objectives, including IMDB, RN BIBREF17 and QC BIBREF18 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the benchmark datasets?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they evaluated?\n< Explanation: We evaluate our model on five benchmark datasets for text classification: BIBREF1, BIBREF2, BIBREF3, BIBREF4, and BIBREF5. The datasets are collected from the UCI repository. The datasets are divided into training and validation sets. The training set is used to train the model, and the validation set is used to evaluate the model. The datasets are publicly available at https://www.github.com/zhengyuanliu/MTL. The datasets are divided into training and validation sets. The training set is used to train the model\n\nAnswer B:\nSST-1 BIBREF14. Explanation: Multi-Cardinality Movie review datasets with different average lengths and class numbers, including SST-1 BIBREF14 , SST-2 and IMDB BIBREF15 .\n\nMulti-Domain Product review datasets on different domains from Multi-Domain Sentiment Dataset BIBREF16 , including Books, DVDs, Electronics and Kitchen.\n\nMulti-Objective Classification datasets with different objectives, including IMDB, RN BIBREF17 and QC BIBREF18 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.02892",
    "question": "What tasks are the models trained on?",
    "gold_answer": "",
    "answer_a": "What are the performance metrics?\n< Explanation: We train the models on the following tasks: (1) text classification, (2) sentiment classification, (3) sentiment classification with emotion, (4) sentiment classification with emotion and (5) sentiment classification with emotion and emotion. The models are trained on the training set and evaluated on the test set. The performance metrics are F1 score and accuracy. The results are shown in Table TABREF11. The results show that our model can significantly improve the performance of related tasks with additional information from others. The models with the best performance are the best models for all tasks. The models",
    "answer_b": "different average lengths and class numbers. Explanation: As Table TABREF35 shows, we select five benchmark datasets for text classification and design three experiment scenarios to evaluate the performances of our model.\n\nMulti-Cardinality Movie review datasets with different average lengths and class numbers, including SST-1 BIBREF14 , SST-2 and IMDB BIBREF15 .\n\nMulti-Domain Product review datasets on different domains from Multi-Domain Sentiment Dataset BIBREF16 , including Books, DVDs, Electronics and Kitchen.\n\nMulti-Objective Classification datasets with different objectives, including IMDB, RN BIBREF17 and QC BIBREF18 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat tasks are the models trained on?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the performance metrics?\n< Explanation: We train the models on the following tasks: (1) text classification, (2) sentiment classification, (3) sentiment classification with emotion, (4) sentiment classification with emotion and (5) sentiment classification with emotion and emotion. The models are trained on the training set and evaluated on the test set. The performance metrics are F1 score and accuracy. The results are shown in Table TABREF11. The results show that our model can significantly improve the performance of related tasks with additional information from others. The models with the best performance are the best models for all tasks. The models\n\nAnswer B:\ndifferent average lengths and class numbers. Explanation: As Table TABREF35 shows, we select five benchmark datasets for text classification and design three experiment scenarios to evaluate the performances of our model.\n\nMulti-Cardinality Movie review datasets with different average lengths and class numbers, including SST-1 BIBREF14 , SST-2 and IMDB BIBREF15 .\n\nMulti-Domain Product review datasets on different domains from Multi-Domain Sentiment Dataset BIBREF16 , including Books, DVDs, Electronics and Kitchen.\n\nMulti-Objective Classification datasets with different objectives, including IMDB, RN BIBREF17 and QC BIBREF18 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.02892",
    "question": "What recurrent neural networks are explored?",
    "gold_answer": "",
    "answer_a": "LSTM. Explanation: Based on the LSTM implementation of BIBREF13 , we propose a generalized multi-task learning architecture for text classification with four types of recurrent neural layers to convey information inside and among tasks.",
    "answer_b": "What are their architectures?\n< Explanation: We propose a multi-task learning architecture with four types of recurrent neural layers to fuse information across multiple related tasks. The architecture is structurally flexible and considers various interactions among tasks, which can be regarded as a generalized case of many previous works. The four types of recurrent neural layers are as follows: (1) a sequence-to-sequence (seq2seq) model with a bidirectional LSTM layer, (2) a transformer model with a transformer encoder and a transformer decoder, (3) a transformer model with a transformer encoder and a transformer decoder, and (4) a transformer model with a transformer",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat recurrent neural networks are explored?\n\nGround Truth Answer:\n\n\nAnswer A:\nLSTM. Explanation: Based on the LSTM implementation of BIBREF13 , we propose a generalized multi-task learning architecture for text classification with four types of recurrent neural layers to convey information inside and among tasks.\n\nAnswer B:\nWhat are their architectures?\n< Explanation: We propose a multi-task learning architecture with four types of recurrent neural layers to fuse information across multiple related tasks. The architecture is structurally flexible and considers various interactions among tasks, which can be regarded as a generalized case of many previous works. The four types of recurrent neural layers are as follows: (1) a sequence-to-sequence (seq2seq) model with a bidirectional LSTM layer, (2) a transformer model with a transformer encoder and a transformer decoder, (3) a transformer model with a transformer encoder and a transformer decoder, and (4) a transformer model with a transformer\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.06851",
    "question": "What extractive models were trained on this dataset?",
    "gold_answer": "",
    "answer_a": "LEAD-$k$. Explanation: For extractive models, we include LEAD-$k$ which is a strong baseline for single document summarization tasks and takes the first $k$ sentences in the document as summary BIBREF5. TextRank BIBREF6 and LexRank BIBREF7 are two graph-based methods, where nodes are text units and edges are defined by a similarity measure. SumBasic BIBREF8 is a frequency-based sentence selection method, which uses a component to re-weigh the word probabilities in order to minimize redundancy. The last extractive baselines are the near state-of-the-art models C_SKIP from rossiello2017centroid and SemSenSum from antognini2019.",
    "answer_b": "What were their performance?\n< Explanation: We train two different models: an abstractive model and an extractive model. We use the following baselines: (1) a simple bag-of-words model, (2) a simple bag-of-words model with a word embedding, (3) a simple bag-of-words model with a word embedding and a word2vec model, (4) a simple bag-of-words model with a word embedding and a word2vec model with a word2vec pre-trained model, (5) a simple bag-of-words model with a word embedding and a word2vec model",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat extractive models were trained on this dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nLEAD-$k$. Explanation: For extractive models, we include LEAD-$k$ which is a strong baseline for single document summarization tasks and takes the first $k$ sentences in the document as summary BIBREF5. TextRank BIBREF6 and LexRank BIBREF7 are two graph-based methods, where nodes are text units and edges are defined by a similarity measure. SumBasic BIBREF8 is a frequency-based sentence selection method, which uses a component to re-weigh the word probabilities in order to minimize redundancy. The last extractive baselines are the near state-of-the-art models C_SKIP from rossiello2017centroid and SemSenSum from antognini2019.\n\nAnswer B:\nWhat were their performance?\n< Explanation: We train two different models: an abstractive model and an extractive model. We use the following baselines: (1) a simple bag-of-words model, (2) a simple bag-of-words model with a word embedding, (3) a simple bag-of-words model with a word embedding and a word2vec model, (4) a simple bag-of-words model with a word embedding and a word2vec model with a word2vec pre-trained model, (5) a simple bag-of-words model with a word embedding and a word2vec model\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.06851",
    "question": "What abstractive models were trained?",
    "gold_answer": "",
    "answer_a": "What abstractive models were trained?\n< Explanation: We train two abstractive models, named AB1 and AB2, on the GameWikiSum dataset. The first model is trained on the reviews and the second on the references. The models are trained using the same hyperparameters, and the results are compared in Table TABREF11 . The results show that the models are able to extract the most important information from the reviews and references. The AB1 model is able to extract the most important information from both the reviews and references, while the AB2 model is able to extract the most important information from the reviews but not from",
    "answer_b": "Conv2Conv. Explanation: We use common abstractive sequence-to-sequence baselines such as Conv2Conv BIBREF9, Transformer BIBREF10 and its language model variant, TransformerLM BIBREF3.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat abstractive models were trained?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat abstractive models were trained?\n< Explanation: We train two abstractive models, named AB1 and AB2, on the GameWikiSum dataset. The first model is trained on the reviews and the second on the references. The models are trained using the same hyperparameters, and the results are compared in Table TABREF11 . The results show that the models are able to extract the most important information from the reviews and references. The AB1 model is able to extract the most important information from both the reviews and references, while the AB2 model is able to extract the most important information from the reviews but not from\n\nAnswer B:\nConv2Conv. Explanation: We use common abstractive sequence-to-sequence baselines such as Conv2Conv BIBREF9, Transformer BIBREF10 and its language model variant, TransformerLM BIBREF3.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.06851",
    "question": "What is the size of this dataset?",
    "gold_answer": "",
    "answer_a": "How is it different from other datasets?\n< Explanation: The dataset consists of 10,000 reviews and 10,000 references. The reviews are in English and the references are in English and Wikipedia. The dataset is divided into 10,000 reviews and 10,000 references. The reviews are in English and the references are in English and Wikipedia. The dataset is divided into 10,000 reviews and 10,000 references. The reviews are in English and the references are in English and Wikipedia. The dataset is divided into 1",
    "answer_b": "14652. Explanation: In contrast, we propose a novel domain-specific dataset containing $14\\,652$ samples, based on professional video game reviews obtained via Metacritic and gameplay sections from Wikipedia.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the size of this dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it different from other datasets?\n< Explanation: The dataset consists of 10,000 reviews and 10,000 references. The reviews are in English and the references are in English and Wikipedia. The dataset is divided into 10,000 reviews and 10,000 references. The reviews are in English and the references are in English and Wikipedia. The dataset is divided into 10,000 reviews and 10,000 references. The reviews are in English and the references are in English and Wikipedia. The dataset is divided into 1\n\nAnswer B:\n14652. Explanation: In contrast, we propose a novel domain-specific dataset containing $14\\,652$ samples, based on professional video game reviews obtained via Metacritic and gameplay sections from Wikipedia.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.11980",
    "question": "What language(s) does the system answer questions in?",
    "gold_answer": "",
    "answer_a": "French. Explanation: We present a spoken conversational question answering system that is able to answer questions about general knowledge in French by calling two distinct QA systems. It solves coreference and ellipsis by modelling context.",
    "answer_b": "Explanation: The system answers questions in English and German. The system is trained on a large amount of data from Wikidata. The data is divided into training, validation and test sets. The training set is used to train the model and the validation set is used to evaluate the model. The test set is used to evaluate the model on unseen data. The test set is split into training and validation sets. The training set is used to train the model and the validation set is used to evaluate the model. The test set is split into training and validation sets. The training set is used to train the model and the validation set is used to",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat language(s) does the system answer questions in?\n\nGround Truth Answer:\n\n\nAnswer A:\nFrench. Explanation: We present a spoken conversational question answering system that is able to answer questions about general knowledge in French by calling two distinct QA systems. It solves coreference and ellipsis by modelling context.\n\nAnswer B:\nExplanation: The system answers questions in English and German. The system is trained on a large amount of data from Wikidata. The data is divided into training, validation and test sets. The training set is used to train the model and the validation set is used to evaluate the model. The test set is used to evaluate the model on unseen data. The test set is split into training and validation sets. The training set is used to train the model and the validation set is used to evaluate the model. The test set is split into training and validation sets. The training set is used to train the model and the validation set is used to\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.11980",
    "question": "What metrics are used for evaluation?",
    "gold_answer": "",
    "answer_a": "macro precision. Explanation: Performance on out-of-context questions was evaluated on Bench'It, a dataset containing 150 open ended questions about general knowledge in French (Figure FIGREF20). The system reached a macro precision, recall and F-1 of $64.14\\%$, $64.33\\%$ and $63.46\\%$ respectively.",
    "answer_b": "Explanation: We use the following metrics for evaluation: Recall, Precision, F1, and NDCG. Recall is the fraction of questions answered correctly, Precision is the fraction of correct answers that are correct, F1 is the harmonic mean of Precision and Recall, and NDCG is the normalized difference between Recall and Precision. We use the following thresholds: Recall = 0.7, Precision = 0.7, F1 = 0.7, and NDCG = 0.7. We also use the following thresholds: Recall = 0.5, Precision = 0.5, F1 =",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat metrics are used for evaluation?\n\nGround Truth Answer:\n\n\nAnswer A:\nmacro precision. Explanation: Performance on out-of-context questions was evaluated on Bench'It, a dataset containing 150 open ended questions about general knowledge in French (Figure FIGREF20). The system reached a macro precision, recall and F-1 of $64.14\\%$, $64.33\\%$ and $63.46\\%$ respectively.\n\nAnswer B:\nExplanation: We use the following metrics for evaluation: Recall, Precision, F1, and NDCG. Recall is the fraction of questions answered correctly, Precision is the fraction of correct answers that are correct, F1 is the harmonic mean of Precision and Recall, and NDCG is the normalized difference between Recall and Precision. We use the following thresholds: Recall = 0.7, Precision = 0.7, F1 = 0.7, and NDCG = 0.7. We also use the following thresholds: Recall = 0.5, Precision = 0.5, F1 =\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.06259",
    "question": "How do they determine that a decoder handles an easier task than the encoder?",
    "gold_answer": "",
    "answer_a": "adding more layers to the encoder achieves larger improvements than adding more layers to the decoder. Explanation: 1) We find that adding more layers to the encoder achieves larger improvements than adding more layers to the decoder. 2) We also compare the training time of the encoder and decoder by fixing the parameters of a well-trained decoder (encoder), and just update the parameters of the encoder (decoder). We found that the decoder converges faster than the encoder.",
    "answer_b": "What is the difference between the two?\n< Explanation: The encoder is responsible for encoding the input sequence into a high-dimensional vector space. The decoder is responsible for decoding the input sequence into a sequence of output tokens. The decoder is more sensitive to the input noise than the encoder, because the decoder is not able to directly process the input sequence. The decoder is also more sensitive to the input sequence than the encoder, because the decoder is able to process the input sequence and then use the information in the sequence to make a prediction. The preceding words/tokens in the decoder provide strong conditional information, which accounts for the two observations above. The",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they determine that a decoder handles an easier task than the encoder?\n\nGround Truth Answer:\n\n\nAnswer A:\nadding more layers to the encoder achieves larger improvements than adding more layers to the decoder. Explanation: 1) We find that adding more layers to the encoder achieves larger improvements than adding more layers to the decoder. 2) We also compare the training time of the encoder and decoder by fixing the parameters of a well-trained decoder (encoder), and just update the parameters of the encoder (decoder). We found that the decoder converges faster than the encoder.\n\nAnswer B:\nWhat is the difference between the two?\n< Explanation: The encoder is responsible for encoding the input sequence into a high-dimensional vector space. The decoder is responsible for decoding the input sequence into a sequence of output tokens. The decoder is more sensitive to the input noise than the encoder, because the decoder is not able to directly process the input sequence. The decoder is also more sensitive to the input sequence than the encoder, because the decoder is able to process the input sequence and then use the information in the sequence to make a prediction. The preceding words/tokens in the decoder provide strong conditional information, which accounts for the two observations above. The\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.06259",
    "question": "How do they measure conditional information strength?",
    "gold_answer": "",
    "answer_a": "What is the definition of conditional information?\n< Explanation: We use the following definition to measure the conditional information of the decoder. Given a decoder output $y$ and a decoder input $x$, the conditional information of the decoder is defined as $I(y|x) = \\log \\frac{P(y|x)}{P(y)}$, where $P(y|x)$ is the conditional probability of the decoder output $y$ given the decoder input $x$. The larger the conditional information, the stronger the conditional information of the decoder. We use the following definition to measure the conditional information of the encoder. Given a decoder output $y$",
    "answer_b": "by masking the previous tokens, and comparing autoregressive NMT with the non-autoregressive counterpart. Explanation: We further analyze why the decoder is more sensitive by masking the previous tokens, and comparing autoregressive NMT with the non-autoregressive counterpart.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they measure conditional information strength?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the definition of conditional information?\n< Explanation: We use the following definition to measure the conditional information of the decoder. Given a decoder output $y$ and a decoder input $x$, the conditional information of the decoder is defined as $I(y|x) = \\log \\frac{P(y|x)}{P(y)}$, where $P(y|x)$ is the conditional probability of the decoder output $y$ given the decoder input $x$. The larger the conditional information, the stronger the conditional information of the decoder. We use the following definition to measure the conditional information of the encoder. Given a decoder output $y$\n\nAnswer B:\nby masking the previous tokens, and comparing autoregressive NMT with the non-autoregressive counterpart. Explanation: We further analyze why the decoder is more sensitive by masking the previous tokens, and comparing autoregressive NMT with the non-autoregressive counterpart.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.06259",
    "question": "How do they generate input noise for the encoder and decoder?",
    "gold_answer": "",
    "answer_a": "What is the noise level?\n< Explanation: The noise level is set to 0.001. The noise is generated by adding Gaussian noise to the input sequence. The noise is added to the input sequence in the following way: for each word, the noise is added to the word with probability 0.5, and the noise is added to the word with probability 0.5. The noise is added to the word in the following way: for each word, the noise is added to the word with probability 0.5, and the noise is added to the word with probability 0.5. The noise",
    "answer_b": "random dropping. Explanation: We simulate the input noise with three typical operations BIBREF20, BIBREF21: 1) random dropping: we randomly drop the input tokens of encoder and decoder respectively with different drop rates; 2) random noising: we randomly select tokens and replace its embedding with random noise; 3) random swapping: we randomly reverse the order for the adjacent tokens.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they generate input noise for the encoder and decoder?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the noise level?\n< Explanation: The noise level is set to 0.001. The noise is generated by adding Gaussian noise to the input sequence. The noise is added to the input sequence in the following way: for each word, the noise is added to the word with probability 0.5, and the noise is added to the word with probability 0.5. The noise is added to the word in the following way: for each word, the noise is added to the word with probability 0.5, and the noise is added to the word with probability 0.5. The noise\n\nAnswer B:\nrandom dropping. Explanation: We simulate the input noise with three typical operations BIBREF20, BIBREF21: 1) random dropping: we randomly drop the input tokens of encoder and decoder respectively with different drop rates; 2) random noising: we randomly select tokens and replace its embedding with random noise; 3) random swapping: we randomly reverse the order for the adjacent tokens.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.07721",
    "question": "How do they perform the joint training?",
    "gold_answer": "",
    "answer_a": "What is the training process?\n< Explanation: The joint training is performed by the following steps: (1) the BERT model is trained on the training set, and the parameters of the BERT model are updated by the following formula: (2) the BERT model is trained on the test set, and the parameters of the BERT model are updated by the following formula: (3) the BERT model is trained on the validation set, and the parameters of the BERT model are updated by the following formula: (4) the BERT model is trained on the test set, and the parameters of the BERT model",
    "answer_b": "They train a single model that integrates a BERT language model as a shared parameter layer on NER and RC tasks.. Explanation: We propose a focused attention model to jointly learn NER and RC task. The model integrates BERT language model as a shared parameter layer to achieve better generalization performance.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they perform the joint training?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the training process?\n< Explanation: The joint training is performed by the following steps: (1) the BERT model is trained on the training set, and the parameters of the BERT model are updated by the following formula: (2) the BERT model is trained on the test set, and the parameters of the BERT model are updated by the following formula: (3) the BERT model is trained on the validation set, and the parameters of the BERT model are updated by the following formula: (4) the BERT model is trained on the test set, and the parameters of the BERT model\n\nAnswer B:\nThey train a single model that integrates a BERT language model as a shared parameter layer on NER and RC tasks.. Explanation: We propose a focused attention model to jointly learn NER and RC task. The model integrates BERT language model as a shared parameter layer to achieve better generalization performance.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.07721",
    "question": "What is the previous model that achieved state-of-the-art?",
    "gold_answer": "",
    "answer_a": "What is the difference between the proposed model and the previous model?\n< Explanation: The previous model is the BERT model. BERT is a transformer-based language model that has been widely used in various NLP tasks. BERT has been trained on the large-scale Chinese medical text corpus. The BERT model is composed of a sequence-to-sequence encoder and a decoder. The encoder is composed of a self-attention layer and a feed-forward layer. The decoder is composed of a self-attention layer and a feed-forward layer. The BERT model has been trained on the large-scale Chinese medical text corpus. The BERT model has",
    "answer_b": "Joint Bi-LSTM. Explanation: FLOAT SELECTED: TABLE V COMPARISONS WITH THE DIFFERENT METHODS ON THE TASK OF JOINT ENTITY AND RELATION EXTRACTION",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the previous model that achieved state-of-the-art?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the proposed model and the previous model?\n< Explanation: The previous model is the BERT model. BERT is a transformer-based language model that has been widely used in various NLP tasks. BERT has been trained on the large-scale Chinese medical text corpus. The BERT model is composed of a sequence-to-sequence encoder and a decoder. The encoder is composed of a self-attention layer and a feed-forward layer. The decoder is composed of a self-attention layer and a feed-forward layer. The BERT model has been trained on the large-scale Chinese medical text corpus. The BERT model has\n\nAnswer B:\nJoint Bi-LSTM. Explanation: FLOAT SELECTED: TABLE V COMPARISONS WITH THE DIFFERENT METHODS ON THE TASK OF JOINT ENTITY AND RELATION EXTRACTION\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.06024",
    "question": "What datasets were used in this work?",
    "gold_answer": "",
    "answer_a": "Explanation: We use the following datasets: (1) the dataset of the 2016 US Census Bureau Hate Crime Survey (HCS) (HCS-2016) (2) the dataset of the 2016 US Census Bureau Hate Crime Survey (HCS-2016) (3) the dataset of the 2016 US Census Bureau Hate Crime Survey (HCS-2016) (4) the dataset of the 2016 US Census Bureau Hate Crime Survey (HCS-2016) (5) the dataset of the",
    "answer_b": "DATA-TWITTER-WH, DATA-TWITTER-W, DATA-TWITTER-DAVID, DATA-TWITTER-F,  DATA-WIKI-ATT,  DATA-WIKI-AGG,  DATA-WIKI-TOX,  DATA-FOX-NEWS,  DATA-GAZZETTA, DATA-FACEBOOK, Arabic News,  GermEval,  Ask.fm.. Explanation: FLOAT SELECTED: Table 1: Links and summaries of datasets mentioned in the paper that are publicly available.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets were used in this work?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We use the following datasets: (1) the dataset of the 2016 US Census Bureau Hate Crime Survey (HCS) (HCS-2016) (2) the dataset of the 2016 US Census Bureau Hate Crime Survey (HCS-2016) (3) the dataset of the 2016 US Census Bureau Hate Crime Survey (HCS-2016) (4) the dataset of the 2016 US Census Bureau Hate Crime Survey (HCS-2016) (5) the dataset of the\n\nAnswer B:\nDATA-TWITTER-WH, DATA-TWITTER-W, DATA-TWITTER-DAVID, DATA-TWITTER-F,  DATA-WIKI-ATT,  DATA-WIKI-AGG,  DATA-WIKI-TOX,  DATA-FOX-NEWS,  DATA-GAZZETTA, DATA-FACEBOOK, Arabic News,  GermEval,  Ask.fm.. Explanation: FLOAT SELECTED: Table 1: Links and summaries of datasets mentioned in the paper that are publicly available.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.06024",
    "question": "How is abuse defined for the purposes of this research?",
    "gold_answer": "",
    "answer_a": "we define abuse as any expression that is meant to denigrate or offend a particular person or group.. Explanation: However, regardless of the specific type, we define abuse as any expression that is meant to denigrate or offend a particular person or group.",
    "answer_b": "Is it based on the content of the message or the content of the sender?\nFor this research, we define abuse as a negative message that is intended to cause harm or distress to the victim. We define abuse as a negative message that is intended to cause harm or distress to the victim. We also define abuse as a negative message that is intended to cause harm or distress to the victim. We define abuse as a negative message that is intended to cause harm or distress to the victim. We define abuse as a negative message that is intended to cause harm or distress to the victim. We define abuse as a negative message that is intended to cause",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is abuse defined for the purposes of this research?\n\nGround Truth Answer:\n\n\nAnswer A:\nwe define abuse as any expression that is meant to denigrate or offend a particular person or group.. Explanation: However, regardless of the specific type, we define abuse as any expression that is meant to denigrate or offend a particular person or group.\n\nAnswer B:\nIs it based on the content of the message or the content of the sender?\nFor this research, we define abuse as a negative message that is intended to cause harm or distress to the victim. We define abuse as a negative message that is intended to cause harm or distress to the victim. We also define abuse as a negative message that is intended to cause harm or distress to the victim. We define abuse as a negative message that is intended to cause harm or distress to the victim. We define abuse as a negative message that is intended to cause harm or distress to the victim. We define abuse as a negative message that is intended to cause\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1705.00571",
    "question": "Which evaluation metrics do they use?",
    "gold_answer": "",
    "answer_a": "Metric 1. Explanation: The main evaluation over the test data is based on the best performing SVR and the two BLSTM models once trained on all of the training data. The result table TABREF28 shows three columns based on the three evaluation metrics that the organisers have used. Metric 1 is the original metric, weighted cosine similarity (the metric used to evaluate the final version of the results, where we were ranked 5th; metric provided on the task website). This was then changed after the evaluation deadline to equation EQREF25 (which we term metric 2; this is what the first version of the results were actually based on, where we were ranked 4th), which then changed by the organisers to their equation as presented in BIBREF18 (which we term metric 3 and what the second version of the results were based on, where we were ranked 5th).\n\nAs you can see from the results table TABREF28 , the difference between the metrics is quite substantial. This is due to the system's optimisation being based on metric 1 rather than 2. Metric 2 is a classification metric for sentences with one aspect as it penalises values that are of opposite sign (giving -1 score) and rewards values with the same sign (giving +1 score). Our systems are not optimised for this because it would predict scores of -0.01 and true value of 0.01 as very close (within vector of other results) with low error whereas metric 2 would give this the highest error rating of -1 as they are not the same sign. Metric 3 is more similar to metric 1 as shown by the results, however the crucial difference is that again if you get opposite signs it will penalise more. We analysed the top 50 errors based on Mean Absolute Error (MAE) in the test dataset specifically to examine the number of sentences containing more than one aspect. Our investigation shows that no one system is better at predicting the sentiment of sentences that have more than one aspect (i.e. company) within them. Within those top 50 errors we found that the BLSTM systems do not know which parts of the sentence are associated to the company the sentiment is with respect to. Also they do not know the strength/existence of certain sentiment words.",
    "answer_b": "What are the results?\n<sep><table cellspacing=\"0\" cellpadding=\"0\" width=\"600\" border=\"0\" align=\"left\">\n<tr>\n<td align=\"left\" valign=\"top\" width=\"100\">Evaluation metrics</td>\n<td align=\"left\" valign=\"top\" width=\"100\">Results</td>\n</tr>\n<tr>\n<td align=\"left\" valign=\"top\" width=\"100\">Accuracy</td>\n<td align=\"left\" valign=\"top\" width=\"100\">0.73</td>\n</tr>\n<tr>\n<td align=\"left\"",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich evaluation metrics do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nMetric 1. Explanation: The main evaluation over the test data is based on the best performing SVR and the two BLSTM models once trained on all of the training data. The result table TABREF28 shows three columns based on the three evaluation metrics that the organisers have used. Metric 1 is the original metric, weighted cosine similarity (the metric used to evaluate the final version of the results, where we were ranked 5th; metric provided on the task website). This was then changed after the evaluation deadline to equation EQREF25 (which we term metric 2; this is what the first version of the results were actually based on, where we were ranked 4th), which then changed by the organisers to their equation as presented in BIBREF18 (which we term metric 3 and what the second version of the results were based on, where we were ranked 5th).\n\nAs you can see from the results table TABREF28 , the difference between the metrics is quite substantial. This is due to the system's optimisation being based on metric 1 rather than 2. Metric 2 is a classification metric for sentences with one aspect as it penalises values that are of opposite sign (giving -1 score) and rewards values with the same sign (giving +1 score). Our systems are not optimised for this because it would predict scores of -0.01 and true value of 0.01 as very close (within vector of other results) with low error whereas metric 2 would give this the highest error rating of -1 as they are not the same sign. Metric 3 is more similar to metric 1 as shown by the results, however the crucial difference is that again if you get opposite signs it will penalise more. We analysed the top 50 errors based on Mean Absolute Error (MAE) in the test dataset specifically to examine the number of sentences containing more than one aspect. Our investigation shows that no one system is better at predicting the sentiment of sentences that have more than one aspect (i.e. company) within them. Within those top 50 errors we found that the BLSTM systems do not know which parts of the sentence are associated to the company the sentiment is with respect to. Also they do not know the strength/existence of certain sentiment words.\n\nAnswer B:\nWhat are the results?\n<sep><table cellspacing=\"0\" cellpadding=\"0\" width=\"600\" border=\"0\" align=\"left\">\n<tr>\n<td align=\"left\" valign=\"top\" width=\"100\">Evaluation metrics</td>\n<td align=\"left\" valign=\"top\" width=\"100\">Results</td>\n</tr>\n<tr>\n<td align=\"left\" valign=\"top\" width=\"100\">Accuracy</td>\n<td align=\"left\" valign=\"top\" width=\"100\">0.73</td>\n</tr>\n<tr>\n<td align=\"left\"\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1705.00571",
    "question": "Which finance specific word embedding model do they use?",
    "gold_answer": "",
    "answer_a": "word2vec. Explanation: We additionally trained a word2vec BIBREF10 word embedding model on a set of 189,206 financial articles containing 161,877,425 tokens, that were manually downloaded from Factiva.",
    "answer_b": "What is the size of the vocabulary?\n< Explanation: We used the Word2Vec model with a size of 1000000. We used the following word embeddings: Word2Vec, GloVe, FastText, FastText-100, FastText-1000, FastText-10000, FastText-100000, FastText-1000000, FastText-10000000, FastText-100000000, FastText-1000000",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich finance specific word embedding model do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nword2vec. Explanation: We additionally trained a word2vec BIBREF10 word embedding model on a set of 189,206 financial articles containing 161,877,425 tokens, that were manually downloaded from Factiva.\n\nAnswer B:\nWhat is the size of the vocabulary?\n< Explanation: We used the Word2Vec model with a size of 1000000. We used the following word embeddings: Word2Vec, GloVe, FastText, FastText-100, FastText-1000, FastText-10000, FastText-100000, FastText-1000000, FastText-10000000, FastText-100000000, FastText-1000000\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.04498",
    "question": "How does lattice rescoring improve inference?",
    "gold_answer": "",
    "answer_a": "By transducing initial hypotheses produced by the biased baseline system to create gender-inflected search spaces which can\nbe rescored by the adapted model. Explanation: We also present a lattice rescoring procedure in which initial hypotheses produced by the biased baseline system are transduced to create gender-inflected search spaces which can be rescored by the adapted model.",
    "answer_b": "What is the performance of the lattice rescoring scheme?\n< Explanation: We propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al (2019) on WinoMT with no degradation of general test set BLEU. We show that the lattice-rescoring scheme outperforms all systems evaluated in Stanovsky et al (2019) on WinoMT with no degradation of general test set BLEU, and that it can be applied to remove gender bias in the output of `black box` online commercial MT systems. We also show that the lattice-rescoring scheme out",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does lattice rescoring improve inference?\n\nGround Truth Answer:\n\n\nAnswer A:\nBy transducing initial hypotheses produced by the biased baseline system to create gender-inflected search spaces which can\nbe rescored by the adapted model. Explanation: We also present a lattice rescoring procedure in which initial hypotheses produced by the biased baseline system are transduced to create gender-inflected search spaces which can be rescored by the adapted model.\n\nAnswer B:\nWhat is the performance of the lattice rescoring scheme?\n< Explanation: We propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al (2019) on WinoMT with no degradation of general test set BLEU. We show that the lattice-rescoring scheme outperforms all systems evaluated in Stanovsky et al (2019) on WinoMT with no degradation of general test set BLEU, and that it can be applied to remove gender bias in the output of `black box` online commercial MT systems. We also show that the lattice-rescoring scheme out\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.04498",
    "question": "What three languages are used in the translation experiments?",
    "gold_answer": "",
    "answer_a": "German. Explanation: We select three pairs for experiments: English to German (en-de), English to Spanish (en-es) and English to Hebrew (en-he).",
    "answer_b": "Explanation: We use the following three languages: English, Spanish, and French. We use the same data for each language, and we use the same model architecture. We use the same dataset for each language, and we use the same evaluation metrics. We use the same evaluation dataset for each language. We use the same evaluation metrics for each language. We use the same evaluation dataset for each language. We use the same evaluation metrics for each language. We use the same evaluation dataset for each language. We use the same evaluation metrics for each language. We use the same evaluation dataset for each language. We use the same evaluation metrics for each",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat three languages are used in the translation experiments?\n\nGround Truth Answer:\n\n\nAnswer A:\nGerman. Explanation: We select three pairs for experiments: English to German (en-de), English to Spanish (en-es) and English to Hebrew (en-he).\n\nAnswer B:\nExplanation: We use the following three languages: English, Spanish, and French. We use the same data for each language, and we use the same model architecture. We use the same dataset for each language, and we use the same evaluation metrics. We use the same evaluation dataset for each language. We use the same evaluation metrics for each language. We use the same evaluation dataset for each language. We use the same evaluation metrics for each language. We use the same evaluation dataset for each language. We use the same evaluation metrics for each language. We use the same evaluation dataset for each language. We use the same evaluation metrics for each\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.04498",
    "question": "What metrics are used to measure bias reduction?",
    "gold_answer": "",
    "answer_a": "Accuracy. Explanation: The authors emphasise the following metrics over the challenge set:\n\nAccuracy \u2013 percentage of hypotheses with the correctly gendered primary entity.\n\n$\\mathbf {\\Delta G}$ \u2013 difference in $F_1$ score between the set of sentences with masculine entities and the set with feminine entities.\n\n$\\mathbf {\\Delta S}$ \u2013 difference in accuracy between the set of sentences with pro-stereotypical (`pro') entities and those with anti-stereotypical (`anti') entities, as determined by BIBREF5 using US labour statistics. For example, the `pro' set contains male doctors and female nurses, while `anti' contains female doctors and male nurses.",
    "answer_b": "What is the performance of the lattice-rescoring scheme?\n< Explanation: We use the BLEU metric to measure debiasing. We use the following formula to compute BLEU:\n\nwhere $B$ is the set of sentences in the target language, $S$ is the set of sentences in the source language, and $B^*$ is the set of sentences in the target language that are equivalent to $S$ in the source language. We use the following threshold for the BLEU score:\n\nwhere $B^*$ is the set of sentences in the target language that are equivalent to $S$ in the source language. We",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat metrics are used to measure bias reduction?\n\nGround Truth Answer:\n\n\nAnswer A:\nAccuracy. Explanation: The authors emphasise the following metrics over the challenge set:\n\nAccuracy \u2013 percentage of hypotheses with the correctly gendered primary entity.\n\n$\\mathbf {\\Delta G}$ \u2013 difference in $F_1$ score between the set of sentences with masculine entities and the set with feminine entities.\n\n$\\mathbf {\\Delta S}$ \u2013 difference in accuracy between the set of sentences with pro-stereotypical (`pro') entities and those with anti-stereotypical (`anti') entities, as determined by BIBREF5 using US labour statistics. For example, the `pro' set contains male doctors and female nurses, while `anti' contains female doctors and male nurses.\n\nAnswer B:\nWhat is the performance of the lattice-rescoring scheme?\n< Explanation: We use the BLEU metric to measure debiasing. We use the following formula to compute BLEU:\n\nwhere $B$ is the set of sentences in the target language, $S$ is the set of sentences in the source language, and $B^*$ is the set of sentences in the target language that are equivalent to $S$ in the source language. We use the following threshold for the BLEU score:\n\nwhere $B^*$ is the set of sentences in the target language that are equivalent to $S$ in the source language. We\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2004.04498",
    "question": "How is the set of trusted, gender-balanced examples selected?",
    "gold_answer": "",
    "answer_a": "What is the size of the set?\n< Explanation: We use the WinoMT challenge set (Stanovsky et al, 2019) to train our models. The challenge set consists of 1000 sentences from 10 different languages, each with 1000 words. We use 1000 sentences from each language, and we use 1000 words from each sentence. We use the first 1000 words of each sentence as training data, and the remaining 1000 words as test data. We use the first 1000 words",
    "answer_b": "create a tiny, handcrafted profession-based dataset. Explanation: To explore this we create a tiny, handcrafted profession-based dataset for transfer learning.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is the set of trusted, gender-balanced examples selected?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the set?\n< Explanation: We use the WinoMT challenge set (Stanovsky et al, 2019) to train our models. The challenge set consists of 1000 sentences from 10 different languages, each with 1000 words. We use 1000 sentences from each language, and we use 1000 words from each sentence. We use the first 1000 words of each sentence as training data, and the remaining 1000 words as test data. We use the first 1000 words\n\nAnswer B:\ncreate a tiny, handcrafted profession-based dataset. Explanation: To explore this we create a tiny, handcrafted profession-based dataset for transfer learning.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1602.03483",
    "question": "Which data sources do they use?",
    "gold_answer": "",
    "answer_a": "Toronto Books Corpus. Explanation: Unless stated above, all models were trained on the Toronto Books Corpus, which has the inter-sentential coherence required for SkipThought and FastSent.",
    "answer_b": "How do they label them?\n< Explanation: We use the following data sources: (i) Wikipedia, (ii) Wikipedia articles, (iii) Wikipedia articles with a single sentence, (iv) Wikipedia articles with multiple sentences, (v) Wikipedia articles with a single sentence and (vi) Wikipedia articles with multiple sentences. We label the Wikipedia data with the Wikipedia article dataset from the Stanford NLP Corpus (SNC) BIBREF10 . We label the Wikipedia articles with the Wikipedia article dataset from the Stanford NLP Corpus BIBREF11 . We label the Wikipedia articles with the Wikipedia article dataset from the Stanford NLP",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich data sources do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nToronto Books Corpus. Explanation: Unless stated above, all models were trained on the Toronto Books Corpus, which has the inter-sentential coherence required for SkipThought and FastSent.\n\nAnswer B:\nHow do they label them?\n< Explanation: We use the following data sources: (i) Wikipedia, (ii) Wikipedia articles, (iii) Wikipedia articles with a single sentence, (iv) Wikipedia articles with multiple sentences, (v) Wikipedia articles with a single sentence and (vi) Wikipedia articles with multiple sentences. We label the Wikipedia data with the Wikipedia article dataset from the Stanford NLP Corpus (SNC) BIBREF10 . We label the Wikipedia articles with the Wikipedia article dataset from the Stanford NLP Corpus BIBREF11 . We label the Wikipedia articles with the Wikipedia article dataset from the Stanford NLP\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1602.03483",
    "question": "Which tasks do they evaluate supervised systems on?",
    "gold_answer": "",
    "answer_a": "How do they evaluate unsupervised systems?\n< Explanation: We evaluate the performance of the models on three tasks: (1) word embedding, (2) sentence embedding, and (3) spatial distance. We use the following metrics: (1) BLEU, (2) F1, and (3) cosine similarity. We also evaluate the performance of the models on three tasks: (1) word embedding, (2) sentence embedding, and (3) spatial distance. We use the following metrics: (1) BLEU, (2) F1, and (3) cosine similarity. We also evaluate the performance",
    "answer_b": "paraphrase identification (MSRP). Explanation: Representations are applied to 6 sentence classification tasks: paraphrase identification (MSRP) BIBREF30 , movie review sentiment (MR) BIBREF31 , product reviews (CR) BIBREF32 , subjectivity classification (SUBJ) BIBREF33 , opinion polarity (MPQA) BIBREF34 and question type classification (TREC) BIBREF35 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich tasks do they evaluate supervised systems on?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate unsupervised systems?\n< Explanation: We evaluate the performance of the models on three tasks: (1) word embedding, (2) sentence embedding, and (3) spatial distance. We use the following metrics: (1) BLEU, (2) F1, and (3) cosine similarity. We also evaluate the performance of the models on three tasks: (1) word embedding, (2) sentence embedding, and (3) spatial distance. We use the following metrics: (1) BLEU, (2) F1, and (3) cosine similarity. We also evaluate the performance\n\nAnswer B:\nparaphrase identification (MSRP). Explanation: Representations are applied to 6 sentence classification tasks: paraphrase identification (MSRP) BIBREF30 , movie review sentiment (MR) BIBREF31 , product reviews (CR) BIBREF32 , subjectivity classification (SUBJ) BIBREF33 , opinion polarity (MPQA) BIBREF34 and question type classification (TREC) BIBREF35 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1602.03483",
    "question": "How do they evaluate domain portability?",
    "gold_answer": "",
    "answer_a": "What is the performance metric?\n< Explanation: We use the following metrics to evaluate the performance of the models: (1) the mean absolute error (MAE) between the learned representations and the ground truth, (2) the mean squared error (MSE) between the learned representations and the ground truth, (3) the mean absolute cosine distance (MAD) between the learned representations and the ground truth, and (4) the mean absolute cosine distance between the learned representations and the ground truth. We also use the following metrics to evaluate the performance of the models: (1) the mean absolute error (MAE) between the",
    "answer_b": "We constrain our comparison to methods that do not require labelled data. Explanation: We constrain our comparison to methods that do not require labelled data gathered for the purpose of training models, since such methods are more cost-effective and applicable across languages and domains.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they evaluate domain portability?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance metric?\n< Explanation: We use the following metrics to evaluate the performance of the models: (1) the mean absolute error (MAE) between the learned representations and the ground truth, (2) the mean squared error (MSE) between the learned representations and the ground truth, (3) the mean absolute cosine distance (MAD) between the learned representations and the ground truth, and (4) the mean absolute cosine distance between the learned representations and the ground truth. We also use the following metrics to evaluate the performance of the models: (1) the mean absolute error (MAE) between the\n\nAnswer B:\nWe constrain our comparison to methods that do not require labelled data. Explanation: We constrain our comparison to methods that do not require labelled data gathered for the purpose of training models, since such methods are more cost-effective and applicable across languages and domains.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1602.03483",
    "question": "Which unsupervised representation-learning objectives do they introduce?",
    "gold_answer": "",
    "answer_a": "What are they?\n< Explanation: We introduce two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance. The first objective is a simple distance metric that optimises the trade-off between the number of words and the number of sentences in the representation. The second objective is a more complex distance metric that optimises the trade-off between the number of words and the number of sentences in the representation. We show that the simple distance metric optimises the trade-off between training time and domain portability, but that the more complex metric optimises the trade-off between training time and performance.",
    "answer_b": "Sequential Denoising Autoencoders (SDAEs) and FastSent. Explanation: We also propose two new phrase or sentence representation learning objectives - Sequential Denoising Autoencoders (SDAEs) and FastSent, a sentence-level log-linear bag-of-words model.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich unsupervised representation-learning objectives do they introduce?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are they?\n< Explanation: We introduce two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance. The first objective is a simple distance metric that optimises the trade-off between the number of words and the number of sentences in the representation. The second objective is a more complex distance metric that optimises the trade-off between the number of words and the number of sentences in the representation. We show that the simple distance metric optimises the trade-off between training time and domain portability, but that the more complex metric optimises the trade-off between training time and performance.\n\nAnswer B:\nSequential Denoising Autoencoders (SDAEs) and FastSent. Explanation: We also propose two new phrase or sentence representation learning objectives - Sequential Denoising Autoencoders (SDAEs) and FastSent, a sentence-level log-linear bag-of-words model.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1710.11334",
    "question": "What methods to they compare to?",
    "gold_answer": "",
    "answer_a": "What are the results?\nThe experimental results show that the proposed method outperforms the state-of-the-art methods in terms of the average precision and F1 score. The average precision of the proposed method is 0.87, which is 1.15 points higher than the state-of-the-art method. The F1 score of the proposed method is 0.87, which is 1.15 points higher than the state-of-the-art method. The average precision and F1 score of the proposed method are all higher than the state-of-the-art methods. The experimental results are shown in Table 1.",
    "answer_b": "(1) Baseline_1, which applies the probability information. Explanation: In order to evaluate the performance of the discourse parser, we compare it with other approaches: (1) Baseline_1, which applies the probability information. The connective identifier predicts the connective according the frequency of the connec-tive in the train set. The arguments identifier takes the immediately previous sentence in which the connective appears as Arg1 and the text span after the connective but in the same sentence with connective as Arg2. The non-explicit identifier labels the ad-jacent sentences according to the frequency of the non-explicit relation. (2) Base-line_2, which is the parser using the Support Vector Maching as the train and predic-tion model with numeric type feature from the hashcode of the textual type feature.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat methods to they compare to?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\nThe experimental results show that the proposed method outperforms the state-of-the-art methods in terms of the average precision and F1 score. The average precision of the proposed method is 0.87, which is 1.15 points higher than the state-of-the-art method. The F1 score of the proposed method is 0.87, which is 1.15 points higher than the state-of-the-art method. The average precision and F1 score of the proposed method are all higher than the state-of-the-art methods. The experimental results are shown in Table 1.\n\nAnswer B:\n(1) Baseline_1, which applies the probability information. Explanation: In order to evaluate the performance of the discourse parser, we compare it with other approaches: (1) Baseline_1, which applies the probability information. The connective identifier predicts the connective according the frequency of the connec-tive in the train set. The arguments identifier takes the immediately previous sentence in which the connective appears as Arg1 and the text span after the connective but in the same sentence with connective as Arg2. The non-explicit identifier labels the ad-jacent sentences according to the frequency of the non-explicit relation. (2) Base-line_2, which is the parser using the Support Vector Maching as the train and predic-tion model with numeric type feature from the hashcode of the textual type feature.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1710.11334",
    "question": "Which dataset to they train and evaluate on?",
    "gold_answer": "",
    "answer_a": "How do they evaluate the performance?\n< Explanation: The dataset used in this paper is the Penn Discourse Treebank (PDTB) BIBREF1 . The PDTB is a large-scale corpus of discourse texts, which is collected from the Stanford Discourse Project (SDP) BIBREF2 . The corpus contains 1,000,000 sentences, which are annotated by 1,000 annotators. The annotations are divided into 100,000 sentences, which are used for training and 100,000 sentences, which are used for testing.",
    "answer_b": "PDTB as training set, Section 22 as testing set. Explanation: In our experiments, we make use of the Section 02-21 in the PDTB as training set, Section 22 as testing set. All of components adopt maximum entropy model.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dataset to they train and evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate the performance?\n< Explanation: The dataset used in this paper is the Penn Discourse Treebank (PDTB) BIBREF1 . The PDTB is a large-scale corpus of discourse texts, which is collected from the Stanford Discourse Project (SDP) BIBREF2 . The corpus contains 1,000,000 sentences, which are annotated by 1,000 annotators. The annotations are divided into 100,000 sentences, which are used for training and 100,000 sentences, which are used for testing.\n\nAnswer B:\nPDTB as training set, Section 22 as testing set. Explanation: In our experiments, we make use of the Section 02-21 in the PDTB as training set, Section 22 as testing set. All of components adopt maximum entropy model.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.11664",
    "question": "What settings did they experiment with?",
    "gold_answer": "",
    "answer_a": "How did they evaluate the performance?\n< Explanation: We evaluate the performance of the proposed methods on three different settings: (1) the original dataset, (2) the dataset with domain shift, and (3) the dataset with domain shift and meta-learning. We use the same hyper-parameters for all the experiments. The results are shown in Table TABREF11. We can see that the proposed methods outperform the baseline methods in all the settings. The meta-learning method outperforms the other methods in the original dataset and the dataset with domain shift. The meta-learning method outperforms the other methods in the dataset with domain",
    "answer_b": "in-domain, out-of-domain and cross-dataset. Explanation: We investigate the effectiveness of the above four strategies under three evaluation settings: in-domain, out-of-domain and cross-dataset.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat settings did they experiment with?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow did they evaluate the performance?\n< Explanation: We evaluate the performance of the proposed methods on three different settings: (1) the original dataset, (2) the dataset with domain shift, and (3) the dataset with domain shift and meta-learning. We use the same hyper-parameters for all the experiments. The results are shown in Table TABREF11. We can see that the proposed methods outperform the baseline methods in all the settings. The meta-learning method outperforms the other methods in the original dataset and the dataset with domain shift. The meta-learning method outperforms the other methods in the dataset with domain\n\nAnswer B:\nin-domain, out-of-domain and cross-dataset. Explanation: We investigate the effectiveness of the above four strategies under three evaluation settings: in-domain, out-of-domain and cross-dataset.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.11664",
    "question": "what domains are explored in this paper?",
    "gold_answer": "",
    "answer_a": "NYTimes. Explanation: We select top ten publications (NYTimes, WashingtonPost, FoxNews, TheGuardian, NYDailyNews, WSJ, USAToday, CNN, Time and Mashable) and process them in the way of BIBREF22.",
    "answer_b": "Explanation: We first extend the conventional definition of the domain from categories into data sources for the text summarization task. We consider the following four categories: news, sports, politics and science. The data sources are the following: news, sports, politics and science. The training data is collected from the following sources: news, sports, politics and science. The training data is divided into training and test sets. The training set is used to train the model and the test set is used to evaluate the model. The training set is divided into training and validation sets. The training and validation sets are used to tune the hyper-parameters of the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat domains are explored in this paper?\n\nGround Truth Answer:\n\n\nAnswer A:\nNYTimes. Explanation: We select top ten publications (NYTimes, WashingtonPost, FoxNews, TheGuardian, NYDailyNews, WSJ, USAToday, CNN, Time and Mashable) and process them in the way of BIBREF22.\n\nAnswer B:\nExplanation: We first extend the conventional definition of the domain from categories into data sources for the text summarization task. We consider the following four categories: news, sports, politics and science. The data sources are the following: news, sports, politics and science. The training data is collected from the following sources: news, sports, politics and science. The training data is divided into training and test sets. The training set is used to train the model and the test set is used to evaluate the model. The training set is divided into training and validation sets. The training and validation sets are used to tune the hyper-parameters of the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.11664",
    "question": "what multi-domain dataset is repurposed?",
    "gold_answer": "",
    "answer_a": "How is it different from the original dataset?\n< Explanation: We use the Multi-SUM dataset BIBREF11 to re-purpose the original dataset BIBREF12. The dataset contains 10000 sentences from 10 different domains, each with 1000 sentences. The sentences are randomly shuffled and the domain is randomly assigned to each sentence. The sentences are further divided into training and test sets. The training set is used to train the summarization model and the test set is used to evaluate the model. The training set is further divided into training and validation sets. The training and validation",
    "answer_b": "MULTI-SUM. Explanation: Based on this, we re-purpose a multi-domain summarization dataset MULTI-SUM and further explore the issue of domain shift.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat multi-domain dataset is repurposed?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it different from the original dataset?\n< Explanation: We use the Multi-SUM dataset BIBREF11 to re-purpose the original dataset BIBREF12. The dataset contains 10000 sentences from 10 different domains, each with 1000 sentences. The sentences are randomly shuffled and the domain is randomly assigned to each sentence. The sentences are further divided into training and test sets. The training set is used to train the summarization model and the test set is used to evaluate the model. The training set is further divided into training and validation sets. The training and validation\n\nAnswer B:\nMULTI-SUM. Explanation: Based on this, we re-purpose a multi-domain summarization dataset MULTI-SUM and further explore the issue of domain shift.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.11664",
    "question": "what four learning strategies are investigated?",
    "gold_answer": "",
    "answer_a": "Model@!START@$^{I}_{Base}$@!END@. Explanation: Model@!START@$^{I}_{Base}$@!END@\nThis is a simple but effective model for multi-domain learning, in which all domains are aggregated together and will be further used for training a set of shared parameters.",
    "answer_b": "Explanation: We investigate four learning strategies: (1) using the same data source for both domains, (2) using the same data source for both domains and different summarization models, (3) using different data sources for both domains, and (4) using different data sources for both domains and different summarization models. For the first two strategies, we use the same data source for both domains and train the summarization model on the same data source. For the third strategy, we use the same data source for both domains and train the summarization model on the same data source. For the fourth strategy, we use the same data",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat four learning strategies are investigated?\n\nGround Truth Answer:\n\n\nAnswer A:\nModel@!START@$^{I}_{Base}$@!END@. Explanation: Model@!START@$^{I}_{Base}$@!END@\nThis is a simple but effective model for multi-domain learning, in which all domains are aggregated together and will be further used for training a set of shared parameters.\n\nAnswer B:\nExplanation: We investigate four learning strategies: (1) using the same data source for both domains, (2) using the same data source for both domains and different summarization models, (3) using different data sources for both domains, and (4) using different data sources for both domains and different summarization models. For the first two strategies, we use the same data source for both domains and train the summarization model on the same data source. For the third strategy, we use the same data source for both domains and train the summarization model on the same data source. For the fourth strategy, we use the same data\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.07076",
    "question": "By how much did the new model outperform multilingual BERT?",
    "gold_answer": "",
    "answer_a": "For POS,  improvements for cased BERT are 1.26 2.52  0.5 for TDT,  FTB and PUD datasets respectively.\nFor NER in-domain test set, improvement is  2.11 F1 and for NER out-of-domain test set, improvement is  5.32 F1.\nFor Dependency parsing, improvements are in range from 3.35 to 6.64 LAS for cased BERT.. Explanation: FLOAT SELECTED: Table 6: Results for POS tagging (standard deviation in parentheses)",
    "answer_b": "What languages are they trained on?\nFor the first time, we show that multilingual BERT can outperform the state-of-the-art multilingual BERT model on a range of tasks. We show that the new model outperforms the multilingual model by 1.5% on part-of-speech tagging, 1.0% on named entity recognition, and 0.5% on dependency parsing. We also show that the new model outperforms the multilingual model by 1.5% on part-of-speech tagging, 1.0% on named entity recognition, and 0.5% on",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much did the new model outperform multilingual BERT?\n\nGround Truth Answer:\n\n\nAnswer A:\nFor POS,  improvements for cased BERT are 1.26 2.52  0.5 for TDT,  FTB and PUD datasets respectively.\nFor NER in-domain test set, improvement is  2.11 F1 and for NER out-of-domain test set, improvement is  5.32 F1.\nFor Dependency parsing, improvements are in range from 3.35 to 6.64 LAS for cased BERT.. Explanation: FLOAT SELECTED: Table 6: Results for POS tagging (standard deviation in parentheses)\n\nAnswer B:\nWhat languages are they trained on?\nFor the first time, we show that multilingual BERT can outperform the state-of-the-art multilingual BERT model on a range of tasks. We show that the new model outperforms the multilingual model by 1.5% on part-of-speech tagging, 1.0% on named entity recognition, and 0.5% on dependency parsing. We also show that the new model outperforms the multilingual model by 1.5% on part-of-speech tagging, 1.0% on named entity recognition, and 0.5% on\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.07076",
    "question": "What previous proposed methods did they explore?",
    "gold_answer": "",
    "answer_a": "What were the results?\n< Explanation: We first compare the performance of the multilingual BERT model with a new Finnish BERT model trained from scratch. We find that the new model outperforms the multilingual model by a large margin on all tasks, with the exception of named entity recognition. We also find that the new model is able to outperform the multilingual model on all tasks, with the exception of part-of-speech tagging. We also find that the new model is able to outperform the multilingual model on all tasks, with the exception of dependency parsing. We also find that the new model is able to out",
    "answer_b": "ELMo. Explanation: .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat previous proposed methods did they explore?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat were the results?\n< Explanation: We first compare the performance of the multilingual BERT model with a new Finnish BERT model trained from scratch. We find that the new model outperforms the multilingual model by a large margin on all tasks, with the exception of named entity recognition. We also find that the new model is able to outperform the multilingual model on all tasks, with the exception of part-of-speech tagging. We also find that the new model is able to outperform the multilingual model on all tasks, with the exception of dependency parsing. We also find that the new model is able to out\n\nAnswer B:\nELMo. Explanation: .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.07076",
    "question": "What was the new Finnish model trained on?",
    "gold_answer": "",
    "answer_a": "How was it trained?\n< Explanation: We trained a new Finnish BERT model on the Finnish Wikipedia corpus, which contains 1.5 million sentences. We used the same architecture as the original BERT model, but with a few modifications to improve the performance on the Finnish Wikipedia corpus. We used the same pre-trained weights for the Finnish Wikipedia corpus, and fine-tuned the weights for the Finnish Wikipedia corpus to the Finnish Wikipedia corpus. We used the same training parameters for the Finnish Wikipedia corpus, and fine-tuned the weights for the Finnish Wikipedia corpus to the Finnish Wikipedia corpus. We used the same training parameters for the Finnish Wikipedia corpus",
    "answer_b": "Yle corpus. Explanation: We combine two major sources of Finnish news: the Yle corpus, an archive of news published by Finland's national public broadcasting company in the years 2011-2018, and The STT corpus of newswire articles sent to media outlets by the Finnish News Agency (STT) between 1992 and 2018.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat was the new Finnish model trained on?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow was it trained?\n< Explanation: We trained a new Finnish BERT model on the Finnish Wikipedia corpus, which contains 1.5 million sentences. We used the same architecture as the original BERT model, but with a few modifications to improve the performance on the Finnish Wikipedia corpus. We used the same pre-trained weights for the Finnish Wikipedia corpus, and fine-tuned the weights for the Finnish Wikipedia corpus to the Finnish Wikipedia corpus. We used the same training parameters for the Finnish Wikipedia corpus, and fine-tuned the weights for the Finnish Wikipedia corpus to the Finnish Wikipedia corpus. We used the same training parameters for the Finnish Wikipedia corpus\n\nAnswer B:\nYle corpus. Explanation: We combine two major sources of Finnish news: the Yle corpus, an archive of news published by Finland's national public broadcasting company in the years 2011-2018, and The STT corpus of newswire articles sent to media outlets by the Finnish News Agency (STT) between 1992 and 2018.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.02378",
    "question": "How many TV series are considered?",
    "gold_answer": "",
    "answer_a": "What are the categories?\n< Explanation: We collected 1000 reviews of 100 TV series from 2016 to 2018. The dataset was divided into training and testing sets. The training set was used to train the models and the testing set was used to evaluate the performance of the models. The dataset was collected from the following websites: http://www.1000reviews.com/ and http://www.1000reviews.com/. The dataset was collected from 100 TV series, which were selected from the following categories: 1) TV series with a",
    "answer_b": "3. Explanation: For the sake of popularity, we choose \u201cThe Journey of Flower\u201d, \u201cNirvana in Fire\u201d and \u201cGood Time\u201d as parts of our movie review dataset, which are the hottest TV series from summer to fall 2015.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many TV series are considered?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the categories?\n< Explanation: We collected 1000 reviews of 100 TV series from 2016 to 2018. The dataset was divided into training and testing sets. The training set was used to train the models and the testing set was used to evaluate the performance of the models. The dataset was collected from the following websites: http://www.1000reviews.com/ and http://www.1000reviews.com/. The dataset was collected from 100 TV series, which were selected from the following categories: 1) TV series with a\n\nAnswer B:\n3. Explanation: For the sake of popularity, we choose \u201cThe Journey of Flower\u201d, \u201cNirvana in Fire\u201d and \u201cGood Time\u201d as parts of our movie review dataset, which are the hottest TV series from summer to fall 2015.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.02378",
    "question": "How long is the dataset?",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n< Explanation: The dataset consists of 1000 reviews of 10 hot TV series in China. The dataset is divided into training and testing sets. The training set contains 700 reviews and the testing set contains 300 reviews. The dataset is available at http://www.csie.ntu.edu.tw/~cjlin/onlineTVseries/onlineTVseries.html\n\nThe size of the dataset is 1000 reviews, which is 700 reviews in training set and 300 reviews in testing set. The average length of the review is",
    "answer_b": "Answer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series.. Explanation: The statistics of our TV series review dataset is shown in Table TABREF7 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long is the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n< Explanation: The dataset consists of 1000 reviews of 10 hot TV series in China. The dataset is divided into training and testing sets. The training set contains 700 reviews and the testing set contains 300 reviews. The dataset is available at http://www.csie.ntu.edu.tw/~cjlin/onlineTVseries/onlineTVseries.html\n\nThe size of the dataset is 1000 reviews, which is 700 reviews in training set and 300 reviews in testing set. The average length of the review is\n\nAnswer B:\nAnswer with content missing: (Table 2) Dataset contains 19062 reviews from 3 tv series.. Explanation: The statistics of our TV series review dataset is shown in Table TABREF7 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.02378",
    "question": "What are the eight predefined categories?",
    "gold_answer": "",
    "answer_a": "How are they defined?\n< Explanation: The eight predefined categories are: 1) Positive, 2) Negative, 3) Neutral, 4) Suspicious, 5) Unrelated, 6) Other, 7) Other, and 8) Other. The first three categories are used to classify positive, negative and neutral reviews, while the last five categories are used to classify other categories. The first three categories are used to classify positive, negative and neutral reviews, while the last five categories are used to classify other categories. The first three categories are used to classify positive, negative and neutral reviews, while the",
    "answer_b": "Plot of the TV series, Actor/actress, Role, Dialogue, Analysis, Platform, Thumb up or down, Noise or others. Explanation: FLOAT SELECTED: Table 3: Categories of Movie Reviews",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the eight predefined categories?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they defined?\n< Explanation: The eight predefined categories are: 1) Positive, 2) Negative, 3) Neutral, 4) Suspicious, 5) Unrelated, 6) Other, 7) Other, and 8) Other. The first three categories are used to classify positive, negative and neutral reviews, while the last five categories are used to classify other categories. The first three categories are used to classify positive, negative and neutral reviews, while the last five categories are used to classify other categories. The first three categories are used to classify positive, negative and neutral reviews, while the\n\nAnswer B:\nPlot of the TV series, Actor/actress, Role, Dialogue, Analysis, Platform, Thumb up or down, Noise or others. Explanation: FLOAT SELECTED: Table 3: Categories of Movie Reviews\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.08392",
    "question": "When the authors say their method largely outperforms the baseline, does this mean that the baseline performed better in some cases? If so, which ones?",
    "gold_answer": "",
    "answer_a": "If not, why not?\n<sep><br></br>Answer: The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model",
    "answer_b": "Baseline performed better in \"Fascinating\" and \"Jaw-dropping\" categories.. Explanation: FLOAT SELECTED: Table 4: Recall for various rating categories. The reason we choose recall is for making comparison with the results reported by Weninger et al. (2013).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhen the authors say their method largely outperforms the baseline, does this mean that the baseline performed better in some cases? If so, which ones?\n\nGround Truth Answer:\n\n\nAnswer A:\nIf not, why not?\n<sep><br></br>Answer: The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model is a simple linear regression model. The baseline model\n\nAnswer B:\nBaseline performed better in \"Fascinating\" and \"Jaw-dropping\" categories.. Explanation: FLOAT SELECTED: Table 4: Recall for various rating categories. The reason we choose recall is for making comparison with the results reported by Weninger et al. (2013).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.08392",
    "question": "What baseline method was used?",
    "gold_answer": "",
    "answer_a": "How was the baseline evaluated?\n< Explanation: We use the baseline method of the previous work (BIBREF10) to evaluate our model. The baseline model is a simple neural network that predicts the ratings using the word embeddings of the TED talk transcripts. The baseline model is trained using the TED talk transcripts and the ratings provided by the online viewers. The baseline model is trained using the TED talk transcripts and the ratings provided by the online viewers. The baseline model is trained using the TED talk transcripts and the ratings provided by the online viewers. The baseline model is trained using the TED talk transcripts and the ratings provided by the online viewers.",
    "answer_b": "LinearSVM, LASSO, Weninger at al. (SVM). Explanation: FLOAT SELECTED: Table 3: Average F-score, Precision, Recall and Accuracy for various models. Due to the choice of the median thresholds, the precision, recall, F-score, and accuracy values are practically identical in our experiments.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat baseline method was used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow was the baseline evaluated?\n< Explanation: We use the baseline method of the previous work (BIBREF10) to evaluate our model. The baseline model is a simple neural network that predicts the ratings using the word embeddings of the TED talk transcripts. The baseline model is trained using the TED talk transcripts and the ratings provided by the online viewers. The baseline model is trained using the TED talk transcripts and the ratings provided by the online viewers. The baseline model is trained using the TED talk transcripts and the ratings provided by the online viewers. The baseline model is trained using the TED talk transcripts and the ratings provided by the online viewers.\n\nAnswer B:\nLinearSVM, LASSO, Weninger at al. (SVM). Explanation: FLOAT SELECTED: Table 3: Average F-score, Precision, Recall and Accuracy for various models. Due to the choice of the median thresholds, the precision, recall, F-score, and accuracy values are practically identical in our experiments.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.08392",
    "question": "What was the motivation for using a dependency tree based recursive architecture?",
    "gold_answer": "",
    "answer_a": "< Explanation: We use a dependency tree based recursive architecture to model the data generating process. The architecture is based on the idea of a tree structure where each node represents a speaker and each edge represents a conversation between two speakers. The nodes are connected by a tree structure where each edge represents a conversation between two speakers. The edges are weighted by the importance of the conversation. The weights are determined by the number of times the speaker has spoken in the conversation. The weights are also determined by the number of times the speaker has spoken in the conversation. The weights are also determined by the number of times the speaker has spoken in the conversation.",
    "answer_b": "It performs better than other models predicting TED talk ratings.. Explanation: Our experiments show that the dependency tree-based model can predict the TED talk ratings with slightly higher performance (average F-score 0.77) than the word sequence model (average F-score 0.76). To the best of our knowledge, this is the best performance in the literature on predicting the TED talk ratings.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat was the motivation for using a dependency tree based recursive architecture?\n\nGround Truth Answer:\n\n\nAnswer A:\n< Explanation: We use a dependency tree based recursive architecture to model the data generating process. The architecture is based on the idea of a tree structure where each node represents a speaker and each edge represents a conversation between two speakers. The nodes are connected by a tree structure where each edge represents a conversation between two speakers. The edges are weighted by the importance of the conversation. The weights are determined by the number of times the speaker has spoken in the conversation. The weights are also determined by the number of times the speaker has spoken in the conversation. The weights are also determined by the number of times the speaker has spoken in the conversation.\n\nAnswer B:\nIt performs better than other models predicting TED talk ratings.. Explanation: Our experiments show that the dependency tree-based model can predict the TED talk ratings with slightly higher performance (average F-score 0.77) than the word sequence model (average F-score 0.76). To the best of our knowledge, this is the best performance in the literature on predicting the TED talk ratings.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.08392",
    "question": "How was a causal diagram used to carefully remove this bias?",
    "gold_answer": "",
    "answer_a": "By confining to transcripts only and normalizing ratings to remove the effects of speaker's reputations, popularity gained by publicity, contemporary hot topics, etc.. Explanation: We use a Causal Diagram BIBREF5 , BIBREF6 to analyze and remove the effects of the data bias (e.g., the speakers' reputations, popularity gained by publicity, etc.) in our prediction model. In order to make the prediction model less biased to the speakers' race and gender, we confine our analysis to the transcripts only. Besides, we normalize the ratings to remove the effects of the unwanted variables such as the speakers' reputations, publicity, contemporary hot topics, etc.",
    "answer_b": "What is the causal diagram?\n< Explanation: We first remove the bias by removing the speaker's reputation from the dataset. We then remove the popularity gained by publicity from the dataset. We then remove the speaker's reputation and popularity from the dataset. We then remove the speaker's reputation and popularity from the dataset. We then remove the speaker's reputation and popularity from the dataset. We then remove the speaker's reputation and popularity from the dataset. We then remove the speaker's reputation and popularity from the dataset. We then remove the speaker's reputation and popularity from the dataset. We then remove the speaker's reputation and popularity from the dataset. We",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow was a causal diagram used to carefully remove this bias?\n\nGround Truth Answer:\n\n\nAnswer A:\nBy confining to transcripts only and normalizing ratings to remove the effects of speaker's reputations, popularity gained by publicity, contemporary hot topics, etc.. Explanation: We use a Causal Diagram BIBREF5 , BIBREF6 to analyze and remove the effects of the data bias (e.g., the speakers' reputations, popularity gained by publicity, etc.) in our prediction model. In order to make the prediction model less biased to the speakers' race and gender, we confine our analysis to the transcripts only. Besides, we normalize the ratings to remove the effects of the unwanted variables such as the speakers' reputations, publicity, contemporary hot topics, etc.\n\nAnswer B:\nWhat is the causal diagram?\n< Explanation: We first remove the bias by removing the speaker's reputation from the dataset. We then remove the popularity gained by publicity from the dataset. We then remove the speaker's reputation and popularity from the dataset. We then remove the speaker's reputation and popularity from the dataset. We then remove the speaker's reputation and popularity from the dataset. We then remove the speaker's reputation and popularity from the dataset. We then remove the speaker's reputation and popularity from the dataset. We then remove the speaker's reputation and popularity from the dataset. We then remove the speaker's reputation and popularity from the dataset. We\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.11161",
    "question": "What is the state-of-the-art approach?",
    "gold_answer": "",
    "answer_a": "What is the difference between the proposed approach and the state-of-the-art approach?\n<sep><hr>\nThe proposed approach is based on the recently released Empathetic-Dialogues corpus. The proposed approach is based on the recently released Empathetic-Dialogues corpus. The proposed approach is based on the recently released Empathetic-Dialogues corpus. The proposed approach is based on the recently released Empathetic-Dialogues corpus. The proposed approach is based on the recently released Empathetic-Dialogues corpus. The proposed approach is based on the recently released Empathetic-Dialogues corpus. The proposed approach is based",
    "answer_b": "Rashkin et al. BIBREF3. Explanation: We first compare the performance of our approach with the baseline results obtained from Rashkin et al. BIBREF3 that uses a full transformer architecture BIBREF26, consisting of an encoder and decoder.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the state-of-the-art approach?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the proposed approach and the state-of-the-art approach?\n<sep><hr>\nThe proposed approach is based on the recently released Empathetic-Dialogues corpus. The proposed approach is based on the recently released Empathetic-Dialogues corpus. The proposed approach is based on the recently released Empathetic-Dialogues corpus. The proposed approach is based on the recently released Empathetic-Dialogues corpus. The proposed approach is based on the recently released Empathetic-Dialogues corpus. The proposed approach is based on the recently released Empathetic-Dialogues corpus. The proposed approach is based\n\nAnswer B:\nRashkin et al. BIBREF3. Explanation: We first compare the performance of our approach with the baseline results obtained from Rashkin et al. BIBREF3 that uses a full transformer architecture BIBREF26, consisting of an encoder and decoder.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1710.07695",
    "question": "what do they mean by description length?",
    "gold_answer": "",
    "answer_a": "the code length of phrases.. Explanation: Given verb phrases, we seek for the best assignment function INLINEFORM0 that minimizes the code length of phrases. Let INLINEFORM1 be the code length derived by INLINEFORM2 .",
    "answer_b": "How is it calculated?\n< Explanation: The description length of a word is the number of bits needed to encode the word in a dictionary. The description length of a word is the number of bits needed to encode the word in a dictionary. The description length of a word is the number of bits needed to encode the word in a dictionary. The description length of a word is the number of bits needed to encode the word in a dictionary. The description length of a word is the number of bits needed to encode the word in a dictionary. The description length of a word is the number of bits needed to encode the word in a dictionary.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat do they mean by description length?\n\nGround Truth Answer:\n\n\nAnswer A:\nthe code length of phrases.. Explanation: Given verb phrases, we seek for the best assignment function INLINEFORM0 that minimizes the code length of phrases. Let INLINEFORM1 be the code length derived by INLINEFORM2 .\n\nAnswer B:\nHow is it calculated?\n< Explanation: The description length of a word is the number of bits needed to encode the word in a dictionary. The description length of a word is the number of bits needed to encode the word in a dictionary. The description length of a word is the number of bits needed to encode the word in a dictionary. The description length of a word is the number of bits needed to encode the word in a dictionary. The description length of a word is the number of bits needed to encode the word in a dictionary. The description length of a word is the number of bits needed to encode the word in a dictionary.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1710.07695",
    "question": "what evaluation metrics are used?",
    "gold_answer": "",
    "answer_a": "Explanation: We use the following metrics to evaluate the performance of the proposed model:\n\n1) BLEU: BLEU is a well-known metric for evaluating the performance of language models. It is defined as:\n\n2) ROUGE: ROUGE is a metric for evaluating the performance of named entity recognition systems. It is defined as:\n\n3) F1: F1 is a metric for evaluating the performance of language models. It is defined as:\n\n4) F1@k: F1@k is a metric for evaluating the performance of language models. It is defined as:\n\n5) F1@k@l: F1@",
    "answer_b": "coverage and precision. Explanation: To evaluate the effectiveness of our pattern summarization approach, we report two metrics: (1) ( INLINEFORM0 ) how much of the verb phrases in natural language our solution can find corresponding patterns (2) ( INLINEFORM1 ) how much of the phrases and their corresponding patterns are correctly matched?",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat evaluation metrics are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We use the following metrics to evaluate the performance of the proposed model:\n\n1) BLEU: BLEU is a well-known metric for evaluating the performance of language models. It is defined as:\n\n2) ROUGE: ROUGE is a metric for evaluating the performance of named entity recognition systems. It is defined as:\n\n3) F1: F1 is a metric for evaluating the performance of language models. It is defined as:\n\n4) F1@k: F1@k is a metric for evaluating the performance of language models. It is defined as:\n\n5) F1@k@l: F1@\n\nAnswer B:\ncoverage and precision. Explanation: To evaluate the effectiveness of our pattern summarization approach, we report two metrics: (1) ( INLINEFORM0 ) how much of the verb phrases in natural language our solution can find corresponding patterns (2) ( INLINEFORM1 ) how much of the phrases and their corresponding patterns are correctly matched?\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.05195",
    "question": "What is the relationship between author and emotional valence?",
    "gold_answer": "",
    "answer_a": "people have different baseline emotional valences from one another. Explanation: Different locations, times and authors have different emotional valences. For instance, people are generally happier on weekends and certain hours of the day, more depressed at the end of summer holidays, and happier in certain states in the United States. Moreover, people have different baseline emotional valences from one another.",
    "answer_b": "How is this relationship measured?\n< Explanation: We found that the emotional valence of tweets is strongly correlated with the author. The authors with the highest valence tend to be more positive and the authors with the lowest valence tend to be more negative. We also found that the valence of tweets is positively correlated with the number of likes and negatively correlated with the number of retweets. We also found that the valence of tweets is positively correlated with the number of mentions and negatively correlated with the number of replies. We also found that the valence of tweets is positively correlated with the number of hashtags and negatively correlated with the number of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the relationship between author and emotional valence?\n\nGround Truth Answer:\n\n\nAnswer A:\npeople have different baseline emotional valences from one another. Explanation: Different locations, times and authors have different emotional valences. For instance, people are generally happier on weekends and certain hours of the day, more depressed at the end of summer holidays, and happier in certain states in the United States. Moreover, people have different baseline emotional valences from one another.\n\nAnswer B:\nHow is this relationship measured?\n< Explanation: We found that the emotional valence of tweets is strongly correlated with the author. The authors with the highest valence tend to be more positive and the authors with the lowest valence tend to be more negative. We also found that the valence of tweets is positively correlated with the number of likes and negatively correlated with the number of retweets. We also found that the valence of tweets is positively correlated with the number of mentions and negatively correlated with the number of replies. We also found that the valence of tweets is positively correlated with the number of hashtags and negatively correlated with the number of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.05195",
    "question": "What is the relationship between time and emotional valence?",
    "gold_answer": "",
    "answer_a": "How is this relationship captured in the data?\n<blockquote>Time is a key factor in determining the emotional valence of a tweet. Tweets are often written in a short burst, with a time interval between the start and end of the tweet. The time interval between tweets is a key factor in determining the emotional valence of a tweet. For example, a tweet written at 10:00am is likely to be more positive than a tweet written at 10:30am. </blockquote>\n\nThe time interval between tweets is a key factor in determining the emotional valence of a tweet. Tweets are often written in",
    "answer_b": "people are generally happier on weekends and certain hours of the day, more depressed at the end of summer holidays. Explanation: Different locations, times and authors have different emotional valences. For instance, people are generally happier on weekends and certain hours of the day, more depressed at the end of summer holidays, and happier in certain states in the United States.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the relationship between time and emotional valence?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is this relationship captured in the data?\n<blockquote>Time is a key factor in determining the emotional valence of a tweet. Tweets are often written in a short burst, with a time interval between the start and end of the tweet. The time interval between tweets is a key factor in determining the emotional valence of a tweet. For example, a tweet written at 10:00am is likely to be more positive than a tweet written at 10:30am. </blockquote>\n\nThe time interval between tweets is a key factor in determining the emotional valence of a tweet. Tweets are often written in\n\nAnswer B:\npeople are generally happier on weekends and certain hours of the day, more depressed at the end of summer holidays. Explanation: Different locations, times and authors have different emotional valences. For instance, people are generally happier on weekends and certain hours of the day, more depressed at the end of summer holidays, and happier in certain states in the United States.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.05195",
    "question": "What is the relationship between location and emotional valence?",
    "gold_answer": "",
    "answer_a": "happier in certain states in the United States. Explanation: Different locations, times and authors have different emotional valences. For instance, people are generally happier on weekends and certain hours of the day, more depressed at the end of summer holidays, and happier in certain states in the United States.",
    "answer_b": "How is this relationship measured?\n< Explanation: We found that the emotional valence of tweets is positively correlated with the location of the tweet. This is shown in Figure 1. The correlation coefficient is 0.60, indicating that tweets from the same location tend to have similar emotional valence. The correlation coefficient is 0.45, indicating that tweets from the same time tend to have similar emotional valence. The correlation coefficient is 0.35, indicating that tweets from the same author tend to have similar emotional valence. The correlation coefficient is 0.25, indicating that tweets from the same author",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the relationship between location and emotional valence?\n\nGround Truth Answer:\n\n\nAnswer A:\nhappier in certain states in the United States. Explanation: Different locations, times and authors have different emotional valences. For instance, people are generally happier on weekends and certain hours of the day, more depressed at the end of summer holidays, and happier in certain states in the United States.\n\nAnswer B:\nHow is this relationship measured?\n< Explanation: We found that the emotional valence of tweets is positively correlated with the location of the tweet. This is shown in Figure 1. The correlation coefficient is 0.60, indicating that tweets from the same location tend to have similar emotional valence. The correlation coefficient is 0.45, indicating that tweets from the same time tend to have similar emotional valence. The correlation coefficient is 0.35, indicating that tweets from the same author tend to have similar emotional valence. The correlation coefficient is 0.25, indicating that tweets from the same author\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1604.05559",
    "question": "What is the computational complexity of old method",
    "gold_answer": "",
    "answer_a": "O(2**N). Explanation: Text: \u201cI like kitties and doggies\u201d\n\nWindow: 2\n\nBigrams: {(I like), (like kitties), (kitties and), (and doggies)} and this one:\n\nText: \u201cI like kitties and doggies\u201d\n\nWindow: 4\n\nBigrams: {(I like), (I kitties), (I and), (like kitties), (like and), (like doggies), (kitties and), (kitties doggies), (and doggies)}.",
    "answer_b": "?\n< Explanation: The old method is based on the following formula: where is the number of words in the corpus, is the number of words in the corpus, is the number of words in the corpus, and is the number of words in the corpus. The old method is based on the following formula: where is the number of words in the corpus, is the number of words in the corpus, is the number of words in the corpus, and is the number of words in the corpus. The old method is based on the following formula: where is the number of words in the corpus, is the number of words in the corpus",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the computational complexity of old method\n\nGround Truth Answer:\n\n\nAnswer A:\nO(2**N). Explanation: Text: \u201cI like kitties and doggies\u201d\n\nWindow: 2\n\nBigrams: {(I like), (like kitties), (kitties and), (and doggies)} and this one:\n\nText: \u201cI like kitties and doggies\u201d\n\nWindow: 4\n\nBigrams: {(I like), (I kitties), (I and), (like kitties), (like and), (like doggies), (kitties and), (kitties doggies), (and doggies)}.\n\nAnswer B:\n?\n< Explanation: The old method is based on the following formula: where is the number of words in the corpus, is the number of words in the corpus, is the number of words in the corpus, and is the number of words in the corpus. The old method is based on the following formula: where is the number of words in the corpus, is the number of words in the corpus, is the number of words in the corpus, and is the number of words in the corpus. The old method is based on the following formula: where is the number of words in the corpus, is the number of words in the corpus\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1604.05559",
    "question": "Could you tell me more about the old method?",
    "gold_answer": "",
    "answer_a": "freq(*, word) = freq(word, *) = freq(word). Explanation: Bigram frequencies are often calculated using the approximation\n\n$$freq(*, word) = freq(word, *) = freq(word)$$ (Eq. 1)\n\nIn a much cited paper, Church and Hanks BIBREF0 use ` $=$ ' in place of ` $\\approx $ ' because the approximation is so good. Indeed, this approximation will only cause errors for the very few words which occur near the beginning or the end of the text. Take for example the text appearing above - the bigram (doggies, *) does not occur once, but the approximation says it does.\n\nAn efficient method for computing the contingency matrix for a bigram (word1, word2) is suggested by the approximation. Store $freq(w1, w2)$ for all bigrams $(w1, w2)$ and the frequencies of all words. Then,\n\nThe statistical importance of miscalculations due to this method diminishes as our text grows larger and larger. Interest is growing in the analysis of small texts, however, and a means of computing bigrams for this type of corpus must be employed. This approximation is implemented in popular NLP libraries and can be seen in many tutorials across the internet. People who use this code, or write their own software, must know when it is appropriate.",
    "answer_b": "Explanation: The old method is based on the fact that the frequency of a word is proportional to the number of times it occurs in the corpus. The frequency of a word is then calculated as the number of times it occurs in the corpus divided by the total number of words in the corpus. The old method is based on the fact that the frequency of a word is proportional to the number of times it occurs in the corpus. The frequency of a word is then calculated as the number of times it occurs in the corpus divided by the total number of words in the corpus. The old method is based on the fact that the frequency of a word",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nCould you tell me more about the old method?\n\nGround Truth Answer:\n\n\nAnswer A:\nfreq(*, word) = freq(word, *) = freq(word). Explanation: Bigram frequencies are often calculated using the approximation\n\n$$freq(*, word) = freq(word, *) = freq(word)$$ (Eq. 1)\n\nIn a much cited paper, Church and Hanks BIBREF0 use ` $=$ ' in place of ` $\\approx $ ' because the approximation is so good. Indeed, this approximation will only cause errors for the very few words which occur near the beginning or the end of the text. Take for example the text appearing above - the bigram (doggies, *) does not occur once, but the approximation says it does.\n\nAn efficient method for computing the contingency matrix for a bigram (word1, word2) is suggested by the approximation. Store $freq(w1, w2)$ for all bigrams $(w1, w2)$ and the frequencies of all words. Then,\n\nThe statistical importance of miscalculations due to this method diminishes as our text grows larger and larger. Interest is growing in the analysis of small texts, however, and a means of computing bigrams for this type of corpus must be employed. This approximation is implemented in popular NLP libraries and can be seen in many tutorials across the internet. People who use this code, or write their own software, must know when it is appropriate.\n\nAnswer B:\nExplanation: The old method is based on the fact that the frequency of a word is proportional to the number of times it occurs in the corpus. The frequency of a word is then calculated as the number of times it occurs in the corpus divided by the total number of words in the corpus. The old method is based on the fact that the frequency of a word is proportional to the number of times it occurs in the corpus. The frequency of a word is then calculated as the number of times it occurs in the corpus divided by the total number of words in the corpus. The old method is based on the fact that the frequency of a word\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.03056",
    "question": "How this system recommend features for the new application?",
    "gold_answer": "",
    "answer_a": "What is the structure of the language?\n< Explanation: We propose a language for specifying NLP features that is based on the structure of a language for specifying NLP features. We call this language the Feature Language (FL). The FL is a language that is used to specify features for a given application. The FL is a language that is used to specify features for a given application. The FL is a language that is used to specify features for a given application. The FL is a language that is used to specify features for a given application. The FL is a language that is used to specify features for a given application. The FL is",
    "answer_b": "estimating its semantic proximity (from the perspective of a NLP data scientist) with existing applications with known features. Explanation: For a new text analytics application requiring feature engineering, it starts with estimating its semantic proximity (from the perspective of a NLP data scientist) with existing applications with known features. Based upon these proximity estimates as well as expected relevance of features for existing applications, system would recommend features for the new application in a ranked order.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow this system recommend features for the new application?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the structure of the language?\n< Explanation: We propose a language for specifying NLP features that is based on the structure of a language for specifying NLP features. We call this language the Feature Language (FL). The FL is a language that is used to specify features for a given application. The FL is a language that is used to specify features for a given application. The FL is a language that is used to specify features for a given application. The FL is a language that is used to specify features for a given application. The FL is a language that is used to specify features for a given application. The FL is\n\nAnswer B:\nestimating its semantic proximity (from the perspective of a NLP data scientist) with existing applications with known features. Explanation: For a new text analytics application requiring feature engineering, it starts with estimating its semantic proximity (from the perspective of a NLP data scientist) with existing applications with known features. Based upon these proximity estimates as well as expected relevance of features for existing applications, system would recommend features for the new application in a ranked order.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.03056",
    "question": "What is the similarity of manually selected features across related applications in different domains?",
    "gold_answer": "",
    "answer_a": "Applications share similar sets of features (of the 7 set of features, 6 selected are the same). Explanation: Table TABREF21 below depicts classes of features selected by authors of these works (as described in the corresponding references above) to highlight the point that despite domain differences, these applications share similar sets of features.",
    "answer_b": "What is the similarity of the features across applications?\n<blockquote>\n<font size=\"2\">The features are mostly related to the domain of the application. For example, the features of the application for the domain of the application are mostly related to the domain of the application. For example, the features of the application for the domain of the application are mostly related to the domain of the application. For example, the features of the application for the domain of the application are mostly related to the domain of the application. For example, the features of the application for the domain of the application are mostly related to the domain of the application. For",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the similarity of manually selected features across related applications in different domains?\n\nGround Truth Answer:\n\n\nAnswer A:\nApplications share similar sets of features (of the 7 set of features, 6 selected are the same). Explanation: Table TABREF21 below depicts classes of features selected by authors of these works (as described in the corresponding references above) to highlight the point that despite domain differences, these applications share similar sets of features.\n\nAnswer B:\nWhat is the similarity of the features across applications?\n<blockquote>\n<font size=\"2\">The features are mostly related to the domain of the application. For example, the features of the application for the domain of the application are mostly related to the domain of the application. For example, the features of the application for the domain of the application are mostly related to the domain of the application. For example, the features of the application for the domain of the application are mostly related to the domain of the application. For example, the features of the application for the domain of the application are mostly related to the domain of the application. For\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.03056",
    "question": "What type of features are extracted with this language?",
    "gold_answer": "",
    "answer_a": "Linguistic, Semantic, and Statistical.. Explanation: FLOAT SELECTED: Figure 4: Association between Different Feature Types and Units of Analysis",
    "answer_b": "What are the advantages of using this language?\n< Explanation: We propose a language for specifying NLP features that is based on the concept of a feature graph. A feature graph is a directed graph where nodes represent features and edges represent relationships between features. The graph is constructed by specifying the features to be used in the application and the relationships between them. The graph is then used to identify the features that are most relevant to the application. We show that this approach is effective in identifying the most relevant features for a given application. We also show that the graph can be used to identify features that are not explicitly specified in the application. We show",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of features are extracted with this language?\n\nGround Truth Answer:\n\n\nAnswer A:\nLinguistic, Semantic, and Statistical.. Explanation: FLOAT SELECTED: Figure 4: Association between Different Feature Types and Units of Analysis\n\nAnswer B:\nWhat are the advantages of using this language?\n< Explanation: We propose a language for specifying NLP features that is based on the concept of a feature graph. A feature graph is a directed graph where nodes represent features and edges represent relationships between features. The graph is constructed by specifying the features to be used in the application and the relationships between them. The graph is then used to identify the features that are most relevant to the application. We show that this approach is effective in identifying the most relevant features for a given application. We also show that the graph can be used to identify features that are not explicitly specified in the application. We show\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.03056",
    "question": "What are meta elements of language for specifying NLP features?",
    "gold_answer": "",
    "answer_a": "What are the advantages of using this language?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Explanation: The language is structured as follows: <br/>1. The language is structured as a tree, with each node representing a feature. <br/>2. Each feature is represented by a node with a label and a list of children. <br/>3. Each child node represents a feature value. <br/>4. The language is structured as a tree, with each node representing a feature. <br/>5. Each feature is represented by a node with a label and a",
    "answer_b": "Analysis Unit (AU). Explanation: Analysis Unit (AU) specifies level at which features have to be extracted. At Corpus level, features are extracted for all the text documents together.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are meta elements of language for specifying NLP features?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the advantages of using this language?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Explanation: The language is structured as follows: <br/>1. The language is structured as a tree, with each node representing a feature. <br/>2. Each feature is represented by a node with a label and a list of children. <br/>3. Each child node represents a feature value. <br/>4. The language is structured as a tree, with each node representing a feature. <br/>5. Each feature is represented by a node with a label and a\n\nAnswer B:\nAnalysis Unit (AU). Explanation: Analysis Unit (AU) specifies level at which features have to be extracted. At Corpus level, features are extracted for all the text documents together.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1904.02306",
    "question": "what previous work do they also look at?",
    "gold_answer": "",
    "answer_a": "< Explanation: We also compare our model to the previous work of BIBREF10, which uses a combination of morphological tagging and lemmatization. BIBREF10 use a combination of morphological tagging and lemmatization to train a neural model for lemmatization. BIBREF10 use a combination of morphological tagging and lemmatization to train a neural model for morphological tagging. BIBREF10 use a combination of morphological tagging and lemmatization to train a neural model for morphological tagging. BIBREF10 use a combination of morphological tagging",
    "answer_b": "N18-1126. Explanation: Baselines (and Related Work)\nWe compare our approach against recent competing methods that report results on UD datasets.\n\nThe current state of the art is held by N18-1126, who, as discussed in sec:introduction, provide a direct context-to-lemma approach, avoiding the use of morphological tags.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat previous work do they also look at?\n\nGround Truth Answer:\n\n\nAnswer A:\n< Explanation: We also compare our model to the previous work of BIBREF10, which uses a combination of morphological tagging and lemmatization. BIBREF10 use a combination of morphological tagging and lemmatization to train a neural model for lemmatization. BIBREF10 use a combination of morphological tagging and lemmatization to train a neural model for morphological tagging. BIBREF10 use a combination of morphological tagging and lemmatization to train a neural model for morphological tagging. BIBREF10 use a combination of morphological tagging\n\nAnswer B:\nN18-1126. Explanation: Baselines (and Related Work)\nWe compare our approach against recent competing methods that report results on UD datasets.\n\nThe current state of the art is held by N18-1126, who, as discussed in sec:introduction, provide a direct context-to-lemma approach, avoiding the use of morphological tags.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1904.02306",
    "question": "what languages did they experiment with?",
    "gold_answer": "",
    "answer_a": "< Explanation: We tested our model on 20 languages from the Universal Dependencies corpus. We used the following criteria for evaluation: (1) the number of correctly tagged words, (2) the number of correctly tagged sentences, and (3) the number of correctly tagged sentences per word. We also used the following metrics: (1) the number of correctly tagged words, (2) the number of correctly tagged sentences, and (3) the number of correctly tagged sentences per word. We also used the following metrics: (1) the number of correctly tagged words, (2) the number of correctly tagged sentences, and",
    "answer_b": "They experiment with: arabic, basque, croatian, dutch, estonian, finnish, german, greek, hindi, hungarian, italian, latvian, polish, portuguese, romanian, russian, slovak, slovenian, turkish and urdu.. Explanation: FLOAT SELECTED: Table 3: Development performance breakdown.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat languages did they experiment with?\n\nGround Truth Answer:\n\n\nAnswer A:\n< Explanation: We tested our model on 20 languages from the Universal Dependencies corpus. We used the following criteria for evaluation: (1) the number of correctly tagged words, (2) the number of correctly tagged sentences, and (3) the number of correctly tagged sentences per word. We also used the following metrics: (1) the number of correctly tagged words, (2) the number of correctly tagged sentences, and (3) the number of correctly tagged sentences per word. We also used the following metrics: (1) the number of correctly tagged words, (2) the number of correctly tagged sentences, and\n\nAnswer B:\nThey experiment with: arabic, basque, croatian, dutch, estonian, finnish, german, greek, hindi, hungarian, italian, latvian, polish, portuguese, romanian, russian, slovak, slovenian, turkish and urdu.. Explanation: FLOAT SELECTED: Table 3: Development performance breakdown.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.00864",
    "question": "What are state of the art results on OSA and PD corpora used for testing?",
    "gold_answer": "",
    "answer_a": "What is the difference between x-vectors and i-vectors?\n< Explanation: We report results for two European Portuguese corpora, for OSA and PD, as well as for an additional Spanish corpus for PD. We also report results for an out-of-domain corpus for OSA. The results are presented in Table 1. We report the average performance of the x-vectors and i-vectors, as well as the average performance of the x-vectors and i-vectors, as a function of the number of features. We also report the average performance of the x-vectors and i-vectors, as a function of the number",
    "answer_b": "PD : i-vectors had segment level F1 score 66.6 and for speaker level had 75.6 F1 score\n\nOSA: For the same levels it had F1 scores of 65.5 and 75.0. Explanation: Until recently, i-vectors have been considered the state-of-the-art method for speaker recognition. An extension of the GMM Supervector, the i-vector approach models the variability present in the Supervector, as a low-rank total variability space. Using factor analysis, it is possible to extract low-dimensional total variability factors, called i-vectors, that provide a powerful and compact representation of speech segments BIBREF23, BIBREF25, BIBREF26.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are state of the art results on OSA and PD corpora used for testing?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between x-vectors and i-vectors?\n< Explanation: We report results for two European Portuguese corpora, for OSA and PD, as well as for an additional Spanish corpus for PD. We also report results for an out-of-domain corpus for OSA. The results are presented in Table 1. We report the average performance of the x-vectors and i-vectors, as well as the average performance of the x-vectors and i-vectors, as a function of the number of features. We also report the average performance of the x-vectors and i-vectors, as a function of the number\n\nAnswer B:\nPD : i-vectors had segment level F1 score 66.6 and for speaker level had 75.6 F1 score\n\nOSA: For the same levels it had F1 scores of 65.5 and 75.0. Explanation: Until recently, i-vectors have been considered the state-of-the-art method for speaker recognition. An extension of the GMM Supervector, the i-vector approach models the variability present in the Supervector, as a low-rank total variability space. Using factor analysis, it is possible to extract low-dimensional total variability factors, called i-vectors, that provide a powerful and compact representation of speech segments BIBREF23, BIBREF25, BIBREF26.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.00864",
    "question": "How better does x-vectors perform than knowlege-based features in same-language corpora?",
    "gold_answer": "",
    "answer_a": "What is the difference between x-vectors and i-vectors?\n< Explanation: We compare the performance of x-vectors and i-vectors in the same-language corpora. We find that x-vectors outperform i-vectors in both the same-language and out-of-domain settings. In the same-language setting, x-vectors outperform i-vectors by 1.5% in the PD corpus and 1.2% in the OSA corpus. In the out-of-domain setting, x-vectors outperform i-vectors by 1.5% in the PD corpus and 1.2% in the OSA",
    "answer_b": "For OSA detection x-vectors outperform all other approaches at the segment level, most importantly they significantly outperform KB features by 8%.. Explanation: Table TABREF21 contains the results for OSA detection with the PSD corpus. For this task, x-vectors outperform all other approaches at the segment level, most importantly they significantly outperform KB features by $\\sim $8%, which further supports our hypothesis.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow better does x-vectors perform than knowlege-based features in same-language corpora?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between x-vectors and i-vectors?\n< Explanation: We compare the performance of x-vectors and i-vectors in the same-language corpora. We find that x-vectors outperform i-vectors in both the same-language and out-of-domain settings. In the same-language setting, x-vectors outperform i-vectors by 1.5% in the PD corpus and 1.2% in the OSA corpus. In the out-of-domain setting, x-vectors outperform i-vectors by 1.5% in the PD corpus and 1.2% in the OSA\n\nAnswer B:\nFor OSA detection x-vectors outperform all other approaches at the segment level, most importantly they significantly outperform KB features by 8%.. Explanation: Table TABREF21 contains the results for OSA detection with the PSD corpus. For this task, x-vectors outperform all other approaches at the segment level, most importantly they significantly outperform KB features by $\\sim $8%, which further supports our hypothesis.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.00864",
    "question": "What is meant by domain missmatch occuring?",
    "gold_answer": "",
    "answer_a": "How is it measured?\n< Explanation: In this work, we focus on the detection of PD and OSA, two of the most common sleep disorders. In both cases, the presence of a sleep disorder is associated with a higher risk of developing other sleep disorders, such as insomnia, sleep apnea, and restless leg syndrome. In addition, sleep disorders are associated with a higher risk of developing other health conditions, such as cardiovascular disease, diabetes, and depression. Therefore, the detection of sleep disorders is of great importance for the health care system. In this work, we focus on the detection of sleep disorders using a non-invasive non",
    "answer_b": "tasks whose domain does not match that of the training data. Explanation: It was also observed that x-vectors are more suitable than i-vectors for tasks whose domain does not match that of the training data, such as verbal task mismatch and cross-lingual experiments.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is meant by domain missmatch occuring?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it measured?\n< Explanation: In this work, we focus on the detection of PD and OSA, two of the most common sleep disorders. In both cases, the presence of a sleep disorder is associated with a higher risk of developing other sleep disorders, such as insomnia, sleep apnea, and restless leg syndrome. In addition, sleep disorders are associated with a higher risk of developing other health conditions, such as cardiovascular disease, diabetes, and depression. Therefore, the detection of sleep disorders is of great importance for the health care system. In this work, we focus on the detection of sleep disorders using a non-invasive non\n\nAnswer B:\ntasks whose domain does not match that of the training data. Explanation: It was also observed that x-vectors are more suitable than i-vectors for tasks whose domain does not match that of the training data, such as verbal task mismatch and cross-lingual experiments.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.00864",
    "question": "How big are OSA and PD corporas used for testing?",
    "gold_answer": "",
    "answer_a": "For Portuguese PD have  for patient 1.24h and for control 1.07 h.\nFor Portuguese OSA have  for patient 1.10h and for control 1.05 h.\nFor Spanish PD have for patient 0.49h  and for control 0.50h.. Explanation: FLOAT SELECTED: TABLE I CORPORA DESCRIPTION.",
    "answer_b": "What is the size of the Spanish corpus?\n< Explanation: The Spanish corpus used for the PD and OSA studies is the Spanish version of the Spanish PD and OSA databases (SPE-PSD and SPE-OSA, respectively). The Spanish version of the SPE-PSD database is a collection of 10,000 speech samples from 100 patients with PD and 100 patients with OSA. The Spanish version of the SPE-OSA database is a collection of 10,000 speech samples from 100 patients with OSA. The Spanish version of the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big are OSA and PD corporas used for testing?\n\nGround Truth Answer:\n\n\nAnswer A:\nFor Portuguese PD have  for patient 1.24h and for control 1.07 h.\nFor Portuguese OSA have  for patient 1.10h and for control 1.05 h.\nFor Spanish PD have for patient 0.49h  and for control 0.50h.. Explanation: FLOAT SELECTED: TABLE I CORPORA DESCRIPTION.\n\nAnswer B:\nWhat is the size of the Spanish corpus?\n< Explanation: The Spanish corpus used for the PD and OSA studies is the Spanish version of the Spanish PD and OSA databases (SPE-PSD and SPE-OSA, respectively). The Spanish version of the SPE-PSD database is a collection of 10,000 speech samples from 100 patients with PD and 100 patients with OSA. The Spanish version of the SPE-OSA database is a collection of 10,000 speech samples from 100 patients with OSA. The Spanish version of the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.04278",
    "question": "How do they think this treebank will support research on second language acquisition?",
    "gold_answer": "",
    "answer_a": "It will improve tagging and parsing performance, syntax based grammatical error correction.. Explanation: Finally, a corpus that is annotated with both grammatical errors and syntactic dependencies paves the way for empirical investigation of the relation between grammaticality and syntax. Understanding this relation is vital for improving tagging and parsing performance on learner language BIBREF8 , syntax based grammatical error correction BIBREF9 , BIBREF10 , and many other fundamental challenges in NLP.",
    "answer_b": "What are the goals of the research?\n<sep><xml:lang=\"en-US\">The treebank will be used to support research on second language acquisition, including the following areas: </xml:section>1. The treebank will be used to support research on second language acquisition, including the following areas: 1. The treebank will be used to support research on second language acquisition, including the following areas: 1. The treebank will be used to support research on second language acquisition, including the following areas: 1. The treebank will be used to support research on second language acquisition, including the following areas",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they think this treebank will support research on second language acquisition?\n\nGround Truth Answer:\n\n\nAnswer A:\nIt will improve tagging and parsing performance, syntax based grammatical error correction.. Explanation: Finally, a corpus that is annotated with both grammatical errors and syntactic dependencies paves the way for empirical investigation of the relation between grammaticality and syntax. Understanding this relation is vital for improving tagging and parsing performance on learner language BIBREF8 , syntax based grammatical error correction BIBREF9 , BIBREF10 , and many other fundamental challenges in NLP.\n\nAnswer B:\nWhat are the goals of the research?\n<sep><xml:lang=\"en-US\">The treebank will be used to support research on second language acquisition, including the following areas: </xml:section>1. The treebank will be used to support research on second language acquisition, including the following areas: 1. The treebank will be used to support research on second language acquisition, including the following areas: 1. The treebank will be used to support research on second language acquisition, including the following areas: 1. The treebank will be used to support research on second language acquisition, including the following areas\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.04278",
    "question": "What are their baseline models?",
    "gold_answer": "",
    "answer_a": "How do they compare to the baseline?\n< Explanation: We compare our baseline models to the following: (1) a baseline model that uses the Universal Dependencies (UD) annotation to annotate the POS tags and (2) a baseline model that uses the Universal Dependencies (UD) annotation to annotate the UD trees. We also compare our baseline models to the following: (3) a baseline model that uses the Universal Dependencies (UD) annotation to annotate the POS tags and (4) a baseline model that uses the Universal Dependencies (UD) annotation to annotate the UD trees. We also compare our baseline models to the following: (5) a",
    "answer_b": "version 2.2 of the Turbo tagger and Turbo parser BIBREF18. Explanation: In this, and subsequent experiments, we utilize version 2.2 of the Turbo tagger and Turbo parser BIBREF18 , state of the art tools for statistical POS tagging and dependency parsing.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are their baseline models?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they compare to the baseline?\n< Explanation: We compare our baseline models to the following: (1) a baseline model that uses the Universal Dependencies (UD) annotation to annotate the POS tags and (2) a baseline model that uses the Universal Dependencies (UD) annotation to annotate the UD trees. We also compare our baseline models to the following: (3) a baseline model that uses the Universal Dependencies (UD) annotation to annotate the POS tags and (4) a baseline model that uses the Universal Dependencies (UD) annotation to annotate the UD trees. We also compare our baseline models to the following: (5) a\n\nAnswer B:\nversion 2.2 of the Turbo tagger and Turbo parser BIBREF18. Explanation: In this, and subsequent experiments, we utilize version 2.2 of the Turbo tagger and Turbo parser BIBREF18 , state of the art tools for statistical POS tagging and dependency parsing.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.04278",
    "question": "How long is the dataset?",
    "gold_answer": "",
    "answer_a": "5124. Explanation: The TLE currently contains 5,124 sentences (97,681 tokens) with POS tag and dependency annotations in the English Universal Dependencies (UD) formalism BIBREF2 , BIBREF3 .",
    "answer_b": "What is the size of the dataset?\n< Explanation: The corpus consists of 5,124 sentences from the Cambridge First Certificate in English (FCE) corpus. The corpus is divided into 1,024 sentences for training, 1,024 for validation, and 1,120 for testing. The corpus is annotated with 1,024 sentences for POS tagging and 1,120 for UD trees. The corpus is annotated with 1,024 sentences for POS tagging and 1,120 for UD trees. The corpus is annotated with 1",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long is the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\n5124. Explanation: The TLE currently contains 5,124 sentences (97,681 tokens) with POS tag and dependency annotations in the English Universal Dependencies (UD) formalism BIBREF2 , BIBREF3 .\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: The corpus consists of 5,124 sentences from the Cambridge First Certificate in English (FCE) corpus. The corpus is divided into 1,024 sentences for training, 1,024 for validation, and 1,120 for testing. The corpus is annotated with 1,024 sentences for POS tagging and 1,120 for UD trees. The corpus is annotated with 1,024 sentences for POS tagging and 1,120 for UD trees. The corpus is annotated with 1\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.09590",
    "question": "How significant are the improvements over previous approaches?",
    "gold_answer": "",
    "answer_a": "What are the baselines?\n< Explanation: We show that our proposed method achieves significant improvements over the standard approach and that attention mechanism is the worst location to inject attributes, contradicting prior work. We also show that these representations transfer well to other tasks. We also show that our proposed method achieves significant improvements over the standard approach and that attention mechanism is the worst location to inject attributes, contradicting prior work. We also show that these representations transfer well to other tasks. We also show that our proposed method achieves significant improvements over the standard approach and that attention mechanism is the worst location to inject attributes, contradicting prior work. We also show",
    "answer_b": "with performance increases of 2.4%, 1.3%, and 1.6% on IMDB, Yelp 2013, and Yelp 2014, respectively. Explanation: On all three datasets, our best results outperform all previous models based on accuracy and RMSE. Among our four models, CHIM-embedding performs the best in terms of accuracy, with performance increases of 2.4%, 1.3%, and 1.6% on IMDB, Yelp 2013, and Yelp 2014, respectively.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow significant are the improvements over previous approaches?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the baselines?\n< Explanation: We show that our proposed method achieves significant improvements over the standard approach and that attention mechanism is the worst location to inject attributes, contradicting prior work. We also show that these representations transfer well to other tasks. We also show that our proposed method achieves significant improvements over the standard approach and that attention mechanism is the worst location to inject attributes, contradicting prior work. We also show that these representations transfer well to other tasks. We also show that our proposed method achieves significant improvements over the standard approach and that attention mechanism is the worst location to inject attributes, contradicting prior work. We also show\n\nAnswer B:\nwith performance increases of 2.4%, 1.3%, and 1.6% on IMDB, Yelp 2013, and Yelp 2014, respectively. Explanation: On all three datasets, our best results outperform all previous models based on accuracy and RMSE. Among our four models, CHIM-embedding performs the best in terms of accuracy, with performance increases of 2.4%, 1.3%, and 1.6% on IMDB, Yelp 2013, and Yelp 2014, respectively.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.09590",
    "question": "Which other tasks are evaluated?",
    "gold_answer": "",
    "answer_a": "product category classification and review headline generation. Explanation: Finally, we show that these attribute representations transfer well to other tasks such as product category classification and review headline generation.",
    "answer_b": "What are the results?\n< Explanation: We evaluate our model on four tasks: sentiment classification, sentiment classification with attributes, sentiment classification with attributes and sentiment classification with attributes and attributes. We also evaluate the performance of the attention mechanism on the sentiment classification task. We use the sentiment classification task to evaluate the performance of the attention mechanism and to demonstrate that it is the worst location to inject attributes. We also evaluate the performance of the attention mechanism on the sentiment classification task and show that it is the worst location to inject attributes. We also evaluate the performance of the attention mechanism on the sentiment classification task and show that it is the worst location to inject",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich other tasks are evaluated?\n\nGround Truth Answer:\n\n\nAnswer A:\nproduct category classification and review headline generation. Explanation: Finally, we show that these attribute representations transfer well to other tasks such as product category classification and review headline generation.\n\nAnswer B:\nWhat are the results?\n< Explanation: We evaluate our model on four tasks: sentiment classification, sentiment classification with attributes, sentiment classification with attributes and sentiment classification with attributes and attributes. We also evaluate the performance of the attention mechanism on the sentiment classification task. We use the sentiment classification task to evaluate the performance of the attention mechanism and to demonstrate that it is the worst location to inject attributes. We also evaluate the performance of the attention mechanism on the sentiment classification task and show that it is the worst location to inject attributes. We also evaluate the performance of the attention mechanism on the sentiment classification task and show that it is the worst location to inject\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.09590",
    "question": "What are the performances associated to different attribute placing?",
    "gold_answer": "",
    "answer_a": "Best accuracy is for proposed CHIM methods (~56% IMDB, ~68.5 YELP datasets), most common bias attention (~53%IMDB, ~65%YELP), and oll others are worse than proposed method.. Explanation: Specifically, we use a smaller version of our base model (with dimensions set to 64) and incorporate the user and product attributes using nine different approaches: (1) bias-attention: the bias-based method injected to the attention mechanism, (2-5) the matrix-based method injected to four different locations (matrix-embedding, matrix-encoder, matrix-attention, matrix-classifier), and (6-9) the CHIM-based method injected to four different locations (CHIM-embedding, CHIM-encoder, CHIM-attention, CHIM-classifier). We then calculate the accuracy of each approach for all datasets.\n\nResults are shown in Figure FIGREF25. The figure shows that bias-attention consistently performs poorly compared to other approaches. As expected, matrix-based representations perform the worst when injected to embeddings and encoder, however we can already see improvements over bias-attention when these representations are injected to attention and classifier.",
    "answer_b": "How do they compare to the standard approach?\n< Explanation: We compare our proposed method with the standard approach in terms of accuracy and F1 score. We also compare our proposed method with the state-of-the-art model in terms of accuracy and F1 score. We also compare our proposed method with the state-of-the-art model in terms of accuracy and F1 score. We also compare our proposed method with the state-of-the-art model in terms of accuracy and F1 score. We also compare our proposed method with the state-of-the-art model in terms of accuracy and F1 score. We also compare our proposed method with the state-of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the performances associated to different attribute placing?\n\nGround Truth Answer:\n\n\nAnswer A:\nBest accuracy is for proposed CHIM methods (~56% IMDB, ~68.5 YELP datasets), most common bias attention (~53%IMDB, ~65%YELP), and oll others are worse than proposed method.. Explanation: Specifically, we use a smaller version of our base model (with dimensions set to 64) and incorporate the user and product attributes using nine different approaches: (1) bias-attention: the bias-based method injected to the attention mechanism, (2-5) the matrix-based method injected to four different locations (matrix-embedding, matrix-encoder, matrix-attention, matrix-classifier), and (6-9) the CHIM-based method injected to four different locations (CHIM-embedding, CHIM-encoder, CHIM-attention, CHIM-classifier). We then calculate the accuracy of each approach for all datasets.\n\nResults are shown in Figure FIGREF25. The figure shows that bias-attention consistently performs poorly compared to other approaches. As expected, matrix-based representations perform the worst when injected to embeddings and encoder, however we can already see improvements over bias-attention when these representations are injected to attention and classifier.\n\nAnswer B:\nHow do they compare to the standard approach?\n< Explanation: We compare our proposed method with the standard approach in terms of accuracy and F1 score. We also compare our proposed method with the state-of-the-art model in terms of accuracy and F1 score. We also compare our proposed method with the state-of-the-art model in terms of accuracy and F1 score. We also compare our proposed method with the state-of-the-art model in terms of accuracy and F1 score. We also compare our proposed method with the state-of-the-art model in terms of accuracy and F1 score. We also compare our proposed method with the state-of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.08584",
    "question": "What architecture is used in the encoder?",
    "gold_answer": "",
    "answer_a": "How does it differ from the standard Transformer architecture?\n<sep><br>The encoder architecture is a simple encoder which is independent of the source language. It consists of a sequence of fully connected layers. The encoder is trained to learn a representation of the source language. The encoder is trained with the same loss function as the standard Transformer model. The encoder is trained with the same architecture as the standard Transformer model. The encoder is trained with the same architecture as the standard Transformer model. The encoder is trained with the same architecture as the standard Transformer model. The encoder is trained with the same architecture as the standard Transformer model. The encoder is trained",
    "answer_b": "Transformer. Explanation: By modifying the Transformer architecture of BIBREF10 to afford a fixed-size representation for the encoder output, we found that we can significantly improve zero-shot performance at the cost of a lower performance on the supervised language pairs.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat architecture is used in the encoder?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow does it differ from the standard Transformer architecture?\n<sep><br>The encoder architecture is a simple encoder which is independent of the source language. It consists of a sequence of fully connected layers. The encoder is trained to learn a representation of the source language. The encoder is trained with the same loss function as the standard Transformer model. The encoder is trained with the same architecture as the standard Transformer model. The encoder is trained with the same architecture as the standard Transformer model. The encoder is trained with the same architecture as the standard Transformer model. The encoder is trained with the same architecture as the standard Transformer model. The encoder is trained\n\nAnswer B:\nTransformer. Explanation: By modifying the Transformer architecture of BIBREF10 to afford a fixed-size representation for the encoder output, we found that we can significantly improve zero-shot performance at the cost of a lower performance on the supervised language pairs.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.09652",
    "question": "How much data do they manage to gather online?",
    "gold_answer": "",
    "answer_a": "INLINEFORM0 bilingual English-Tamil and INLINEFORM1 English-Hindi titles on the Wikimedia. Explanation: Subsequently, we extracted parallel sentences using the trained model, and parallel articles collected from Wikipedia. There were INLINEFORM0 bilingual English-Tamil and INLINEFORM1 English-Hindi titles on the Wikimedia dumps collected in December 2017.",
    "answer_b": "What is the size of the dataset?\n< Explanation: We collected 1000 parallel sentences from Wikipedia articles in English and Hindi, and 1000 parallel sentences from English and Tamil. We also collected 1000 parallel sentences from Wikipedia articles in English and Tamil. The total size of the dataset is 100000 sentences. We used the Wikipedia articles as the training set, and the parallel sentences as the test set. We used the Wikipedia articles as the training set, and the parallel sentences as the test set. We used the Wikipedia articles as the training set, and the parallel sentences as",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much data do they manage to gather online?\n\nGround Truth Answer:\n\n\nAnswer A:\nINLINEFORM0 bilingual English-Tamil and INLINEFORM1 English-Hindi titles on the Wikimedia. Explanation: Subsequently, we extracted parallel sentences using the trained model, and parallel articles collected from Wikipedia. There were INLINEFORM0 bilingual English-Tamil and INLINEFORM1 English-Hindi titles on the Wikimedia dumps collected in December 2017.\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: We collected 1000 parallel sentences from Wikipedia articles in English and Hindi, and 1000 parallel sentences from English and Tamil. We also collected 1000 parallel sentences from Wikipedia articles in English and Tamil. The total size of the dataset is 100000 sentences. We used the Wikipedia articles as the training set, and the parallel sentences as the test set. We used the Wikipedia articles as the training set, and the parallel sentences as the test set. We used the Wikipedia articles as the training set, and the parallel sentences as\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.09652",
    "question": "Which models do they use for phrase-based SMT?",
    "gold_answer": "",
    "answer_a": "Phrase-Based SMT systems were trained using Moses, grow-diag-final-and heuristic were used for extracting phrases,  and lexicalised reordering and Batch MIRA for tuning.. Explanation: Phrase-Based SMT systems were trained using Moses BIBREF14 . We used the grow-diag-final-and heuristic for extracting phrases, lexicalised reordering and Batch MIRA BIBREF15 for tuning (the default parameters on Moses).",
    "answer_b": "What is the performance of the models?\n< Explanation: We have used the following models for phrase-based SMT: (i) a simple rule-based model, (ii) a rule-based model with a simple rule set, (iii) a rule-based model with a rule set that is based on the word embeddings, and (iv) a rule-based model with a rule set that is based on the word embeddings and the word embeddings of the target language. We have also used a rule-based model with a rule set that is based on the word embeddings and the word embeddings of the target language. The rule-based models are trained on the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich models do they use for phrase-based SMT?\n\nGround Truth Answer:\n\n\nAnswer A:\nPhrase-Based SMT systems were trained using Moses, grow-diag-final-and heuristic were used for extracting phrases,  and lexicalised reordering and Batch MIRA for tuning.. Explanation: Phrase-Based SMT systems were trained using Moses BIBREF14 . We used the grow-diag-final-and heuristic for extracting phrases, lexicalised reordering and Batch MIRA BIBREF15 for tuning (the default parameters on Moses).\n\nAnswer B:\nWhat is the performance of the models?\n< Explanation: We have used the following models for phrase-based SMT: (i) a simple rule-based model, (ii) a rule-based model with a simple rule set, (iii) a rule-based model with a rule set that is based on the word embeddings, and (iv) a rule-based model with a rule set that is based on the word embeddings and the word embeddings of the target language. We have also used a rule-based model with a rule set that is based on the word embeddings and the word embeddings of the target language. The rule-based models are trained on the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.09652",
    "question": "Which models do they use for NMT?",
    "gold_answer": "",
    "answer_a": "TensorFlow BIBREF17 implementation of OpenNMT. Explanation: For training neural machine translation models, we used the TensorFlow BIBREF17 implementation of OpenNMT BIBREF18 with attention-based transformer architecture BIBREF19 .",
    "answer_b": "Which models do they use for phrase-based SMT?\n< Explanation: We have used the following models for NMT: (1) a bidirectional LSTM model with a single hidden layer, (2) a bidirectional LSTM model with a single hidden layer and a dropout layer, (3) a bidirectional LSTM model with a single hidden layer and a dropout layer and (4) a bidirectional LSTM model with a single hidden layer and a dropout layer and a linear layer. We have also used the following models for phrase-based SMT: (1) a bidirectional LSTM model with a single hidden layer, (2) a bid",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich models do they use for NMT?\n\nGround Truth Answer:\n\n\nAnswer A:\nTensorFlow BIBREF17 implementation of OpenNMT. Explanation: For training neural machine translation models, we used the TensorFlow BIBREF17 implementation of OpenNMT BIBREF18 with attention-based transformer architecture BIBREF19 .\n\nAnswer B:\nWhich models do they use for phrase-based SMT?\n< Explanation: We have used the following models for NMT: (1) a bidirectional LSTM model with a single hidden layer, (2) a bidirectional LSTM model with a single hidden layer and a dropout layer, (3) a bidirectional LSTM model with a single hidden layer and a dropout layer and (4) a bidirectional LSTM model with a single hidden layer and a dropout layer and a linear layer. We have also used the following models for phrase-based SMT: (1) a bidirectional LSTM model with a single hidden layer, (2) a bid\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.09652",
    "question": "What are the BLEU performance improvements they achieve?",
    "gold_answer": "",
    "answer_a": "How are they measured?\n< Explanation: We have evaluated the performance of the proposed method on the English--Hindi and English--Tamil language pairs. We have used the English Wikipedia articles as the training set and the corresponding Hindi and Tamil Wikipedia articles as the test set. The results are shown in Table 1. We have observed that the proposed method outperforms the baseline system on both the English--Hindi and English--Tamil language pairs. The BLEU scores of the proposed method are 0.73 and 0.75, respectively, compared to 0.63 and 0.64",
    "answer_b": "11.03% and 14.7% for en\u2013ta and en\u2013hi pairs respectively. Explanation: We demonstrated a percentage increase in BLEU scores of 11.03% and 14.7% for en\u2013ta and en\u2013hi pairs respectively, due to the use of parallel-sentence pairs extracted from comparable corpora using the neural architecture.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the BLEU performance improvements they achieve?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they measured?\n< Explanation: We have evaluated the performance of the proposed method on the English--Hindi and English--Tamil language pairs. We have used the English Wikipedia articles as the training set and the corresponding Hindi and Tamil Wikipedia articles as the test set. The results are shown in Table 1. We have observed that the proposed method outperforms the baseline system on both the English--Hindi and English--Tamil language pairs. The BLEU scores of the proposed method are 0.73 and 0.75, respectively, compared to 0.63 and 0.64\n\nAnswer B:\n11.03% and 14.7% for en\u2013ta and en\u2013hi pairs respectively. Explanation: We demonstrated a percentage increase in BLEU scores of 11.03% and 14.7% for en\u2013ta and en\u2013hi pairs respectively, due to the use of parallel-sentence pairs extracted from comparable corpora using the neural architecture.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1703.04357",
    "question": "What is the architecture of the model?",
    "gold_answer": "",
    "answer_a": "attentional encoder\u2013decoder. Explanation: Nematus implements an attentional encoder\u2013decoder architecture similar to the one described by DBLP:journals/corr/BahdanauCB14, but with several implementation differences.",
    "answer_b": "Output: The model consists of a sequence-to-sequence encoder, a sequence-to-sequence decoder, and a language model. The encoder and decoder are both recurrent neural networks. The language model is a fully connected neural network. The model is trained using a backpropagation algorithm. The model is trained using a backpropagation algorithm. The model is trained using a backpropagation algorithm. The model is trained using a backpropagation algorithm. The model is trained using a backpropagation algorithm. The model is trained using a backpropagation algorithm. The model is trained using a backpropagation algorithm. The model is trained using",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the architecture of the model?\n\nGround Truth Answer:\n\n\nAnswer A:\nattentional encoder\u2013decoder. Explanation: Nematus implements an attentional encoder\u2013decoder architecture similar to the one described by DBLP:journals/corr/BahdanauCB14, but with several implementation differences.\n\nAnswer B:\nOutput: The model consists of a sequence-to-sequence encoder, a sequence-to-sequence decoder, and a language model. The encoder and decoder are both recurrent neural networks. The language model is a fully connected neural network. The model is trained using a backpropagation algorithm. The model is trained using a backpropagation algorithm. The model is trained using a backpropagation algorithm. The model is trained using a backpropagation algorithm. The model is trained using a backpropagation algorithm. The model is trained using a backpropagation algorithm. The model is trained using a backpropagation algorithm. The model is trained using\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.02914",
    "question": "Which movie subtitles dataset did they use?",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n<sep>Answer: The movie subtitles dataset is from the movie subtitles dataset from the Movie Subtitles Project. The dataset contains 1000000 movie subtitles and 10000000 words. The dataset is divided into 1000000000000000000000000000000000000000000000000000000000000000000",
    "answer_b": "the OpenSubtitles dataset BIBREF18. Explanation: For our QA system we used the DBpedia repository and the Subtitles corpus. Due to the nature of training a sequence-to-sequence neural model, questions and answers need to be aligned. The statistics on the used data are shown in Table TABREF5 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich movie subtitles dataset did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n<sep>Answer: The movie subtitles dataset is from the movie subtitles dataset from the Movie Subtitles Project. The dataset contains 1000000 movie subtitles and 10000000 words. The dataset is divided into 1000000000000000000000000000000000000000000000000000000000000000000\n\nAnswer B:\nthe OpenSubtitles dataset BIBREF18. Explanation: For our QA system we used the DBpedia repository and the Subtitles corpus. Due to the nature of training a sequence-to-sequence neural model, questions and answers need to be aligned. The statistics on the used data are shown in Table TABREF5 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.00175",
    "question": "What are the other two Vietnamese datasets?",
    "gold_answer": "",
    "answer_a": "How are they different from UIT-ViIC?\n< Explanation: There are two other Vietnamese datasets which are built by different methods. The first one is the dataset of images captioned by a human. This dataset is called Human Captioning dataset. It is built by the same method as UIT-ViIC, but it is built by a human annotator. The second dataset is called Image Captioning dataset. This dataset is built by a machine learning model. It is built by a deep neural network model. The dataset is called Deep Neural Network Captioning dataset. The dataset is built by a deep neural network model. The dataset is built",
    "answer_b": "MS-COCO dataset translated to Vietnamese using Google Translate and through human annotation. Explanation: We conduct our experiments and do comparisons through three datasets with the same size and images of sportball category: Two Vietnamese datasets generated by two methods (translated by Google Translation service and annotated by human) and the original MS-COCO English dataset.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the other two Vietnamese datasets?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they different from UIT-ViIC?\n< Explanation: There are two other Vietnamese datasets which are built by different methods. The first one is the dataset of images captioned by a human. This dataset is called Human Captioning dataset. It is built by the same method as UIT-ViIC, but it is built by a human annotator. The second dataset is called Image Captioning dataset. This dataset is built by a machine learning model. It is built by a deep neural network model. The dataset is called Deep Neural Network Captioning dataset. The dataset is built by a deep neural network model. The dataset is built\n\nAnswer B:\nMS-COCO dataset translated to Vietnamese using Google Translate and through human annotation. Explanation: We conduct our experiments and do comparisons through three datasets with the same size and images of sportball category: Two Vietnamese datasets generated by two methods (translated by Google Translation service and annotated by human) and the original MS-COCO English dataset.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.00175",
    "question": "Which English dataset do they evaluate on?",
    "gold_answer": "",
    "answer_a": "How do they compare it with UIT-ViIC?\n< Explanation: We evaluate our dataset on deep neural network models and do comparisons with English dataset and two Vietnamese datasets built by different methods. We use the following datasets for our experiments: English dataset: English Captioning dataset (ECCV 2015) and English Captioning dataset (ECCV 2016) BIBREF10. Vietnamese dataset: Vietnamese Captioning dataset (VCS) BIBREF11. We use the following datasets for our experiments: English dataset: English Captioning dataset (ECCV 2015)",
    "answer_b": "the original MS-COCO English dataset. Explanation: We conduct our experiments and do comparisons through three datasets with the same size and images of sportball category: Two Vietnamese datasets generated by two methods (translated by Google Translation service and annotated by human) and the original MS-COCO English dataset. The three sets are distributed into three subsets: 2,695 images for the training set, 924 images for validation set and 231 images for test set.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich English dataset do they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they compare it with UIT-ViIC?\n< Explanation: We evaluate our dataset on deep neural network models and do comparisons with English dataset and two Vietnamese datasets built by different methods. We use the following datasets for our experiments: English dataset: English Captioning dataset (ECCV 2015) and English Captioning dataset (ECCV 2016) BIBREF10. Vietnamese dataset: Vietnamese Captioning dataset (VCS) BIBREF11. We use the following datasets for our experiments: English dataset: English Captioning dataset (ECCV 2015)\n\nAnswer B:\nthe original MS-COCO English dataset. Explanation: We conduct our experiments and do comparisons through three datasets with the same size and images of sportball category: Two Vietnamese datasets generated by two methods (translated by Google Translation service and annotated by human) and the original MS-COCO English dataset. The three sets are distributed into three subsets: 2,695 images for the training set, 924 images for validation set and 231 images for test set.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.00175",
    "question": "What neural network models do they use in their evaluation?",
    "gold_answer": "",
    "answer_a": "Explanation: We use the following models for evaluation: (1) CNN (2) LSTM (3) RNN (4) BERT (5) Transformer (6) BERT-UNet (7) BERT-UNet-2 (8) BERT-UNet-3 (9) BERT-UNet-4 (10) BERT-UNet-5 (11) BERT-UNet-6 (12) BERT-UNet-7 (13) BERT-UNet-8 (14) BERT-UNet-9 (15",
    "answer_b": "CNN. Explanation: Overall, CNN is first used for extracting image features for encoder part. The image features which are presented in vectors will be used as layers for decoding. For decoder part, RNN - LSTM are used to embed the vectors to target sentences using words/tokens provided in vocabulary.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat neural network models do they use in their evaluation?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We use the following models for evaluation: (1) CNN (2) LSTM (3) RNN (4) BERT (5) Transformer (6) BERT-UNet (7) BERT-UNet-2 (8) BERT-UNet-3 (9) BERT-UNet-4 (10) BERT-UNet-5 (11) BERT-UNet-6 (12) BERT-UNet-7 (13) BERT-UNet-8 (14) BERT-UNet-9 (15\n\nAnswer B:\nCNN. Explanation: Overall, CNN is first used for extracting image features for encoder part. The image features which are presented in vectors will be used as layers for decoding. For decoder part, RNN - LSTM are used to embed the vectors to target sentences using words/tokens provided in vocabulary.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.00175",
    "question": "What methods are used to build two other Viatnamese datsets?",
    "gold_answer": "",
    "answer_a": "What are the results of these datasets?\n< Explanation: We build two other Vietnamese datasets by using the same method as UIT-ViIC. The first dataset is called VIT-ViIC, which is built by using the same method as UIT-ViIC, but with a different dataset. The second dataset is called VIT-ViIC, which is built by using the same method as UIT-ViIC, but with a different dataset. The third dataset is called VIT-ViIC, which is built by using the same method as UIT-ViIC, but with a different dataset. The fourth dataset is called VIT",
    "answer_b": "Translation and annotation.. Explanation: Two Vietnamese datasets generated by two methods (translated by Google Translation service and annotated by human) and the original MS-COCO English dataset.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat methods are used to build two other Viatnamese datsets?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results of these datasets?\n< Explanation: We build two other Vietnamese datasets by using the same method as UIT-ViIC. The first dataset is called VIT-ViIC, which is built by using the same method as UIT-ViIC, but with a different dataset. The second dataset is called VIT-ViIC, which is built by using the same method as UIT-ViIC, but with a different dataset. The third dataset is called VIT-ViIC, which is built by using the same method as UIT-ViIC, but with a different dataset. The fourth dataset is called VIT\n\nAnswer B:\nTranslation and annotation.. Explanation: Two Vietnamese datasets generated by two methods (translated by Google Translation service and annotated by human) and the original MS-COCO English dataset.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.00175",
    "question": "What deep neural network models are used in evaluation?",
    "gold_answer": "",
    "answer_a": "encoder-decoder architecture of CNN for encoding and LSTM for decoding. Explanation: To accomplish this task, we train and evaluate our dataset with two published Image Captioning models applying encoder-decoder architecture. The models we propose are Neural Image Captioning (NIC) model BIBREF14, Image Captioning model from the Pytorch-tutorial BIBREF15 by Yunjey.\n\nOverall, CNN is first used for extracting image features for encoder part. The image features which are presented in vectors will be used as layers for decoding. For decoder part, RNN - LSTM are used to embed the vectors to target sentences using words/tokens provided in vocabulary.",
    "answer_b": "Explanation: We use the following deep neural network models for evaluation: (1) CNN (2) LSTM (3) RNN (4) BERT (5) Transformer (6) GPT (7) GPT2 (8) GPT3 (9) BERT (10) BERT (11) BERT (12) BERT (13) BERT (14) BERT (15) BERT (16) BERT (17) BERT (18) BERT (19) BERT (20) BERT (21) B",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat deep neural network models are used in evaluation?\n\nGround Truth Answer:\n\n\nAnswer A:\nencoder-decoder architecture of CNN for encoding and LSTM for decoding. Explanation: To accomplish this task, we train and evaluate our dataset with two published Image Captioning models applying encoder-decoder architecture. The models we propose are Neural Image Captioning (NIC) model BIBREF14, Image Captioning model from the Pytorch-tutorial BIBREF15 by Yunjey.\n\nOverall, CNN is first used for extracting image features for encoder part. The image features which are presented in vectors will be used as layers for decoding. For decoder part, RNN - LSTM are used to embed the vectors to target sentences using words/tokens provided in vocabulary.\n\nAnswer B:\nExplanation: We use the following deep neural network models for evaluation: (1) CNN (2) LSTM (3) RNN (4) BERT (5) Transformer (6) GPT (7) GPT2 (8) GPT3 (9) BERT (10) BERT (11) BERT (12) BERT (13) BERT (14) BERT (15) BERT (16) BERT (17) BERT (18) BERT (19) BERT (20) BERT (21) B\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.00175",
    "question": "How authors evaluate datasets using models trained on different datasets?",
    "gold_answer": "",
    "answer_a": "What are the results?\n< Explanation: We evaluate our dataset on deep neural network models and do comparisons with English dataset and two Vietnamese datasets built by different methods. We use the following models: CNN, LSTM, RNN, and CNN-LSTM. We use the following datasets: CNN, CNN-LSTM, LSTM, and RNN. We use the following evaluation metrics: BLEU, METEOR, and F1. We use the following evaluation metrics: BLEU, METEOR, and F1. We use the following evaluation metrics: BLEU, METEOR, and F1. We use the following evaluation metrics",
    "answer_b": "The two models are trained with three mentioned datasets, then validated on  subset for each dataset and evaluated using BLEU, ROUGE and CIDEr measures.. Explanation: The two models are trained with three mentioned datasets, which are English-sportball, GT-sportball, UIT-ViIC. After training, 924 images from validation subset for each dataset are used to validate the our models.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow authors evaluate datasets using models trained on different datasets?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n< Explanation: We evaluate our dataset on deep neural network models and do comparisons with English dataset and two Vietnamese datasets built by different methods. We use the following models: CNN, LSTM, RNN, and CNN-LSTM. We use the following datasets: CNN, CNN-LSTM, LSTM, and RNN. We use the following evaluation metrics: BLEU, METEOR, and F1. We use the following evaluation metrics: BLEU, METEOR, and F1. We use the following evaluation metrics: BLEU, METEOR, and F1. We use the following evaluation metrics\n\nAnswer B:\nThe two models are trained with three mentioned datasets, then validated on  subset for each dataset and evaluated using BLEU, ROUGE and CIDEr measures.. Explanation: The two models are trained with three mentioned datasets, which are English-sportball, GT-sportball, UIT-ViIC. After training, 924 images from validation subset for each dataset are used to validate the our models.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.09381",
    "question": "Do they evaluate their model on datasets other than RACE?",
    "gold_answer": "",
    "answer_a": "Yes, they also evaluate on the ROCStories\n(Spring 2016) dataset which collects 50k five sentence commonsense stories.",
    "answer_b": "Explanation: We evaluate our model on the following datasets: (1) the Stanford Question Answering Dataset (SQuAD) BIBREF10, (2) the Stanford Multiple Choice Reading Comprehension (SCRC) BIBREF11, (3) the Stanford Multiple Choice Reading Comprehension with Answering (SCRC-A) BIBREF12, (4) the Stanford Multiple Choice Reading Comprehension with Answering and Question Answering (SCRC-AQA) BIBREF13, (5) the Stanford Multiple Choice Reading Comprehension with Answering and Question Answering and Answering",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nDo they evaluate their model on datasets other than RACE?\n\nGround Truth Answer:\n\n\nAnswer A:\nYes, they also evaluate on the ROCStories\n(Spring 2016) dataset which collects 50k five sentence commonsense stories.\n\nAnswer B:\nExplanation: We evaluate our model on the following datasets: (1) the Stanford Question Answering Dataset (SQuAD) BIBREF10, (2) the Stanford Multiple Choice Reading Comprehension (SCRC) BIBREF11, (3) the Stanford Multiple Choice Reading Comprehension with Answering (SCRC-A) BIBREF12, (4) the Stanford Multiple Choice Reading Comprehension with Answering and Question Answering (SCRC-AQA) BIBREF13, (5) the Stanford Multiple Choice Reading Comprehension with Answering and Question Answering and Answering\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.09381",
    "question": "What is their model's performance on RACE?",
    "gold_answer": "",
    "answer_a": "Explanation: We evaluate our model on RACE dataset. The dataset contains 10,000 passages, 10,000 questions and 10,000 answer pairs. The dataset is divided into training set, validation set and test set. The training set contains 6,000 passages, 6,000 questions and 6,000 answer pairs. The validation set contains 2,000 passages, 2,000 questions and 2,000 answer pairs. The test set contains 2,000 passages, 2",
    "answer_b": "Model's performance ranges from 67.0% to 82.8%.. Explanation: FLOAT SELECTED: Table 4: Experiment results on RACE test set. All the results are from single models. PSS : Passage Sentence Selection; AOI : Answer Option Interaction. \u2217 indicates our implementation.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is their model's performance on RACE?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We evaluate our model on RACE dataset. The dataset contains 10,000 passages, 10,000 questions and 10,000 answer pairs. The dataset is divided into training set, validation set and test set. The training set contains 6,000 passages, 6,000 questions and 6,000 answer pairs. The validation set contains 2,000 passages, 2,000 questions and 2,000 answer pairs. The test set contains 2,000 passages, 2\n\nAnswer B:\nModel's performance ranges from 67.0% to 82.8%.. Explanation: FLOAT SELECTED: Table 4: Experiment results on RACE test set. All the results are from single models. PSS : Passage Sentence Selection; AOI : Answer Option Interaction. \u2217 indicates our implementation.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1705.10272",
    "question": "What deep learning models do they plan to use?",
    "gold_answer": "",
    "answer_a": "CNNs in combination with LSTMs. Explanation: CNNs in combination with LSTMs seem to be a particularly promising way to overcome these limitations (e.g., BIBREF12 ) which we will explore and compare to our existing results.",
    "answer_b": "Explanation: We propose a deep learning model that uses a recurrent neural network (RNN) to learn a sequence of words that are associated with a humorous statement. We use a sequence-to-sequence (Seq2Seq) model to learn the sequence of words, and a recurrent neural network (RNN) to learn the meaning of the words. We use a sequence-to-sequence (Seq2Seq) model to learn the sequence of words, and a recurrent neural network (RNN) to learn the meaning of the words. We use a sequence-to-sequence (Seq2Seq) model to learn the sequence of words, and a",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat deep learning models do they plan to use?\n\nGround Truth Answer:\n\n\nAnswer A:\nCNNs in combination with LSTMs. Explanation: CNNs in combination with LSTMs seem to be a particularly promising way to overcome these limitations (e.g., BIBREF12 ) which we will explore and compare to our existing results.\n\nAnswer B:\nExplanation: We propose a deep learning model that uses a recurrent neural network (RNN) to learn a sequence of words that are associated with a humorous statement. We use a sequence-to-sequence (Seq2Seq) model to learn the sequence of words, and a recurrent neural network (RNN) to learn the meaning of the words. We use a sequence-to-sequence (Seq2Seq) model to learn the sequence of words, and a recurrent neural network (RNN) to learn the meaning of the words. We use a sequence-to-sequence (Seq2Seq) model to learn the sequence of words, and a\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1705.10272",
    "question": "How are the language models used to make predictions on humorous statements?",
    "gold_answer": "",
    "answer_a": "scored tweets by assigning them a probability based on each model. Explanation: We scored tweets by assigning them a probability based on each model. Tweets that have a higher probability according to the funny tweet model are considered funnier since they are more like the humorous training data. However, tweets that have a lower probability according to the news language model are viewed as funnier since they are least like the (unfunny) news corpus.",
    "answer_b": "Explanation: The language models are trained on a corpus of humorous and non-humorous statements. The models are trained on a corpus of humorous statements and then fine-tuned on a corpus of non-humorous statements. The models are trained on a corpus of humorous statements and then fine-tuned on a corpus of non-humorous statements. The models are trained on a corpus of humorous statements and then fine-tuned on a corpus of non-humorous statements. The models are trained on a corpus of humorous statements and then fine-tuned on a corpus of non-humorous statements. The models are trained on a corpus of humorous",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are the language models used to make predictions on humorous statements?\n\nGround Truth Answer:\n\n\nAnswer A:\nscored tweets by assigning them a probability based on each model. Explanation: We scored tweets by assigning them a probability based on each model. Tweets that have a higher probability according to the funny tweet model are considered funnier since they are more like the humorous training data. However, tweets that have a lower probability according to the news language model are viewed as funnier since they are least like the (unfunny) news corpus.\n\nAnswer B:\nExplanation: The language models are trained on a corpus of humorous and non-humorous statements. The models are trained on a corpus of humorous statements and then fine-tuned on a corpus of non-humorous statements. The models are trained on a corpus of humorous statements and then fine-tuned on a corpus of non-humorous statements. The models are trained on a corpus of humorous statements and then fine-tuned on a corpus of non-humorous statements. The models are trained on a corpus of humorous statements and then fine-tuned on a corpus of non-humorous statements. The models are trained on a corpus of humorous\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1705.10272",
    "question": "What type of language models are used? e.g. trigrams, bigrams?",
    "gold_answer": "",
    "answer_a": "bigrams and trigrams as features. Explanation: We took a standard approach to language modeling and used bigrams and trigrams as features in our models. We used KenLM BIBREF8 with modified Kneser-Ney smoothing and a back-off technique as our language modeling tool.",
    "answer_b": "What are the advantages of using trigrams over bigrams?\n<sep><br/ >Answer: We use a trigram model, which is a sequence of trigrams. The trigram model is a simple model that uses a sequence of trigrams to predict the next word in the sequence. The trigram model is a simple model that uses a sequence of trigrams to predict the next word in the sequence. The trigram model is a simple model that uses a sequence of trigrams to predict the next word in the sequence. The trigram model is a simple model that uses a sequence of trigrams to predict the next word in",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of language models are used? e.g. trigrams, bigrams?\n\nGround Truth Answer:\n\n\nAnswer A:\nbigrams and trigrams as features. Explanation: We took a standard approach to language modeling and used bigrams and trigrams as features in our models. We used KenLM BIBREF8 with modified Kneser-Ney smoothing and a back-off technique as our language modeling tool.\n\nAnswer B:\nWhat are the advantages of using trigrams over bigrams?\n<sep><br/ >Answer: We use a trigram model, which is a sequence of trigrams. The trigram model is a simple model that uses a sequence of trigrams to predict the next word in the sequence. The trigram model is a simple model that uses a sequence of trigrams to predict the next word in the sequence. The trigram model is a simple model that uses a sequence of trigrams to predict the next word in the sequence. The trigram model is a simple model that uses a sequence of trigrams to predict the next word in\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01615",
    "question": "How do attention, recurrent and convolutional networks differ on the language classes they accept?",
    "gold_answer": "",
    "answer_a": "What is the relationship between neural networks and natural language grammar?\n< Explanation: We define the following classes of languages:\n\n1. Regular languages: These are languages that can be recognized by a finite automaton. We denote this class by R.\n\n2. Context-free languages: These are languages that can be recognized by a pushdown automaton. We denote this class by C.\n\n3. Context-sensitive languages: These are languages that can be recognized by a context-sensitive grammar. We denote this class by S.\n\n4. Context-free context-sensitive languages: These are languages that can be recognized by a pushdown automaton and a context-sensitive grammar.",
    "answer_b": "Attention neural networks can represent more languages than other networks. Simple recurring networks can describe regular languages. CNNs can describe only strictly local languages.. Explanation: The exponential state complexity provided by attention enables copying, which we can view as a simplified version of machine translation. Thus, it makes sense that attention is almost universal in machine translation architectures. The additional memory introduced by attention might also allow more complex hierarchical representations.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do attention, recurrent and convolutional networks differ on the language classes they accept?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the relationship between neural networks and natural language grammar?\n< Explanation: We define the following classes of languages:\n\n1. Regular languages: These are languages that can be recognized by a finite automaton. We denote this class by R.\n\n2. Context-free languages: These are languages that can be recognized by a pushdown automaton. We denote this class by C.\n\n3. Context-sensitive languages: These are languages that can be recognized by a context-sensitive grammar. We denote this class by S.\n\n4. Context-free context-sensitive languages: These are languages that can be recognized by a pushdown automaton and a context-sensitive grammar.\n\nAnswer B:\nAttention neural networks can represent more languages than other networks. Simple recurring networks can describe regular languages. CNNs can describe only strictly local languages.. Explanation: The exponential state complexity provided by attention enables copying, which we can view as a simplified version of machine translation. Thus, it makes sense that attention is almost universal in machine translation architectures. The additional memory introduced by attention might also allow more complex hierarchical representations.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.01615",
    "question": "What type of languages do they test LSTMs on?",
    "gold_answer": "",
    "answer_a": "What is the measure of network memory?\n< Explanation: We test LSTMs on the following languages: English, French, German, Spanish, and Chinese. We use the following metrics to measure network memory: the number of states, the number of transitions, and the number of states and transitions per unit time. We also measure the number of states and transitions per unit time for the following networks: LSTMs, RNNs, and CNNs. We also measure the number of states and transitions per unit time for the following networks: RNNs, CNNs, and attention networks. We also measure the number of states and",
    "answer_b": "Counting. Explanation: Combining these results, we see that the asymptotic expressiveness of the LSTM falls somewhere between the general and simplified counter languages. This suggests counting is a good way to understand the behavior of LSTMs.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of languages do they test LSTMs on?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the measure of network memory?\n< Explanation: We test LSTMs on the following languages: English, French, German, Spanish, and Chinese. We use the following metrics to measure network memory: the number of states, the number of transitions, and the number of states and transitions per unit time. We also measure the number of states and transitions per unit time for the following networks: LSTMs, RNNs, and CNNs. We also measure the number of states and transitions per unit time for the following networks: RNNs, CNNs, and attention networks. We also measure the number of states and\n\nAnswer B:\nCounting. Explanation: Combining these results, we see that the asymptotic expressiveness of the LSTM falls somewhere between the general and simplified counter languages. This suggests counting is a good way to understand the behavior of LSTMs.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.10487",
    "question": "What is possible future improvement for proposed method/s?",
    "gold_answer": "",
    "answer_a": "Explanation: We propose two different architectures for storing and retrieving dialogue history, and compare them to existing baselines. We find that the proposed architecture is more flexible and easier to implement, and that it is able to maintain a consistent format over time. We also find that the proposed architecture is able to store a larger amount of information, and that it is able to maintain a higher level of coherence. We also find that the proposed architecture is able to maintain a higher level of coherence, and that it is able to maintain a higher level of coherence over time. We also find that the proposed architecture is able to maintain a higher level of coherence,",
    "answer_b": "memory module could be applied to other domains such as summary generation. Explanation: In future work, the memory module could be applied to other domains such as summary generation. While memory modules are able to capture neural vectors of information, they may not easily capture specific words for later use. A possible future approach might combine memory module architectures with pointer softmax networks BIBREF16 to allow memory models to store information about which words from previous utterances of the conversation to use in future responses.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is possible future improvement for proposed method/s?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We propose two different architectures for storing and retrieving dialogue history, and compare them to existing baselines. We find that the proposed architecture is more flexible and easier to implement, and that it is able to maintain a consistent format over time. We also find that the proposed architecture is able to store a larger amount of information, and that it is able to maintain a higher level of coherence. We also find that the proposed architecture is able to maintain a higher level of coherence, and that it is able to maintain a higher level of coherence over time. We also find that the proposed architecture is able to maintain a higher level of coherence,\n\nAnswer B:\nmemory module could be applied to other domains such as summary generation. Explanation: In future work, the memory module could be applied to other domains such as summary generation. While memory modules are able to capture neural vectors of information, they may not easily capture specific words for later use. A possible future approach might combine memory module architectures with pointer softmax networks BIBREF16 to allow memory models to store information about which words from previous utterances of the conversation to use in future responses.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.10487",
    "question": "What is percentage change in performance for better model when compared to baseline?",
    "gold_answer": "",
    "answer_a": "What is percentage change in performance for worse model when compared to baseline?\n< Explanation: We report perplexity performance for both models, and compare them to existing baselines. We report perplexity performance for both models, and compare them to existing baselines. We report perplexity performance for both models, and compare them to existing baselines. We report perplexity performance for both models, and compare them to existing baselines. We report perplexity performance for both models, and compare them to existing baselines. We report perplexity performance for both models, and compare them to existing baselines. We report perplexity performance for both models,",
    "answer_b": "9.2% reduction in perplexity. Explanation: FLOAT SELECTED: Table 1: Word-level perplexity evaluation on proposed model and two selected baselines.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is percentage change in performance for better model when compared to baseline?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is percentage change in performance for worse model when compared to baseline?\n< Explanation: We report perplexity performance for both models, and compare them to existing baselines. We report perplexity performance for both models, and compare them to existing baselines. We report perplexity performance for both models, and compare them to existing baselines. We report perplexity performance for both models, and compare them to existing baselines. We report perplexity performance for both models, and compare them to existing baselines. We report perplexity performance for both models, and compare them to existing baselines. We report perplexity performance for both models,\n\nAnswer B:\n9.2% reduction in perplexity. Explanation: FLOAT SELECTED: Table 1: Word-level perplexity evaluation on proposed model and two selected baselines.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.10487",
    "question": "Which of two design architectures have better performance?",
    "gold_answer": "",
    "answer_a": "Why?\n<sep><solution> The first design architecture, which uses a sequence-to-sequence architecture with two separate NTM modules, is shown in Figure 1. The second design architecture, which uses a single NTM module, is shown in Figure 2. The second design architecture is shown to have better performance, as it is able to store more context information and is able to learn from the conversation history. The second design architecture is also able to learn from the conversation history, as it is able to learn from the conversation history. The second design architecture is also able to learn from the conversation history, as it is able to",
    "answer_b": "NTM-LM. Explanation: The best performing model was the NTM-LM architecture.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich of two design architectures have better performance?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhy?\n<sep><solution> The first design architecture, which uses a sequence-to-sequence architecture with two separate NTM modules, is shown in Figure 1. The second design architecture, which uses a single NTM module, is shown in Figure 2. The second design architecture is shown to have better performance, as it is able to store more context information and is able to learn from the conversation history. The second design architecture is also able to learn from the conversation history, as it is able to learn from the conversation history. The second design architecture is also able to learn from the conversation history, as it is able to\n\nAnswer B:\nNTM-LM. Explanation: The best performing model was the NTM-LM architecture.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.09616",
    "question": "What evaluation metrics did they use?",
    "gold_answer": "",
    "answer_a": "How did they evaluate the performance of their model?\n< Explanation: We used the following evaluation metrics to evaluate the performance of our model: Recall, F1, and Precision. Recall is the ratio of the number of correct responses to the total number of responses. F1 is the harmonic mean of precision and recall. Precision is the ratio of the number of correct responses to the total number of utterances. The higher the better. Precision is the ratio of the number of correct responses to the total number of utterances. Precision is the ratio of the number of correct responses to the total number of utterances. The higher the better. Precision",
    "answer_b": "Bilingual Evaluation Understudy (BLEU) BIBREF22. Explanation: To compare with dataset baselines in multiple dimensions and test the model's performance, we use the overall Bilingual Evaluation Understudy (BLEU) BIBREF22 to evaluate the imaginators' generation performance. As for arbitrator, we use accuracy score of the classification to evaluate. Accuracy in our experiments is the correct ratio in all samples.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat evaluation metrics did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow did they evaluate the performance of their model?\n< Explanation: We used the following evaluation metrics to evaluate the performance of our model: Recall, F1, and Precision. Recall is the ratio of the number of correct responses to the total number of responses. F1 is the harmonic mean of precision and recall. Precision is the ratio of the number of correct responses to the total number of utterances. The higher the better. Precision is the ratio of the number of correct responses to the total number of utterances. Precision is the ratio of the number of correct responses to the total number of utterances. The higher the better. Precision\n\nAnswer B:\nBilingual Evaluation Understudy (BLEU) BIBREF22. Explanation: To compare with dataset baselines in multiple dimensions and test the model's performance, we use the overall Bilingual Evaluation Understudy (BLEU) BIBREF22 to evaluate the imaginators' generation performance. As for arbitrator, we use accuracy score of the classification to evaluate. Accuracy in our experiments is the correct ratio in all samples.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.09616",
    "question": "By how much does their model outperform the baseline?",
    "gold_answer": "",
    "answer_a": "What are the baseline models?\n< Explanation: We compare our model with the following models: (1) a baseline model that only uses the utterance history to decide whether to reply or not, (2) a baseline model that only uses the utterance history to decide whether to make a response or not, (3) a baseline model that only uses the utterance history to decide whether to wait or not, (4) a baseline model that only uses the utterance history to decide whether to make a response or not, (5) a baseline model that only uses the utterance history to decide whether to wait or not, (",
    "answer_b": "Best model outperforms baseline by 1.98% on MultiWoz dataset and .67% on DailyDialogue dataset. Explanation: FLOAT SELECTED: Table 3: Accuracy Results on Two datasets. Better results between baselines and corresponding ITA models are in BOLD and best results on datasets are in RED. Random result is the accuracy of script that making random decisions.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much does their model outperform the baseline?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the baseline models?\n< Explanation: We compare our model with the following models: (1) a baseline model that only uses the utterance history to decide whether to reply or not, (2) a baseline model that only uses the utterance history to decide whether to make a response or not, (3) a baseline model that only uses the utterance history to decide whether to wait or not, (4) a baseline model that only uses the utterance history to decide whether to make a response or not, (5) a baseline model that only uses the utterance history to decide whether to wait or not, (\n\nAnswer B:\nBest model outperforms baseline by 1.98% on MultiWoz dataset and .67% on DailyDialogue dataset. Explanation: FLOAT SELECTED: Table 3: Accuracy Results on Two datasets. Better results between baselines and corresponding ITA models are in BOLD and best results on datasets are in RED. Random result is the accuracy of script that making random decisions.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.09616",
    "question": "Which models did they compare with?",
    "gold_answer": "",
    "answer_a": "What are the results?\n< Explanation: We compared our model with the following models: (1) a simple model that only uses the user's utterance as the input of the arbitrator, (2) a simple model that only uses the agent's utterance as the input of the arbitrator, (3) a simple model that only uses the agent's utterance as the input of the arbitrator, (4) a simple model that only uses the agent's utterance as the input of the arbitrator, (5) a simple model that only uses the agent's utterance as the input of the arbitrator, (",
    "answer_b": "Bidirectional Gated Recurrent Units (Bi-GRUs) BIBREF23, TextCNNs BIBREF12 and BERT BIBREF14. Explanation: Baseline models are Bidirectional Gated Recurrent Units (Bi-GRUs) BIBREF23, TextCNNs BIBREF12 and BERT BIBREF14.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich models did they compare with?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n< Explanation: We compared our model with the following models: (1) a simple model that only uses the user's utterance as the input of the arbitrator, (2) a simple model that only uses the agent's utterance as the input of the arbitrator, (3) a simple model that only uses the agent's utterance as the input of the arbitrator, (4) a simple model that only uses the agent's utterance as the input of the arbitrator, (5) a simple model that only uses the agent's utterance as the input of the arbitrator, (\n\nAnswer B:\nBidirectional Gated Recurrent Units (Bi-GRUs) BIBREF23, TextCNNs BIBREF12 and BERT BIBREF14. Explanation: Baseline models are Bidirectional Gated Recurrent Units (Bi-GRUs) BIBREF23, TextCNNs BIBREF12 and BERT BIBREF14.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.09616",
    "question": "What is the source of their datasets?",
    "gold_answer": "",
    "answer_a": "human-to-human conversations. Explanation: Both datasets are collected from human-to-human conversations.",
    "answer_b": "How are they constructed?\n< Explanation: We collected two datasets from the online forums. The first dataset is from the forum of \u201cThe Daily News\u201d and the second one is from the forum of \u201cThe Daily Telegraph\u201d. The two datasets are collected from the forums of the two newspapers. The two datasets are collected from the forums of the two newspapers. The two datasets are collected from the forums of the two newspapers. The two datasets are collected from the forums of the two newspapers. The two datasets are collected from the forums of the two newspapers. The two datasets are collected from the forums of the two newspapers. The two datasets are collected from",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the source of their datasets?\n\nGround Truth Answer:\n\n\nAnswer A:\nhuman-to-human conversations. Explanation: Both datasets are collected from human-to-human conversations.\n\nAnswer B:\nHow are they constructed?\n< Explanation: We collected two datasets from the online forums. The first dataset is from the forum of \u201cThe Daily News\u201d and the second one is from the forum of \u201cThe Daily Telegraph\u201d. The two datasets are collected from the forums of the two newspapers. The two datasets are collected from the forums of the two newspapers. The two datasets are collected from the forums of the two newspapers. The two datasets are collected from the forums of the two newspapers. The two datasets are collected from the forums of the two newspapers. The two datasets are collected from the forums of the two newspapers. The two datasets are collected from\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1612.00866",
    "question": "What new advances are included in this dataset?",
    "gold_answer": "",
    "answer_a": "Explanation: The dataset includes improvements in the underlying news collection process and event coding software, along with the creation of a general processing pipeline necessary to produce daily-updated data. The dataset includes the following improvements:\n\n1. The dataset includes a new, more comprehensive news collection process. This process is based on the news collection process used by the National Public Radio (NPR) and the BBC, and is based on the news collection process used by the BBC. The dataset includes a new, more comprehensive news collection process, which is based on the news collection process used by the BBC. This process is based on the news collection process used by the",
    "answer_b": "PETRARCH. Explanation: Advances\nPETRARCH (Python Engine for Text Resolution And Related Coding Hierarchy) is the new generation of event-data coding software that is the successor to the TABARI software.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat new advances are included in this dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The dataset includes improvements in the underlying news collection process and event coding software, along with the creation of a general processing pipeline necessary to produce daily-updated data. The dataset includes the following improvements:\n\n1. The dataset includes a new, more comprehensive news collection process. This process is based on the news collection process used by the National Public Radio (NPR) and the BBC, and is based on the news collection process used by the BBC. The dataset includes a new, more comprehensive news collection process, which is based on the news collection process used by the BBC. This process is based on the news collection process used by the\n\nAnswer B:\nPETRARCH. Explanation: Advances\nPETRARCH (Python Engine for Text Resolution And Related Coding Hierarchy) is the new generation of event-data coding software that is the successor to the TABARI software.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1612.00866",
    "question": "What language is this dataset in?",
    "gold_answer": "",
    "answer_a": "Explanation: The dataset is in English. The dataset is a subset of the Phoenix dataset, which is in Spanish. The dataset is a subset of the Phoenix dataset, which is in Spanish. The dataset is a subset of the Phoenix dataset, which is in Spanish. The dataset is a subset of the Phoenix dataset, which is in Spanish. The dataset is a subset of the Phoenix dataset, which is in Spanish. The dataset is a subset of the Phoenix dataset, which is in Spanish. The dataset is a subset of the Phoenix dataset, which is in Spanish. The dataset is a subset of the Phoenix dataset, which is in Spanish.",
    "answer_b": "English. Explanation: The dataset makes use of 450 English-language news sites, which are each scraped every hour for new content.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat language is this dataset in?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The dataset is in English. The dataset is a subset of the Phoenix dataset, which is in Spanish. The dataset is a subset of the Phoenix dataset, which is in Spanish. The dataset is a subset of the Phoenix dataset, which is in Spanish. The dataset is a subset of the Phoenix dataset, which is in Spanish. The dataset is a subset of the Phoenix dataset, which is in Spanish. The dataset is a subset of the Phoenix dataset, which is in Spanish. The dataset is a subset of the Phoenix dataset, which is in Spanish. The dataset is a subset of the Phoenix dataset, which is in Spanish.\n\nAnswer B:\nEnglish. Explanation: The dataset makes use of 450 English-language news sites, which are each scraped every hour for new content.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.06349",
    "question": "How do they prove that RNNs with arbitrary precision are as powerful as a pushdown automata?",
    "gold_answer": "",
    "answer_a": "What is the computational power of a pushdown automata?\n< Explanation: We prove that finite precision RNNs with one hidden layer and ReLU activation and finite precision GRUs are exactly as computationally powerful as deterministic finite automata. Allowing arbitrary precision, we prove that RNNs with one hidden layer and ReLU activation are at least as computationally powerful as pushdown automata. If we also allow infinite precision, infinite edge weights, and nonlinear output activation functions, we prove that GRUs are at least as computationally powerful as pushdown automata. All results are shown constructively.\n\nWe show that the computational power",
    "answer_b": "Theorem 1.11. For any context-free language INLINEFORM0 , suppose we relabel and write INLINEFORM1 for some regular language INLINEFORM2 , whose corresponding minimum-size DFA has INLINEFORM3 states. Then there exists a simple RNN with a hidden layer of size INLINEFORM4 whose INLINEFORM5 -language is INLINEFORM6 .. Explanation: Theorem 1.11. For any context-free language INLINEFORM0 , suppose we relabel and write INLINEFORM1 for some regular language INLINEFORM2 , whose corresponding minimum-size DFA has INLINEFORM3 states. Then there exists a simple RNN with a hidden layer of size INLINEFORM4 whose INLINEFORM5 -language is INLINEFORM6 .\n\nProof. Consider the simple RNN with INLINEFORM0 as its INLINEFORM1 -language described in the proof of Theorem 1.1 and the simple RNN with INLINEFORM2 as its INLINEFORM3 -language constructed to prove Theorem 1.10. Merge the INLINEFORM4 nodes in the input layer corresponding to the input and merge the single output nodes of both RNNs. Stack the two hidden layers, and add no new edges. There were INLINEFORM5 hidden nodes in the first RNN and INLINEFORM6 in the second, so altogether the new RNN has INLINEFORM7 hidden nodes.\n\nThe output of the new RNN is equal to the summed output of the two original RNNs, and from the proofs of Theorems 1.1 and 1.10 these outputs are always nonnegative. Thus the output of the new RNN is INLINEFORM0 if and only if the outputs of both old RNNs were INLINEFORM1 , immediately proving the theorem. INLINEFORM2\n\nDiscussion 1.12. This result shows that simple RNNs with arbitrary precision are at least as computationally powerful as PDAs.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they prove that RNNs with arbitrary precision are as powerful as a pushdown automata?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the computational power of a pushdown automata?\n< Explanation: We prove that finite precision RNNs with one hidden layer and ReLU activation and finite precision GRUs are exactly as computationally powerful as deterministic finite automata. Allowing arbitrary precision, we prove that RNNs with one hidden layer and ReLU activation are at least as computationally powerful as pushdown automata. If we also allow infinite precision, infinite edge weights, and nonlinear output activation functions, we prove that GRUs are at least as computationally powerful as pushdown automata. All results are shown constructively.\n\nWe show that the computational power\n\nAnswer B:\nTheorem 1.11. For any context-free language INLINEFORM0 , suppose we relabel and write INLINEFORM1 for some regular language INLINEFORM2 , whose corresponding minimum-size DFA has INLINEFORM3 states. Then there exists a simple RNN with a hidden layer of size INLINEFORM4 whose INLINEFORM5 -language is INLINEFORM6 .. Explanation: Theorem 1.11. For any context-free language INLINEFORM0 , suppose we relabel and write INLINEFORM1 for some regular language INLINEFORM2 , whose corresponding minimum-size DFA has INLINEFORM3 states. Then there exists a simple RNN with a hidden layer of size INLINEFORM4 whose INLINEFORM5 -language is INLINEFORM6 .\n\nProof. Consider the simple RNN with INLINEFORM0 as its INLINEFORM1 -language described in the proof of Theorem 1.1 and the simple RNN with INLINEFORM2 as its INLINEFORM3 -language constructed to prove Theorem 1.10. Merge the INLINEFORM4 nodes in the input layer corresponding to the input and merge the single output nodes of both RNNs. Stack the two hidden layers, and add no new edges. There were INLINEFORM5 hidden nodes in the first RNN and INLINEFORM6 in the second, so altogether the new RNN has INLINEFORM7 hidden nodes.\n\nThe output of the new RNN is equal to the summed output of the two original RNNs, and from the proofs of Theorems 1.1 and 1.10 these outputs are always nonnegative. Thus the output of the new RNN is INLINEFORM0 if and only if the outputs of both old RNNs were INLINEFORM1 , immediately proving the theorem. INLINEFORM2\n\nDiscussion 1.12. This result shows that simple RNNs with arbitrary precision are at least as computationally powerful as PDAs.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.06493",
    "question": "What does post-processing do to the output?",
    "gold_answer": "",
    "answer_a": "How is it done?\n< Explanation: The post-processing step is a simple linear SVM classifier, which is trained on the output of the first classifier. The output of the first classifier is a vector of 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
    "answer_b": "Set treshold  for prediction.. Explanation: Many classifiers can predict a score or confidence about the prediction. Turning this score into the prediction is usually performed by setting a threshold, such as 0 and 0.5, so labels which have a score assigned greater than that are assigned to the sample. This might be not the optimal threshold in the multi-label classification setup and there are many approaches to set it (BIBREF9). Although these methods concentrate in the sample or label, we have had good results with a much more general approach.\n\nAs described in BIBREF1, Read and Pfahringer BIBREF10 introduce a method (referred hereinafter to as LCA) to estimate the threshold globally. Their method chooses the threshold that minimizes the difference between the label cardinality of the training set and the predicted set.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat does post-processing do to the output?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it done?\n< Explanation: The post-processing step is a simple linear SVM classifier, which is trained on the output of the first classifier. The output of the first classifier is a vector of 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n\nAnswer B:\nSet treshold  for prediction.. Explanation: Many classifiers can predict a score or confidence about the prediction. Turning this score into the prediction is usually performed by setting a threshold, such as 0 and 0.5, so labels which have a score assigned greater than that are assigned to the sample. This might be not the optimal threshold in the multi-label classification setup and there are many approaches to set it (BIBREF9). Although these methods concentrate in the sample or label, we have had good results with a much more general approach.\n\nAs described in BIBREF1, Read and Pfahringer BIBREF10 introduce a method (referred hereinafter to as LCA) to estimate the threshold globally. Their method chooses the threshold that minimizes the difference between the label cardinality of the training set and the predicted set.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.12208",
    "question": "What supports the claim that enhancement in training is advisable as long as enhancement in test is at least as strong as in training?",
    "gold_answer": "",
    "answer_a": "we performed an extensive experimental evaluation on the acoustically very challenging CHiME-5 dinner party data. Explanation: Experiments were performed using the CHiME-5 data. Distant microphone recordings (U data) during training and/or testing were processed using the speech enhancement methods depicted in Table TABREF6. Speech was either left unprocessed, enhanced using a weighted delay-and-sum beamformer (BFIt) BIBREF21 with or without dereverberation (WPE), or processed using the guided source separation (GSS) approach described in Section SECREF3. In Table TABREF6, the strength of the enhancement increases from top to bottom, i.e., GSS6 signals are much cleaner than the unprocessed ones.",
    "answer_b": "There is no discussion of the results of the experiment.\n< Explanation: We show that the performance of the acoustic model is significantly better on the test set than on the training set. This is in line with the claim that enhancement in training is advisable as long as enhancement in test is at least as strong as in training. We also show that the acoustic model is able to learn from the test set, which is in line with the claim that the acoustic model is able to learn from the test set. Finally, we show that the acoustic model is able to learn from the test set, which is in line with the claim that the acoustic model",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat supports the claim that enhancement in training is advisable as long as enhancement in test is at least as strong as in training?\n\nGround Truth Answer:\n\n\nAnswer A:\nwe performed an extensive experimental evaluation on the acoustically very challenging CHiME-5 dinner party data. Explanation: Experiments were performed using the CHiME-5 data. Distant microphone recordings (U data) during training and/or testing were processed using the speech enhancement methods depicted in Table TABREF6. Speech was either left unprocessed, enhanced using a weighted delay-and-sum beamformer (BFIt) BIBREF21 with or without dereverberation (WPE), or processed using the guided source separation (GSS) approach described in Section SECREF3. In Table TABREF6, the strength of the enhancement increases from top to bottom, i.e., GSS6 signals are much cleaner than the unprocessed ones.\n\nAnswer B:\nThere is no discussion of the results of the experiment.\n< Explanation: We show that the performance of the acoustic model is significantly better on the test set than on the training set. This is in line with the claim that enhancement in training is advisable as long as enhancement in test is at least as strong as in training. We also show that the acoustic model is able to learn from the test set, which is in line with the claim that the acoustic model is able to learn from the test set. Finally, we show that the acoustic model is able to learn from the test set, which is in line with the claim that the acoustic model\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.12208",
    "question": "How does this single-system compares to system combination ones?",
    "gold_answer": "",
    "answer_a": "What is the state-of-the-art?\n< Explanation: We compare our approach to the state-of-the-art system combination approach of BIBREF10, which uses a CNN followed by a factorized TDNN. We also compare our approach to the state-of-the-art system combination approach of BIBREF11, which uses a CNN followed by a factorized TDNN followed by a CNN followed by a factorized TDNN. We also compare our approach to the state-of-the-art system combination approach of BIBREF12, which uses a CNN followed by a factorized TDNN followed by a CNN followed by a factorized",
    "answer_b": "in terms of WER for the DEV (EVAL) set, the single proposed model (GSS1) has higher WER than the multiple proposed model  (GSS6) by 7.4% (4.1%).. Explanation: To facilitate comparison with the recently published top-line in BIBREF12 (H/UPB), we have conducted a more focused set of experiments whose results are depicted in Table TABREF14. As explained in Section SECREF16, we opted for BIBREF12 instead of BIBREF13 as baseline because the former system is stronger. The experiments include refining the GSS enhancement using time annotations from ASR output (GSS w/ ASR), performing discriminative training on top of the AMs trained with LF-MMI and performing RNN LM rescoring. All the above helped further improve ASR performance. We report performance of our system on both single and multiple array tracks. To have a fair comparison, the results are compared with the single-system performance reported inBIBREF12.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does this single-system compares to system combination ones?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the state-of-the-art?\n< Explanation: We compare our approach to the state-of-the-art system combination approach of BIBREF10, which uses a CNN followed by a factorized TDNN. We also compare our approach to the state-of-the-art system combination approach of BIBREF11, which uses a CNN followed by a factorized TDNN followed by a CNN followed by a factorized TDNN. We also compare our approach to the state-of-the-art system combination approach of BIBREF12, which uses a CNN followed by a factorized TDNN followed by a CNN followed by a factorized\n\nAnswer B:\nin terms of WER for the DEV (EVAL) set, the single proposed model (GSS1) has higher WER than the multiple proposed model  (GSS6) by 7.4% (4.1%).. Explanation: To facilitate comparison with the recently published top-line in BIBREF12 (H/UPB), we have conducted a more focused set of experiments whose results are depicted in Table TABREF14. As explained in Section SECREF16, we opted for BIBREF12 instead of BIBREF13 as baseline because the former system is stronger. The experiments include refining the GSS enhancement using time annotations from ASR output (GSS w/ ASR), performing discriminative training on top of the AMs trained with LF-MMI and performing RNN LM rescoring. All the above helped further improve ASR performance. We report performance of our system on both single and multiple array tracks. To have a fair comparison, the results are compared with the single-system performance reported inBIBREF12.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.12208",
    "question": "What was previous single-system state of the art result on the CHiME-5 data?",
    "gold_answer": "",
    "answer_a": "What was the state of the art on the CHiME-5 data?\n< Explanation: The state of the art on the CHiME-5 data was achieved by the single-system state-of-the-art model of BIBREF10, which achieved 43.2 % WER on the DEV set and 41.6 % WER on the EVAL set. The state of the art on the CHiME-5 data was achieved by the single-system state-of-the-art model of BIBREF10, which achieved 43.2 % WER on the DEV set and 41.",
    "answer_b": "BIBREF12 (H/UPB). Explanation: To facilitate comparison with the recently published top-line in BIBREF12 (H/UPB), we have conducted a more focused set of experiments whose results are depicted in Table TABREF14. As explained in Section SECREF16, we opted for BIBREF12 instead of BIBREF13 as baseline because the former system is stronger. The experiments include refining the GSS enhancement using time annotations from ASR output (GSS w/ ASR), performing discriminative training on top of the AMs trained with LF-MMI and performing RNN LM rescoring.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat was previous single-system state of the art result on the CHiME-5 data?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat was the state of the art on the CHiME-5 data?\n< Explanation: The state of the art on the CHiME-5 data was achieved by the single-system state-of-the-art model of BIBREF10, which achieved 43.2 % WER on the DEV set and 41.6 % WER on the EVAL set. The state of the art on the CHiME-5 data was achieved by the single-system state-of-the-art model of BIBREF10, which achieved 43.2 % WER on the DEV set and 41.\n\nAnswer B:\nBIBREF12 (H/UPB). Explanation: To facilitate comparison with the recently published top-line in BIBREF12 (H/UPB), we have conducted a more focused set of experiments whose results are depicted in Table TABREF14. As explained in Section SECREF16, we opted for BIBREF12 instead of BIBREF13 as baseline because the former system is stronger. The experiments include refining the GSS enhancement using time annotations from ASR output (GSS w/ ASR), performing discriminative training on top of the AMs trained with LF-MMI and performing RNN LM rescoring.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.12208",
    "question": "How much is error rate reduced by cleaning up training data?",
    "gold_answer": "",
    "answer_a": "Explanation: The average WER on the DEV set is 41.6 %, and on the EVAL set is 43.2 %. This is a 8 % relative improvement compared to the best word error rate published so far for a speech recognizer without system combination. This is a 1.6 % relative improvement compared to the best single-system state-of-the-art result on the CHiME-5 data. This is a 1.6 % relative improvement compared to the best single-system state-of-the-art result on the CHiME-5 data. This is a 1.6 % relative improvement compared to",
    "answer_b": "In case of singe model the WER was better by 10.%  (6.4%) and in case of multi model it was 3.5% ( 4.1%). Explanation: FLOAT SELECTED: Table 5: Comparison of the reference [13] and proposed systems in terms of amount of training data.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much is error rate reduced by cleaning up training data?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The average WER on the DEV set is 41.6 %, and on the EVAL set is 43.2 %. This is a 8 % relative improvement compared to the best word error rate published so far for a speech recognizer without system combination. This is a 1.6 % relative improvement compared to the best single-system state-of-the-art result on the CHiME-5 data. This is a 1.6 % relative improvement compared to the best single-system state-of-the-art result on the CHiME-5 data. This is a 1.6 % relative improvement compared to\n\nAnswer B:\nIn case of singe model the WER was better by 10.%  (6.4%) and in case of multi model it was 3.5% ( 4.1%). Explanation: FLOAT SELECTED: Table 5: Comparison of the reference [13] and proposed systems in terms of amount of training data.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.09821",
    "question": "Which baselines were they used for evaluation?",
    "gold_answer": "",
    "answer_a": "What were their results?\n< Explanation: We use the following baselines: (i) a baseline based on the word embeddings of the English and German corpora (BIBREF10), (ii) a baseline based on the sentence embeddings of the English and German corpora (BIBREF11), (iii) a baseline based on the word embeddings of the English and Japanese corpora (BIBREF12), (iv) a baseline based on the sentence embeddings of the English and Japanese corpora (BIBREF13), (v) a baseline based on the word embeddings of the English and Chinese corpora",
    "answer_b": "aggregation of multilingual word embeddings. Explanation: In this paper, we propose initial strong baselines which represent two complementary directions of research: one based on the aggregation of multilingual word embeddings, and another one, which directly learns multilingual sentence representations.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich baselines were they used for evaluation?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat were their results?\n< Explanation: We use the following baselines: (i) a baseline based on the word embeddings of the English and German corpora (BIBREF10), (ii) a baseline based on the sentence embeddings of the English and German corpora (BIBREF11), (iii) a baseline based on the word embeddings of the English and Japanese corpora (BIBREF12), (iv) a baseline based on the sentence embeddings of the English and Japanese corpora (BIBREF13), (v) a baseline based on the word embeddings of the English and Chinese corpora\n\nAnswer B:\naggregation of multilingual word embeddings. Explanation: In this paper, we propose initial strong baselines which represent two complementary directions of research: one based on the aggregation of multilingual word embeddings, and another one, which directly learns multilingual sentence representations.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.09821",
    "question": "What is the difference in size compare to the previous model?",
    "gold_answer": "",
    "answer_a": "larger. Explanation: Split the data into train, development and test corpus: for each languages, we provide training data of different sizes (1k, 2k, 5k and 10k stories), a development (1k) and a test corpus (4k);",
    "answer_b": "What is the size of the new model?\n< Explanation: The size of the previous model is 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the difference in size compare to the previous model?\n\nGround Truth Answer:\n\n\nAnswer A:\nlarger. Explanation: Split the data into train, development and test corpus: for each languages, we provide training data of different sizes (1k, 2k, 5k and 10k stories), a development (1k) and a test corpus (4k);\n\nAnswer B:\nWhat is the size of the new model?\n< Explanation: The size of the previous model is 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.07212",
    "question": "What languages are used as input?",
    "gold_answer": "",
    "answer_a": "How are they annotated?\n< Explanation: We use English, Spanish, French, German, Italian, and Portuguese. We annotate tweets with the following features:\n\n1. Tweet text: the text of the tweet, including the tweet ID, the author's name, and the tweet's content.\n\n2. Tweet sentiment: the sentiment of the tweet, which is a numerical value between -1 and 1, where -1 indicates a negative sentiment and 1 indicates a positive sentiment.\n\n3. Tweet veridicality: the veridicality of the tweet, which is a numerical value between 0 and 1, where 0",
    "answer_b": "English. Explanation: We restricted the data to English tweets only, as tagged by langid.py BIBREF18 . Jaccard similarity was computed between messages to identify and remove duplicates. We removed URLs and preserved only tweets that mention contenders in the text. This automatic post-processing left us with 57,711 tweets for all winners and 55,558 tweets for losers (contenders who did not win) across all events. Table TABREF17 gives the data distribution across event categories.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat languages are used as input?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they annotated?\n< Explanation: We use English, Spanish, French, German, Italian, and Portuguese. We annotate tweets with the following features:\n\n1. Tweet text: the text of the tweet, including the tweet ID, the author's name, and the tweet's content.\n\n2. Tweet sentiment: the sentiment of the tweet, which is a numerical value between -1 and 1, where -1 indicates a negative sentiment and 1 indicates a positive sentiment.\n\n3. Tweet veridicality: the veridicality of the tweet, which is a numerical value between 0 and 1, where 0\n\nAnswer B:\nEnglish. Explanation: We restricted the data to English tweets only, as tagged by langid.py BIBREF18 . Jaccard similarity was computed between messages to identify and remove duplicates. We removed URLs and preserved only tweets that mention contenders in the text. This automatic post-processing left us with 57,711 tweets for all winners and 55,558 tweets for losers (contenders who did not win) across all events. Table TABREF17 gives the data distribution across event categories.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.07212",
    "question": "What are the components of the classifier?",
    "gold_answer": "",
    "answer_a": "log-linear model. Explanation: We model the conditional distribution over a tweet's veridicality toward a candidate INLINEFORM0 winning a contest against a set of opponents, INLINEFORM1 , using a log-linear model: INLINEFORM2\n\nwhere INLINEFORM0 is the veridicality (positive, negative or neutral).",
    "answer_b": "How is the wisdom of crowds used?\n< Explanation: The classifier consists of a simple logistic regression model that predicts the winner of a given contest based on the tweet content. The classifier is trained on a set of tweets annotated for veridicality. The classifier is trained on a set of tweets annotated for veridicality. The classifier is trained on a set of tweets annotated for veridicality. The classifier is trained on a set of tweets annotated for veridicality. The classifier is trained on a set of tweets annotated for veridicality. The classifier is trained on a set of tweets annotated for veridical",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the components of the classifier?\n\nGround Truth Answer:\n\n\nAnswer A:\nlog-linear model. Explanation: We model the conditional distribution over a tweet's veridicality toward a candidate INLINEFORM0 winning a contest against a set of opponents, INLINEFORM1 , using a log-linear model: INLINEFORM2\n\nwhere INLINEFORM0 is the veridicality (positive, negative or neutral).\n\nAnswer B:\nHow is the wisdom of crowds used?\n< Explanation: The classifier consists of a simple logistic regression model that predicts the winner of a given contest based on the tweet content. The classifier is trained on a set of tweets annotated for veridicality. The classifier is trained on a set of tweets annotated for veridicality. The classifier is trained on a set of tweets annotated for veridicality. The classifier is trained on a set of tweets annotated for veridicality. The classifier is trained on a set of tweets annotated for veridicality. The classifier is trained on a set of tweets annotated for veridical\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.07212",
    "question": "Which uncertain outcomes are forecast using the wisdom of crowds?",
    "gold_answer": "",
    "answer_a": "Explanation: We then forecast uncertain outcomes using the wisdom of crowds, by aggregating users' explicit predictions. We use the following three methods to aggregate predictions:\n\n1) Majority vote: We use the majority vote of all tweets that contain the word \"winner\" to forecast the winner. This method is simple and straightforward, but it is not robust to noise. It is also not informative, as it does not take into account the uncertainty of individual predictions.\n\n2) Simple majority: We use the simple majority vote of all tweets that contain the word \"winner\" to forecast the winner. This method is robust to noise, but it is not informative",
    "answer_b": "neutral (\u201cUncertain about the outcome\"). Explanation: For the purpose of our experiments, we collapsed the five labels for veridicality into three: positive veridicality (\u201cDefinitely Yes\" and \u201cProbably Yes\"), neutral (\u201cUncertain about the outcome\") and negative veridicality (\u201cDefinitely No\" and \u201cProbably No\").",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich uncertain outcomes are forecast using the wisdom of crowds?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We then forecast uncertain outcomes using the wisdom of crowds, by aggregating users' explicit predictions. We use the following three methods to aggregate predictions:\n\n1) Majority vote: We use the majority vote of all tweets that contain the word \"winner\" to forecast the winner. This method is simple and straightforward, but it is not robust to noise. It is also not informative, as it does not take into account the uncertainty of individual predictions.\n\n2) Simple majority: We use the simple majority vote of all tweets that contain the word \"winner\" to forecast the winner. This method is robust to noise, but it is not informative\n\nAnswer B:\nneutral (\u201cUncertain about the outcome\"). Explanation: For the purpose of our experiments, we collapsed the five labels for veridicality into three: positive veridicality (\u201cDefinitely Yes\" and \u201cProbably Yes\"), neutral (\u201cUncertain about the outcome\") and negative veridicality (\u201cDefinitely No\" and \u201cProbably No\").\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1810.12897",
    "question": "What set topics are looked at?",
    "gold_answer": "",
    "answer_a": "How are sentiments expressed towards them?\n<blockquote>topics: \u201cclimate change\u201d, \u201cclimate change policy\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform",
    "answer_b": "We obtained 50 topics using LDA. Explanation: We obtained 50 topics using LDA from Mallet run over the training dataset. The topic-sentiment matrix was obtained using the Stanford CoreNLP sentiment API BIBREF9 which provides probability distributions over a set of five sentiment polarity classes.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat set topics are looked at?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are sentiments expressed towards them?\n<blockquote>topics: \u201cclimate change\u201d, \u201cclimate change policy\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\u201d, \u201cclimate change policy reform\n\nAnswer B:\nWe obtained 50 topics using LDA. Explanation: We obtained 50 topics using LDA from Mallet run over the training dataset. The topic-sentiment matrix was obtained using the Stanford CoreNLP sentiment API BIBREF9 which provides probability distributions over a set of five sentiment polarity classes.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1810.12897",
    "question": "What were the baselines?",
    "gold_answer": "",
    "answer_a": "How were they evaluated?\n< Explanation: We compare our approach to the following baselines: (1) a simple sentiment analysis model that uses the sentiment polarity of a document to predict the sentiment of a topic (i.e., a bag-of-words model); (2) a simple topic model that uses the topic distribution of a document to predict the topic of a topic (i.e., a Markov chain model); (3) a simple topic model that uses the topic distribution of a document to predict the topic of a topic (i.e., a Markov chain model); (4) a simple topic model that uses the topic distribution",
    "answer_b": "We use pre-trained GloVe BIBREF10 word embeddings to compute vector representation of each VPD by averaging the GloVe vectors for all words in the document. A logistic regression classifier is then trained on the vector representations thus obtained.. Explanation: In order to evaluate our proposed TSM-based methods - viz., nearest class (NC) and logistic regression (LR) - we use the following methods in our empirical evaluation.\n\nGloVe-d2v: We use pre-trained GloVe BIBREF10 word embeddings to compute vector representation of each VPD by averaging the GloVe vectors for all words in the document. A logistic regression classifier is then trained on the vector representations thus obtained.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat were the baselines?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow were they evaluated?\n< Explanation: We compare our approach to the following baselines: (1) a simple sentiment analysis model that uses the sentiment polarity of a document to predict the sentiment of a topic (i.e., a bag-of-words model); (2) a simple topic model that uses the topic distribution of a document to predict the topic of a topic (i.e., a Markov chain model); (3) a simple topic model that uses the topic distribution of a document to predict the topic of a topic (i.e., a Markov chain model); (4) a simple topic model that uses the topic distribution\n\nAnswer B:\nWe use pre-trained GloVe BIBREF10 word embeddings to compute vector representation of each VPD by averaging the GloVe vectors for all words in the document. A logistic regression classifier is then trained on the vector representations thus obtained.. Explanation: In order to evaluate our proposed TSM-based methods - viz., nearest class (NC) and logistic regression (LR) - we use the following methods in our empirical evaluation.\n\nGloVe-d2v: We use pre-trained GloVe BIBREF10 word embeddings to compute vector representation of each VPD by averaging the GloVe vectors for all words in the document. A logistic regression classifier is then trained on the vector representations thus obtained.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1810.12897",
    "question": "Which widely used dataset did the authors use?",
    "gold_answer": "",
    "answer_a": "Convote dataset BIBREF3. Explanation: We used the publicly available Convote dataset BIBREF3 for our experiments.",
    "answer_b": "What is the size of the dataset?\n<blockquote>\n<font size=\"2\">We use the dataset of <a href=\"http://www.cs.cornell.edu/~gkatz/indieweb.html\">Indieweb</a> which contains 100,000 tweets from 10000 users. The dataset is available at <a href=\"http://www.cs.cornell.edu/~gkatz/indieweb.html\">http://www.cs.cornell.edu/~gkatz/indieweb.html</a>. </font>\n</blockquote>\nThe dataset is available at",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich widely used dataset did the authors use?\n\nGround Truth Answer:\n\n\nAnswer A:\nConvote dataset BIBREF3. Explanation: We used the publicly available Convote dataset BIBREF3 for our experiments.\n\nAnswer B:\nWhat is the size of the dataset?\n<blockquote>\n<font size=\"2\">We use the dataset of <a href=\"http://www.cs.cornell.edu/~gkatz/indieweb.html\">Indieweb</a> which contains 100,000 tweets from 10000 users. The dataset is available at <a href=\"http://www.cs.cornell.edu/~gkatz/indieweb.html\">http://www.cs.cornell.edu/~gkatz/indieweb.html</a>. </font>\n</blockquote>\nThe dataset is available at\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.01884",
    "question": "How do they perform semi-supervised learning?",
    "gold_answer": "",
    "answer_a": "What is the dataset used?\n<sep><EXPERIMENT RESULT PAGE FACEBOOK> Explanation: We use the dataset of 10000 sentences and 10000 documents from the Stanford Sentiment Treebank (SST) BIBREF10 . We use the following steps to train the model:\n\n1. We randomly split the dataset into training and validation sets, with 80% of the data used for training and 20% for validation.\n\n2. We train the model using the training set, and then use the validation set to tune the hyperparameters.\n\n3. We use the best model",
    "answer_b": "On each step, a generative network is used to generate samples, then a classifier labels them to an extra class. A mix of generated data and real data is combined into a batch, then a gradient descent is performed on the batch,  and the parameters are updated.. Explanation: Our semi-supervised text classification framewrok is inspired by works BIBREF18 , BIBREF19 . We assume the original classifier classify a sample into one of INLINEFORM0 possible classes. So we can do semi-supervised learning by simply adding samples from a generative network G to our dataset and labeling them to an extra class INLINEFORM1 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they perform semi-supervised learning?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the dataset used?\n<sep><EXPERIMENT RESULT PAGE FACEBOOK> Explanation: We use the dataset of 10000 sentences and 10000 documents from the Stanford Sentiment Treebank (SST) BIBREF10 . We use the following steps to train the model:\n\n1. We randomly split the dataset into training and validation sets, with 80% of the data used for training and 20% for validation.\n\n2. We train the model using the training set, and then use the validation set to tune the hyperparameters.\n\n3. We use the best model\n\nAnswer B:\nOn each step, a generative network is used to generate samples, then a classifier labels them to an extra class. A mix of generated data and real data is combined into a batch, then a gradient descent is performed on the batch,  and the parameters are updated.. Explanation: Our semi-supervised text classification framewrok is inspired by works BIBREF18 , BIBREF19 . We assume the original classifier classify a sample into one of INLINEFORM0 possible classes. So we can do semi-supervised learning by simply adding samples from a generative network G to our dataset and labeling them to an extra class INLINEFORM1 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.01884",
    "question": "What are the five evaluated tasks?",
    "gold_answer": "",
    "answer_a": "Model is evaluated on six tasks: TREC, MR, SST-1, SST-2, SUBJ and YELP13.. Explanation: Results of our models against other methods are listed in table TABREF16 .",
    "answer_b": "What are the results?\n<sep><font size=\"2\">Sentiment Analysis</font>:\nSentiment analysis is a task that aims to classify the sentiment of a given text. The task is to classify the sentiment of a given text into positive, negative, or neutral. The dataset used in this task is the Stanford Sentiment Treebank (SST). The dataset contains 10000 sentences and 10000 labels. The dataset is divided into training and testing sets. The training set contains 7000 sentences and 7000 labels, while the testing set contains 30",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the five evaluated tasks?\n\nGround Truth Answer:\n\n\nAnswer A:\nModel is evaluated on six tasks: TREC, MR, SST-1, SST-2, SUBJ and YELP13.. Explanation: Results of our models against other methods are listed in table TABREF16 .\n\nAnswer B:\nWhat are the results?\n<sep><font size=\"2\">Sentiment Analysis</font>:\nSentiment analysis is a task that aims to classify the sentiment of a given text. The task is to classify the sentiment of a given text into positive, negative, or neutral. The dataset used in this task is the Stanford Sentiment Treebank (SST). The dataset contains 10000 sentences and 10000 labels. The dataset is divided into training and testing sets. The training set contains 7000 sentences and 7000 labels, while the testing set contains 30\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.09692",
    "question": "What downstream tasks are explored?",
    "gold_answer": "",
    "answer_a": "How are stability measured?\n< Explanation: We consider three tasks: (1) word similarity, (2) word classification, and (3) word embedding. We use the following metrics to measure the stability of the embeddings: (1) cosine similarity between the embeddings of two words, (2) the number of words that are not in the embedding space of the other word, and (3) the number of words that are not in the embedding space of the other word. We also use the following metrics to measure the stability of the embeddings: (1) the number of words that are not in the embedding space of the other word",
    "answer_b": "word similarity. Explanation: We study two downstream tasks to see whether stability impacts performance.\n\nSince we are interested in seeing the impact of word vector stability, we choose tasks that have an intuitive evaluation at the word level: word similarity and POS tagging.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat downstream tasks are explored?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are stability measured?\n< Explanation: We consider three tasks: (1) word similarity, (2) word classification, and (3) word embedding. We use the following metrics to measure the stability of the embeddings: (1) cosine similarity between the embeddings of two words, (2) the number of words that are not in the embedding space of the other word, and (3) the number of words that are not in the embedding space of the other word. We also use the following metrics to measure the stability of the embeddings: (1) the number of words that are not in the embedding space of the other word\n\nAnswer B:\nword similarity. Explanation: We study two downstream tasks to see whether stability impacts performance.\n\nSince we are interested in seeing the impact of word vector stability, we choose tasks that have an intuitive evaluation at the word level: word similarity and POS tagging.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.09692",
    "question": "What factors contribute to the stability of the word embeddings?",
    "gold_answer": "",
    "answer_a": "curriculum learning. Explanation: This indicates that curriculum learning is a factor in stability.",
    "answer_b": "What are the empirical evidence?\n< Explanation: We show that the stability of word embeddings is highly dependent on the frequency of the words. We find that the stability of the embeddings is significantly worse for words with low frequency (100-200 occurrences) than for words with high frequency (1000-2000 occurrences). We also show that the stability of the embeddings is significantly worse for words that are not in the vocabulary of the model. We find that the stability of the embeddings is significantly worse for words that are not in the vocabulary of the model. We also show that the stability of the embeddings is",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat factors contribute to the stability of the word embeddings?\n\nGround Truth Answer:\n\n\nAnswer A:\ncurriculum learning. Explanation: This indicates that curriculum learning is a factor in stability.\n\nAnswer B:\nWhat are the empirical evidence?\n< Explanation: We show that the stability of word embeddings is highly dependent on the frequency of the words. We find that the stability of the embeddings is significantly worse for words with low frequency (100-200 occurrences) than for words with high frequency (1000-2000 occurrences). We also show that the stability of the embeddings is significantly worse for words that are not in the vocabulary of the model. We find that the stability of the embeddings is significantly worse for words that are not in the vocabulary of the model. We also show that the stability of the embeddings is\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.09692",
    "question": "How is unstability defined?",
    "gold_answer": "",
    "answer_a": "We define stability as the percent overlap between nearest neighbors in an embedding space.. Explanation: Defining Stability\nWe define stability as the percent overlap between nearest neighbors in an embedding space. Given a word INLINEFORM0 and two embedding spaces INLINEFORM1 and INLINEFORM2 , take the ten nearest neighbors of INLINEFORM3 in both INLINEFORM4 and INLINEFORM5 . Let the stability of INLINEFORM6 be the percent overlap between these two lists of nearest neighbors. 100% stability indicates perfect agreement between the two embedding spaces, while 0% stability indicates complete disagreement.",
    "answer_b": "What are the factors that contribute to the stability of word embeddings?\n< Explanation: We define unstability as the probability that a word is not represented by its embedding. We define the probability of a word being represented by its embedding as the probability of the word being in the embedding space. We define the stability of a word embedding as the probability that a word is not represented by its embedding. We define the probability of a word being represented by its embedding as the probability of the word being in the embedding space. We define the stability of a word embedding as the probability that a word is not represented by its embedding. We define the probability of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is unstability defined?\n\nGround Truth Answer:\n\n\nAnswer A:\nWe define stability as the percent overlap between nearest neighbors in an embedding space.. Explanation: Defining Stability\nWe define stability as the percent overlap between nearest neighbors in an embedding space. Given a word INLINEFORM0 and two embedding spaces INLINEFORM1 and INLINEFORM2 , take the ten nearest neighbors of INLINEFORM3 in both INLINEFORM4 and INLINEFORM5 . Let the stability of INLINEFORM6 be the percent overlap between these two lists of nearest neighbors. 100% stability indicates perfect agreement between the two embedding spaces, while 0% stability indicates complete disagreement.\n\nAnswer B:\nWhat are the factors that contribute to the stability of word embeddings?\n< Explanation: We define unstability as the probability that a word is not represented by its embedding. We define the probability of a word being represented by its embedding as the probability of the word being in the embedding space. We define the stability of a word embedding as the probability that a word is not represented by its embedding. We define the probability of a word being represented by its embedding as the probability of the word being in the embedding space. We define the stability of a word embedding as the probability that a word is not represented by its embedding. We define the probability of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.09692",
    "question": "What embedding algorithms are explored?",
    "gold_answer": "",
    "answer_a": "How are they evaluated?\n< Explanation: We evaluate the stability of word embeddings using the following metrics: (1) the cosine similarity between the embedding and the ground truth word embeddings, (2) the cosine similarity between the embedding and the ground truth word embeddings, and (3) the cosine similarity between the embedding and the ground truth word embeddings, where the cosine similarity is computed using the Euclidean distance. We also evaluate the stability of word embeddings using the following metrics: (1) the cosine similarity between the embedding and the ground truth word embeddings, (2) the cosine similarity between the embedding and the ground truth word embeddings, and (",
    "answer_b": "word2vec, GloVe, and PPMI. Explanation: Here, we consider three embedding algorithms, word2vec, GloVe, and PPMI.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat embedding algorithms are explored?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they evaluated?\n< Explanation: We evaluate the stability of word embeddings using the following metrics: (1) the cosine similarity between the embedding and the ground truth word embeddings, (2) the cosine similarity between the embedding and the ground truth word embeddings, and (3) the cosine similarity between the embedding and the ground truth word embeddings, where the cosine similarity is computed using the Euclidean distance. We also evaluate the stability of word embeddings using the following metrics: (1) the cosine similarity between the embedding and the ground truth word embeddings, (2) the cosine similarity between the embedding and the ground truth word embeddings, and (\n\nAnswer B:\nword2vec, GloVe, and PPMI. Explanation: Here, we consider three embedding algorithms, word2vec, GloVe, and PPMI.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.12016",
    "question": "Which data-selection algorithms do they use?",
    "gold_answer": "",
    "answer_a": "What are their advantages?\n< Explanation: We use the following data-selection algorithms: (1) Random Selection: randomly selects 10% of the sentences from the training set. (2) Random Selection with Replacement: randomly selects 10% of the sentences from the training set, and then selects 10% of the remaining sentences. (3) Random Selection with Replacement and Reversal: randomly selects 10% of the sentences from the training set, and then selects 10% of the remaining sentences, and then reverses the order of the selected sentences. (4) Random Selection with Replacement and Re",
    "answer_b": "Infrequent N-gram Recovery (INR). Explanation: As we want to build task-specific NMT models, in this work we explore two data-selection algorithms that are classified as Transductive Algorithms (TA): Infrequent N-gram Recovery (INR) and Feature Decay Algorithms (FDA).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich data-selection algorithms do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are their advantages?\n< Explanation: We use the following data-selection algorithms: (1) Random Selection: randomly selects 10% of the sentences from the training set. (2) Random Selection with Replacement: randomly selects 10% of the sentences from the training set, and then selects 10% of the remaining sentences. (3) Random Selection with Replacement and Reversal: randomly selects 10% of the sentences from the training set, and then selects 10% of the remaining sentences, and then reverses the order of the selected sentences. (4) Random Selection with Replacement and Re\n\nAnswer B:\nInfrequent N-gram Recovery (INR). Explanation: As we want to build task-specific NMT models, in this work we explore two data-selection algorithms that are classified as Transductive Algorithms (TA): Infrequent N-gram Recovery (INR) and Feature Decay Algorithms (FDA).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.12016",
    "question": "How are the artificial sentences generated?",
    "gold_answer": "",
    "answer_a": "they can be considered as candidate sentences for a data-selection algorithm to decide which sentence-pairs should be used to fine-tune the NMT model. Explanation: For this reason, we explore an alternative approach to better use the artificially-generated training instances to improve NMT models. In particular, we propose that instead of blindly adding back-translated sentences into the training set they can be considered as candidate sentences for a data-selection algorithm to decide which sentence-pairs should be used to fine-tune the NMT model.",
    "answer_b": "What are the data-selection algorithms used?\n< Explanation: We generate the artificial sentences by randomly selecting 1000 sentences from the German Wikipedia corpus. We then randomly select 1000 sentences from the German Wikipedia corpus. We then randomly select 1000 sentences from the German Wikipedia corpus. We then randomly select 1000 sentences from the German Wikipedia corpus. We then randomly select 1000 sentences from the German Wikipedia corpus. We then randomly select 1000 sentences from the German Wikipedia corpus. We then randomly select 1000 sentences from the German Wikipedia corpus. We",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are the artificial sentences generated?\n\nGround Truth Answer:\n\n\nAnswer A:\nthey can be considered as candidate sentences for a data-selection algorithm to decide which sentence-pairs should be used to fine-tune the NMT model. Explanation: For this reason, we explore an alternative approach to better use the artificially-generated training instances to improve NMT models. In particular, we propose that instead of blindly adding back-translated sentences into the training set they can be considered as candidate sentences for a data-selection algorithm to decide which sentence-pairs should be used to fine-tune the NMT model.\n\nAnswer B:\nWhat are the data-selection algorithms used?\n< Explanation: We generate the artificial sentences by randomly selecting 1000 sentences from the German Wikipedia corpus. We then randomly select 1000 sentences from the German Wikipedia corpus. We then randomly select 1000 sentences from the German Wikipedia corpus. We then randomly select 1000 sentences from the German Wikipedia corpus. We then randomly select 1000 sentences from the German Wikipedia corpus. We then randomly select 1000 sentences from the German Wikipedia corpus. We then randomly select 1000 sentences from the German Wikipedia corpus. We\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.12016",
    "question": "What domain is their test set?",
    "gold_answer": "",
    "answer_a": "biomedical. Explanation: Test sets: We evaluate the models with two test sets in different domains:\n\nBIO test set: the Cochrane dataset from the WMT 2017 biomedical translation shared task BIBREF20.\n\nNEWS test set: The test set provided in WMT 2015 News Translation Task.",
    "answer_b": "How many sentences are in the test set?\n< Explanation: We use the German-English NMT dataset of BIBREF11. The dataset consists of 1000 sentences, each of which is a translation of a German sentence into English. The sentences are randomly selected from the dataset, and the test set consists of 100 sentences. The test set is used to evaluate the performance of the trained models. We use the German-English NMT dataset of BIBREF11. The dataset consists of 1000 sentences, each of which is a translation of a German sentence into English. The",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat domain is their test set?\n\nGround Truth Answer:\n\n\nAnswer A:\nbiomedical. Explanation: Test sets: We evaluate the models with two test sets in different domains:\n\nBIO test set: the Cochrane dataset from the WMT 2017 biomedical translation shared task BIBREF20.\n\nNEWS test set: The test set provided in WMT 2015 News Translation Task.\n\nAnswer B:\nHow many sentences are in the test set?\n< Explanation: We use the German-English NMT dataset of BIBREF11. The dataset consists of 1000 sentences, each of which is a translation of a German sentence into English. The sentences are randomly selected from the dataset, and the test set consists of 100 sentences. The test set is used to evaluate the performance of the trained models. We use the German-English NMT dataset of BIBREF11. The dataset consists of 1000 sentences, each of which is a translation of a German sentence into English. The\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1606.02892",
    "question": "What morphological features are considered?",
    "gold_answer": "",
    "answer_a": "case. Explanation: For German INLINEFORM0 English, the parser annotates the German input with morphological features. Different word types have different sets of features \u2013 for instance, nouns have case, number and gender, while verbs have person, number, tense and aspect \u2013 and features may be underspecified.",
    "answer_b": "What are the features used?\n<blockquote>Part of speech tags are considered. The tags are: Noun, Verb, Adjective, Adverb, Preposition, Conjunction, Interjection, and Pronoun. The tags are: Noun, Verb, Adjective, Adverb, Preposition, Conjunction, Interjection, and Pronoun. The tags are: Noun, Verb, Adjective, Adverb, Preposition, Conjunction, Interjection, and Pronoun. The tags are: Noun, Verb, Adjective, Adverb, Preposition, Conjunction, Interjection, and Pronoun.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat morphological features are considered?\n\nGround Truth Answer:\n\n\nAnswer A:\ncase. Explanation: For German INLINEFORM0 English, the parser annotates the German input with morphological features. Different word types have different sets of features \u2013 for instance, nouns have case, number and gender, while verbs have person, number, tense and aspect \u2013 and features may be underspecified.\n\nAnswer B:\nWhat are the features used?\n<blockquote>Part of speech tags are considered. The tags are: Noun, Verb, Adjective, Adverb, Preposition, Conjunction, Interjection, and Pronoun. The tags are: Noun, Verb, Adjective, Adverb, Preposition, Conjunction, Interjection, and Pronoun. The tags are: Noun, Verb, Adjective, Adverb, Preposition, Conjunction, Interjection, and Pronoun. The tags are: Noun, Verb, Adjective, Adverb, Preposition, Conjunction, Interjection, and Pronoun.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1606.02892",
    "question": "What type of attention do they use in the decoder?",
    "gold_answer": "",
    "answer_a": "Generalized attention. Explanation: The weight of each annotation INLINEFORM7 is computed through an alignment model INLINEFORM8 , which models the probability that INLINEFORM9 is aligned to INLINEFORM10 . The alignment model is a single-layer feedforward neural network that is learned jointly with the rest of the network through backpropagation.",
    "answer_b": "How does it differ from the attention used in the encoder?\n< Explanation: The attention mechanism in the decoder is based on the attention mechanism in the encoder. The encoder uses a bidirectional LSTM to encode the input sequence. The decoder uses a bidirectional LSTM to encode the input sequence. The decoder uses the attention mechanism to combine the encoder outputs to produce the decoder output. The attention mechanism in the decoder is based on the attention mechanism in the encoder. The encoder uses a bidirectional LSTM to encode the input sequence. The decoder uses a bidirectional LSTM to encode the input sequence. The decoder uses the attention mechanism to combine the encoder outputs to",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of attention do they use in the decoder?\n\nGround Truth Answer:\n\n\nAnswer A:\nGeneralized attention. Explanation: The weight of each annotation INLINEFORM7 is computed through an alignment model INLINEFORM8 , which models the probability that INLINEFORM9 is aligned to INLINEFORM10 . The alignment model is a single-layer feedforward neural network that is learned jointly with the rest of the network through backpropagation.\n\nAnswer B:\nHow does it differ from the attention used in the encoder?\n< Explanation: The attention mechanism in the decoder is based on the attention mechanism in the encoder. The encoder uses a bidirectional LSTM to encode the input sequence. The decoder uses a bidirectional LSTM to encode the input sequence. The decoder uses the attention mechanism to combine the encoder outputs to produce the decoder output. The attention mechanism in the decoder is based on the attention mechanism in the encoder. The encoder uses a bidirectional LSTM to encode the input sequence. The decoder uses a bidirectional LSTM to encode the input sequence. The decoder uses the attention mechanism to combine the encoder outputs to\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.09716",
    "question": "What set of semantic tags did they use?",
    "gold_answer": "",
    "answer_a": "Tags categories ranging from anaphoric (definite, possessive pronoun), attribute (colour, concrete quantity, intersective, relation), unnamed entity (concept), logical (alternative, disjunction), discourse (subordinate relation), events (present simple, past simple), etc.. Explanation: The semantic tagset is language-neutral, abstracts over part-of-speech and named-entity classes, and includes fine-grained semantic information. The tagset consists of 80 semantic tags grouped in 13 coarse-grained classes. The tagset originated in the Parallel Meaning Bank (PMB) project BIBREF9 , where it contributes to compositional semantics and cross-lingual projection of semantic representations.",
    "answer_b": "How many were there?\n< Explanation: We use the following 1000 tags: 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat set of semantic tags did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nTags categories ranging from anaphoric (definite, possessive pronoun), attribute (colour, concrete quantity, intersective, relation), unnamed entity (concept), logical (alternative, disjunction), discourse (subordinate relation), events (present simple, past simple), etc.. Explanation: The semantic tagset is language-neutral, abstracts over part-of-speech and named-entity classes, and includes fine-grained semantic information. The tagset consists of 80 semantic tags grouped in 13 coarse-grained classes. The tagset originated in the Parallel Meaning Bank (PMB) project BIBREF9 , where it contributes to compositional semantics and cross-lingual projection of semantic representations.\n\nAnswer B:\nHow many were there?\n< Explanation: We use the following 1000 tags: 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.09716",
    "question": "How much improvement did they see on the NLI task?",
    "gold_answer": "",
    "answer_a": "What was the size of the improvement?\n< Explanation: We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task.",
    "answer_b": "0.5 improvement with LWS over the single-task model. Explanation: FLOAT SELECTED: Table 1: Results for single-task models (ST), fullyshared networks (FSN), partially-shared networks (PSN), and learning what to share (LWS). All scores are reported as accuracy, except UD DEP for which we report LAS/UAS F1 score.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much improvement did they see on the NLI task?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat was the size of the improvement?\n< Explanation: We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task. We report the results of the experiments on the NLI task.\n\nAnswer B:\n0.5 improvement with LWS over the single-task model. Explanation: FLOAT SELECTED: Table 1: Results for single-task models (ST), fullyshared networks (FSN), partially-shared networks (PSN), and learning what to share (LWS). All scores are reported as accuracy, except UD DEP for which we report LAS/UAS F1 score.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.10210",
    "question": "How better are results of new model compared to competitive methods?",
    "gold_answer": "",
    "answer_a": "What is the state-of-the-art result?\n<blockquote>Our model achieves a new state-of-the-art result on the sentence-level dataset, with a F1 score of 0.938, which is 0.001 better than the state-of-the-art result on the dataset. Our model also achieves a new state-of-the-art result on the document-level dataset, with a F1 score of 0.938, which is 0.001 better than the state-of-the-art result on the dataset. </blockquote>\n<blockquote>Our model achieves a new state-of-the-art result on",
    "answer_b": "For Document- level comparison, the model achieves highest CS precision and  F1 score and it achieves higher BLEU score that TMTE, Coatt, CCDT, and HEDT. \nIn terms of Human Evaluation, the model had the highest average score, the highest Fluency score, and the second highest Content Fidelity. \nIn terms of Sentence-level comparison the model had the highest Recall and F1 scores for Content Fidelity.. Explanation: Document-level text manipulation experimental results are given in Table 2. The first block shows two slot filling methods, which can reach the maximum BLEU (100) after masking out record tokens. It is because that both methods only replace records without modifying other parts of the reference text. Moreover, Copy-SF achieves reasonably good performance on multiple metrics, setting a strong baseline for content fidelity and content selection. For two data-to-text generation methods CCDT and HEDT, the latter one is consistently better than the former, which verifies the proposed hierarchical record encoder is more powerful. However, their Style BLEU scores are particularly low, which demonstrates that direct supervised learning is incapable of controlling the text expression. In comparison, our proposed models achieve better Style BLEU and Content Selection F%. The superior performance of our full model compared to the variant ours-w/o-InterAtt, TMTE and Coatt demonstrates the usefulness of the interactive attention mechanism.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow better are results of new model compared to competitive methods?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the state-of-the-art result?\n<blockquote>Our model achieves a new state-of-the-art result on the sentence-level dataset, with a F1 score of 0.938, which is 0.001 better than the state-of-the-art result on the dataset. Our model also achieves a new state-of-the-art result on the document-level dataset, with a F1 score of 0.938, which is 0.001 better than the state-of-the-art result on the dataset. </blockquote>\n<blockquote>Our model achieves a new state-of-the-art result on\n\nAnswer B:\nFor Document- level comparison, the model achieves highest CS precision and  F1 score and it achieves higher BLEU score that TMTE, Coatt, CCDT, and HEDT. \nIn terms of Human Evaluation, the model had the highest average score, the highest Fluency score, and the second highest Content Fidelity. \nIn terms of Sentence-level comparison the model had the highest Recall and F1 scores for Content Fidelity.. Explanation: Document-level text manipulation experimental results are given in Table 2. The first block shows two slot filling methods, which can reach the maximum BLEU (100) after masking out record tokens. It is because that both methods only replace records without modifying other parts of the reference text. Moreover, Copy-SF achieves reasonably good performance on multiple metrics, setting a strong baseline for content fidelity and content selection. For two data-to-text generation methods CCDT and HEDT, the latter one is consistently better than the former, which verifies the proposed hierarchical record encoder is more powerful. However, their Style BLEU scores are particularly low, which demonstrates that direct supervised learning is incapable of controlling the text expression. In comparison, our proposed models achieve better Style BLEU and Content Selection F%. The superior performance of our full model compared to the variant ours-w/o-InterAtt, TMTE and Coatt demonstrates the usefulness of the interactive attention mechanism.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.10210",
    "question": "What is the metrics used for benchmarking methods?",
    "gold_answer": "",
    "answer_a": "What is the performance of the proposed model?\n<blockquote>Table 1: Performance of the proposed model and the state-of-the-art methods on the basketball game report dataset. The results are reported as mean average precision (mAP) and standard deviation (STD). The results are reported as mean average precision (mAP) and standard deviation (STD). The results are reported as mean average precision (mAP) and standard deviation (STD). The results are reported as mean average precision (mAP) and standard deviation (STD). The results are reported as mean average precision (mAP) and standard deviation (STD). The results are",
    "answer_b": "Content Fidelity (CF). Explanation: We use the same evaluation metrics employed in BIBREF1. Content Fidelity (CF) is an information extraction (IE) approach used in BIBREF10 to measure model's ability to generate text containing factual records. That is, precision and recall (or number) of unique records extracted from the generated text $z$ via an IE model also appear in source recordset $x$. Style Preservation is used to measure how many stylistic properties of the reference are retained in the generated text. In this paper, we calculate BLEU score between the generated text and the reference to reflect model's ability on style preservation. Furthermore, in order to measure model's ability on content selection, we adopt another IE-based evaluation metric, named Content selection, (CS), which is used for data-to-text generation BIBREF10. It is measured in terms of precision and recall by comparing records in generated text $z$ with records in the auxiliary reference $y_{aux}$.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the metrics used for benchmarking methods?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of the proposed model?\n<blockquote>Table 1: Performance of the proposed model and the state-of-the-art methods on the basketball game report dataset. The results are reported as mean average precision (mAP) and standard deviation (STD). The results are reported as mean average precision (mAP) and standard deviation (STD). The results are reported as mean average precision (mAP) and standard deviation (STD). The results are reported as mean average precision (mAP) and standard deviation (STD). The results are reported as mean average precision (mAP) and standard deviation (STD). The results are\n\nAnswer B:\nContent Fidelity (CF). Explanation: We use the same evaluation metrics employed in BIBREF1. Content Fidelity (CF) is an information extraction (IE) approach used in BIBREF10 to measure model's ability to generate text containing factual records. That is, precision and recall (or number) of unique records extracted from the generated text $z$ via an IE model also appear in source recordset $x$. Style Preservation is used to measure how many stylistic properties of the reference are retained in the generated text. In this paper, we calculate BLEU score between the generated text and the reference to reflect model's ability on style preservation. Furthermore, in order to measure model's ability on content selection, we adopt another IE-based evaluation metric, named Content selection, (CS), which is used for data-to-text generation BIBREF10. It is measured in terms of precision and recall by comparing records in generated text $z$ with records in the auxiliary reference $y_{aux}$.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.10210",
    "question": "What are other competitive methods?",
    "gold_answer": "",
    "answer_a": "Rule-based Slot Filling Method (Rule-SF). Explanation: We compare with the following baseline methods on the document-level text manipulation.\n\n(1) Rule-based Slot Filling Method (Rule-SF) is a straightforward way for text manipulation. Firstly, It masks the record information $x^{\\prime }$ in the $y^{\\prime }$ and build a mapping between $x$ and $x^{\\prime }$ through their data types. Afterwards, select the suitable records from $x$ to fill in the reference y with masked slots. The method is also used in sentence-level task BIBREF1.\n\n(2) Copy-based Slot Filling Method (Copy-SF) is a data-driven slot filling method. It is derived from BIBREF21, which first generates a template text with data slots to be filled and then leverages a delayed copy mechanism to fill in the slots with proper data records.\n\n(3) Conditional Copy based Data-To-Text (CCDT) is a classical neural model for data-to-text generation BIBREF10. (4) Hierarchical Encoder for Data-To-Text (HEDT) is also a data-to-text method, which adopts the same hierarchical encoder in our model.\n\n(5) Text Manipulation with Table Encoder (TMTE) extends sentence-level text editing method BIBREF1 by equipping a more powerful hierarchical table encoder.\n\n(6) Co-attention-based Method (Coatt): a variation of our model by replacing interactive attention with another co-attention model BIBREF22.\n\n(7) Ours w/o Interactive Attention (-InterAtt) is our model without interactive attention.\n\n(8) Ours w/o Back-translation (-BackT) is also a variation of our model by omitting back-translation loss.\n\nIn addition, for sentence-level task, we adopt the same baseline methods as the paper BIBREF1, including an attention-based Seq2Seq method with copy mechanism BIBREF23, a rule-based method, two style transfer methods, MAST BIBREF24 and AdvST BIBREF25, as well as their state-of-the-art method, abbreviate as S-SOTA.",
    "answer_b": "How do they compare to the model in this paper?\n< Explanation: The baseline model is a simple neural model with a single layer of fully connected layers. The model is trained using the same training data as the baseline model. The baseline model is trained using the same training data as the baseline model. The baseline model is trained using the same training data as the baseline model. The baseline model is trained using the same training data as the baseline model. The baseline model is trained using the same training data as the baseline model. The baseline model is trained using the same training data as the baseline model. The baseline model is trained using the same",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are other competitive methods?\n\nGround Truth Answer:\n\n\nAnswer A:\nRule-based Slot Filling Method (Rule-SF). Explanation: We compare with the following baseline methods on the document-level text manipulation.\n\n(1) Rule-based Slot Filling Method (Rule-SF) is a straightforward way for text manipulation. Firstly, It masks the record information $x^{\\prime }$ in the $y^{\\prime }$ and build a mapping between $x$ and $x^{\\prime }$ through their data types. Afterwards, select the suitable records from $x$ to fill in the reference y with masked slots. The method is also used in sentence-level task BIBREF1.\n\n(2) Copy-based Slot Filling Method (Copy-SF) is a data-driven slot filling method. It is derived from BIBREF21, which first generates a template text with data slots to be filled and then leverages a delayed copy mechanism to fill in the slots with proper data records.\n\n(3) Conditional Copy based Data-To-Text (CCDT) is a classical neural model for data-to-text generation BIBREF10. (4) Hierarchical Encoder for Data-To-Text (HEDT) is also a data-to-text method, which adopts the same hierarchical encoder in our model.\n\n(5) Text Manipulation with Table Encoder (TMTE) extends sentence-level text editing method BIBREF1 by equipping a more powerful hierarchical table encoder.\n\n(6) Co-attention-based Method (Coatt): a variation of our model by replacing interactive attention with another co-attention model BIBREF22.\n\n(7) Ours w/o Interactive Attention (-InterAtt) is our model without interactive attention.\n\n(8) Ours w/o Back-translation (-BackT) is also a variation of our model by omitting back-translation loss.\n\nIn addition, for sentence-level task, we adopt the same baseline methods as the paper BIBREF1, including an attention-based Seq2Seq method with copy mechanism BIBREF23, a rule-based method, two style transfer methods, MAST BIBREF24 and AdvST BIBREF25, as well as their state-of-the-art method, abbreviate as S-SOTA.\n\nAnswer B:\nHow do they compare to the model in this paper?\n< Explanation: The baseline model is a simple neural model with a single layer of fully connected layers. The model is trained using the same training data as the baseline model. The baseline model is trained using the same training data as the baseline model. The baseline model is trained using the same training data as the baseline model. The baseline model is trained using the same training data as the baseline model. The baseline model is trained using the same training data as the baseline model. The baseline model is trained using the same training data as the baseline model. The baseline model is trained using the same\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.10210",
    "question": "What is the size of built dataset?",
    "gold_answer": "",
    "answer_a": "Document-level dataset has total of 4821 instances. \nSentence-level dataset has total of 45583 instances.. Explanation: We use an existing information extraction tool BIBREF10 to extract record types from the reference text. Table TABREF3 shows the statistics of constructed document-level dataset and a sentence-level benchmark dataset BIBREF1. We can see that the proposed document-level text manipulation problem is more difficult than sentence-level, both in terms of the complexity of input records and the length of generated text.",
    "answer_b": "How many records and references are there?\n< Explanation: The dataset consists of 1000 records and 1000 references. The records are randomly generated from the basketball game report corpus, and the references are generated from the corresponding reference text. The records and references are randomly distributed in the dataset. The dataset is divided into training, validation and test sets. The training set is 80%, the validation set is 10%, and the test set is 10%. The training set is used to train the model, the validation set is used to evaluate the model, and the test set is used to evaluate",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the size of built dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nDocument-level dataset has total of 4821 instances. \nSentence-level dataset has total of 45583 instances.. Explanation: We use an existing information extraction tool BIBREF10 to extract record types from the reference text. Table TABREF3 shows the statistics of constructed document-level dataset and a sentence-level benchmark dataset BIBREF1. We can see that the proposed document-level text manipulation problem is more difficult than sentence-level, both in terms of the complexity of input records and the length of generated text.\n\nAnswer B:\nHow many records and references are there?\n< Explanation: The dataset consists of 1000 records and 1000 references. The records are randomly generated from the basketball game report corpus, and the references are generated from the corresponding reference text. The records and references are randomly distributed in the dataset. The dataset is divided into training, validation and test sets. The training set is 80%, the validation set is 10%, and the test set is 10%. The training set is used to train the model, the validation set is used to evaluate the model, and the test set is used to evaluate\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.11706",
    "question": "which had better results, the svm or the random forest model?",
    "gold_answer": "",
    "answer_a": "SVM. Explanation: Support vector machine trained on human labeled data performed 0.9572 while the same model trained on the data labeled by our method resulted 0.9931. Also, the Random forest model trained on human labeled data performed 0.9504 while the same model trained on the data labeled by our method did 0.9759.",
    "answer_b": "What is the difference between the two models?\n< Explanation: The results of the SVM and Random Forest models are shown in Table 1. The results show that the Random Forest model outperforms the SVM model by 2.68-3.75% of classification accuracy. The results of the two models are similar, but the Random Forest model outperforms the SVM model by 2.68-3.75% of classification accuracy. The results of the two models are similar, but the Random Forest model outperforms the SVM model by 2.68-3.75% of classification",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhich had better results, the svm or the random forest model?\n\nGround Truth Answer:\n\n\nAnswer A:\nSVM. Explanation: Support vector machine trained on human labeled data performed 0.9572 while the same model trained on the data labeled by our method resulted 0.9931. Also, the Random forest model trained on human labeled data performed 0.9504 while the same model trained on the data labeled by our method did 0.9759.\n\nAnswer B:\nWhat is the difference between the two models?\n< Explanation: The results of the SVM and Random Forest models are shown in Table 1. The results show that the Random Forest model outperforms the SVM model by 2.68-3.75% of classification accuracy. The results of the two models are similar, but the Random Forest model outperforms the SVM model by 2.68-3.75% of classification accuracy. The results of the two models are similar, but the Random Forest model outperforms the SVM model by 2.68-3.75% of classification\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.11706",
    "question": "which network community detection dataset was used?",
    "gold_answer": "",
    "answer_a": "Text data from Pypestream. Explanation: Gathered a set of text data that was used to develop a particular conversational intelligence(chatbot) system from an artificial intelligence company, Pypestream.",
    "answer_b": "Explanation: We use the following dataset for our experiments: (1) the dataset of the 2009-2010 news articles from the New York Times, (2) the dataset of the 2009-2010 news articles from the New York Times, (3) the dataset of the 2009-2010 news articles from the New York Times, (4) the dataset of the 2009-2010 news articles from the New York Times, (5) the dataset of the 2009-2010",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhich network community detection dataset was used?\n\nGround Truth Answer:\n\n\nAnswer A:\nText data from Pypestream. Explanation: Gathered a set of text data that was used to develop a particular conversational intelligence(chatbot) system from an artificial intelligence company, Pypestream.\n\nAnswer B:\nExplanation: We use the following dataset for our experiments: (1) the dataset of the 2009-2010 news articles from the New York Times, (2) the dataset of the 2009-2010 news articles from the New York Times, (3) the dataset of the 2009-2010 news articles from the New York Times, (4) the dataset of the 2009-2010 news articles from the New York Times, (5) the dataset of the 2009-2010\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.11706",
    "question": "how many classes are they classifying?",
    "gold_answer": "",
    "answer_a": "Explanation: The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass",
    "answer_b": "18. Explanation: The network constructed with no threshold in place was detected to have 18 distinct communities with three single node communities.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow many classes are they classifying?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass text classification problem. The proposed method is applied to the multiclass\n\nAnswer B:\n18. Explanation: The network constructed with no threshold in place was detected to have 18 distinct communities with three single node communities.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.00461",
    "question": "What metrics of gender bias amplification are used to demonstrate the effectiveness of this approach?",
    "gold_answer": "",
    "answer_a": "the bias score of a word $x$ considering its word embedding $h^{fair}(x)$ and two gender indicators (words man and woman). Explanation: As originally introduced by BIBREF1, we compute the bias score of a word $x$ considering its word embedding $h^{fair}(x)$ and two gender indicators (words man and woman). For example, the bias score of scientist is:\n\nIf the bias score during testing is greater than the one during training,\n\nthen the bias of man towards scientist has been amplified by the model while learning such representation, given training and testing datasets similarly distributed.",
    "answer_b": "What are the results?\n< Explanation: We evaluate the impact of our approach on the gender bias amplification in the automatic generation of articles news. We use the same dataset as in the previous work (BIBREF10) and we use the same model architecture (BIBREF11) to train the model. We use the same hyperparameters (BIBREF12) and we use the same evaluation metrics (BIBREF13). We use the same evaluation criteria (BIBREF14) and we use the same evaluation metrics (BIBREF15). We use the same evaluation criteria (BIB",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat metrics of gender bias amplification are used to demonstrate the effectiveness of this approach?\n\nGround Truth Answer:\n\n\nAnswer A:\nthe bias score of a word $x$ considering its word embedding $h^{fair}(x)$ and two gender indicators (words man and woman). Explanation: As originally introduced by BIBREF1, we compute the bias score of a word $x$ considering its word embedding $h^{fair}(x)$ and two gender indicators (words man and woman). For example, the bias score of scientist is:\n\nIf the bias score during testing is greater than the one during training,\n\nthen the bias of man towards scientist has been amplified by the model while learning such representation, given training and testing datasets similarly distributed.\n\nAnswer B:\nWhat are the results?\n< Explanation: We evaluate the impact of our approach on the gender bias amplification in the automatic generation of articles news. We use the same dataset as in the previous work (BIBREF10) and we use the same model architecture (BIBREF11) to train the model. We use the same hyperparameters (BIBREF12) and we use the same evaluation metrics (BIBREF13). We use the same evaluation criteria (BIBREF14) and we use the same evaluation metrics (BIBREF15). We use the same evaluation criteria (BIB\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.00461",
    "question": "How is representation learning decoupled from memory management in this architecture?",
    "gold_answer": "",
    "answer_a": "What is the ratio of gender types that is updated?\n< Explanation: The architecture is composed of two parts: a memory module and a language model. The memory module is responsible for storing the latent representation of the input text. The language model is responsible for generating the output text. The architecture is composed of two parts: a memory module and a language model. The memory module is responsible for storing the latent representation of the input text. The language model is responsible for generating the output text. The architecture is composed of two parts: a memory module and a language model. The memory module is responsible for storing the latent representation of the input text",
    "answer_b": "considers the notion of a Fair Region to update a subset of the trainable parameters of a Memory Network. Explanation: We introduce a novel architecture that considers the notion of a Fair Region to update a subset of the trainable parameters of a Memory Network.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is representation learning decoupled from memory management in this architecture?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the ratio of gender types that is updated?\n< Explanation: The architecture is composed of two parts: a memory module and a language model. The memory module is responsible for storing the latent representation of the input text. The language model is responsible for generating the output text. The architecture is composed of two parts: a memory module and a language model. The memory module is responsible for storing the latent representation of the input text. The language model is responsible for generating the output text. The architecture is composed of two parts: a memory module and a language model. The memory module is responsible for storing the latent representation of the input text\n\nAnswer B:\nconsiders the notion of a Fair Region to update a subset of the trainable parameters of a Memory Network. Explanation: We introduce a novel architecture that considers the notion of a Fair Region to update a subset of the trainable parameters of a Memory Network.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.07568",
    "question": "What method did the highest scoring team use?",
    "gold_answer": "",
    "answer_a": "CRF model that used morphosyntactic and distributional features, as well as word clusters based on these learned representations.. Explanation: Submission 3 BIBREF4 , ranked first, employ CRF as a learning model. In the feature engineering process they use morphosyntactic features, distributional ones as well as word clusters based on these learned representations.",
    "answer_b": "What was the score?\n< Explanation: The highest scoring team was the team of Nicolas BIBBET, who used the following methods: (1) a bag of words approach, (2) a bag of words with a word embedding, (3) a bag of words with a word embedding and a word embedding with a word embedding, (4) a bag of words with a word embedding and a word embedding with a word embedding and (5) a bag of words with a word embedding and a word embedding with a word embedding and a word embedding with a word embedding. The team achieved a F$_1$ score of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat method did the highest scoring team use?\n\nGround Truth Answer:\n\n\nAnswer A:\nCRF model that used morphosyntactic and distributional features, as well as word clusters based on these learned representations.. Explanation: Submission 3 BIBREF4 , ranked first, employ CRF as a learning model. In the feature engineering process they use morphosyntactic features, distributional ones as well as word clusters based on these learned representations.\n\nAnswer B:\nWhat was the score?\n< Explanation: The highest scoring team was the team of Nicolas BIBBET, who used the following methods: (1) a bag of words approach, (2) a bag of words with a word embedding, (3) a bag of words with a word embedding and a word embedding with a word embedding, (4) a bag of words with a word embedding and a word embedding with a word embedding and (5) a bag of words with a word embedding and a word embedding with a word embedding and a word embedding with a word embedding. The team achieved a F$_1$ score of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.07568",
    "question": "What descriptive statistics are provided about the data?",
    "gold_answer": "",
    "answer_a": "What are the inter-annotator agreement scores?\n<sep><xml:lang=\"en\">\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score",
    "answer_b": "the number of entities, unique entities in the training and test sets. Explanation: As shown in Figure 1, the training and the test set have a similar distribution in terms of named entity types. The training set contains 2,902 entities among 1,656 unique entities (i.e. 57,1%). The test set contains 3,660 entities among 2,264 unique entities (i.e. 61,8%). Only 15,7% of named entities are in both datasets (i.e. 307 named entities). Finally we notice that less than 2% of seen entities are ambiguous on the testset.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat descriptive statistics are provided about the data?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the inter-annotator agreement scores?\n<sep><xml:lang=\"en\">\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score>\n<score>0.99</score\n\nAnswer B:\nthe number of entities, unique entities in the training and test sets. Explanation: As shown in Figure 1, the training and the test set have a similar distribution in terms of named entity types. The training set contains 2,902 entities among 1,656 unique entities (i.e. 57,1%). The test set contains 3,660 entities among 2,264 unique entities (i.e. 61,8%). Only 15,7% of named entities are in both datasets (i.e. 307 named entities). Finally we notice that less than 2% of seen entities are ambiguous on the testset.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.07568",
    "question": "What was the level of inter-annotator agreement?",
    "gold_answer": "",
    "answer_a": "Average Cohen\u2019s Kappa score of inter-annotator agreement was 0.655. Explanation: FLOAT SELECTED: Table 2: Cohen\u2019s Kappa for the interannotator agreement. \u201cAnn\" stands for the annotator. The Table is symmetric.",
    "answer_b": "How was it measured?\n< Explanation: The inter-annotator agreement was measured by the Cohen's Kappa statistic. The Kappa statistic is a measure of agreement between two raters. It is calculated as the ratio of the sum of the products of the ranks of the two raters' agreement to the sum of the ranks of the two raters' disagreement. The Kappa statistic ranges from 0 to 1, with 1 indicating perfect agreement and 0 indicating perfect disagreement. The Kappa statistic is calculated as follows:\n\n$$Kappa = \\frac{TP \\times TN + FP \\times FN}{TP + FP",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat was the level of inter-annotator agreement?\n\nGround Truth Answer:\n\n\nAnswer A:\nAverage Cohen\u2019s Kappa score of inter-annotator agreement was 0.655. Explanation: FLOAT SELECTED: Table 2: Cohen\u2019s Kappa for the interannotator agreement. \u201cAnn\" stands for the annotator. The Table is symmetric.\n\nAnswer B:\nHow was it measured?\n< Explanation: The inter-annotator agreement was measured by the Cohen's Kappa statistic. The Kappa statistic is a measure of agreement between two raters. It is calculated as the ratio of the sum of the products of the ranks of the two raters' agreement to the sum of the ranks of the two raters' disagreement. The Kappa statistic ranges from 0 to 1, with 1 indicating perfect agreement and 0 indicating perfect disagreement. The Kappa statistic is calculated as follows:\n\n$$Kappa = \\frac{TP \\times TN + FP \\times FN}{TP + FP\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.07568",
    "question": "What questions were asked in the annotation process?",
    "gold_answer": "",
    "answer_a": "determine entities and annotate them based on the description that matched the type of entity. Explanation: In the framework of the challenge, we were required to first identify the entities occurring in the dataset and, then, annotate them with of the 13 possible types. Table TABREF12 provides a description for each type of entity that we made available both to the annotators and to the participants of the challenge.",
    "answer_b": "What were the necessary decisions made?\n< Explanation: The annotation process was divided into three phases: (1) the annotation of the entities, (2) the annotation of the types of entities, and (3) the annotation of the types of tweets. The annotation of the entities was done by the annotators in the first phase. The annotation of the types of entities was done by the annotators in the second phase. The annotation of the types of tweets was done by the annotators in the third phase. The annotation of the types of tweets was done by the annotators in the third phase. The annotation of the types of tweets",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat questions were asked in the annotation process?\n\nGround Truth Answer:\n\n\nAnswer A:\ndetermine entities and annotate them based on the description that matched the type of entity. Explanation: In the framework of the challenge, we were required to first identify the entities occurring in the dataset and, then, annotate them with of the 13 possible types. Table TABREF12 provides a description for each type of entity that we made available both to the annotators and to the participants of the challenge.\n\nAnswer B:\nWhat were the necessary decisions made?\n< Explanation: The annotation process was divided into three phases: (1) the annotation of the entities, (2) the annotation of the types of entities, and (3) the annotation of the types of tweets. The annotation of the entities was done by the annotators in the first phase. The annotation of the types of entities was done by the annotators in the second phase. The annotation of the types of tweets was done by the annotators in the third phase. The annotation of the types of tweets was done by the annotators in the third phase. The annotation of the types of tweets\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.07568",
    "question": "Why is NER for tweets more challenging as the number of entities increases?",
    "gold_answer": "",
    "answer_a": "What are the reasons?\nFor the challenge, we have 13 types of entities, which are the following: person, organization, location, date, time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time",
    "answer_b": "tweets contain informal text with multilingual content that becomes more difficult to classify when there are more options to choose from. Explanation: Named Entity Recognition (NER) is a fundamental step for most of the information extraction pipelines. Importantly, the terse and difficult text style of tweets presents serious challenges to NER systems, which are usually trained using more formal text sources such as newswire articles or Wikipedia entries that follow particular morpho-syntactic rules. As a result, off-the-self tools trained on such data perform poorly BIBREF0 . The problem becomes more intense as the number of entities to be identified increases, moving from the traditional setting of very few entities (persons, organization, time, location) to problems with more. Furthermore, most of the resources (e.g., software tools) and benchmarks for NER are for text written in English. As the multilingual content online increases, and English may not be anymore the lingua franca of the Web. Therefore, having resources and benchmarks in other languages is crucial for enabling information access worldwide.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhy is NER for tweets more challenging as the number of entities increases?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the reasons?\nFor the challenge, we have 13 types of entities, which are the following: person, organization, location, date, time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time, date and time\n\nAnswer B:\ntweets contain informal text with multilingual content that becomes more difficult to classify when there are more options to choose from. Explanation: Named Entity Recognition (NER) is a fundamental step for most of the information extraction pipelines. Importantly, the terse and difficult text style of tweets presents serious challenges to NER systems, which are usually trained using more formal text sources such as newswire articles or Wikipedia entries that follow particular morpho-syntactic rules. As a result, off-the-self tools trained on such data perform poorly BIBREF0 . The problem becomes more intense as the number of entities to be identified increases, moving from the traditional setting of very few entities (persons, organization, time, location) to problems with more. Furthermore, most of the resources (e.g., software tools) and benchmarks for NER are for text written in English. As the multilingual content online increases, and English may not be anymore the lingua franca of the Web. Therefore, having resources and benchmarks in other languages is crucial for enabling information access worldwide.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1707.07568",
    "question": "What data preparation steps were used to construct the dataset?",
    "gold_answer": "",
    "answer_a": "The tweets were gathered using Twitter API plus tweets provided by the French National Railway Corporation; the tweets were split into training and test sets, and then annotated.. Explanation: Complementary to the collection of tweets using the Twitter API, we used 886 tweets provided by the \u201cSoci\u00e9t\u00e9 Nationale des Chemins de fer Fran\u00e7ais\u201d (SNCF), that is the French National Railway Corporation.",
    "answer_b": "What was the annotation process?\n< Explanation: The data preparation steps we followed for constructing the dataset are as follows: (1) we downloaded the tweets from the Twitter API (Twitter API v2.0) and (2) we removed the tweets containing the word \"France\" or \"France\" followed by a space. We also removed the tweets containing the word \"France\" or \"France\" followed by a comma. We removed the tweets containing the word \"France\" or \"France\" followed by a colon. We removed the tweets containing the word \"France\" or \"France\" followed by a semicolon. We removed the tweets",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat data preparation steps were used to construct the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe tweets were gathered using Twitter API plus tweets provided by the French National Railway Corporation; the tweets were split into training and test sets, and then annotated.. Explanation: Complementary to the collection of tweets using the Twitter API, we used 886 tweets provided by the \u201cSoci\u00e9t\u00e9 Nationale des Chemins de fer Fran\u00e7ais\u201d (SNCF), that is the French National Railway Corporation.\n\nAnswer B:\nWhat was the annotation process?\n< Explanation: The data preparation steps we followed for constructing the dataset are as follows: (1) we downloaded the tweets from the Twitter API (Twitter API v2.0) and (2) we removed the tweets containing the word \"France\" or \"France\" followed by a space. We also removed the tweets containing the word \"France\" or \"France\" followed by a comma. We removed the tweets containing the word \"France\" or \"France\" followed by a colon. We removed the tweets containing the word \"France\" or \"France\" followed by a semicolon. We removed the tweets\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.00202",
    "question": "What is the training objective in the method introduced in this paper?",
    "gold_answer": "",
    "answer_a": "How is the fine-tuning process different from the one in the previous work?\n< Explanation: The training objective in the method introduced in this paper is to minimize the mean cross-entropy loss of the fine-tuned model on the target domain. The fine-tuning process is different from the one in the previous work in that the fine-tuning process is not limited to the target domain. Instead, the fine-tuning process is limited to the source domain. The source domain is used to train the fine-tuned model, and the fine-tuned model is used to predict the target domain. The source domain is used to fine-tune",
    "answer_b": "we explore strategies to reduce forgetting for comprehension systems during domain adaption. Our goal is to preserve the source domain's performance as much as possible, while keeping target domain's performance optimal and assuming no access to the source data.. Explanation: In this paper, we explore strategies to reduce forgetting for comprehension systems during domain adaption. Our goal is to preserve the source domain's performance as much as possible, while keeping target domain's performance optimal and assuming no access to the source data. We experiment with a number of auxiliary penalty terms to regularise the fine-tuning process for three modern RC models: QANet BIBREF10, decaNLP BIBREF11 and BERT BIBREF12. We observe that combining different auxiliary penalty terms results in the best performance, outperforming benchmark methods that require source data.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the training objective in the method introduced in this paper?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is the fine-tuning process different from the one in the previous work?\n< Explanation: The training objective in the method introduced in this paper is to minimize the mean cross-entropy loss of the fine-tuned model on the target domain. The fine-tuning process is different from the one in the previous work in that the fine-tuning process is not limited to the target domain. Instead, the fine-tuning process is limited to the source domain. The source domain is used to train the fine-tuned model, and the fine-tuned model is used to predict the target domain. The source domain is used to fine-tune\n\nAnswer B:\nwe explore strategies to reduce forgetting for comprehension systems during domain adaption. Our goal is to preserve the source domain's performance as much as possible, while keeping target domain's performance optimal and assuming no access to the source data.. Explanation: In this paper, we explore strategies to reduce forgetting for comprehension systems during domain adaption. Our goal is to preserve the source domain's performance as much as possible, while keeping target domain's performance optimal and assuming no access to the source data. We experiment with a number of auxiliary penalty terms to regularise the fine-tuning process for three modern RC models: QANet BIBREF10, decaNLP BIBREF11 and BERT BIBREF12. We observe that combining different auxiliary penalty terms results in the best performance, outperforming benchmark methods that require source data.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2001.04346",
    "question": "What kind of baseline model do they compare against?",
    "gold_answer": "",
    "answer_a": "What are the results?\n< Explanation: We compare our model with the following baselines: (1) a simple bag-of-words model that learns to extract the most salient contents of a review with respect to the target item, (2) a simple bag-of-words model that learns to extract the most salient contents of a review with respect to the target item, (3) a simple bag-of-words model that learns to extract the most salient contents of a review with respect to the target item, (4) a simple bag-of-words model that learns to extract the most salient contents of a review with",
    "answer_b": "Factorization Machines (FM) BIBREF5, SVD BIBREF0, Probabilistic Matrix Factorization (PMF) BIBREF24, Nonnegative Matrix Factorization (NMF) BIBREF25, DeepCoNN BIBREF1, D-ATT BIBREF3, MPCN BIBREF17, and HUITA BIBREF16. Explanation: We compare our model with both conventional approaches and state-of-the-art approaches, including Factorization Machines (FM) BIBREF5, SVD BIBREF0, Probabilistic Matrix Factorization (PMF) BIBREF24, Nonnegative Matrix Factorization (NMF) BIBREF25, DeepCoNN BIBREF1, D-ATT BIBREF3, MPCN BIBREF17, and HUITA BIBREF16.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat kind of baseline model do they compare against?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n< Explanation: We compare our model with the following baselines: (1) a simple bag-of-words model that learns to extract the most salient contents of a review with respect to the target item, (2) a simple bag-of-words model that learns to extract the most salient contents of a review with respect to the target item, (3) a simple bag-of-words model that learns to extract the most salient contents of a review with respect to the target item, (4) a simple bag-of-words model that learns to extract the most salient contents of a review with\n\nAnswer B:\nFactorization Machines (FM) BIBREF5, SVD BIBREF0, Probabilistic Matrix Factorization (PMF) BIBREF24, Nonnegative Matrix Factorization (NMF) BIBREF25, DeepCoNN BIBREF1, D-ATT BIBREF3, MPCN BIBREF17, and HUITA BIBREF16. Explanation: We compare our model with both conventional approaches and state-of-the-art approaches, including Factorization Machines (FM) BIBREF5, SVD BIBREF0, Probabilistic Matrix Factorization (PMF) BIBREF24, Nonnegative Matrix Factorization (NMF) BIBREF25, DeepCoNN BIBREF1, D-ATT BIBREF3, MPCN BIBREF17, and HUITA BIBREF16.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2001.04346",
    "question": "Which set of datasets do they use?",
    "gold_answer": "",
    "answer_a": "9 Amazon product review datasets for 9 different domains. Explanation: We conducted experiments on 10 different datasets, including 9 Amazon product review datasets for 9 different domains, and the large-scale Yelp challenge dataset on restaurant reviews.",
    "answer_b": "How do they evaluate the performance of their model?\n< Explanation: We use the following datasets: (1) Amazon reviews dataset (Amazon) BIBREF10 , which contains 100,000 reviews of 100,000 items. (2) Yelp reviews dataset (Yelp) BIBREF11 , which contains 100,000 reviews of 100,000 items. (3) IMDB reviews dataset (IMDB) BIBREF12 , which contains 100,000 reviews of 100,",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich set of datasets do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\n9 Amazon product review datasets for 9 different domains. Explanation: We conducted experiments on 10 different datasets, including 9 Amazon product review datasets for 9 different domains, and the large-scale Yelp challenge dataset on restaurant reviews.\n\nAnswer B:\nHow do they evaluate the performance of their model?\n< Explanation: We use the following datasets: (1) Amazon reviews dataset (Amazon) BIBREF10 , which contains 100,000 reviews of 100,000 items. (2) Yelp reviews dataset (Yelp) BIBREF11 , which contains 100,000 reviews of 100,000 items. (3) IMDB reviews dataset (IMDB) BIBREF12 , which contains 100,000 reviews of 100,\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2001.11316",
    "question": "How long is the dataset?",
    "gold_answer": "",
    "answer_a": "SemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences. Explanation: A summary of these datasets can be seen in Tables TABREF8 and TABREF8.",
    "answer_b": "What is the size of the dataset?\n< Explanation: The dataset consists of 10000 sentences, each of which is labeled with one of the following sentiment labels: positive, negative, or neutral. The dataset is split into training and validation sets. The training set consists of 7000 sentences, and the validation set consists of 3000 sentences. The training set is used to train the BERT model, and the validation set is used to evaluate the model. The dataset is available at https://github.com/abhishek1010/Aspect-Based_Sentiment_Analysis.\n\nThe",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long is the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nSemEval 2016 contains 6521 sentences, SemEval 2014 contains 7673 sentences. Explanation: A summary of these datasets can be seen in Tables TABREF8 and TABREF8.\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: The dataset consists of 10000 sentences, each of which is labeled with one of the following sentiment labels: positive, negative, or neutral. The dataset is split into training and validation sets. The training set consists of 7000 sentences, and the validation set consists of 3000 sentences. The training set is used to train the BERT model, and the validation set is used to evaluate the model. The dataset is available at https://github.com/abhishek1010/Aspect-Based_Sentiment_Analysis.\n\nThe\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2001.11316",
    "question": "How are adversarial examples generated?",
    "gold_answer": "",
    "answer_a": "What is the difference between the two methods?\n< Explanation: The adversarial examples are generated by taking the negative of the original examples. The original examples are used to train the model and the negative examples are used to generate adversarial examples. The negative examples are generated by taking the negative of the original examples. The negative examples are generated by taking the negative of the original examples. The original examples are used to train the model and the negative examples are used to generate adversarial examples. The negative examples are generated by taking the negative of the original examples. The negative examples are generated by taking the negative of the original examples. The original examples",
    "answer_b": "we are searching for the worst perturbations while trying to minimize the loss of the model. Explanation: In this work, we utilize a white-box method working on the embedding level. In order to create adversarial examples, we utilize the formula used by BIBREF16, where the perturbations are created using gradient of the loss function. Assuming $p(y|x;\\theta )$ is the probability of label $y$ given the input $x$ and the model parameters $\\theta $, in order to find the adversarial examples the following minimization problem should be solved:\n\nwhere $r$ denotes the perturbations on the input and $\\hat{\\theta }$ is a constant copy of $\\theta $ in order not to allow the gradients to propagate in the process of constructing the artificial examples. Solving the above minimization problem means that we are searching for the worst perturbations while trying to minimize the loss of the model. An approximate solution for Equation DISPLAY_FORM3 is found by linearizing $\\log p(y|x;\\theta )$ around $x$ BIBREF0. Therefore, the following perturbations are added to the input embeddings to create new adversarial sentences in the embedding space.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are adversarial examples generated?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the two methods?\n< Explanation: The adversarial examples are generated by taking the negative of the original examples. The original examples are used to train the model and the negative examples are used to generate adversarial examples. The negative examples are generated by taking the negative of the original examples. The negative examples are generated by taking the negative of the original examples. The original examples are used to train the model and the negative examples are used to generate adversarial examples. The negative examples are generated by taking the negative of the original examples. The negative examples are generated by taking the negative of the original examples. The original examples\n\nAnswer B:\nwe are searching for the worst perturbations while trying to minimize the loss of the model. Explanation: In this work, we utilize a white-box method working on the embedding level. In order to create adversarial examples, we utilize the formula used by BIBREF16, where the perturbations are created using gradient of the loss function. Assuming $p(y|x;\\theta )$ is the probability of label $y$ given the input $x$ and the model parameters $\\theta $, in order to find the adversarial examples the following minimization problem should be solved:\n\nwhere $r$ denotes the perturbations on the input and $\\hat{\\theta }$ is a constant copy of $\\theta $ in order not to allow the gradients to propagate in the process of constructing the artificial examples. Solving the above minimization problem means that we are searching for the worst perturbations while trying to minimize the loss of the model. An approximate solution for Equation DISPLAY_FORM3 is found by linearizing $\\log p(y|x;\\theta )$ around $x$ BIBREF0. Therefore, the following perturbations are added to the input embeddings to create new adversarial sentences in the embedding space.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2001.11316",
    "question": "What are the modifications made to post-trained BERT?",
    "gold_answer": "",
    "answer_a": "How is adversarial training applied?\n< Explanation: We first train BERT on the pre-trained BERT-PT model on the Stanford Sentiment Treebank (SST) dataset. We then use the pre-trained BERT model as the base model for our adversarial training. We train the BERT model with the following hyperparameters: batch size = 16, learning rate = 1e-5, number of epochs = 100, and the number of training examples = 100000. We use the Adam optimizer with a learning rate of 1e-5. We use the cross",
    "answer_b": "adversarial examples from BERT embeddings using the gradient of the loss. Explanation: As can be seen, we create adversarial examples from BERT embeddings using the gradient of the loss. Then, we feed the perturbed examples to the BERT encoder to calculate the adversarial loss. In the end, the backpropagation algorithm is applied to the sum of both losses.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the modifications made to post-trained BERT?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is adversarial training applied?\n< Explanation: We first train BERT on the pre-trained BERT-PT model on the Stanford Sentiment Treebank (SST) dataset. We then use the pre-trained BERT model as the base model for our adversarial training. We train the BERT model with the following hyperparameters: batch size = 16, learning rate = 1e-5, number of epochs = 100, and the number of training examples = 100000. We use the Adam optimizer with a learning rate of 1e-5. We use the cross\n\nAnswer B:\nadversarial examples from BERT embeddings using the gradient of the loss. Explanation: As can be seen, we create adversarial examples from BERT embeddings using the gradient of the loss. Then, we feed the perturbed examples to the BERT encoder to calculate the adversarial loss. In the end, the backpropagation algorithm is applied to the sum of both losses.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.07149",
    "question": "What datasets are used?",
    "gold_answer": "",
    "answer_a": "How are they used?\n< Explanation: We use the following datasets: (1) the Open Domain Corpus (ODC) BIBREF1 , which is a collection of 10,000 utterances from 10,000 users, and (2) the Open Domain Corpus with a Fewer Users (ODC-FM) BIBREF2 , which is a collection of 10,000 utterances from 10,000 users, but with a smaller number of users. The ODC-FM dataset is used to train the RNN-based reply generator, while",
    "answer_b": "They create their own datasets from online text.. Explanation: To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, such as Sina Weibo, Baidu Zhidao, and Baidu Tieba.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they used?\n< Explanation: We use the following datasets: (1) the Open Domain Corpus (ODC) BIBREF1 , which is a collection of 10,000 utterances from 10,000 users, and (2) the Open Domain Corpus with a Fewer Users (ODC-FM) BIBREF2 , which is a collection of 10,000 utterances from 10,000 users, but with a smaller number of users. The ODC-FM dataset is used to train the RNN-based reply generator, while\n\nAnswer B:\nThey create their own datasets from online text.. Explanation: To construct a database for information retrieval, we collected human-human utterances from massive online forums, microblogs, and question-answering communities, such as Sina Weibo, Baidu Zhidao, and Baidu Tieba.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1901.03253",
    "question": "Where can I access the dataset?",
    "gold_answer": "",
    "answer_a": "Explanation: The dataset is available at https://github.com/alexander-lee/HS-2019-01-01-Satirical-Headlines. The dataset is a subset of the original dataset, and is available at https://github.com/alexander-lee/HS-2019-01-01-Satirical-Headlines. The dataset is a subset of the original dataset, and is available at https://github.com/alexander-lee/HS-2019-01-01-Satirical-Headlines. The dataset is a subset of",
    "answer_b": "BIBREF9. Explanation: We make the data collected via Unfun.me, as well as our code for analyzing it, publicly available online BIBREF9 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhere can I access the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The dataset is available at https://github.com/alexander-lee/HS-2019-01-01-Satirical-Headlines. The dataset is a subset of the original dataset, and is available at https://github.com/alexander-lee/HS-2019-01-01-Satirical-Headlines. The dataset is a subset of the original dataset, and is available at https://github.com/alexander-lee/HS-2019-01-01-Satirical-Headlines. The dataset is a subset of\n\nAnswer B:\nBIBREF9. Explanation: We make the data collected via Unfun.me, as well as our code for analyzing it, publicly available online BIBREF9 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.10564",
    "question": "What sources did they get the data from?",
    "gold_answer": "",
    "answer_a": "online public-domain sources, private sources and actual books. Explanation: FLOAT SELECTED: Table 2: Data sources, prevalence and category of text",
    "answer_b": "How did they do this?\n< Explanation: We collected data from the web and personal libraries of Yor\\`ub\\'a speakers. We used the following sources: (1) Wikipedia, (2) Wikipedia articles, (3) Wikipedia articles with a link to a Wikipedia article, (4) Wikipedia articles with a link to a Wikipedia article with a link to a Wikipedia article, (5) Wikipedia articles with a link to a Wikipedia article with a link to a Wikipedia article with a link to a Wikipedia article, (6) Wikipedia articles with a link to a Wikipedia article with a link to a Wikipedia article with a link to",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat sources did they get the data from?\n\nGround Truth Answer:\n\n\nAnswer A:\nonline public-domain sources, private sources and actual books. Explanation: FLOAT SELECTED: Table 2: Data sources, prevalence and category of text\n\nAnswer B:\nHow did they do this?\n< Explanation: We collected data from the web and personal libraries of Yor\\`ub\\'a speakers. We used the following sources: (1) Wikipedia, (2) Wikipedia articles, (3) Wikipedia articles with a link to a Wikipedia article, (4) Wikipedia articles with a link to a Wikipedia article with a link to a Wikipedia article, (5) Wikipedia articles with a link to a Wikipedia article with a link to a Wikipedia article with a link to a Wikipedia article, (6) Wikipedia articles with a link to a Wikipedia article with a link to a Wikipedia article with a link to\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1902.10246",
    "question": "How much lower is the computational cost of the proposed model?",
    "gold_answer": "",
    "answer_a": "What is the computational cost of the state-of-the-art approach?\n< Explanation: We compare our proposed method with the state-of-the-art approach (SOTA) in terms of computational cost. The SOTA model is trained on the Penn Treebank (PTB) data set, which has 10,000 instances. The SOTA model is trained using a standard FFNN with 100 hidden units and 1000 epochs. The proposed model is trained using a FOFE-based FFNN with 100 hidden units and 1000 epochs. The proposed model is trained using a standard",
    "answer_b": "BIBREF9 took approximately 4.5 months even after applying optimization of trimming sentences, while the training process of our FOFE-based model took around 3 days. Explanation: The training process of the LSTM model by BIBREF9 took approximately 4.5 months even after applying optimization of trimming sentences, while the training process of our FOFE-based model took around 3 days to produce the claimed results.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much lower is the computational cost of the proposed model?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the computational cost of the state-of-the-art approach?\n< Explanation: We compare our proposed method with the state-of-the-art approach (SOTA) in terms of computational cost. The SOTA model is trained on the Penn Treebank (PTB) data set, which has 10,000 instances. The SOTA model is trained using a standard FFNN with 100 hidden units and 1000 epochs. The proposed model is trained using a FOFE-based FFNN with 100 hidden units and 1000 epochs. The proposed model is trained using a standard\n\nAnswer B:\nBIBREF9 took approximately 4.5 months even after applying optimization of trimming sentences, while the training process of our FOFE-based model took around 3 days. Explanation: The training process of the LSTM model by BIBREF9 took approximately 4.5 months even after applying optimization of trimming sentences, while the training process of our FOFE-based model took around 3 days to produce the claimed results.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1902.10246",
    "question": "What is the state-of-the-art model?",
    "gold_answer": "",
    "answer_a": "BIBREF4. Explanation: Recently, BIBREF4 improved the performance by pre-training a LSTM language model with a large unlabelled corpus, and using this model to generate sense vectors for further WSD predictions.",
    "answer_b": "What is the difference between the proposed method and the state-of-the-art model?\n< Explanation: The state-of-the-art model is the one proposed by BIBREF10, which is a simple feed-forward neural network (FFNN) with a single hidden layer. The FFNN is trained using the word embedding and the word sense disambiguation (WSD) dataset. The FFNN is trained using the word embedding and the word sense disambiguation (WSD) dataset. The FFNN is trained using the word embedding and the word sense disambiguation (WSD) dataset. The FFNN is trained",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the state-of-the-art model?\n\nGround Truth Answer:\n\n\nAnswer A:\nBIBREF4. Explanation: Recently, BIBREF4 improved the performance by pre-training a LSTM language model with a large unlabelled corpus, and using this model to generate sense vectors for further WSD predictions.\n\nAnswer B:\nWhat is the difference between the proposed method and the state-of-the-art model?\n< Explanation: The state-of-the-art model is the one proposed by BIBREF10, which is a simple feed-forward neural network (FFNN) with a single hidden layer. The FFNN is trained using the word embedding and the word sense disambiguation (WSD) dataset. The FFNN is trained using the word embedding and the word sense disambiguation (WSD) dataset. The FFNN is trained using the word embedding and the word sense disambiguation (WSD) dataset. The FFNN is trained\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1902.10246",
    "question": "What is a pseudo language model?",
    "gold_answer": "",
    "answer_a": "different from language model in terms of that the sense prediction of a target word depends on its surrounding sequence rather than only preceding sequence. Explanation: A language model represents the probability distribution of a given sequence of words, and it is commonly used in predicting the subsequent word given preceding sequence. BIBREF5 proposed a FOFE-based neural network language model by feeding FOFE code of preceding sequence into FFNN. WSD is different from language model in terms of that the sense prediction of a target word depends on its surrounding sequence rather than only preceding sequence. Hence, we build a pseudo language model that uses both preceding and succeeding sequence to accommodate the purpose of WSD tasks.",
    "answer_b": "How is it used?\n< Explanation: A pseudo language model is a model that is trained to abstract the surrounding context of polyseme instances in labelled corpus into context embeddings. The model is trained to learn the context embeddings of polyseme instances in labelled corpus, and then the learned context embeddings are used to abstract the surrounding context of polyseme instances in unlabelled corpus. The pseudo language model is trained to learn the context embeddings of polyseme instances in labelled corpus, and then the learned context embeddings are used to abstract the surrounding context of polyseme instances in unlabelled corpus. The pseudo language model is trained to",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is a pseudo language model?\n\nGround Truth Answer:\n\n\nAnswer A:\ndifferent from language model in terms of that the sense prediction of a target word depends on its surrounding sequence rather than only preceding sequence. Explanation: A language model represents the probability distribution of a given sequence of words, and it is commonly used in predicting the subsequent word given preceding sequence. BIBREF5 proposed a FOFE-based neural network language model by feeding FOFE code of preceding sequence into FFNN. WSD is different from language model in terms of that the sense prediction of a target word depends on its surrounding sequence rather than only preceding sequence. Hence, we build a pseudo language model that uses both preceding and succeeding sequence to accommodate the purpose of WSD tasks.\n\nAnswer B:\nHow is it used?\n< Explanation: A pseudo language model is a model that is trained to abstract the surrounding context of polyseme instances in labelled corpus into context embeddings. The model is trained to learn the context embeddings of polyseme instances in labelled corpus, and then the learned context embeddings are used to abstract the surrounding context of polyseme instances in unlabelled corpus. The pseudo language model is trained to learn the context embeddings of polyseme instances in labelled corpus, and then the learned context embeddings are used to abstract the surrounding context of polyseme instances in unlabelled corpus. The pseudo language model is trained to\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1706.02222",
    "question": "How significant is the performance compared to LSTM model?",
    "gold_answer": "",
    "answer_a": "What is the baseline model?\n< Explanation: We conducted experiments with our proposed models on word-level and character-level language modeling tasks and revealed that our proposed models significantly improved their performance compared to our baseline models. We compared our proposed models with the LSTM model and the GRU model. The LSTM model was trained with the same parameters as our proposed models. The GRU model was trained with the same parameters as our proposed models. The results of our proposed models are shown in Table 1. The results of our proposed models are better than the baseline models. The results of our proposed models are better than the baseline models. The results of our",
    "answer_b": "0.03 absolute / 2.22% relative BPC. Explanation: GRURNTN reduced the BPC from 1.39 to 1.33 (0.06 absolute / 4.32% relative BPC) from the baseline GRURNN, and LSTMRNTN reduced the BPC from 1.37 to 1.34 (0.03 absolute / 2.22% relative BPC) from the baseline LSTMRNN.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow significant is the performance compared to LSTM model?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the baseline model?\n< Explanation: We conducted experiments with our proposed models on word-level and character-level language modeling tasks and revealed that our proposed models significantly improved their performance compared to our baseline models. We compared our proposed models with the LSTM model and the GRU model. The LSTM model was trained with the same parameters as our proposed models. The GRU model was trained with the same parameters as our proposed models. The results of our proposed models are shown in Table 1. The results of our proposed models are better than the baseline models. The results of our proposed models are better than the baseline models. The results of our\n\nAnswer B:\n0.03 absolute / 2.22% relative BPC. Explanation: GRURNTN reduced the BPC from 1.39 to 1.33 (0.06 absolute / 4.32% relative BPC) from the baseline GRURNN, and LSTMRNTN reduced the BPC from 1.37 to 1.34 (0.03 absolute / 2.22% relative BPC) from the baseline LSTMRNN.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1706.02222",
    "question": "How does the introduced model combine the both factors?",
    "gold_answer": "",
    "answer_a": "in this paper we proposed a Gated Recurrent Neural Tensor Network (GRURNTN) to combine these two advantages into an RNN architecture. In this architecture, the tensor product operation is applied between the current input and previous hidden layer multiplied by the reset gates for calculating the current candidate hidden layer values.. Explanation: in this paper we proposed a Gated Recurrent Neural Tensor Network (GRURNTN) to combine these two advantages into an RNN architecture. In this architecture, the tensor product operation is applied between the current input and previous hidden layer multiplied by the reset gates for calculating the current candidate hidden layer values.",
    "answer_b": "What are the differences between the proposed models and the baseline models?\n< Explanation: The proposed models are made by combining the LSTM and GRU RNN models with the tensor product. The LSTM RNN model is used to model the long-term dependencies in the dataset and the GRU RNN model is used to model the short-term dependencies. The tensor product is used to combine the two models. The proposed models are made by combining the LSTM and GRU RNN models with the tensor product. The LSTM RNN model is used to model the long-term dependencies in the dataset and the GRU RNN model is used to model the short",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does the introduced model combine the both factors?\n\nGround Truth Answer:\n\n\nAnswer A:\nin this paper we proposed a Gated Recurrent Neural Tensor Network (GRURNTN) to combine these two advantages into an RNN architecture. In this architecture, the tensor product operation is applied between the current input and previous hidden layer multiplied by the reset gates for calculating the current candidate hidden layer values.. Explanation: in this paper we proposed a Gated Recurrent Neural Tensor Network (GRURNTN) to combine these two advantages into an RNN architecture. In this architecture, the tensor product operation is applied between the current input and previous hidden layer multiplied by the reset gates for calculating the current candidate hidden layer values.\n\nAnswer B:\nWhat are the differences between the proposed models and the baseline models?\n< Explanation: The proposed models are made by combining the LSTM and GRU RNN models with the tensor product. The LSTM RNN model is used to model the long-term dependencies in the dataset and the GRU RNN model is used to model the short-term dependencies. The tensor product is used to combine the two models. The proposed models are made by combining the LSTM and GRU RNN models with the tensor product. The LSTM RNN model is used to model the long-term dependencies in the dataset and the GRU RNN model is used to model the short\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1706.02222",
    "question": "How much improvement do the introduced model achieve compared to the previous models?",
    "gold_answer": "",
    "answer_a": "What are the baselines?\n< Explanation: Table 1 shows the results of our proposed models on word-level and character-level language modeling tasks. We can see that our proposed models significantly outperform the baseline models. The results of our proposed models are better than the baseline models by 0.000000000000000000000000000000000000000000000000000000000000000000000",
    "answer_b": "we report our experiment results on PTB character-level language modeling using our baseline models GRURNN and LSTMRNN as well as our proposed models GRURNTN and LSTMRNTN.. Explanation: we report our experiment results on PTB character-level language modeling using our baseline models GRURNN and LSTMRNN as well as our proposed models GRURNTN and LSTMRNTN.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much improvement do the introduced model achieve compared to the previous models?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the baselines?\n< Explanation: Table 1 shows the results of our proposed models on word-level and character-level language modeling tasks. We can see that our proposed models significantly outperform the baseline models. The results of our proposed models are better than the baseline models by 0.000000000000000000000000000000000000000000000000000000000000000000000\n\nAnswer B:\nwe report our experiment results on PTB character-level language modeling using our baseline models GRURNN and LSTMRNN as well as our proposed models GRURNTN and LSTMRNTN.. Explanation: we report our experiment results on PTB character-level language modeling using our baseline models GRURNN and LSTMRNN as well as our proposed models GRURNTN and LSTMRNTN.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.00982",
    "question": "what is the architecture of their model?",
    "gold_answer": "",
    "answer_a": "bidirectional LSTM. Explanation: The model encodes the entity using a bidirectional LSTM (BiLSTM), which is then used to initialize a BiLSTM that encodes the article and produces a prediction.",
    "answer_b": "Explanation: The model is a convolutional neural network (CNN) with a 360-degree view of the news. The CNN is trained to predict the 360-degree view of the news, and the output is a 360-degree view of the news. The CNN is trained on a dataset of 1000 news articles, and the CNN is trained on a dataset of 1000 news articles. The CNN is trained on a dataset of 1000 news articles, and the CNN is trained on a dataset of 1000 news articles. The CNN is trained on",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the architecture of their model?\n\nGround Truth Answer:\n\n\nAnswer A:\nbidirectional LSTM. Explanation: The model encodes the entity using a bidirectional LSTM (BiLSTM), which is then used to initialize a BiLSTM that encodes the article and produces a prediction.\n\nAnswer B:\nExplanation: The model is a convolutional neural network (CNN) with a 360-degree view of the news. The CNN is trained to predict the 360-degree view of the news, and the output is a 360-degree view of the news. The CNN is trained on a dataset of 1000 news articles, and the CNN is trained on a dataset of 1000 news articles. The CNN is trained on a dataset of 1000 news articles, and the CNN is trained on a dataset of 1000 news articles. The CNN is trained on\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.00982",
    "question": "what dataset did they use for this tool?",
    "gold_answer": "",
    "answer_a": "They collect data using the AYLIEN News API,  which provides search capabilities for news articles enriched with extracted entities and other metadata and take a step to compile a curated list of topics. The final dataset consists of 32,227 pairs of news articles and topics annotated with their stance.. Explanation: We collect data using the AYLIEN News API, which provides search capabilities for news articles enriched with extracted entities and other metadata. As most extracted entities have a neutral stance or might not be of interest to users, we take steps to compile a curated list of topics, which we detail in the following.",
    "answer_b": "(if the answer is \"the 2016 US presidential election data\", explain why)\nThe dataset used for this tool is the 2016 US presidential election data. The dataset contains 1000 news articles from the 2016 US presidential election. The dataset is available at https://www.kaggle.com/c/2016-us-election-polling-data. The dataset is divided into 1000 news articles, each containing 1000 words. The dataset is available at https://www.kaggle.com/c/2016-us-election-poll",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat dataset did they use for this tool?\n\nGround Truth Answer:\n\n\nAnswer A:\nThey collect data using the AYLIEN News API,  which provides search capabilities for news articles enriched with extracted entities and other metadata and take a step to compile a curated list of topics. The final dataset consists of 32,227 pairs of news articles and topics annotated with their stance.. Explanation: We collect data using the AYLIEN News API, which provides search capabilities for news articles enriched with extracted entities and other metadata. As most extracted entities have a neutral stance or might not be of interest to users, we take steps to compile a curated list of topics, which we detail in the following.\n\nAnswer B:\n(if the answer is \"the 2016 US presidential election data\", explain why)\nThe dataset used for this tool is the 2016 US presidential election data. The dataset contains 1000 news articles from the 2016 US presidential election. The dataset is available at https://www.kaggle.com/c/2016-us-election-polling-data. The dataset is divided into 1000 news articles, each containing 1000 words. The dataset is available at https://www.kaggle.com/c/2016-us-election-poll\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1809.06083",
    "question": "How large is the dataset?",
    "gold_answer": "",
    "answer_a": "How is it constructed?\n< Explanation: The dataset is constructed from the training corpus of English and German news articles. The training corpus is a combination of 10000 English and 10000 German news articles. The training corpus is split into 10000 training documents and 10000 test documents. The training documents are used to train the model and the test documents are used to evaluate the model. The training corpus is then used to build the training set for the model. The training set is then used to build the test set for the model. The test set is then used",
    "answer_b": "70287. Explanation: FLOAT SELECTED: Table 1: News articles",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow large is the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it constructed?\n< Explanation: The dataset is constructed from the training corpus of English and German news articles. The training corpus is a combination of 10000 English and 10000 German news articles. The training corpus is split into 10000 training documents and 10000 test documents. The training documents are used to train the model and the test documents are used to evaluate the model. The training corpus is then used to build the training set for the model. The training set is then used to build the test set for the model. The test set is then used\n\nAnswer B:\n70287. Explanation: FLOAT SELECTED: Table 1: News articles\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.04005",
    "question": "what is the average number of speakers in the dataset?",
    "gold_answer": "",
    "answer_a": "How many speakers are there in the dataset?\n< Explanation: The dataset consists of 1000 conversations, each with 100 messages. The average number of speakers is 10. The dataset is divided into 1000 conversations, each with 100 messages. The number of messages in each conversation is 100. The number of speakers in each conversation is 10. The number of messages in each conversation is 100. The number of speakers in each conversation is 10. The number of messages in each conversation is 100. The number of speakers",
    "answer_b": "26.8. Explanation: FLOAT SELECTED: Table 3: Data Statistics. \u201cAdrMention Freq\u201d is the frequency of explicit addressee mention.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the average number of speakers in the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow many speakers are there in the dataset?\n< Explanation: The dataset consists of 1000 conversations, each with 100 messages. The average number of speakers is 10. The dataset is divided into 1000 conversations, each with 100 messages. The number of messages in each conversation is 100. The number of speakers in each conversation is 10. The number of messages in each conversation is 100. The number of speakers in each conversation is 10. The number of messages in each conversation is 100. The number of speakers\n\nAnswer B:\n26.8. Explanation: FLOAT SELECTED: Table 3: Data Statistics. \u201cAdrMention Freq\u201d is the frequency of explicit addressee mention.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.04005",
    "question": "by how much is accuracy improved?",
    "gold_answer": "",
    "answer_a": "What is the difference between the previous state-of-the-art system and SI-RNN?\n< Explanation: We compare our model with the previous state-of-the-art system (SIR) and the state-of-the-art system (SOS) in the multi-party conversation setting. The previous state-of-the-art system is a simple encoder-decoder model that updates speaker embeddings only for the sender. The state-of-the-art system is a multi-layered encoder-decoder model that updates speaker embeddings in a role-sensitive way. We compare our model with the previous state-of-the-art system and the state-of-the-art system in the multi-party conversation setting.",
    "answer_b": "In addressee selection around 12% in RES-CAND = 2 and 10% in RES-CAND = 10, in candidate responses around 2% in RES-CAND = 2 and 4% in RES-CAND = 10. Explanation: As shown in Table TABREF23 , SI-RNN significantly improves upon the previous state-of-the-art. In particular, addressee selection (ADR) benefits most, with different number of candidate responses (denoted as RES-CAND): around 12% in RES-CAND INLINEFORM0 and more than 10% in RES-CAND INLINEFORM1 . Response selection (RES) is also improved, suggesting role-sensitive GRUs and joint selection are helpful for response selection as well. The improvement is more obvious with more candidate responses (2% in RES-CAND INLINEFORM2 and 4% in RES-CAND INLINEFORM3 ).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nby how much is accuracy improved?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the previous state-of-the-art system and SI-RNN?\n< Explanation: We compare our model with the previous state-of-the-art system (SIR) and the state-of-the-art system (SOS) in the multi-party conversation setting. The previous state-of-the-art system is a simple encoder-decoder model that updates speaker embeddings only for the sender. The state-of-the-art system is a multi-layered encoder-decoder model that updates speaker embeddings in a role-sensitive way. We compare our model with the previous state-of-the-art system and the state-of-the-art system in the multi-party conversation setting.\n\nAnswer B:\nIn addressee selection around 12% in RES-CAND = 2 and 10% in RES-CAND = 10, in candidate responses around 2% in RES-CAND = 2 and 4% in RES-CAND = 10. Explanation: As shown in Table TABREF23 , SI-RNN significantly improves upon the previous state-of-the-art. In particular, addressee selection (ADR) benefits most, with different number of candidate responses (denoted as RES-CAND): around 12% in RES-CAND INLINEFORM0 and more than 10% in RES-CAND INLINEFORM1 . Response selection (RES) is also improved, suggesting role-sensitive GRUs and joint selection are helpful for response selection as well. The improvement is more obvious with more candidate responses (2% in RES-CAND INLINEFORM2 and 4% in RES-CAND INLINEFORM3 ).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.04005",
    "question": "what are the previous state of the art systems?",
    "gold_answer": "",
    "answer_a": "Dynamic-RNN model from BIBREF4. Explanation: The previous state-of-the-art Dynamic-RNN model from BIBREF4 ouchi-tsuboi:2016:EMNLP2016 maintains speaker embeddings to track each speaker status, which dynamically changes across time steps.",
    "answer_b": "What are the differences between the previous state of the art systems?\n<sep><table cellspacing=0 cellpadding=4>\n<tr>\n<td align=left style=\"text-align:left;vertical-align:middle;\">\nFigure 1: Previous state of the art systems\n</td>\n</tr>\n<tr>\n<td align=left style=\"text-align:left;vertical-align:middle;\">\nFigure 2: Previous state of the art systems\n</td>\n</tr>\n<tr>\n<td align=left style=\"text-align:left;vertical-align:middle;\">\nFigure 3: Previous state of the art systems\n</td>\n</tr>\n<tr>",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the previous state of the art systems?\n\nGround Truth Answer:\n\n\nAnswer A:\nDynamic-RNN model from BIBREF4. Explanation: The previous state-of-the-art Dynamic-RNN model from BIBREF4 ouchi-tsuboi:2016:EMNLP2016 maintains speaker embeddings to track each speaker status, which dynamically changes across time steps.\n\nAnswer B:\nWhat are the differences between the previous state of the art systems?\n<sep><table cellspacing=0 cellpadding=4>\n<tr>\n<td align=left style=\"text-align:left;vertical-align:middle;\">\nFigure 1: Previous state of the art systems\n</td>\n</tr>\n<tr>\n<td align=left style=\"text-align:left;vertical-align:middle;\">\nFigure 2: Previous state of the art systems\n</td>\n</tr>\n<tr>\n<td align=left style=\"text-align:left;vertical-align:middle;\">\nFigure 3: Previous state of the art systems\n</td>\n</tr>\n<tr>\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.02855",
    "question": "What are the three SOTA models evaluated?",
    "gold_answer": "",
    "answer_a": "What are their performance?\n<sep><br/ >Answer: The three SOTA models are: (1) the BIBREF10, (2) the BIBREF11, (3) the BIBREF12. The BIBREF10 model is a simple rule-based model that uses a set of rules to translate inflections. The BIBREF11 model is a rule-based model that uses a set of rules to translate inflections. The BIBREF12 model is a rule-based model that uses a set of rules to translate inflections. The BIBREF10 model is",
    "answer_b": "BIBREF3, BIBREF4, BIBREF5. Explanation: We use our dictionaries to train and evaluate three of the best performing BLI models BIBREF3, BIBREF4, BIBREF5 on all 40 language pairs.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the three SOTA models evaluated?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are their performance?\n<sep><br/ >Answer: The three SOTA models are: (1) the BIBREF10, (2) the BIBREF11, (3) the BIBREF12. The BIBREF10 model is a simple rule-based model that uses a set of rules to translate inflections. The BIBREF11 model is a rule-based model that uses a set of rules to translate inflections. The BIBREF12 model is a rule-based model that uses a set of rules to translate inflections. The BIBREF10 model is\n\nAnswer B:\nBIBREF3, BIBREF4, BIBREF5. Explanation: We use our dictionaries to train and evaluate three of the best performing BLI models BIBREF3, BIBREF4, BIBREF5 on all 40 language pairs.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.02855",
    "question": "What is the morphological constraint added?",
    "gold_answer": "",
    "answer_a": "How is it evaluated?\n< Explanation: We introduce a simple morphological constraint at training time that is evaluated by a simple rule-based method. The constraint is to add a morpheme to the word that is not in the dictionary. We evaluate the performance of the model on the training set and on the test set. We find that the performance of the model drops significantly when evaluated on infrequent morphological inflections. We also find that the performance of the model drops significantly when evaluated on infrequent morphological inflections. We conclude that the performance of the model drops significantly when evaluated on infrequent morphological inflections. We also find that",
    "answer_b": "Aligned words must share the same morphosyntactic category. Explanation: We adapt the self-learning models of BIBREF4 and BIBREF5 so that at each iteration they can align two words only if they share the same morphosyntactic category.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the morphological constraint added?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it evaluated?\n< Explanation: We introduce a simple morphological constraint at training time that is evaluated by a simple rule-based method. The constraint is to add a morpheme to the word that is not in the dictionary. We evaluate the performance of the model on the training set and on the test set. We find that the performance of the model drops significantly when evaluated on infrequent morphological inflections. We also find that the performance of the model drops significantly when evaluated on infrequent morphological inflections. We conclude that the performance of the model drops significantly when evaluated on infrequent morphological inflections. We also find that\n\nAnswer B:\nAligned words must share the same morphosyntactic category. Explanation: We adapt the self-learning models of BIBREF4 and BIBREF5 so that at each iteration they can align two words only if they share the same morphosyntactic category.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.04707",
    "question": "How do they interpret the model?",
    "gold_answer": "",
    "answer_a": "What is the difference between the two applications?\n<sep><br/ >Answer: We show that the neural network models are interpretable, and that the neural network models are able to learn the salience of the inference. We also show that the neural network models are able to learn the salience of the inference, and that the neural network models are able to learn the salience of the inference. We show that the neural network models are able to learn the salience of the inference, and that the neural network models are able to learn the salience of the inference. We show that the neural network models are able to learn the",
    "answer_b": "They find relations that connect questions to the answer-options.. Explanation: Applications of Neuro-symbolism ::: Application II: Neural Question-Answering using Commonsense Knowledge Bases ::: Knowledge elicitation\nConceptNet. We identify ConceptNet relations that connect questions to the answer-options. The intuition is that these relation paths would provide explicit evidence that would help the model find the answer.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they interpret the model?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the two applications?\n<sep><br/ >Answer: We show that the neural network models are interpretable, and that the neural network models are able to learn the salience of the inference. We also show that the neural network models are able to learn the salience of the inference, and that the neural network models are able to learn the salience of the inference. We show that the neural network models are able to learn the salience of the inference, and that the neural network models are able to learn the salience of the inference. We show that the neural network models are able to learn the\n\nAnswer B:\nThey find relations that connect questions to the answer-options.. Explanation: Applications of Neuro-symbolism ::: Application II: Neural Question-Answering using Commonsense Knowledge Bases ::: Knowledge elicitation\nConceptNet. We identify ConceptNet relations that connect questions to the answer-options. The intuition is that these relation paths would provide explicit evidence that would help the model find the answer.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.04707",
    "question": "What are the two applications of neuro-symbolism?",
    "gold_answer": "",
    "answer_a": "Learning a Knowledge Graph Embedding Space for Context Understanding in Automotive Driving Scenes. Explanation: Applications of Neuro-symbolism ::: Application I: Learning a Knowledge Graph Embedding Space for Context Understanding in Automotive Driving Scenes ::: Introduction",
    "answer_b": "How do they compare to the state-of-the-art?\n< Explanation: The first application is the use of neuro-symbolism to guide the learning progress of deep neural networks. In this case, we use the concept of neuro-symbolism to guide the learning progress of a deep neural network. We show that the use of neuro-symbolism can be used to guide the learning progress of deep neural networks, and that the use of neuro-symbolism can be used to guide the learning progress of deep neural networks, and that the use of neuro-symbolism can be used to guide the learning progress of deep neural networks. The second application is the use of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the two applications of neuro-symbolism?\n\nGround Truth Answer:\n\n\nAnswer A:\nLearning a Knowledge Graph Embedding Space for Context Understanding in Automotive Driving Scenes. Explanation: Applications of Neuro-symbolism ::: Application I: Learning a Knowledge Graph Embedding Space for Context Understanding in Automotive Driving Scenes ::: Introduction\n\nAnswer B:\nHow do they compare to the state-of-the-art?\n< Explanation: The first application is the use of neuro-symbolism to guide the learning progress of deep neural networks. In this case, we use the concept of neuro-symbolism to guide the learning progress of a deep neural network. We show that the use of neuro-symbolism can be used to guide the learning progress of deep neural networks, and that the use of neuro-symbolism can be used to guide the learning progress of deep neural networks, and that the use of neuro-symbolism can be used to guide the learning progress of deep neural networks. The second application is the use of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.05166",
    "question": "what elements of each profile did they use?",
    "gold_answer": "",
    "answer_a": "Explanation: The models were evaluated using the following metrics: precision, recall, F1 score, and accuracy. Precision is the ratio of the number of correct matches to the total number of matches. Recall is the ratio of the number of correct matches to the number of users in the dataset. F1 score is the harmonic mean of precision and recall. Accuracy is the number of correct matches divided by the total number of matches. The models were evaluated using the following metrics: precision, recall, F1 score, and accuracy. Precision is the ratio of the number of correct matches to the total number of matches. Recall is the ratio of the",
    "answer_b": "No profile elements. Explanation: The only information that was used in our models were the time and the linguistic content of posts by the users. We intentionally did not use any other information, especially the potentially personally identifiable information that was explicitly provided by the user, such as the screen name, birthday or location.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat elements of each profile did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The models were evaluated using the following metrics: precision, recall, F1 score, and accuracy. Precision is the ratio of the number of correct matches to the total number of matches. Recall is the ratio of the number of correct matches to the number of users in the dataset. F1 score is the harmonic mean of precision and recall. Accuracy is the number of correct matches divided by the total number of matches. The models were evaluated using the following metrics: precision, recall, F1 score, and accuracy. Precision is the ratio of the number of correct matches to the total number of matches. Recall is the ratio of the\n\nAnswer B:\nNo profile elements. Explanation: The only information that was used in our models were the time and the linguistic content of posts by the users. We intentionally did not use any other information, especially the potentially personally identifiable information that was explicitly provided by the user, such as the screen name, birthday or location.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.05166",
    "question": "How is the gold standard defined?",
    "gold_answer": "",
    "answer_a": "We used a third party social media site (i.e., Google Plus), one that was not used in our analysis to compile our ground truth. Explanation: For the purposes of this paper, we focused on matching accounts between two of the largest social networks: Twitter and Facebook. In order to proceed with our study, we needed a sizeable (few thousand) number of English speaking users with accounts on both Twitter and Facebook. We also needed to know the precise matching between the Twitter and Facebook accounts for our ground truth.\n\nTo that end, we crawled publicly available, English-language, Google Plus accounts using the Google Plus API and scraped links to the users' other social media profiles. (Note that one of the reasons why we used Twitter and Facebook is that they were two of the most common sites linked to on Google Plus). We used a third party social media site (i.e., Google Plus), one that was not used in our analysis to compile our ground truth in order to limit selection bias in our data collection.\n\nWe discarded all users who did not link to an account for both Twitter and Facebook and those whose accounts on either of these sites were not public. We then used the APIs of Twitter and Facebook to collect posts made by the users on these sites.",
    "answer_b": "What is the gold standard for the task?\n<sep><table cellspacing=\"0\" cellpadding=\"0\" width=\"600\" border=\"0\" align=\"center\">\n<tr>\n<td align=\"center\">\nFigure 1: The gold standard for the task is the user account of the person who is the closest match to the user account of the person in the dataset. The user account of the person who is the closest match to the user account of the person in the dataset is called the gold standard. The gold standard is used to evaluate the performance of the models. The gold standard is not used to evaluate the performance of the models",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is the gold standard defined?\n\nGround Truth Answer:\n\n\nAnswer A:\nWe used a third party social media site (i.e., Google Plus), one that was not used in our analysis to compile our ground truth. Explanation: For the purposes of this paper, we focused on matching accounts between two of the largest social networks: Twitter and Facebook. In order to proceed with our study, we needed a sizeable (few thousand) number of English speaking users with accounts on both Twitter and Facebook. We also needed to know the precise matching between the Twitter and Facebook accounts for our ground truth.\n\nTo that end, we crawled publicly available, English-language, Google Plus accounts using the Google Plus API and scraped links to the users' other social media profiles. (Note that one of the reasons why we used Twitter and Facebook is that they were two of the most common sites linked to on Google Plus). We used a third party social media site (i.e., Google Plus), one that was not used in our analysis to compile our ground truth in order to limit selection bias in our data collection.\n\nWe discarded all users who did not link to an account for both Twitter and Facebook and those whose accounts on either of these sites were not public. We then used the APIs of Twitter and Facebook to collect posts made by the users on these sites.\n\nAnswer B:\nWhat is the gold standard for the task?\n<sep><table cellspacing=\"0\" cellpadding=\"0\" width=\"600\" border=\"0\" align=\"center\">\n<tr>\n<td align=\"center\">\nFigure 1: The gold standard for the task is the user account of the person who is the closest match to the user account of the person in the dataset. The user account of the person who is the closest match to the user account of the person in the dataset is called the gold standard. The gold standard is used to evaluate the performance of the models. The gold standard is not used to evaluate the performance of the models\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1607.00167",
    "question": "What is the timeframe of the current events?",
    "gold_answer": "",
    "answer_a": "from January 2014 to December 2015. Explanation: In this particular scenario, we use tweets from January 2014 to December 2015.",
    "answer_b": "How are they created?\n<sep><br/ > The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the timeframe of the current events?\n\nGround Truth Answer:\n\n\nAnswer A:\nfrom January 2014 to December 2015. Explanation: In this particular scenario, we use tweets from January 2014 to December 2015.\n\nAnswer B:\nHow are they created?\n<sep><br/ > The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API. The data is collected from the Twitter API.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1607.00167",
    "question": "What model was used for sentiment analysis?",
    "gold_answer": "",
    "answer_a": "A word-level sentiment analysis was made, using Sentilex-PT BIBREF7 - a sentiment lexicon for the portuguese language, which can be used to determine the sentiment polarity of each word, i.e. a value of -1 for negative words, 0 for neutral words and 1 for positive words. Explanation: Sentiment Analysis\nA word-level sentiment analysis was made, using Sentilex-PT BIBREF7 - a sentiment lexicon for the portuguese language, which can be used to determine the sentiment polarity of each word, i.e. a value of -1 for negative words, 0 for neutral words and 1 for positive words. A visualization system was created that displays the most mentioned words for each entity/day and their respective polarity using correspondingly colored and sized circles, which are called SentiBubbles.",
    "answer_b": "How was it evaluated?\n< Explanation: We used a simple bag of words model for sentiment analysis. We used the bag of words model to analyze the sentiment of the tweets. The bag of words model is a simple model that uses a set of words to represent a sentence. The model is based on the assumption that the sentiment of a sentence is determined by the presence of certain words in the sentence. The bag of words model is a simple model that uses a set of words to represent a sentence. The model is based on the assumption that the sentiment of a sentence is determined by the presence of certain words in the sentence. The bag of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat model was used for sentiment analysis?\n\nGround Truth Answer:\n\n\nAnswer A:\nA word-level sentiment analysis was made, using Sentilex-PT BIBREF7 - a sentiment lexicon for the portuguese language, which can be used to determine the sentiment polarity of each word, i.e. a value of -1 for negative words, 0 for neutral words and 1 for positive words. Explanation: Sentiment Analysis\nA word-level sentiment analysis was made, using Sentilex-PT BIBREF7 - a sentiment lexicon for the portuguese language, which can be used to determine the sentiment polarity of each word, i.e. a value of -1 for negative words, 0 for neutral words and 1 for positive words. A visualization system was created that displays the most mentioned words for each entity/day and their respective polarity using correspondingly colored and sized circles, which are called SentiBubbles.\n\nAnswer B:\nHow was it evaluated?\n< Explanation: We used a simple bag of words model for sentiment analysis. We used the bag of words model to analyze the sentiment of the tweets. The bag of words model is a simple model that uses a set of words to represent a sentence. The model is based on the assumption that the sentiment of a sentence is determined by the presence of certain words in the sentence. The bag of words model is a simple model that uses a set of words to represent a sentence. The model is based on the assumption that the sentiment of a sentence is determined by the presence of certain words in the sentence. The bag of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1607.00167",
    "question": "What language are the tweets in?",
    "gold_answer": "",
    "answer_a": "Portuguese. Explanation: Entity related data is provided from a knowledge base of Portuguese entities.",
    "answer_b": "Explanation: The tweets are in English. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat language are the tweets in?\n\nGround Truth Answer:\n\n\nAnswer A:\nPortuguese. Explanation: Entity related data is provided from a knowledge base of Portuguese entities.\n\nAnswer B:\nExplanation: The tweets are in English. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API. The tweets are from the Twitter API.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1903.04329",
    "question": "What explanation do the authors offer for the super or sublinear urban scaling?",
    "gold_answer": "",
    "answer_a": "abundance or lack of the elements of urban lifestyle. Explanation: A visible group of words that are amongst the most super- or sublinearly scaling words are related to the abundance or lack of the elements of urban lifestyle (e.g. deer, fitness).",
    "answer_b": "What is the relationship between the parameters for Zipf's law and Heaps law?\n< Explanation: We propose that the super- or sublinear urban scaling is due to the fact that the number of words in a city is proportional to the square of the city population. We also propose that the parameters for Zipf's law and Heaps law differ on Twitter from that of other texts, and that the exponent of Zipf's law changes with city size. We also show that the parameters for Zipf's law and Heaps law are not constant across cities. We find that the parameters for Zipf's law and Heaps law are",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat explanation do the authors offer for the super or sublinear urban scaling?\n\nGround Truth Answer:\n\n\nAnswer A:\nabundance or lack of the elements of urban lifestyle. Explanation: A visible group of words that are amongst the most super- or sublinearly scaling words are related to the abundance or lack of the elements of urban lifestyle (e.g. deer, fitness).\n\nAnswer B:\nWhat is the relationship between the parameters for Zipf's law and Heaps law?\n< Explanation: We propose that the super- or sublinear urban scaling is due to the fact that the number of words in a city is proportional to the square of the city population. We also propose that the parameters for Zipf's law and Heaps law differ on Twitter from that of other texts, and that the exponent of Zipf's law changes with city size. We also show that the parameters for Zipf's law and Heaps law are not constant across cities. We find that the parameters for Zipf's law and Heaps law are\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.05156",
    "question": "What syntactic and semantic features are proposed?",
    "gold_answer": "",
    "answer_a": "What are the differences between the classifiers?\n< Explanation: We used the following features: (1) the number of words, (2) the number of characters, (3) the number of hashtags, (4) the number of mentions, (5) the number of URLs, (6) the number of emojis, (7) the number of emojis with emoticons, (8) the number of emojis with emoticons and (9) the number of emojis with emoticons and emojis with emoticons. We also used the following features: (10) the number of hashtags, (11) the number of mentions,",
    "answer_b": "Opinion Words. Explanation: Semantic Features\nOpinion Words: We used the \"Harvard General Inquirer\" lexicon BIBREF8 , which is a dataset used commonly in sentiment classification tasks, to identify 2442 strong, negative and positive opinion words (such as robust, terrible, untrustworthy, etc).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat syntactic and semantic features are proposed?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the differences between the classifiers?\n< Explanation: We used the following features: (1) the number of words, (2) the number of characters, (3) the number of hashtags, (4) the number of mentions, (5) the number of URLs, (6) the number of emojis, (7) the number of emojis with emoticons, (8) the number of emojis with emoticons and (9) the number of emojis with emoticons and emojis with emoticons. We also used the following features: (10) the number of hashtags, (11) the number of mentions,\n\nAnswer B:\nOpinion Words. Explanation: Semantic Features\nOpinion Words: We used the \"Harvard General Inquirer\" lexicon BIBREF8 , which is a dataset used commonly in sentiment classification tasks, to identify 2442 strong, negative and positive opinion words (such as robust, terrible, untrustworthy, etc).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.05156",
    "question": "Which six speech acts are included in the taxonomy?",
    "gold_answer": "",
    "answer_a": "What are the features used?\n< Explanation: We created a taxonomy of six speech acts for Twitter and proposed a set of semantic and syntactic features. The six speech acts are: (1) asking for information, (2) requesting information, (3) requesting a favor, (4) requesting a favor, (5) requesting a favor, and (6) requesting a favor. The features used are: (1) the number of words in the tweet, (2) the number of characters in the tweet, (3) the number of hashtags in the tweet, (4) the number of emojis in the tweet, (",
    "answer_b": "Assertion. Explanation: Using Searle's speech act taxonomy BIBREF3 , we established a list of six speech act categories that are commonly seen on Twitter: Assertion, Recommendation Expression, Question, Request, and Miscellaneous.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich six speech acts are included in the taxonomy?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the features used?\n< Explanation: We created a taxonomy of six speech acts for Twitter and proposed a set of semantic and syntactic features. The six speech acts are: (1) asking for information, (2) requesting information, (3) requesting a favor, (4) requesting a favor, (5) requesting a favor, and (6) requesting a favor. The features used are: (1) the number of words in the tweet, (2) the number of characters in the tweet, (3) the number of hashtags in the tweet, (4) the number of emojis in the tweet, (\n\nAnswer B:\nAssertion. Explanation: Using Searle's speech act taxonomy BIBREF3 , we established a list of six speech act categories that are commonly seen on Twitter: Assertion, Recommendation Expression, Question, Request, and Miscellaneous.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.05156",
    "question": "what classifier had better performance?",
    "gold_answer": "",
    "answer_a": "How did they compare?\n< Explanation: We compared the performance of the three classifiers with different granularities. We found that the best classifier was a bag of words classifier with 1000000 words. The bag of words classifier achieved an average F1 score of 0.71. The bag of words classifier was also the best classifier in terms of the number of tweets it classified. The bag of words classifier was also the best classifier in terms of the number of tweets it classified. The bag of words classifier was also the best classifier in terms of the number of tweets it classified. The bag of words classifier",
    "answer_b": "logistic regression. Explanation: We trained four different classifiers on our 3,313 binary features using the following methods: naive bayes (NB), decision tree (DT), logistic regression (LR), SVM, and a baseline max classifier BL. We trained classifiers across three granularities: Twitter-wide, Type-specific, and Topic-specific.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat classifier had better performance?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow did they compare?\n< Explanation: We compared the performance of the three classifiers with different granularities. We found that the best classifier was a bag of words classifier with 1000000 words. The bag of words classifier achieved an average F1 score of 0.71. The bag of words classifier was also the best classifier in terms of the number of tweets it classified. The bag of words classifier was also the best classifier in terms of the number of tweets it classified. The bag of words classifier was also the best classifier in terms of the number of tweets it classified. The bag of words classifier\n\nAnswer B:\nlogistic regression. Explanation: We trained four different classifiers on our 3,313 binary features using the following methods: naive bayes (NB), decision tree (DT), logistic regression (LR), SVM, and a baseline max classifier BL. We trained classifiers across three granularities: Twitter-wide, Type-specific, and Topic-specific.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.05156",
    "question": "how many tweets were labeled?",
    "gold_answer": "",
    "answer_a": "Explanation: We collected 1000 tweets from Twitter using the Twitter API. We manually labeled 1000 tweets as positive or negative. We then used the Twitter API to extract the text of the remaining 1000 tweets. We then used the Twitter API to extract the hashtags and the mentions from the tweets. We then used the Twitter API to extract the user IDs of the users who posted the tweets. We then used the Twitter API to extract the user IDs of the users who liked the tweets. We then used the Twitter API to extract the user IDs of the users who retweeted the tweets. We then",
    "answer_b": "7,563. Explanation: For training, we used the label that the majority of annotators agreed upon (7,563 total tweets).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow many tweets were labeled?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We collected 1000 tweets from Twitter using the Twitter API. We manually labeled 1000 tweets as positive or negative. We then used the Twitter API to extract the text of the remaining 1000 tweets. We then used the Twitter API to extract the hashtags and the mentions from the tweets. We then used the Twitter API to extract the user IDs of the users who posted the tweets. We then used the Twitter API to extract the user IDs of the users who liked the tweets. We then used the Twitter API to extract the user IDs of the users who retweeted the tweets. We then\n\nAnswer B:\n7,563. Explanation: For training, we used the label that the majority of annotators agreed upon (7,563 total tweets).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.05156",
    "question": "how many annotators were there?",
    "gold_answer": "",
    "answer_a": "What was the average accuracy of the classifier?\n< Explanation: We used 100 annotators to label the tweets. The average accuracy of the classifier was 0.71. The results are shown in Table 1. The results show that the classifier is able to achieve a high accuracy on the test set. The results are also consistent with the results of the previous work on Twitter sentiment analysis. The authors of that work also found that the classifier is able to achieve a high accuracy on the test set. The results of the classifier are also consistent with the results of the previous work on Twitter sentiment analysis. The authors of that",
    "answer_b": "three. Explanation: We then asked three undergraduate annotators to independently annotate each of the tweets with one of the speech act categories described earlier.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow many annotators were there?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat was the average accuracy of the classifier?\n< Explanation: We used 100 annotators to label the tweets. The average accuracy of the classifier was 0.71. The results are shown in Table 1. The results show that the classifier is able to achieve a high accuracy on the test set. The results are also consistent with the results of the previous work on Twitter sentiment analysis. The authors of that work also found that the classifier is able to achieve a high accuracy on the test set. The results of the classifier are also consistent with the results of the previous work on Twitter sentiment analysis. The authors of that\n\nAnswer B:\nthree. Explanation: We then asked three undergraduate annotators to independently annotate each of the tweets with one of the speech act categories described earlier.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.05156",
    "question": "who labelled the tweets?",
    "gold_answer": "",
    "answer_a": "What was their dataset?\n< Explanation: We collected tweets from Twitter using the Twitter API. We used the Twitter API to collect tweets from the following hashtags: #news, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #pol",
    "answer_b": "three undergraduate annotators. Explanation: We then asked three undergraduate annotators to independently annotate each of the tweets with one of the speech act categories described earlier.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwho labelled the tweets?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat was their dataset?\n< Explanation: We collected tweets from Twitter using the Twitter API. We used the Twitter API to collect tweets from the following hashtags: #news, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #politics, #pol\n\nAnswer B:\nthree undergraduate annotators. Explanation: We then asked three undergraduate annotators to independently annotate each of the tweets with one of the speech act categories described earlier.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.05156",
    "question": "what are the proposed semantic features?",
    "gold_answer": "",
    "answer_a": "What are the proposed syntactic features?\n<blockquote>We propose a set of semantic features that capture the meaning of the utterance. We use the following features: (1) the number of words in the utterance, (2) the number of words in the tweet, (3) the number of words in the tweet that are not in the vocabulary, (4) the number of words in the tweet that are in the vocabulary, (5) the number of words in the tweet that are in the vocabulary and not in the tweet, (6) the number of words in the tweet that are in the vocabulary and not in",
    "answer_b": "Opinion Words. Explanation: We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the proposed semantic features?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the proposed syntactic features?\n<blockquote>We propose a set of semantic features that capture the meaning of the utterance. We use the following features: (1) the number of words in the utterance, (2) the number of words in the tweet, (3) the number of words in the tweet that are not in the vocabulary, (4) the number of words in the tweet that are in the vocabulary, (5) the number of words in the tweet that are in the vocabulary and not in the tweet, (6) the number of words in the tweet that are in the vocabulary and not in\n\nAnswer B:\nOpinion Words. Explanation: We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.05156",
    "question": "what syntactic features are proposed?",
    "gold_answer": "",
    "answer_a": "Explanation: We used the following syntactic features: 1) number of words, 2) number of characters, 3) number of hashtags, 4) number of emojis, 5) number of URLs, 6) number of URLs, 7) number of URLs, 8) number of URLs, 9) number of URLs, 10) number of URLs, 11) number of URLs, 12) number of URLs, 13) number of URLs, 14) number of URLs, 15) number of URLs, 16) number of URLs",
    "answer_b": "Punctuations. Explanation: We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat syntactic features are proposed?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We used the following syntactic features: 1) number of words, 2) number of characters, 3) number of hashtags, 4) number of emojis, 5) number of URLs, 6) number of URLs, 7) number of URLs, 8) number of URLs, 9) number of URLs, 10) number of URLs, 11) number of URLs, 12) number of URLs, 13) number of URLs, 14) number of URLs, 15) number of URLs, 16) number of URLs\n\nAnswer B:\nPunctuations. Explanation: We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1605.05156",
    "question": "what datasets were used?",
    "gold_answer": "",
    "answer_a": "A dataset they annotated, \"Harvard General Inquirer\" Lexicon for opinion words, a collection of vulgar words, an online collection of text-based emoticons, and Wierzbicka's collection of English speech act verbs. Explanation: Given the diversity of topics talked about on Twitter, we wanted to explore topic and type dependent speech act classifiers.",
    "answer_b": "Explanation: We used the Twitter dataset from the Stanford Sentiment Treebank (SST) BIBREF11 . The dataset contains 100,000 tweets from 2012 to 2013. We used the first 100,000 tweets for training and the remaining 10,000 tweets for testing. We used the Twitter API to download the tweets. We removed the tweets that contained URLs, mentions, or hashtags. We also removed the tweets that contained the word \u201c#\u201d or the word \u201c#\u201d followed by a hashtag. We removed the tweets",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat datasets were used?\n\nGround Truth Answer:\n\n\nAnswer A:\nA dataset they annotated, \"Harvard General Inquirer\" Lexicon for opinion words, a collection of vulgar words, an online collection of text-based emoticons, and Wierzbicka's collection of English speech act verbs. Explanation: Given the diversity of topics talked about on Twitter, we wanted to explore topic and type dependent speech act classifiers.\n\nAnswer B:\nExplanation: We used the Twitter dataset from the Stanford Sentiment Treebank (SST) BIBREF11 . The dataset contains 100,000 tweets from 2012 to 2013. We used the first 100,000 tweets for training and the remaining 10,000 tweets for testing. We used the Twitter API to download the tweets. We removed the tweets that contained URLs, mentions, or hashtags. We also removed the tweets that contained the word \u201c#\u201d or the word \u201c#\u201d followed by a hashtag. We removed the tweets\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.05306",
    "question": "What was the baseline?",
    "gold_answer": "",
    "answer_a": "What was the performance of the proposed model?\n< Explanation: The baseline model is a simple LSTM model with 100 hidden units and 10000 training examples. The model is trained with the Adam optimizer with a learning rate of 0.0001. The model is trained for 100 epochs. The model is trained with the Adam optimizer with a learning rate of 0.0001. The model is trained with the Adam optimizer with a learning rate of 0.0001. The model is trained with the Adam optimizer with a learning rate of 0.0",
    "answer_b": "Model A, which was trained on LibriSpeech data with SAT, together with the language model also trained with LibriSpeech.. Explanation: Row(1) is for Model A in Fig. FIGREF5 taken as the baseline, which was trained on LibriSpeech data with SAT, together with the language model also trained with LibriSpeech.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat was the baseline?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat was the performance of the proposed model?\n< Explanation: The baseline model is a simple LSTM model with 100 hidden units and 10000 training examples. The model is trained with the Adam optimizer with a learning rate of 0.0001. The model is trained for 100 epochs. The model is trained with the Adam optimizer with a learning rate of 0.0001. The model is trained with the Adam optimizer with a learning rate of 0.0001. The model is trained with the Adam optimizer with a learning rate of 0.0\n\nAnswer B:\nModel A, which was trained on LibriSpeech data with SAT, together with the language model also trained with LibriSpeech.. Explanation: Row(1) is for Model A in Fig. FIGREF5 taken as the baseline, which was trained on LibriSpeech data with SAT, together with the language model also trained with LibriSpeech.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.05306",
    "question": "How many songs were collected?",
    "gold_answer": "",
    "answer_a": "110 pieces of music-removed version of commercial English popular songs. Explanation: To make our work easier and compatible to more available singing content, we collected 130 music-removed (or vocal-only) English songs from www.youtube.com so as to consider only the vocal line.The music-removing processes are conducted by the video owners, containing the original vocal recordings by the singers and vocal elements for remix purpose.\n\nAfter initial test by speech recognition system trained with LibriSpeech BIBREF20 , we dropped 20 songs, with WERs exceeding 95%. The remaining 110 pieces of music-removed version of commercial English popular songs were produced by 15 male singers, 28 female singers and 19 groups. The term group means by more than one person. No any further preprocessing was performed on the data, so the data preserves many characteristics of the vocal extracted from commercial polyphonic music, such as harmony, scat, and silent parts. Some pieces also contain overlapping verses and residual background music, and some frequency components may be truncated.",
    "answer_b": "What were the features used?\n< Explanation: We collected 100 songs from the commercial singing content. The songs were randomly selected from the 1000 songs in the dataset. The songs were divided into training set (70 songs) and test set (30 songs). The training set was used to train the model and the test set was used to evaluate the model. The features used were the following: 1) the duration of the voice, 2) the duration of the voice in the pitch contour, 3) the duration of the voice in the pitch contour, 4) the duration of the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many songs were collected?\n\nGround Truth Answer:\n\n\nAnswer A:\n110 pieces of music-removed version of commercial English popular songs. Explanation: To make our work easier and compatible to more available singing content, we collected 130 music-removed (or vocal-only) English songs from www.youtube.com so as to consider only the vocal line.The music-removing processes are conducted by the video owners, containing the original vocal recordings by the singers and vocal elements for remix purpose.\n\nAfter initial test by speech recognition system trained with LibriSpeech BIBREF20 , we dropped 20 songs, with WERs exceeding 95%. The remaining 110 pieces of music-removed version of commercial English popular songs were produced by 15 male singers, 28 female singers and 19 groups. The term group means by more than one person. No any further preprocessing was performed on the data, so the data preserves many characteristics of the vocal extracted from commercial polyphonic music, such as harmony, scat, and silent parts. Some pieces also contain overlapping verses and residual background music, and some frequency components may be truncated.\n\nAnswer B:\nWhat were the features used?\n< Explanation: We collected 100 songs from the commercial singing content. The songs were randomly selected from the 1000 songs in the dataset. The songs were divided into training set (70 songs) and test set (30 songs). The training set was used to train the model and the test set was used to evaluate the model. The features used were the following: 1) the duration of the voice, 2) the duration of the voice in the pitch contour, 3) the duration of the voice in the pitch contour, 4) the duration of the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.02614",
    "question": "how does end of utterance and token tags affect the performance",
    "gold_answer": "",
    "answer_a": "Performance degrades if the tags are not used.. Explanation: It can be observed that the performance is significantly degraded without two special tags.",
    "answer_b": "?\n< Explanation: We evaluated the performance of our proposed method on both Ubuntu dialogue corpus and Douban conversation corpus. We found that the proposed method outperformed the original ESIM and the new model by 1.2% and 1.4% on Ubuntu dialogue corpus and 1.3% and 1.5% on Douban conversation corpus. The results of the new model are comparable to the original ESIM on both datasets. The proposed method outperformed the original ESIM by 1.2% and 1.4% on Ubuntu dialogue corpus and 1.3% and 1.5",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow does end of utterance and token tags affect the performance\n\nGround Truth Answer:\n\n\nAnswer A:\nPerformance degrades if the tags are not used.. Explanation: It can be observed that the performance is significantly degraded without two special tags.\n\nAnswer B:\n?\n< Explanation: We evaluated the performance of our proposed method on both Ubuntu dialogue corpus and Douban conversation corpus. We found that the proposed method outperformed the original ESIM and the new model by 1.2% and 1.4% on Ubuntu dialogue corpus and 1.3% and 1.5% on Douban conversation corpus. The results of the new model are comparable to the original ESIM on both datasets. The proposed method outperformed the original ESIM by 1.2% and 1.4% on Ubuntu dialogue corpus and 1.3% and 1.5\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.02614",
    "question": "what are the baselines?",
    "gold_answer": "",
    "answer_a": "ESIM. Explanation: We integrated our methods with ESIM(baseline model) BIBREF10 .",
    "answer_b": "What are the results?\n<sep><font size=\"2\">Baseline</font>:\nESIM: Chen et al. (2017) [1] (original ESIM) BIBREF1\nESIM with character embedding: Chen et al. (2017) [1] (ESIM with character embedding) BIBREF2\nESIM with character embedding and word embedding: Chen et al. (2017) [1] (ESIM with character embedding and word embedding) BIBREF3\nESIM with character embedding and word embedding with word embedding: Chen et al.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the baselines?\n\nGround Truth Answer:\n\n\nAnswer A:\nESIM. Explanation: We integrated our methods with ESIM(baseline model) BIBREF10 .\n\nAnswer B:\nWhat are the results?\n<sep><font size=\"2\">Baseline</font>:\nESIM: Chen et al. (2017) [1] (original ESIM) BIBREF1\nESIM with character embedding: Chen et al. (2017) [1] (ESIM with character embedding) BIBREF2\nESIM with character embedding and word embedding: Chen et al. (2017) [1] (ESIM with character embedding and word embedding) BIBREF3\nESIM with character embedding and word embedding with word embedding: Chen et al.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.02614",
    "question": "what kind of conversations are in the douban conversation corpus?",
    "gold_answer": "",
    "answer_a": "How many are there?\n< Explanation: The Douban conversation corpus contains 10,000 conversations. Each conversation consists of 10 utterances. The utterances are divided into 10 categories: 1) question, 2) answer, 3) question-answer, 4) question-answer-explanation, 5) explanation, 6) explanation-explanation, 7) explanation-explanation-explanation, 8) explanation-explanation-explanation-explanation, 9) explanation-explanation-explanation-explanation-explanation, 10) explanation-explanation-explanation-explanation-explanation-explanation",
    "answer_b": "Conversations that are typical for a social networking service.. Explanation: Douban conversation corpus BIBREF11 which are constructed from Douban group (a popular social networking service in China) is also used in experiments.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat kind of conversations are in the douban conversation corpus?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow many are there?\n< Explanation: The Douban conversation corpus contains 10,000 conversations. Each conversation consists of 10 utterances. The utterances are divided into 10 categories: 1) question, 2) answer, 3) question-answer, 4) question-answer-explanation, 5) explanation, 6) explanation-explanation, 7) explanation-explanation-explanation, 8) explanation-explanation-explanation-explanation, 9) explanation-explanation-explanation-explanation-explanation, 10) explanation-explanation-explanation-explanation-explanation-explanation\n\nAnswer B:\nConversations that are typical for a social networking service.. Explanation: Douban conversation corpus BIBREF11 which are constructed from Douban group (a popular social networking service in China) is also used in experiments.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1802.02614",
    "question": "what pretrained word embeddings are used?",
    "gold_answer": "",
    "answer_a": "GloVe. Explanation: Word embedding matrix was initialized with pre-trained 300-dimensional GloVe vectors BIBREF28 .",
    "answer_b": "Explanation: We used the word embeddings from the BERT model (Liu et al., 2018) to generate the word embeddings for the training set. The word embeddings were pre-trained on the BERT model and were obtained by training the BERT model on the English Wikipedia corpus. The word embeddings were then used to generate the word embeddings for the training set. The word embeddings were then used to generate the word embeddings for the test set. The word embeddings were then used to generate the word embeddings for the test set. The word embeddings were then used to generate the word embeddings for the test set. The word embeddings were",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat pretrained word embeddings are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nGloVe. Explanation: Word embedding matrix was initialized with pre-trained 300-dimensional GloVe vectors BIBREF28 .\n\nAnswer B:\nExplanation: We used the word embeddings from the BERT model (Liu et al., 2018) to generate the word embeddings for the training set. The word embeddings were pre-trained on the BERT model and were obtained by training the BERT model on the English Wikipedia corpus. The word embeddings were then used to generate the word embeddings for the training set. The word embeddings were then used to generate the word embeddings for the test set. The word embeddings were then used to generate the word embeddings for the test set. The word embeddings were then used to generate the word embeddings for the test set. The word embeddings were\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.12203",
    "question": "What other evaluation metrics are reported?",
    "gold_answer": "",
    "answer_a": "Precision and recall for 2-way classification and F1 for 4-way classification.. Explanation: FLOAT SELECTED: Table 2: 2-way classification results on SLN. *n-fold cross validation (precision, recall) as reported in SoTA.",
    "answer_b": "How do they compare to existing methods?\n< Explanation: We evaluate our model on the following metrics: F1 score, Precision, Recall, and Accuracy. We also report the F1 score of the baseline model. We also report the F1 score of the baseline model on the out-of-domain test set. We also report the F1 score of the baseline model on the out-of-domain test set. We also report the F1 score of the baseline model on the out-of-domain test set. We also report the F1 score of the baseline model on the out-of-domain test set. We also report the F1 score of the",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat other evaluation metrics are reported?\n\nGround Truth Answer:\n\n\nAnswer A:\nPrecision and recall for 2-way classification and F1 for 4-way classification.. Explanation: FLOAT SELECTED: Table 2: 2-way classification results on SLN. *n-fold cross validation (precision, recall) as reported in SoTA.\n\nAnswer B:\nHow do they compare to existing methods?\n< Explanation: We evaluate our model on the following metrics: F1 score, Precision, Recall, and Accuracy. We also report the F1 score of the baseline model. We also report the F1 score of the baseline model on the out-of-domain test set. We also report the F1 score of the baseline model on the out-of-domain test set. We also report the F1 score of the baseline model on the out-of-domain test set. We also report the F1 score of the baseline model on the out-of-domain test set. We also report the F1 score of the\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.12203",
    "question": "What out of domain scenarios did they evaluate on?",
    "gold_answer": "",
    "answer_a": "How do they evaluate it?\n< Explanation: We evaluate our model on two out of domain scenarios: (1) fake news detection on a news article dataset (2) fake news detection on a news article dataset with a different set of fake news categories. We also evaluate our model on a news article dataset with a different set of fake news categories. We evaluate our model on two out of domain scenarios: (1) fake news detection on a news article dataset (2) fake news detection on a news article dataset with a different set of fake news categories. We also evaluate our model on a news article dataset with a different set of fake",
    "answer_b": "In 2-way classification they used LUN-train for training, LUN-test for development and the entire SLN dataset for testing. In 4-way classification they used LUN-train for training and development and LUN-test for testing.. Explanation: 2-way classification b/w satire and trusted articles: We use the satirical and trusted news articles from LUN-train for training, and from LUN-test as the development set. We evaluate our model on the entire SLN dataset. This is done to emulate a real-world scenario where we want to see the performance of our classifier on an out of domain dataset.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat out of domain scenarios did they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate it?\n< Explanation: We evaluate our model on two out of domain scenarios: (1) fake news detection on a news article dataset (2) fake news detection on a news article dataset with a different set of fake news categories. We also evaluate our model on a news article dataset with a different set of fake news categories. We evaluate our model on two out of domain scenarios: (1) fake news detection on a news article dataset (2) fake news detection on a news article dataset with a different set of fake news categories. We also evaluate our model on a news article dataset with a different set of fake\n\nAnswer B:\nIn 2-way classification they used LUN-train for training, LUN-test for development and the entire SLN dataset for testing. In 4-way classification they used LUN-train for training and development and LUN-test for testing.. Explanation: 2-way classification b/w satire and trusted articles: We use the satirical and trusted news articles from LUN-train for training, and from LUN-test as the development set. We evaluate our model on the entire SLN dataset. This is done to emulate a real-world scenario where we want to see the performance of our classifier on an out of domain dataset.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.12203",
    "question": "What was their state of the art accuracy score?",
    "gold_answer": "",
    "answer_a": "In 2-way classification precision score was 88% and recall 82%. In 4-way classification on LUN-dev F1-score was 91% and on LUN-test F1-score was 65%.. Explanation: FLOAT SELECTED: Table 2: 2-way classification results on SLN. *n-fold cross validation (precision, recall) as reported in SoTA.",
    "answer_b": "How did they do?\n< Explanation: We report the results of our model on the following datasets: (1) News2018, (2) News2019, (3) News2020, (4) News2021, (5) News2022, (6) News2023, (7) News2024, (8) News2025, (9) News2026, (10) News2027, (11) News2028, (12) News20",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat was their state of the art accuracy score?\n\nGround Truth Answer:\n\n\nAnswer A:\nIn 2-way classification precision score was 88% and recall 82%. In 4-way classification on LUN-dev F1-score was 91% and on LUN-test F1-score was 65%.. Explanation: FLOAT SELECTED: Table 2: 2-way classification results on SLN. *n-fold cross validation (precision, recall) as reported in SoTA.\n\nAnswer B:\nHow did they do?\n< Explanation: We report the results of our model on the following datasets: (1) News2018, (2) News2019, (3) News2020, (4) News2021, (5) News2022, (6) News2023, (7) News2024, (8) News2025, (9) News2026, (10) News2027, (11) News2028, (12) News20\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.12203",
    "question": "Which datasets did they use?",
    "gold_answer": "",
    "answer_a": "How did they evaluate the performance of their model?\n< Explanation: We use the following datasets: (1) News2018: News articles from the 2018 News2018 dataset. (2) News2019: News articles from the 2019 News2019 dataset. (3) News2020: News articles from the 2020 News2020 dataset. (4) News2021: News articles from the 2021 News2021 dataset. (5) News2022:",
    "answer_b": "Satirical and Legitimate News Database. Explanation: We use SLN: Satirical and Legitimate News Database BIBREF2, RPN: Random Political News Dataset BIBREF10 and LUN: Labeled Unreliable News Dataset BIBREF0 for our experiments.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich datasets did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow did they evaluate the performance of their model?\n< Explanation: We use the following datasets: (1) News2018: News articles from the 2018 News2018 dataset. (2) News2019: News articles from the 2019 News2019 dataset. (3) News2020: News articles from the 2020 News2020 dataset. (4) News2021: News articles from the 2021 News2021 dataset. (5) News2022:\n\nAnswer B:\nSatirical and Legitimate News Database. Explanation: We use SLN: Satirical and Legitimate News Database BIBREF2, RPN: Random Political News Dataset BIBREF10 and LUN: Labeled Unreliable News Dataset BIBREF0 for our experiments.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.12203",
    "question": "What are the neural baselines mentioned?",
    "gold_answer": "",
    "answer_a": "CNN. Explanation: Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines,\n\nCNN: In this model, we apply a 1-d CNN (Convolutional Neural Network) layer BIBREF11 with filter size 3 over the word embeddings of the sentences within a document. This is followed by a max-pooling layer to get a single document vector which is passed to a fully connected projection layer to get the logits over output classes.\n\nLSTM: In this model, we encode the document using a LSTM (Long Short-Term Memory) layer BIBREF12. We use the hidden state at the last time step as the document vector which is passed to a fully connected projection layer to get the logits over output classes.\n\nBERT: In this model, we extract the sentence vector (representation corresponding to [CLS] token) using BERT (Bidirectional Encoder Representations from Transformers) BIBREF4 for each sentence in the document. We then apply a LSTM layer on the sentence embeddings, followed by a projection layer to make the prediction for each document.",
    "answer_b": "How do they compare to the proposed model?\n< Explanation: We compare our model with the following baselines: (1) a baseline model which uses only the sentence embeddings of the news article as input, (2) a baseline model which uses only the sentence embeddings of the news article and the document embeddings of the document as input, (3) a baseline model which uses only the document embeddings of the document as input, (4) a baseline model which uses only the document embeddings of the document and the document embeddings of the document as input, (5) a baseline model which uses only the document embeddings of the document and the sentence",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the neural baselines mentioned?\n\nGround Truth Answer:\n\n\nAnswer A:\nCNN. Explanation: Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines,\n\nCNN: In this model, we apply a 1-d CNN (Convolutional Neural Network) layer BIBREF11 with filter size 3 over the word embeddings of the sentences within a document. This is followed by a max-pooling layer to get a single document vector which is passed to a fully connected projection layer to get the logits over output classes.\n\nLSTM: In this model, we encode the document using a LSTM (Long Short-Term Memory) layer BIBREF12. We use the hidden state at the last time step as the document vector which is passed to a fully connected projection layer to get the logits over output classes.\n\nBERT: In this model, we extract the sentence vector (representation corresponding to [CLS] token) using BERT (Bidirectional Encoder Representations from Transformers) BIBREF4 for each sentence in the document. We then apply a LSTM layer on the sentence embeddings, followed by a projection layer to make the prediction for each document.\n\nAnswer B:\nHow do they compare to the proposed model?\n< Explanation: We compare our model with the following baselines: (1) a baseline model which uses only the sentence embeddings of the news article as input, (2) a baseline model which uses only the sentence embeddings of the news article and the document embeddings of the document as input, (3) a baseline model which uses only the document embeddings of the document as input, (4) a baseline model which uses only the document embeddings of the document and the document embeddings of the document as input, (5) a baseline model which uses only the document embeddings of the document and the sentence\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.07620",
    "question": "What regularization methods are used?",
    "gold_answer": "",
    "answer_a": "dropout. Explanation: For all of our models, we employ dropout on the fully-connected layer for regularization.",
    "answer_b": "How are they evaluated?\n< Explanation: We use the L1 and L2 regularization methods. We use the L1 regularization method to prevent overfitting and the L2 regularization method to prevent overfitting. We use the L1 regularization method to prevent overfitting and the L2 regularization method to prevent overfitting. We use the L1 regularization method to prevent overfitting and the L2 regularization method to prevent overfitting. We use the L1 regularization method to prevent overfitting and the L2 regularization method to prevent overfitting. We use the L1 regularization method to prevent overfitting and",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat regularization methods are used?\n\nGround Truth Answer:\n\n\nAnswer A:\ndropout. Explanation: For all of our models, we employ dropout on the fully-connected layer for regularization.\n\nAnswer B:\nHow are they evaluated?\n< Explanation: We use the L1 and L2 regularization methods. We use the L1 regularization method to prevent overfitting and the L2 regularization method to prevent overfitting. We use the L1 regularization method to prevent overfitting and the L2 regularization method to prevent overfitting. We use the L1 regularization method to prevent overfitting and the L2 regularization method to prevent overfitting. We use the L1 regularization method to prevent overfitting and the L2 regularization method to prevent overfitting. We use the L1 regularization method to prevent overfitting and\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.07620",
    "question": "What metrics are used?",
    "gold_answer": "",
    "answer_a": "Accuracy, Precision, Recall, F1-score. Explanation: The results for all of our models on both the ground-truth and augmented datasets are given in Table TABREF22.",
    "answer_b": "How are they evaluated?\n< Explanation: We use the following metrics to evaluate our models:\n\nPrecision: The precision of the model is the ratio of the number of correct predictions to the total number of predictions. The precision is calculated as follows:\n\nPrecision = TP / (TP + FP)\n\nRecall: Recall is the ratio of the number of correct predictions to the total number of actual positives. The recall is calculated as follows:\n\nRecall = TP / (TP + FN)\n\nF1: The F1 score is the harmonic mean of precision and recall. The F1 score is calculated as follows:\n\nF1 = 2 * (Precision",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat metrics are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nAccuracy, Precision, Recall, F1-score. Explanation: The results for all of our models on both the ground-truth and augmented datasets are given in Table TABREF22.\n\nAnswer B:\nHow are they evaluated?\n< Explanation: We use the following metrics to evaluate our models:\n\nPrecision: The precision of the model is the ratio of the number of correct predictions to the total number of predictions. The precision is calculated as follows:\n\nPrecision = TP / (TP + FP)\n\nRecall: Recall is the ratio of the number of correct predictions to the total number of actual positives. The recall is calculated as follows:\n\nRecall = TP / (TP + FN)\n\nF1: The F1 score is the harmonic mean of precision and recall. The F1 score is calculated as follows:\n\nF1 = 2 * (Precision\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.07620",
    "question": "How long is the dataset?",
    "gold_answer": "",
    "answer_a": "almost doubles the number of commits in the training split to 1493. Explanation: For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23. These repositories are split into training, validation, and test splits containing 808, 265, and 264 commits, respectively.",
    "answer_b": "What is the size of the dataset?\n< Explanation: We use the Java source code of the open-source project \u201cJavaFX\u201d as our dataset. The dataset contains 1000 commits, and each commit is a Java source code file. The dataset is available at https://github.com/JavaFX/JavaFX. The dataset is divided into 1000 commits, each of which is a Java source code file. Each commit is annotated with a security threat, and the annotations are provided in the form of a binary label. The annotations are provided in the form of a binary label, and the label is either \u201c",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long is the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nalmost doubles the number of commits in the training split to 1493. Explanation: For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23. These repositories are split into training, validation, and test splits containing 808, 265, and 264 commits, respectively.\n\nAnswer B:\nWhat is the size of the dataset?\n< Explanation: We use the Java source code of the open-source project \u201cJavaFX\u201d as our dataset. The dataset contains 1000 commits, and each commit is a Java source code file. The dataset is available at https://github.com/JavaFX/JavaFX. The dataset is divided into 1000 commits, each of which is a Java source code file. Each commit is annotated with a security threat, and the annotations are provided in the form of a binary label. The annotations are provided in the form of a binary label, and the label is either \u201c\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.07620",
    "question": "What dataset do they use?",
    "gold_answer": "",
    "answer_a": "How do they evaluate their models?\n< Explanation: We use the Java source code of the open-source project \u201cJavaFX\u201d as our dataset. The source code is available at https://github.com/JavaFX/JavaFX. We use the commit diff of the source code as our input data. We also use the source code of the open-source project \u201cJavaFX\u201d as our baseline. We use the commit diff of the source code of the JavaFX project as our baseline. We use the commit diff of the source code of the JavaFX project as our baseline. We use the commit diff of the source code of the JavaFX",
    "answer_b": "manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them. Explanation: For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat dataset do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate their models?\n< Explanation: We use the Java source code of the open-source project \u201cJavaFX\u201d as our dataset. The source code is available at https://github.com/JavaFX/JavaFX. We use the commit diff of the source code as our input data. We also use the source code of the open-source project \u201cJavaFX\u201d as our baseline. We use the commit diff of the source code of the JavaFX project as our baseline. We use the commit diff of the source code of the JavaFX project as our baseline. We use the commit diff of the source code of the JavaFX\n\nAnswer B:\nmanually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them. Explanation: For training our classification models, we use a manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them, provided by BIBREF23.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.03353",
    "question": "What simplification of the architecture is performed that resulted in same performance?",
    "gold_answer": "",
    "answer_a": "randomly sampling them rather than enumerate them all. Explanation: Because we know the exact label of each sample in the training phase, so we can train the model in a particular way. For those negative samples, which means each span does not belong to any entity class, we randomly sampling them rather than enumerate them all.",
    "answer_b": "Explanation: We simplify the original network architecture by removing the last fully connected layer and adding a linear layer to the output layer. The linear layer is used to reduce the search space. The linear layer is trained to minimize the cross-entropy loss. The linear layer is trained to minimize the cross-entropy loss. The linear layer is trained to minimize the cross-entropy loss. The linear layer is trained to minimize the cross-entropy loss. The linear layer is trained to minimize the cross-entropy loss. The linear layer is trained to minimize the cross-entropy loss. The linear layer is trained to minimize the cross-entropy loss. The",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat simplification of the architecture is performed that resulted in same performance?\n\nGround Truth Answer:\n\n\nAnswer A:\nrandomly sampling them rather than enumerate them all. Explanation: Because we know the exact label of each sample in the training phase, so we can train the model in a particular way. For those negative samples, which means each span does not belong to any entity class, we randomly sampling them rather than enumerate them all.\n\nAnswer B:\nExplanation: We simplify the original network architecture by removing the last fully connected layer and adding a linear layer to the output layer. The linear layer is used to reduce the search space. The linear layer is trained to minimize the cross-entropy loss. The linear layer is trained to minimize the cross-entropy loss. The linear layer is trained to minimize the cross-entropy loss. The linear layer is trained to minimize the cross-entropy loss. The linear layer is trained to minimize the cross-entropy loss. The linear layer is trained to minimize the cross-entropy loss. The linear layer is trained to minimize the cross-entropy loss. The\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.03353",
    "question": "How much better is performance of SEPT compared to previous state-of-the-art?",
    "gold_answer": "",
    "answer_a": "What is the state-of-the-art?\n< Explanation: We compare our model with the state-of-the-art results of BERT and the previous state-of-the-art results of BERT+BERT. The results are shown in Table TABREF11 . We can see that our model achieves a new state of the art result in scientific named entity recognition even without relation information involved. The state-of-the-art results of BERT+BERT are 9.1% and 10.1% better than our model and BERT respectively. The state-of-the-art results of BERT are 10.1% better than our model",
    "answer_b": "SEPT have improvement for Recall  3.9%  and F1 1.3%  over the best performing baseline (SCIIE(SciBERT)). Explanation: FLOAT SELECTED: Table 1: Overall performance of scientific named entity recognition task. We report micro F1 score following the convention of NER task. All scores are taken from the test set with the corresponding highest development score.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much better is performance of SEPT compared to previous state-of-the-art?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the state-of-the-art?\n< Explanation: We compare our model with the state-of-the-art results of BERT and the previous state-of-the-art results of BERT+BERT. The results are shown in Table TABREF11 . We can see that our model achieves a new state of the art result in scientific named entity recognition even without relation information involved. The state-of-the-art results of BERT+BERT are 9.1% and 10.1% better than our model and BERT respectively. The state-of-the-art results of BERT are 10.1% better than our model\n\nAnswer B:\nSEPT have improvement for Recall  3.9%  and F1 1.3%  over the best performing baseline (SCIIE(SciBERT)). Explanation: FLOAT SELECTED: Table 1: Overall performance of scientific named entity recognition task. We report micro F1 score following the convention of NER task. All scores are taken from the test set with the corresponding highest development score.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.04236",
    "question": "How many actions are present in the dataset?",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n< Explanation: We collected 1000 videos from YouTube, and manually annotated 1000 of them for the presence of visible actions. The dataset contains 1000 videos, each with 1000 frames. The frames are labeled with the action type (e.g., walking, running, jumping, etc.) and the corresponding frame number. The dataset contains 1000 videos, each with 1000 frames. The frames are labeled with the action type (e.g., walking, running, jumping, etc.) and the corresponding frame number",
    "answer_b": "14,769. Explanation: The dataset includes a total of 14,769 actions, 4,340 of which are visible.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many actions are present in the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n< Explanation: We collected 1000 videos from YouTube, and manually annotated 1000 of them for the presence of visible actions. The dataset contains 1000 videos, each with 1000 frames. The frames are labeled with the action type (e.g., walking, running, jumping, etc.) and the corresponding frame number. The dataset contains 1000 videos, each with 1000 frames. The frames are labeled with the action type (e.g., walking, running, jumping, etc.) and the corresponding frame number\n\nAnswer B:\n14,769. Explanation: The dataset includes a total of 14,769 actions, 4,340 of which are visible.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.04236",
    "question": "How many videos did they use?",
    "gold_answer": "",
    "answer_a": "177. Explanation: FLOAT SELECTED: Table 3: Data statistics.",
    "answer_b": "What is the size of the dataset?\nFor the dataset, we used 1000 videos from the YouTube platform. We also collected 1000 videos from the YouTube channel of the author of this paper. The videos were selected based on the following criteria: (1) they were from the same author, (2) they were from the same channel, (3) they were from the same genre, (4) they were from the same time period, (5) they were from the same country, (6) they were from the same language, (7) they were from the same genre, (8",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many videos did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\n177. Explanation: FLOAT SELECTED: Table 3: Data statistics.\n\nAnswer B:\nWhat is the size of the dataset?\nFor the dataset, we used 1000 videos from the YouTube platform. We also collected 1000 videos from the YouTube channel of the author of this paper. The videos were selected based on the following criteria: (1) they were from the same author, (2) they were from the same channel, (3) they were from the same genre, (4) they were from the same time period, (5) they were from the same country, (6) they were from the same language, (7) they were from the same genre, (8\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.04236",
    "question": "What unimodal algorithms do they compare with?",
    "gold_answer": "",
    "answer_a": "Concreteness. Explanation: Concreteness. We label as visible all the actions that have a concreteness score above a certain threshold, and label as non-visible the remaining ones.",
    "answer_b": "What is the performance of their algorithm?\n< Explanation: We compare our algorithm with the following unimodal algorithms: (1) a baseline algorithm that uses only the visual description of the video to identify visible actions, (2) a baseline algorithm that uses only the linguistic description of the video to identify visible actions, (3) a baseline algorithm that uses both the visual and linguistic descriptions of the video to identify visible actions, and (4) a baseline algorithm that uses both the visual and linguistic descriptions of the video to identify visible actions, but additionally uses a human expert to annotate the actions. We also compare our algorithm with the following multim",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat unimodal algorithms do they compare with?\n\nGround Truth Answer:\n\n\nAnswer A:\nConcreteness. Explanation: Concreteness. We label as visible all the actions that have a concreteness score above a certain threshold, and label as non-visible the remaining ones.\n\nAnswer B:\nWhat is the performance of their algorithm?\n< Explanation: We compare our algorithm with the following unimodal algorithms: (1) a baseline algorithm that uses only the visual description of the video to identify visible actions, (2) a baseline algorithm that uses only the linguistic description of the video to identify visible actions, (3) a baseline algorithm that uses both the visual and linguistic descriptions of the video to identify visible actions, and (4) a baseline algorithm that uses both the visual and linguistic descriptions of the video to identify visible actions, but additionally uses a human expert to annotate the actions. We also compare our algorithm with the following multim\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.04236",
    "question": "What platform was used for crowdsourcing?",
    "gold_answer": "",
    "answer_a": "Amazon Mechanical Turk (AMT). Explanation: We create an annotation task on Amazon Mechanical Turk (AMT) to identify actions that are visible.",
    "answer_b": "How many people participated?\n< Explanation: We use the YouTube API to construct a dataset of 1000 videos of 1000 people performing 1000 actions. We use the YouTube API to construct a dataset of 1000 videos of 1000 people performing 1000 actions. We use the YouTube API to construct a dataset of 1000 videos of 1000 people performing 1000 actions. We use the YouTube API to construct a dataset of 1000 videos of 1000 people performing 10",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat platform was used for crowdsourcing?\n\nGround Truth Answer:\n\n\nAnswer A:\nAmazon Mechanical Turk (AMT). Explanation: We create an annotation task on Amazon Mechanical Turk (AMT) to identify actions that are visible.\n\nAnswer B:\nHow many people participated?\n< Explanation: We use the YouTube API to construct a dataset of 1000 videos of 1000 people performing 1000 actions. We use the YouTube API to construct a dataset of 1000 videos of 1000 people performing 1000 actions. We use the YouTube API to construct a dataset of 1000 videos of 1000 people performing 1000 actions. We use the YouTube API to construct a dataset of 1000 videos of 1000 people performing 10\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1906.04236",
    "question": "How long are the videos?",
    "gold_answer": "",
    "answer_a": "What is the average length?\n< Explanation: The dataset contains 1000 videos, each of which is 10 minutes long. The videos are randomly selected from a pool of 1000 videos, and are then manually annotated for visible actions. The dataset is available at https://github.com/ucar/visible-actions.\n\nThe dataset contains 1000 videos, each of which is 10 minutes long. The videos are randomly selected from a pool of 1000 videos, and are then manually annotated for visible actions. The dataset is available at https://github.com/ucar/",
    "answer_b": "length of our collected videos varies from two minutes to twenty minutes. Explanation: The length of our collected videos varies from two minutes to twenty minutes.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long are the videos?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the average length?\n< Explanation: The dataset contains 1000 videos, each of which is 10 minutes long. The videos are randomly selected from a pool of 1000 videos, and are then manually annotated for visible actions. The dataset is available at https://github.com/ucar/visible-actions.\n\nThe dataset contains 1000 videos, each of which is 10 minutes long. The videos are randomly selected from a pool of 1000 videos, and are then manually annotated for visible actions. The dataset is available at https://github.com/ucar/\n\nAnswer B:\nlength of our collected videos varies from two minutes to twenty minutes. Explanation: The length of our collected videos varies from two minutes to twenty minutes.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1812.05813",
    "question": "How were missing hypotheses discovered?",
    "gold_answer": "",
    "answer_a": "The problems were then re-rated by experts in logic and/or linguistics.. Explanation: The problems were then re-rated by experts in logic and/or linguistics. For each problem, three experts were consulted, and each expert rated 30 problems. More precisely, the experts were instructed to re-consider each problem and be especially wary of missing hypotheses. If they considered the entailment to hold, we still gave the instruction to optionally mention any additional implicit hypothesis that they would be using.",
    "answer_b": "What is the size of the corpus?\n< Explanation: We used a crowd of experts to identify missing hypotheses. We used a simple heuristic to identify missing hypotheses. We asked the experts to identify the missing hypotheses in the corpus. We then used the results of the crowd to identify the missing hypotheses in the corpus. We used a simple heuristic to identify missing hypotheses. We asked the experts to identify the missing hypotheses in the corpus. We then used the results of the crowd to identify the missing hypotheses in the corpus. We used a simple heuristic to identify missing hypotheses. We asked the experts to identify the missing hypotheses in the corpus. We then used",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow were missing hypotheses discovered?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe problems were then re-rated by experts in logic and/or linguistics.. Explanation: The problems were then re-rated by experts in logic and/or linguistics. For each problem, three experts were consulted, and each expert rated 30 problems. More precisely, the experts were instructed to re-consider each problem and be especially wary of missing hypotheses. If they considered the entailment to hold, we still gave the instruction to optionally mention any additional implicit hypothesis that they would be using.\n\nAnswer B:\nWhat is the size of the corpus?\n< Explanation: We used a crowd of experts to identify missing hypotheses. We used a simple heuristic to identify missing hypotheses. We asked the experts to identify the missing hypotheses in the corpus. We then used the results of the crowd to identify the missing hypotheses in the corpus. We used a simple heuristic to identify missing hypotheses. We asked the experts to identify the missing hypotheses in the corpus. We then used the results of the crowd to identify the missing hypotheses in the corpus. We used a simple heuristic to identify missing hypotheses. We asked the experts to identify the missing hypotheses in the corpus. We then used\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.07042",
    "question": "Which aspects of response generation do they evaluate on?",
    "gold_answer": "",
    "answer_a": "Explanation: We evaluate the performance of the proposed model on the following aspects: (1) the diversity of the generated responses, (2) the informativeness of the generated responses, (3) the quality of the generated responses, and (4) the performance of the retrieval-based models. We also evaluate the performance of the proposed model on the following aspects: (1) the diversity of the generated responses, (2) the informativeness of the generated responses, (3) the quality of the generated responses, and (4) the performance of the retrieval-based models. We also evaluate the performance of the proposed model on the following aspects",
    "answer_b": "fluency. Explanation: We evaluate our model on four criteria: fluency, relevance, diversity and originality. We employ Embedding Average (Average), Embedding Extrema (Extrema), and Embedding Greedy (Greedy) BIBREF35 to evaluate response relevance, which are better correlated with human judgment than BLEU.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich aspects of response generation do they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We evaluate the performance of the proposed model on the following aspects: (1) the diversity of the generated responses, (2) the informativeness of the generated responses, (3) the quality of the generated responses, and (4) the performance of the retrieval-based models. We also evaluate the performance of the proposed model on the following aspects: (1) the diversity of the generated responses, (2) the informativeness of the generated responses, (3) the quality of the generated responses, and (4) the performance of the retrieval-based models. We also evaluate the performance of the proposed model on the following aspects\n\nAnswer B:\nfluency. Explanation: We evaluate our model on four criteria: fluency, relevance, diversity and originality. We employ Embedding Average (Average), Embedding Extrema (Extrema), and Embedding Greedy (Greedy) BIBREF35 to evaluate response relevance, which are better correlated with human judgment than BLEU.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.07042",
    "question": "Which dataset do they evaluate on?",
    "gold_answer": "",
    "answer_a": "How large is the dataset?\n< Explanation: We use the dataset of 100,000 responses from 100,000 users. The dataset is divided into training and test sets. The training set is used to train the generative model, and the test set is used to evaluate the performance of the generative model. The test set is divided into 1000 training and 1000 test sets. The training set is used to train the retrieval model, and the test set is used to evaluate the performance of the retrieval model. The test set is divided into 10",
    "answer_b": "a large scale Chinese conversation corpus. Explanation: Our experiments are conducted on a large scale Chinese conversation corpus comprised of 20 million context-response pairs. We compare our model with generative models and retrieval models in terms of fluency, relevance, diversity and originality. The experiments show that our method outperforms traditional generative models on relevance, diversity and originality. We further find that the revised response achieves better relevance compared to its prototype and other retrieval results, demonstrating that the editing process does not only promote response originality but also improve the relevance of retrieval results.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dataset do they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow large is the dataset?\n< Explanation: We use the dataset of 100,000 responses from 100,000 users. The dataset is divided into training and test sets. The training set is used to train the generative model, and the test set is used to evaluate the performance of the generative model. The test set is divided into 1000 training and 1000 test sets. The training set is used to train the retrieval model, and the test set is used to evaluate the performance of the retrieval model. The test set is divided into 10\n\nAnswer B:\na large scale Chinese conversation corpus. Explanation: Our experiments are conducted on a large scale Chinese conversation corpus comprised of 20 million context-response pairs. We compare our model with generative models and retrieval models in terms of fluency, relevance, diversity and originality. The experiments show that our method outperforms traditional generative models on relevance, diversity and originality. We further find that the revised response achieves better relevance compared to its prototype and other retrieval results, demonstrating that the editing process does not only promote response originality but also improve the relevance of retrieval results.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.07042",
    "question": "What model architecture do they use for the decoder?",
    "gold_answer": "",
    "answer_a": "What is the output of the decoder?\n< Explanation: The decoder is a simple recurrent neural network (RNN) with a single hidden layer. The input is a context vector and the output is a response vector. The decoder is trained using the following loss function: where is the decoder output, is the decoder input, is the decoder hidden state, is the decoder output, is the decoder input, is the decoder hidden state, is the decoder output, is the decoder input, is the decoder hidden state, is the decoder output, is the decoder input, is the decoder hidden state, is the decoder output, is the decoder input,",
    "answer_b": "a GRU language model. Explanation: . The decoder is a GRU language model, that regards the concatenation of last step word embedding and the edit vector as inputs, and predicts the next word with an attention mechanism.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat model architecture do they use for the decoder?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the output of the decoder?\n< Explanation: The decoder is a simple recurrent neural network (RNN) with a single hidden layer. The input is a context vector and the output is a response vector. The decoder is trained using the following loss function: where is the decoder output, is the decoder input, is the decoder hidden state, is the decoder output, is the decoder input, is the decoder hidden state, is the decoder output, is the decoder input, is the decoder hidden state, is the decoder output, is the decoder input, is the decoder hidden state, is the decoder output, is the decoder input,\n\nAnswer B:\na GRU language model. Explanation: . The decoder is a GRU language model, that regards the concatenation of last step word embedding and the edit vector as inputs, and predicts the next word with an attention mechanism.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1806.07042",
    "question": "What do they use as the pre-defined index of prototype responses?",
    "gold_answer": "",
    "answer_a": "similar context INLINEFORM1 and its associated response INLINEFORM2. Explanation: Given a conversational context INLINEFORM0 , we first retrieve a similar context INLINEFORM1 and its associated response INLINEFORM2 from a pre-defined index, which are called prototype context and prototype response respectively.",
    "answer_b": "How do they design the edit vector?\n< Explanation: We use the following pre-defined index of prototype responses: (1) the number of words in the response, (2) the number of words in the response, (3) the number of words in the response, (4) the number of words in the response, (5) the number of words in the response, (6) the number of words in the response, (7) the number of words in the response, (8) the number of words in the response, (9) the number of words in the response, (10) the number of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat do they use as the pre-defined index of prototype responses?\n\nGround Truth Answer:\n\n\nAnswer A:\nsimilar context INLINEFORM1 and its associated response INLINEFORM2. Explanation: Given a conversational context INLINEFORM0 , we first retrieve a similar context INLINEFORM1 and its associated response INLINEFORM2 from a pre-defined index, which are called prototype context and prototype response respectively.\n\nAnswer B:\nHow do they design the edit vector?\n< Explanation: We use the following pre-defined index of prototype responses: (1) the number of words in the response, (2) the number of words in the response, (3) the number of words in the response, (4) the number of words in the response, (5) the number of words in the response, (6) the number of words in the response, (7) the number of words in the response, (8) the number of words in the response, (9) the number of words in the response, (10) the number of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.09241",
    "question": "what are all the datasets they experiment with?",
    "gold_answer": "",
    "answer_a": "CoQA. Explanation: We use 10 datasets. For answer extraction datasets in which a reader chooses a text span in a given context, we use (1) CoQA BIBREF17, (2) DuoRC BIBREF18, (3) HotpotQA (distractor) BIBREF19, (4) SQuAD v1.1 BIBREF0, and (5) SQuAD v2.0 BIBREF20. For multiple choice datasets in which a reader chooses a correct option from multiple options, we use (6) ARC (Challenge) BIBREF21, (7) MCTest BIBREF22, (8) MultiRC BIBREF23, (9) RACE BIBREF24, and (10) SWAG BIBREF25.",
    "answer_b": "< Explanation: We evaluate the proposed method on 10 datasets (e.g., CoQA, SQuAD v2.0, and RACE) with a strong baseline model. The datasets are collected from the following sources: CoQA (CoQA), SQuAD v2.0 (SQuAD v2.0), and RACE (RACE). The datasets are collected from the following sources: CoQA (CoQA), SQuAD v2.0 (SQuAD v2.0), and RACE (RACE). The datasets are collected from the following sources: CoQA (Co",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are all the datasets they experiment with?\n\nGround Truth Answer:\n\n\nAnswer A:\nCoQA. Explanation: We use 10 datasets. For answer extraction datasets in which a reader chooses a text span in a given context, we use (1) CoQA BIBREF17, (2) DuoRC BIBREF18, (3) HotpotQA (distractor) BIBREF19, (4) SQuAD v1.1 BIBREF0, and (5) SQuAD v2.0 BIBREF20. For multiple choice datasets in which a reader chooses a correct option from multiple options, we use (6) ARC (Challenge) BIBREF21, (7) MCTest BIBREF22, (8) MultiRC BIBREF23, (9) RACE BIBREF24, and (10) SWAG BIBREF25.\n\nAnswer B:\n< Explanation: We evaluate the proposed method on 10 datasets (e.g., CoQA, SQuAD v2.0, and RACE) with a strong baseline model. The datasets are collected from the following sources: CoQA (CoQA), SQuAD v2.0 (SQuAD v2.0), and RACE (RACE). The datasets are collected from the following sources: CoQA (CoQA), SQuAD v2.0 (SQuAD v2.0), and RACE (RACE). The datasets are collected from the following sources: CoQA (Co\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.09241",
    "question": "what was the baseline model?",
    "gold_answer": "",
    "answer_a": "BERT-large BIBREF3. Explanation: Models. As the baseline model, we used BERT-large BIBREF3. We fine-tuned it on the original training set of each dataset and evaluated it on a modified development set. For $\\sigma _4$ vocabulary anonymization, we train the model after the anonymization.",
    "answer_b": "What was the baseline model?\n< Explanation: The baseline model is a simple model that uses the same features as the model in the original paper. The model is trained on the same dataset as the model in the original paper. The baseline model is trained on the same dataset as the model in the original paper. The baseline model is trained on the same dataset as the model in the original paper. The baseline model is trained on the same dataset as the model in the original paper. The baseline model is trained on the same dataset as the model in the original paper. The baseline model is trained on the same dataset as the model in the original",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat was the baseline model?\n\nGround Truth Answer:\n\n\nAnswer A:\nBERT-large BIBREF3. Explanation: Models. As the baseline model, we used BERT-large BIBREF3. We fine-tuned it on the original training set of each dataset and evaluated it on a modified development set. For $\\sigma _4$ vocabulary anonymization, we train the model after the anonymization.\n\nAnswer B:\nWhat was the baseline model?\n< Explanation: The baseline model is a simple model that uses the same features as the model in the original paper. The model is trained on the same dataset as the model in the original paper. The baseline model is trained on the same dataset as the model in the original paper. The baseline model is trained on the same dataset as the model in the original paper. The baseline model is trained on the same dataset as the model in the original paper. The baseline model is trained on the same dataset as the model in the original paper. The baseline model is trained on the same dataset as the model in the original\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.11637",
    "question": "What do they mean by explicit selection of most relevant segments?",
    "gold_answer": "",
    "answer_a": "Output: The most relevant segments are selected by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a",
    "answer_b": "It is meant that only most contributive k elements are reserved, while other elements are removed.. Explanation: Explicit Sparse Transformer is still based on the Transformer framework. The difference is in the implementation of self-attention. The attention is degenerated to the sparse attention through top-$k$ selection. In this way, the most contributive components for attention are reserved and the other irrelevant information are removed.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat do they mean by explicit selection of most relevant segments?\n\nGround Truth Answer:\n\n\nAnswer A:\nOutput: The most relevant segments are selected by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a weighted sum of the attention weights. The weights are computed by a\n\nAnswer B:\nIt is meant that only most contributive k elements are reserved, while other elements are removed.. Explanation: Explicit Sparse Transformer is still based on the Transformer framework. The difference is in the implementation of self-attention. The attention is degenerated to the sparse attention through top-$k$ selection. In this way, the most contributive components for attention are reserved and the other irrelevant information are removed.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.11637",
    "question": "What datasets they used for evaluation?",
    "gold_answer": "",
    "answer_a": "For En-De translation the newstest 2014 set from WMT 2014 En-De translation dataset, for En-Vi translation the tst2013 from IWSLT 2015 dataset. and for De-En translation the teste set from IWSLT 2014.. Explanation: To evaluate the performance of Explicit Sparse Transformer in NMT, we conducted experiments on three NMT tasks, English-to-German translation (En-De) with a large dataset, English-to-Vietnamese (En-Vi) translation and German-to-English translation (De-En) with two datasets of medium size. For En-De, we trained Explicit Sparse Transformer on the standard dataset for WMT 2014 En-De translation. The dataset consists of around 4.5 million sentence pairs. The source and target languages share a vocabulary of 32K sub-word units. We used the newstest 2013 for validation and the newstest 2014 as our test set. We report the results on the test set.\n\nFor En-Vi, we trained our model on the dataset in IWSLT 2015 BIBREF20. The dataset consists of around 133K sentence pairs from translated TED talks. The vocabulary size for source language is around 17,200 and that for target language is around 7,800. We used tst2012 for validation, and tst2013 for testing and report the testing results. For De-En, we used the dataset in IWSLT 2014. The training set contains 160K sentence pairs and the validation set contains 7K sentences. Following BIBREF21, we used the same test set with around 7K sentences.",
    "answer_b": "< Explanation: We use the following datasets for evaluation: (1) The Stanford Sentiment Treebank (SST) BIBREF10 , (2) The Stanford Sentiment Treebank (SST) BIBREF11 , (3) The Stanford Sentiment Treebank (SST) BIBREF12 , (4) The Stanford Sentiment Treebank (SST) BIBREF13 , (5) The Stanford Sentiment Treebank (SST) BIBREF14 , (6) The Stanford Sentiment Treebank (SST) BIBREF15 , (7",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets they used for evaluation?\n\nGround Truth Answer:\n\n\nAnswer A:\nFor En-De translation the newstest 2014 set from WMT 2014 En-De translation dataset, for En-Vi translation the tst2013 from IWSLT 2015 dataset. and for De-En translation the teste set from IWSLT 2014.. Explanation: To evaluate the performance of Explicit Sparse Transformer in NMT, we conducted experiments on three NMT tasks, English-to-German translation (En-De) with a large dataset, English-to-Vietnamese (En-Vi) translation and German-to-English translation (De-En) with two datasets of medium size. For En-De, we trained Explicit Sparse Transformer on the standard dataset for WMT 2014 En-De translation. The dataset consists of around 4.5 million sentence pairs. The source and target languages share a vocabulary of 32K sub-word units. We used the newstest 2013 for validation and the newstest 2014 as our test set. We report the results on the test set.\n\nFor En-Vi, we trained our model on the dataset in IWSLT 2015 BIBREF20. The dataset consists of around 133K sentence pairs from translated TED talks. The vocabulary size for source language is around 17,200 and that for target language is around 7,800. We used tst2012 for validation, and tst2013 for testing and report the testing results. For De-En, we used the dataset in IWSLT 2014. The training set contains 160K sentence pairs and the validation set contains 7K sentences. Following BIBREF21, we used the same test set with around 7K sentences.\n\nAnswer B:\n< Explanation: We use the following datasets for evaluation: (1) The Stanford Sentiment Treebank (SST) BIBREF10 , (2) The Stanford Sentiment Treebank (SST) BIBREF11 , (3) The Stanford Sentiment Treebank (SST) BIBREF12 , (4) The Stanford Sentiment Treebank (SST) BIBREF13 , (5) The Stanford Sentiment Treebank (SST) BIBREF14 , (6) The Stanford Sentiment Treebank (SST) BIBREF15 , (7\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.04326",
    "question": "How big is this dataset?",
    "gold_answer": "",
    "answer_a": "What are the evaluation metrics?\n< Explanation: The dataset contains 10,000 passages, each of which is annotated by 100 students. The passages are divided into 10 sets, each of which contains 100 passages. The evaluation metrics are as follows: Recall, Precision, F1-score, and Average Precision. The evaluation results are shown in Table TABREF10. The results show that the state-of-the-art models have an outstanding ability to capture biases contained in the dataset with high accuracy on EASY set. However, they struggle on HARD set with poor performance near that of random guess",
    "answer_b": "6,138 logical reasoning questions. Explanation: We construct a dataset containing 6,138 logical reasoning questions sourced from open websites and books.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big is this dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the evaluation metrics?\n< Explanation: The dataset contains 10,000 passages, each of which is annotated by 100 students. The passages are divided into 10 sets, each of which contains 100 passages. The evaluation metrics are as follows: Recall, Precision, F1-score, and Average Precision. The evaluation results are shown in Table TABREF10. The results show that the state-of-the-art models have an outstanding ability to capture biases contained in the dataset with high accuracy on EASY set. However, they struggle on HARD set with poor performance near that of random guess\n\nAnswer B:\n6,138 logical reasoning questions. Explanation: We construct a dataset containing 6,138 logical reasoning questions sourced from open websites and books.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.04326",
    "question": "How are biases identified in the dataset?",
    "gold_answer": "",
    "answer_a": "What are the biases?\n< Explanation: We first identify the bias points in the dataset by comparing the predicted probabilities of the model with the ground truth. We then separate the dataset into EASY and HARD sets. The EASY set is used to evaluate the model's performance on the ground truth, and the HARD set is used to evaluate the model's performance on the predicted probabilities. The model is trained on the EASY set and tested on the HARD set. The model's performance on the HARD set is then used to evaluate the model's performance on the EASY set. The model is then retrained on the EASY set and tested",
    "answer_b": "we feed the five strong baseline models (GPT, GPT-2, BERT$_{\\small \\textsc {BASE}}$, XLNet$_{\\small \\textsc {BASE}}$ and RoBERTa$_{\\small \\textsc {BASE}}$) with ONLY THE ANSWER OPTIONS for each problem. Explanation: As mentioned earlier, biases prevalently exist in human-annotated datasets BIBREF16, BIBREF17, BIBREF18, BIBREF42, which are often exploited by models to perform well without truly understanding the text. Therefore, it is necessary to find out the biased data points in ReClor in order to evaluate models in a more comprehensive manner BIBREF43. To this end, we feed the five strong baseline models (GPT, GPT-2, BERT$_{\\small \\textsc {BASE}}$, XLNet$_{\\small \\textsc {BASE}}$ and RoBERTa$_{\\small \\textsc {BASE}}$) with ONLY THE ANSWER OPTIONS for each problem. In other words, we purposely remove the context and question in the inputs. In this way, we are able to identify those problems that can be answered correctly by merely exploiting the biases in answer options without knowing the relevant context and question. However, the setting of this task is a multiple-choice question with 4 probable options, and even a chance baseline could have 25% probability to get it right. To eliminate the effect of random guess, we set four different random seeds for each model and pick the data points that are predicted correctly in all four cases to form the EASY set. Then, the data points which are predicted correctly by the models at random could be nearly eliminated, since any data point only has a probability of $(25\\%)^{4}=0.39\\%$ to be guessed right consecutively for four times. Then we unite the sets of data points that are consistently predicted right by each model, because intuitively different models may learn different biases of the dataset.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are biases identified in the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the biases?\n< Explanation: We first identify the bias points in the dataset by comparing the predicted probabilities of the model with the ground truth. We then separate the dataset into EASY and HARD sets. The EASY set is used to evaluate the model's performance on the ground truth, and the HARD set is used to evaluate the model's performance on the predicted probabilities. The model is trained on the EASY set and tested on the HARD set. The model's performance on the HARD set is then used to evaluate the model's performance on the EASY set. The model is then retrained on the EASY set and tested\n\nAnswer B:\nwe feed the five strong baseline models (GPT, GPT-2, BERT$_{\\small \\textsc {BASE}}$, XLNet$_{\\small \\textsc {BASE}}$ and RoBERTa$_{\\small \\textsc {BASE}}$) with ONLY THE ANSWER OPTIONS for each problem. Explanation: As mentioned earlier, biases prevalently exist in human-annotated datasets BIBREF16, BIBREF17, BIBREF18, BIBREF42, which are often exploited by models to perform well without truly understanding the text. Therefore, it is necessary to find out the biased data points in ReClor in order to evaluate models in a more comprehensive manner BIBREF43. To this end, we feed the five strong baseline models (GPT, GPT-2, BERT$_{\\small \\textsc {BASE}}$, XLNet$_{\\small \\textsc {BASE}}$ and RoBERTa$_{\\small \\textsc {BASE}}$) with ONLY THE ANSWER OPTIONS for each problem. In other words, we purposely remove the context and question in the inputs. In this way, we are able to identify those problems that can be answered correctly by merely exploiting the biases in answer options without knowing the relevant context and question. However, the setting of this task is a multiple-choice question with 4 probable options, and even a chance baseline could have 25% probability to get it right. To eliminate the effect of random guess, we set four different random seeds for each model and pick the data points that are predicted correctly in all four cases to form the EASY set. Then, the data points which are predicted correctly by the models at random could be nearly eliminated, since any data point only has a probability of $(25\\%)^{4}=0.39\\%$ to be guessed right consecutively for four times. Then we unite the sets of data points that are consistently predicted right by each model, because intuitively different models may learn different biases of the dataset.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.04128",
    "question": "How do they deal with imbalanced datasets?",
    "gold_answer": "",
    "answer_a": "What is the performance of the system on the dataset?\n< Explanation: We first introduce a new dataset for text normalization, which is called the News Dataset. The dataset contains 1000 news articles from the Chinese news website of the Chinese Academy of Sciences. The dataset is divided into 1000 sentences, each of which is 100 words long. The dataset contains 1000 sentences, and each sentence is labeled as either positive or negative. The dataset is divided into 1000 sentences, each of which is labeled as either positive or negative. The dataset contains 1000",
    "answer_b": "data expansion using oversampling. Explanation: We have tried different methods to deal with this problem. The first method is data expansion using oversampling. Attempts include duplicating the text with low pattern proportion, replacing first few characters with paddings in the window, randomly changing digits, and shifting the context window. The other method is to add loss control in the model as mentioned in SECREF2.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they deal with imbalanced datasets?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of the system on the dataset?\n< Explanation: We first introduce a new dataset for text normalization, which is called the News Dataset. The dataset contains 1000 news articles from the Chinese news website of the Chinese Academy of Sciences. The dataset is divided into 1000 sentences, each of which is 100 words long. The dataset contains 1000 sentences, and each sentence is labeled as either positive or negative. The dataset is divided into 1000 sentences, each of which is labeled as either positive or negative. The dataset contains 1000\n\nAnswer B:\ndata expansion using oversampling. Explanation: We have tried different methods to deal with this problem. The first method is data expansion using oversampling. Attempts include duplicating the text with low pattern proportion, replacing first few characters with paddings in the window, randomly changing digits, and shifting the context window. The other method is to add loss control in the model as mentioned in SECREF2.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.04128",
    "question": "What models do they compare to?",
    "gold_answer": "",
    "answer_a": "What is the baseline?\n< Explanation: We compare our proposed system with the following baselines: (1) a rule-based system (RBS) that uses a set of hand-written rules to normalize text, (2) a neural system that uses a neural model to normalize text, and (3) a hybrid system that combines the two models. We also compare our proposed system with the following baselines: (1) a rule-based system (RBS) that uses a set of hand-written rules to normalize text, (2) a neural system that uses a neural model to normalize text, and (3) a hybrid system that",
    "answer_b": "rule-based TN model. Explanation: The rule-based TN model can handle the TN task alone and is the baseline in our experiments.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat models do they compare to?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the baseline?\n< Explanation: We compare our proposed system with the following baselines: (1) a rule-based system (RBS) that uses a set of hand-written rules to normalize text, (2) a neural system that uses a neural model to normalize text, and (3) a hybrid system that combines the two models. We also compare our proposed system with the following baselines: (1) a rule-based system (RBS) that uses a set of hand-written rules to normalize text, (2) a neural system that uses a neural model to normalize text, and (3) a hybrid system that\n\nAnswer B:\nrule-based TN model. Explanation: The rule-based TN model can handle the TN task alone and is the baseline in our experiments.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.04128",
    "question": "What text preprocessing tasks do they focus on?",
    "gold_answer": "",
    "answer_a": "normalize unreadable numbers, symbols or characters. Explanation: In Text-To-Speech (TTS), text normalization is an essential procedure to normalize unreadable numbers, symbols or characters, such as transforming \u201c$20\u201d to \u201ctwenty dollars\u201d and \u201c@\u201d to \u201cat\u201d, into words that can be used in speech synthesis.",
    "answer_b": "Explanation: The proposed system focuses on text normalization tasks, which are divided into two categories: text normalization and text preprocessing. The text normalization task is to convert the text into a standardized form. The text preprocessing task is to convert the text into a form that is easier to process. The two tasks are combined in the proposed system. The text normalization task is divided into two subtasks: rule-based normalization and neural normalization. The rule-based normalization task is to use a set of hand-written rules to normalize the text. The neural normalization task is to use a neural model to normalize the text. The neural model is a multi-head self-attention",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat text preprocessing tasks do they focus on?\n\nGround Truth Answer:\n\n\nAnswer A:\nnormalize unreadable numbers, symbols or characters. Explanation: In Text-To-Speech (TTS), text normalization is an essential procedure to normalize unreadable numbers, symbols or characters, such as transforming \u201c$20\u201d to \u201ctwenty dollars\u201d and \u201c@\u201d to \u201cat\u201d, into words that can be used in speech synthesis.\n\nAnswer B:\nExplanation: The proposed system focuses on text normalization tasks, which are divided into two categories: text normalization and text preprocessing. The text normalization task is to convert the text into a standardized form. The text preprocessing task is to convert the text into a form that is easier to process. The two tasks are combined in the proposed system. The text normalization task is divided into two subtasks: rule-based normalization and neural normalization. The rule-based normalization task is to use a set of hand-written rules to normalize the text. The neural normalization task is to use a neural model to normalize the text. The neural model is a multi-head self-attention\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.07940",
    "question": "How many tweets are in the dataset?",
    "gold_answer": "",
    "answer_a": "What is the average length of a tweet?\nThere are 1,000,000 tweets in the dataset. The average length of a tweet is 100 characters.\n\nQuestion: What is the average length of a tweet in terms of words?\nThe average length of a tweet is 100 characters. The average length of a tweet in terms of words is 100 words.\n\nExplanation: The average length of a tweet is 100 characters. The average length of a tweet in terms of words is 100 words. The average length of a tweet is 100 characters",
    "answer_b": "670 tweets. Explanation: However, in this study, we target only at the first three categories which amounts to a total of 980 annotations in 670 tweets in Turkish.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many tweets are in the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the average length of a tweet?\nThere are 1,000,000 tweets in the dataset. The average length of a tweet is 100 characters.\n\nQuestion: What is the average length of a tweet in terms of words?\nThe average length of a tweet is 100 characters. The average length of a tweet in terms of words is 100 words.\n\nExplanation: The average length of a tweet is 100 characters. The average length of a tweet in terms of words is 100 words. The average length of a tweet is 100 characters\n\nAnswer B:\n670 tweets. Explanation: However, in this study, we target only at the first three categories which amounts to a total of 980 annotations in 670 tweets in Turkish.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.07940",
    "question": "What categories do they look at?",
    "gold_answer": "",
    "answer_a": "PERSON, LOCATION, and ORGANIZATION. Explanation: In this study, we analyze the basic named entities (of type PERSON, LOCATION, and ORGANIZATION, henceforth, PLOs) in the annotated dataset compiled in BIBREF5, with respect to their being well-formed canonical names or name variants.",
    "answer_b": "How are they annotated?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Table 1: The categories of named entities in the dataset. </td>\n</tr>\n</table>\nExplanation: The categories of named entities in the dataset are as follows: 1) names of people, 2) names of places, 3) names of organizations, 4) names of animals, 5) names of plants, 6) names of animals and plants, 7) names of animals and plants and their synonyms, 8) names of animals and plants and their synonyms and",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat categories do they look at?\n\nGround Truth Answer:\n\n\nAnswer A:\nPERSON, LOCATION, and ORGANIZATION. Explanation: In this study, we analyze the basic named entities (of type PERSON, LOCATION, and ORGANIZATION, henceforth, PLOs) in the annotated dataset compiled in BIBREF5, with respect to their being well-formed canonical names or name variants.\n\nAnswer B:\nHow are they annotated?\n<sep><table cellspacing=\"0\" cellpadding=\"0\">\n<tr>\n<td align=\"left\">Table 1: The categories of named entities in the dataset. </td>\n</tr>\n</table>\nExplanation: The categories of named entities in the dataset are as follows: 1) names of people, 2) names of places, 3) names of organizations, 4) names of animals, 5) names of plants, 6) names of animals and plants, 7) names of animals and plants and their synonyms, 8) names of animals and plants and their synonyms and\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1606.07947",
    "question": "Which knowledge destilation methods do they introduce?",
    "gold_answer": "",
    "answer_a": "standard knowledge distillation for NMT. Explanation: With this difference in mind, we experiment with standard knowledge distillation for NMT and also propose two new versions of the approach that attempt to approximately match the sequence-level (as opposed to word-level) distribution of the teacher network.",
    "answer_b": "What are the results?\nYes, we introduce two new sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich knowledge destilation methods do they introduce?\n\nGround Truth Answer:\n\n\nAnswer A:\nstandard knowledge distillation for NMT. Explanation: With this difference in mind, we experiment with standard knowledge distillation for NMT and also propose two new versions of the approach that attempt to approximately match the sequence-level (as opposed to word-level) distribution of the teacher network.\n\nAnswer B:\nWhat are the results?\nYes, we introduce two new sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1606.07947",
    "question": "What type of weight pruning do they use?",
    "gold_answer": "",
    "answer_a": "How effective is it?\n< Explanation: We use the weight pruning method described in Bucila et al. (2006) to reduce the size of the teacher model. We use the same weight pruning method to reduce the size of the student model. We use the same weight pruning method to reduce the size of the teacher model. We use the same weight pruning method to reduce the size of the student model. We use the same weight pruning method to reduce the size of the teacher model. We use the same weight pruning method to reduce the size of the student model. We use the same weight pruning method to reduce the size",
    "answer_b": "pruning parameters by removing the weights with the lowest absolute values. Explanation: We take our best English INLINEFORM1 German student model ( INLINEFORM2 Seq-KD INLINEFORM3 Seq-Inter) and prune INLINEFORM4 of the parameters by removing the weights with the lowest absolute values.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of weight pruning do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow effective is it?\n< Explanation: We use the weight pruning method described in Bucila et al. (2006) to reduce the size of the teacher model. We use the same weight pruning method to reduce the size of the student model. We use the same weight pruning method to reduce the size of the teacher model. We use the same weight pruning method to reduce the size of the student model. We use the same weight pruning method to reduce the size of the teacher model. We use the same weight pruning method to reduce the size of the student model. We use the same weight pruning method to reduce the size\n\nAnswer B:\npruning parameters by removing the weights with the lowest absolute values. Explanation: We take our best English INLINEFORM1 German student model ( INLINEFORM2 Seq-KD INLINEFORM3 Seq-Inter) and prune INLINEFORM4 of the parameters by removing the weights with the lowest absolute values.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1606.07947",
    "question": "Which dataset do they train on?",
    "gold_answer": "",
    "answer_a": "How do they evaluate their models?\n< Explanation: We use the English to German NMT dataset from the Stanford NMT corpus (Stoer et al., 2014). We use the German to English NMT dataset from the Stanford NMT corpus (Stoer et al., 2014) and the German to English NMT dataset from the Stanford NMT corpus (Stoer et al., 2014) for training and evaluation, respectively. We use the German to English NMT dataset from the Stanford NMT corpus (Stoer et al., 2014)",
    "answer_b": "WMT 2014. Explanation: To test out these approaches, we conduct two sets of NMT experiments: high resource (English INLINEFORM0 German) and low resource (Thai INLINEFORM1 English).\n\nThe English-German data comes from WMT 2014.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich dataset do they train on?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate their models?\n< Explanation: We use the English to German NMT dataset from the Stanford NMT corpus (Stoer et al., 2014). We use the German to English NMT dataset from the Stanford NMT corpus (Stoer et al., 2014) and the German to English NMT dataset from the Stanford NMT corpus (Stoer et al., 2014) for training and evaluation, respectively. We use the German to English NMT dataset from the Stanford NMT corpus (Stoer et al., 2014)\n\nAnswer B:\nWMT 2014. Explanation: To test out these approaches, we conduct two sets of NMT experiments: high resource (English INLINEFORM0 German) and low resource (Thai INLINEFORM1 English).\n\nThe English-German data comes from WMT 2014.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.00957",
    "question": "What baselines is the neural relation extractor compared to?",
    "gold_answer": "",
    "answer_a": "What is the performance of the neural relation extractor?\n<sep>baseline: a simple relation extractor that uses the Wikipedia article as a reference. Explanation: We compare our method with the following baselines: (1) a simple relation extractor that uses the Wikipedia article as a reference. (2) a simple relation extractor that uses the Wikipedia article as a reference and a Wikipedia article as a reference. (3) a simple relation extractor that uses the Wikipedia article as a reference and a Wikipedia article as a reference. (4) a simple relation extractor that uses the Wikipedia article as a reference and a Wikipedia article as a reference. (5)",
    "answer_b": "BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10. Explanation: (",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat baselines is the neural relation extractor compared to?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of the neural relation extractor?\n<sep>baseline: a simple relation extractor that uses the Wikipedia article as a reference. Explanation: We compare our method with the following baselines: (1) a simple relation extractor that uses the Wikipedia article as a reference. (2) a simple relation extractor that uses the Wikipedia article as a reference and a Wikipedia article as a reference. (3) a simple relation extractor that uses the Wikipedia article as a reference and a Wikipedia article as a reference. (4) a simple relation extractor that uses the Wikipedia article as a reference and a Wikipedia article as a reference. (5)\n\nAnswer B:\nBIBREF7 , BIBREF8 , BIBREF9 , BIBREF10. Explanation: (\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.00957",
    "question": "What additional evidence they use?",
    "gold_answer": "",
    "answer_a": "Wikipedia sentences that validate or support KB facts. Explanation: these",
    "answer_b": "What is the Wikipedia evidence?\n< Explanation: We use the Wikipedia page for each question to retrieve the candidate answers. We use the following evidence to validate the answers: (1) the Wikipedia page for each question; (2) the Wikipedia page for each answer; (3) the Wikipedia page for each candidate answer. We use the Wikipedia page for each question to retrieve the candidate answers. We use the following evidence to validate the answers: (1) the Wikipedia page for each question; (2) the Wikipedia page for each answer; (3) the Wikipedia page for each candidate answer. We use the following evidence to validate the answers",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat additional evidence they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nWikipedia sentences that validate or support KB facts. Explanation: these\n\nAnswer B:\nWhat is the Wikipedia evidence?\n< Explanation: We use the Wikipedia page for each question to retrieve the candidate answers. We use the following evidence to validate the answers: (1) the Wikipedia page for each question; (2) the Wikipedia page for each answer; (3) the Wikipedia page for each candidate answer. We use the Wikipedia page for each question to retrieve the candidate answers. We use the following evidence to validate the answers: (1) the Wikipedia page for each question; (2) the Wikipedia page for each answer; (3) the Wikipedia page for each candidate answer. We use the following evidence to validate the answers\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.00957",
    "question": "How much improvement they get from the previous state-of-the-art?",
    "gold_answer": "",
    "answer_a": "What is the state-of-the-art?\n< Explanation: We compare our method with the state-of-the-art method of BIBREF10, which is based on a relation extraction method that uses a large amount of annotated training data. We also compare our method with the state-of-the-art method of BIBREF11, which is based on a relation extraction method that uses a large amount of annotated training data. We also compare our method with the state-of-the-art method of BIBREF12, which is based on a relation extraction method that uses a large amount of annotated training data. We also compare our method with the",
    "answer_b": "0.8 point improvement. Explanation: FLOAT SELECTED: Table 1: Results on the test set.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much improvement they get from the previous state-of-the-art?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the state-of-the-art?\n< Explanation: We compare our method with the state-of-the-art method of BIBREF10, which is based on a relation extraction method that uses a large amount of annotated training data. We also compare our method with the state-of-the-art method of BIBREF11, which is based on a relation extraction method that uses a large amount of annotated training data. We also compare our method with the state-of-the-art method of BIBREF12, which is based on a relation extraction method that uses a large amount of annotated training data. We also compare our method with the\n\nAnswer B:\n0.8 point improvement. Explanation: FLOAT SELECTED: Table 1: Results on the test set.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1603.00957",
    "question": "What is the previous state-of-the-art?",
    "gold_answer": "",
    "answer_a": "F1 score of 39.9 for semantic-based parsing methods. For information extraction methods, 49.4 using relation extraction, 40.8 using distributed representations, and 52.5 using neural networks models. Explanation: The state-of-the-art methods for this task can be roughly categorized into two streams. The first is based on semantic parsing BIBREF3 , BIBREF4 , which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem BIBREF4 , BIBREF5 , BIBREF6 .",
    "answer_b": "What is the difference between the proposed method and the state-of-the-art?\n<sep>Previous state-of-the-art: The previous state-of-the-art is the method of [1] which uses a simple relation extraction model to retrieve the candidate answers from Freebase. The method of [2] uses a relation extraction model to retrieve the candidate answers from Wikipedia. The method of [3] uses a relation extraction model to retrieve the candidate answers from Wikipedia and then uses a semantic parsing model to validate the answers. The method of [4] uses a relation extraction model to retrieve the candidate answers from Wikipedia and then uses a semantic parsing model to",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the previous state-of-the-art?\n\nGround Truth Answer:\n\n\nAnswer A:\nF1 score of 39.9 for semantic-based parsing methods. For information extraction methods, 49.4 using relation extraction, 40.8 using distributed representations, and 52.5 using neural networks models. Explanation: The state-of-the-art methods for this task can be roughly categorized into two streams. The first is based on semantic parsing BIBREF3 , BIBREF4 , which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem BIBREF4 , BIBREF5 , BIBREF6 .\n\nAnswer B:\nWhat is the difference between the proposed method and the state-of-the-art?\n<sep>Previous state-of-the-art: The previous state-of-the-art is the method of [1] which uses a simple relation extraction model to retrieve the candidate answers from Freebase. The method of [2] uses a relation extraction model to retrieve the candidate answers from Wikipedia. The method of [3] uses a relation extraction model to retrieve the candidate answers from Wikipedia and then uses a semantic parsing model to validate the answers. The method of [4] uses a relation extraction model to retrieve the candidate answers from Wikipedia and then uses a semantic parsing model to\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.08000",
    "question": "What is the architecture of the model?",
    "gold_answer": "",
    "answer_a": "What is the size of the model?\n< Explanation: We propose a neural architecture for fine-grained entity typing that leverages a greater amount of semantic context than prior work. We use a Transformer-based architecture with a Transformer encoder and a Transformer decoder. We use a Transformer encoder with a Transformer decoder. We use a Transformer encoder with a Transformer decoder. We use a Transformer encoder with a Transformer decoder. We use a Transformer encoder with a Transformer decoder. We use a Transformer encoder with a Transformer decoder. We use a Transformer encoder with a Transformer decoder. We use a Transformer encoder with a Transformer decoder. We use a Transformer encoder with a Transformer",
    "answer_b": "logistic regression. Explanation: General Model\nGiven a type embedding vector INLINEFORM0 and a featurizer INLINEFORM1 that takes entity INLINEFORM2 and its context INLINEFORM3 , we employ the logistic regression (as shown in fig:arch) to model the probability of INLINEFORM4 assigned INLINEFORM5 (i.e., INLINEFORM6 ) DISPLAYFORM0\n\nand we seek to learn a type embedding matrix INLINEFORM0 and a featurizer INLINEFORM1 such that DISPLAYFORM0",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the architecture of the model?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the model?\n< Explanation: We propose a neural architecture for fine-grained entity typing that leverages a greater amount of semantic context than prior work. We use a Transformer-based architecture with a Transformer encoder and a Transformer decoder. We use a Transformer encoder with a Transformer decoder. We use a Transformer encoder with a Transformer decoder. We use a Transformer encoder with a Transformer decoder. We use a Transformer encoder with a Transformer decoder. We use a Transformer encoder with a Transformer decoder. We use a Transformer encoder with a Transformer decoder. We use a Transformer encoder with a Transformer decoder. We use a Transformer encoder with a Transformer\n\nAnswer B:\nlogistic regression. Explanation: General Model\nGiven a type embedding vector INLINEFORM0 and a featurizer INLINEFORM1 that takes entity INLINEFORM2 and its context INLINEFORM3 , we employ the logistic regression (as shown in fig:arch) to model the probability of INLINEFORM4 assigned INLINEFORM5 (i.e., INLINEFORM6 ) DISPLAYFORM0\n\nand we seek to learn a type embedding matrix INLINEFORM0 and a featurizer INLINEFORM1 such that DISPLAYFORM0\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.08000",
    "question": "What fine-grained semantic types are considered?",
    "gold_answer": "",
    "answer_a": "Explanation: We consider three types of fine-grained semantic types: entity types, entity types and entity types. We use the following definitions for each type:\n\nEntity Types: We consider the following types of entity types:\n\nEntity Types: We consider the following types of entity types:\n\nEntity Types: We consider the following types of entity types:\n\nEntity Types: We consider the following types of entity types:\n\nEntity Types: We consider the following types of entity types:\n\nEntity Types: We consider the following types of entity types:\n\nEntity Types: We consider the following types of entity types:\n\nEntity Types: We consider the following types of entity types:\n\nEntity",
    "answer_b": "/other/event/accident, /person/artist/music, /other/product/mobile phone, /other/event/sports event, /other/product/car. Explanation: FLOAT SELECTED: Table 6: Type similarity.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat fine-grained semantic types are considered?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: We consider three types of fine-grained semantic types: entity types, entity types and entity types. We use the following definitions for each type:\n\nEntity Types: We consider the following types of entity types:\n\nEntity Types: We consider the following types of entity types:\n\nEntity Types: We consider the following types of entity types:\n\nEntity Types: We consider the following types of entity types:\n\nEntity Types: We consider the following types of entity types:\n\nEntity Types: We consider the following types of entity types:\n\nEntity Types: We consider the following types of entity types:\n\nEntity Types: We consider the following types of entity types:\n\nEntity\n\nAnswer B:\n/other/event/accident, /person/artist/music, /other/product/mobile phone, /other/event/sports event, /other/product/car. Explanation: FLOAT SELECTED: Table 6: Type similarity.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1804.08000",
    "question": "What hand-crafted features do other approaches use?",
    "gold_answer": "",
    "answer_a": "lexical and syntactic features. Explanation: These representations are then combined with hand-crafted features (e.g., lexical and syntactic features), and fed into a linear classifier with a fixed threshold.",
    "answer_b": "How do they compare to the neural architecture?\n< Explanation: We compare our approach to the following hand-crafted features: (1) word embeddings, (2) word embeddings with a bag of words representation, (3) word embeddings with a bag of words representation with a bag of word embeddings, (4) word embeddings with a bag of words representation with a bag of word embeddings with a bag of word embeddings with a bag of word embeddings with a bag of word embeddings with a bag of word embeddings with a bag of word embeddings with a bag of word embeddings with a bag of word embeddings with a bag of word embeddings with a bag",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat hand-crafted features do other approaches use?\n\nGround Truth Answer:\n\n\nAnswer A:\nlexical and syntactic features. Explanation: These representations are then combined with hand-crafted features (e.g., lexical and syntactic features), and fed into a linear classifier with a fixed threshold.\n\nAnswer B:\nHow do they compare to the neural architecture?\n< Explanation: We compare our approach to the following hand-crafted features: (1) word embeddings, (2) word embeddings with a bag of words representation, (3) word embeddings with a bag of words representation with a bag of word embeddings, (4) word embeddings with a bag of words representation with a bag of word embeddings with a bag of word embeddings with a bag of word embeddings with a bag of word embeddings with a bag of word embeddings with a bag of word embeddings with a bag of word embeddings with a bag of word embeddings with a bag of word embeddings with a bag\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.05803",
    "question": "What is the strong baseline model used?",
    "gold_answer": "",
    "answer_a": "What is the baseline model's performance?\n<sep>Answer: The baseline model is a simple model that uses a simple rule-based approach to resolve coreference. The model is trained on a small dataset of 10K questions. The baseline model is trained on a small dataset of 10K questions. The baseline model is trained on a small dataset of 10K questions. The baseline model is trained on a small dataset of 10K questions. The baseline model is trained on a small dataset of 10K questions. The baseline model is trained on a small dataset of 10K questions. The",
    "answer_b": "an uncased base BERT QA model BIBREF9 trained on SQuAD 1.1 BIBREF0. Explanation: We used an uncased base BERT QA model BIBREF9 trained on SQuAD 1.1 BIBREF0 as an adversary running in the background that attempted to answer the questions written by workers in real time, and the workers were able to submit their questions only if their answer did not match the adversary's prediction.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the strong baseline model used?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the baseline model's performance?\n<sep>Answer: The baseline model is a simple model that uses a simple rule-based approach to resolve coreference. The model is trained on a small dataset of 10K questions. The baseline model is trained on a small dataset of 10K questions. The baseline model is trained on a small dataset of 10K questions. The baseline model is trained on a small dataset of 10K questions. The baseline model is trained on a small dataset of 10K questions. The baseline model is trained on a small dataset of 10K questions. The\n\nAnswer B:\nan uncased base BERT QA model BIBREF9 trained on SQuAD 1.1 BIBREF0. Explanation: We used an uncased base BERT QA model BIBREF9 trained on SQuAD 1.1 BIBREF0 as an adversary running in the background that attempted to answer the questions written by workers in real time, and the workers were able to submit their questions only if their answer did not match the adversary's prediction.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.05803",
    "question": "What crowdsourcing platform did they obtain the data from?",
    "gold_answer": "",
    "answer_a": "Mechanical Turk. Explanation: We crowdsourced questions about these paragraphs on Mechanical Turk.",
    "answer_b": "What is the size of the dataset?\n<sep>Answer: We obtained the data from the Wikipedia Coreference Dataset (WCD) (Wikipedia, 2017). The dataset contains 24K span-selection questions that require resolving coreference among entities in over 4.7K paragraphs from Wikipedia. The dataset is publicly available at https://github.com/alexanderkowalski/wcd. The dataset is divided into 10K training and 14K test sets. The training set contains 10K questions, while the test set contains 4K questions. The dataset is annotated by",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat crowdsourcing platform did they obtain the data from?\n\nGround Truth Answer:\n\n\nAnswer A:\nMechanical Turk. Explanation: We crowdsourced questions about these paragraphs on Mechanical Turk.\n\nAnswer B:\nWhat is the size of the dataset?\n<sep>Answer: We obtained the data from the Wikipedia Coreference Dataset (WCD) (Wikipedia, 2017). The dataset contains 24K span-selection questions that require resolving coreference among entities in over 4.7K paragraphs from Wikipedia. The dataset is publicly available at https://github.com/alexanderkowalski/wcd. The dataset is divided into 10K training and 14K test sets. The training set contains 10K questions, while the test set contains 4K questions. The dataset is annotated by\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.02677",
    "question": "How large is the test set?",
    "gold_answer": "",
    "answer_a": "359 samples. Explanation: Our models are trained and evaluated on the WikiLarge dataset BIBREF10 which contains 296,402/2,000/359 samples (train/validation/test).",
    "answer_b": "What is the size of the WikiLarge dataset?\n< Explanation: The WikiLarge dataset contains 10,000 sentences, each of which is 100 words long. The dataset is split into 1000 training and 1000 test sentences. The training set is used to train a model and the test set is used to evaluate the model. The dataset is available at http://www.cs.cornell.edu/~gabriel/seq2seq/wiki-large.html\n\nThe WikiLarge dataset contains 10,000 sentences, each of which is 100 words long.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow large is the test set?\n\nGround Truth Answer:\n\n\nAnswer A:\n359 samples. Explanation: Our models are trained and evaluated on the WikiLarge dataset BIBREF10 which contains 296,402/2,000/359 samples (train/validation/test).\n\nAnswer B:\nWhat is the size of the WikiLarge dataset?\n< Explanation: The WikiLarge dataset contains 10,000 sentences, each of which is 100 words long. The dataset is split into 1000 training and 1000 test sentences. The training set is used to train a model and the test set is used to evaluate the model. The dataset is available at http://www.cs.cornell.edu/~gabriel/seq2seq/wiki-large.html\n\nThe WikiLarge dataset contains 10,000 sentences, each of which is 100 words long.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.02677",
    "question": "What does SARI measure?",
    "gold_answer": "",
    "answer_a": "How is it computed?\n< Explanation: The SARI metric is a standard metric for evaluating the performance of a sequence-to-sequence model. It is defined as the average of the log-likelihood of the target sequence given the source sequence and the model's output. The SARI metric is computed as follows: for each sentence in the test set, the SARI is computed as the average of the log-likelihood of the target sequence given the source sequence and the model's output. The SARI is computed as follows: for each sentence in the test set, the SARI is computed as the average of the log-likelihood of the",
    "answer_b": "SARI compares the predicted simplification with both the source and the target references. Explanation: On the other hand SARI compares the predicted simplification with both the source and the target references. It is an average of F1 scores for three $n$-gram operations: additions, keeps and deletions. For each operation, these scores are then averaged for all $n$-gram orders (from 1 to 4) to get the overall F1 score.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat does SARI measure?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it computed?\n< Explanation: The SARI metric is a standard metric for evaluating the performance of a sequence-to-sequence model. It is defined as the average of the log-likelihood of the target sequence given the source sequence and the model's output. The SARI metric is computed as follows: for each sentence in the test set, the SARI is computed as the average of the log-likelihood of the target sequence given the source sequence and the model's output. The SARI is computed as follows: for each sentence in the test set, the SARI is computed as the average of the log-likelihood of the\n\nAnswer B:\nSARI compares the predicted simplification with both the source and the target references. Explanation: On the other hand SARI compares the predicted simplification with both the source and the target references. It is an average of F1 scores for three $n$-gram operations: additions, keeps and deletions. For each operation, these scores are then averaged for all $n$-gram orders (from 1 to 4) to get the overall F1 score.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.02677",
    "question": "What are the baseline models?",
    "gold_answer": "",
    "answer_a": "PBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS. Explanation: Table TABREF24 compares our best model to state-of-the-art methods:\n\nBIBREF12\n\nPhrase-Based MT system with candidate reranking. Dissimilar candidates are favored based on their Levenshtein distance to the source.\n\nBIBREF33\n\nDeep semantics sentence representation fed to a monolingual MT system.\n\nBIBREF9\n\nSyntax-based MT model augmented using the PPDB paraphrase database BIBREF34 and fine-tuned towards SARI.\n\nBIBREF10\n\nSeq2Seq trained with reinforcement learning, combined with a lexical simplification model.\n\nBIBREF17\n\nSeq2Seq model based on the pointer-copy mechanism and trained via multi-task learning on the Entailment and Paraphrase Generation tasks.\n\nBIBREF15\n\nStandard Seq2Seq model. The second beam search hypothesis is selected during decoding; the hypothesis number is an hyper-parameter fine-tuned with SARI.\n\nBIBREF35\n\nSeq2Seq with a memory-augmented Neural Semantic Encoder, tuned with SARI.\n\nBIBREF16\n\nSeq2Seq integrating the simple PPDB simplification database BIBREF36 as a dynamic memory. The database is also used to modify the loss and re-weight word probabilities to favor simpler words.",
    "answer_b": "How do they compare to ACCESS?\n< Explanation: We compare our model to the following baselines: (1) a standard sequence-to-sequence model (SToSe) that uses a standard encoder-decoder architecture, (2) a standard sequence-to-sequence model (SToSe) that uses a standard encoder-decoder architecture, (3) a standard sequence-to-sequence model (SToSe) that uses a standard encoder-decoder architecture, (4) a standard sequence-to-sequence model (SToSe) that uses a standard encoder-decoder architecture, (5) a standard sequence-to-sequence model (ST",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the baseline models?\n\nGround Truth Answer:\n\n\nAnswer A:\nPBMT-R, Hybrid, SBMT+PPDB+SARI, DRESS-LS, Pointer+Ent+Par, NTS+SARI, NSELSTM-S and DMASS+DCSS. Explanation: Table TABREF24 compares our best model to state-of-the-art methods:\n\nBIBREF12\n\nPhrase-Based MT system with candidate reranking. Dissimilar candidates are favored based on their Levenshtein distance to the source.\n\nBIBREF33\n\nDeep semantics sentence representation fed to a monolingual MT system.\n\nBIBREF9\n\nSyntax-based MT model augmented using the PPDB paraphrase database BIBREF34 and fine-tuned towards SARI.\n\nBIBREF10\n\nSeq2Seq trained with reinforcement learning, combined with a lexical simplification model.\n\nBIBREF17\n\nSeq2Seq model based on the pointer-copy mechanism and trained via multi-task learning on the Entailment and Paraphrase Generation tasks.\n\nBIBREF15\n\nStandard Seq2Seq model. The second beam search hypothesis is selected during decoding; the hypothesis number is an hyper-parameter fine-tuned with SARI.\n\nBIBREF35\n\nSeq2Seq with a memory-augmented Neural Semantic Encoder, tuned with SARI.\n\nBIBREF16\n\nSeq2Seq integrating the simple PPDB simplification database BIBREF36 as a dynamic memory. The database is also used to modify the loss and re-weight word probabilities to favor simpler words.\n\nAnswer B:\nHow do they compare to ACCESS?\n< Explanation: We compare our model to the following baselines: (1) a standard sequence-to-sequence model (SToSe) that uses a standard encoder-decoder architecture, (2) a standard sequence-to-sequence model (SToSe) that uses a standard encoder-decoder architecture, (3) a standard sequence-to-sequence model (SToSe) that uses a standard encoder-decoder architecture, (4) a standard sequence-to-sequence model (SToSe) that uses a standard encoder-decoder architecture, (5) a standard sequence-to-sequence model (ST\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.04374",
    "question": "What datasets are used?",
    "gold_answer": "",
    "answer_a": "PC-GITA corpus. Explanation: The Spanish data consider the PC-GITA corpus BIBREF5, which contains utterances from 50 PD patients and 50 HC, Colombian Spanish native speakers",
    "answer_b": "Explanation: The datasets used in this work are the following: (1) the dataset of Parkinson's disease patients from the National Parkinson's Disease Research Center (NPDRC) in the United States, (2) the dataset of Parkinson's disease patients from the University of California, San Francisco (UCSF) in the United States, (3) the dataset of Parkinson's disease patients from the University of California, Los Angeles (UCLA) in the United States, (4) the dataset of Parkinson's disease patients from the University of California, San Diego (UCSD) in the United States, (5) the dataset of Parkinson",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nPC-GITA corpus. Explanation: The Spanish data consider the PC-GITA corpus BIBREF5, which contains utterances from 50 PD patients and 50 HC, Colombian Spanish native speakers\n\nAnswer B:\nExplanation: The datasets used in this work are the following: (1) the dataset of Parkinson's disease patients from the National Parkinson's Disease Research Center (NPDRC) in the United States, (2) the dataset of Parkinson's disease patients from the University of California, San Francisco (UCSF) in the United States, (3) the dataset of Parkinson's disease patients from the University of California, Los Angeles (UCLA) in the United States, (4) the dataset of Parkinson's disease patients from the University of California, San Diego (UCSD) in the United States, (5) the dataset of Parkinson\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.08211",
    "question": "How do they split the dataset when training and evaluating their models?",
    "gold_answer": "",
    "answer_a": "What is the size of the training set?\n< Explanation: We use the Twitter dataset from the Twitter Corpus (TWC) [1] to train our model. The dataset consists of 100,000 tweets, which are divided into 100,000 training tweets and 100,000 test tweets. The training tweets are used to train the model, and the test tweets are used to evaluate the model. The training tweets are further divided into 10,000 training tweets and 10,000 test tweets. The training tweets and test tweets are",
    "answer_b": "SemEval-2017 task 8  dataset includes 325 rumorous conversation threads, and has been split into training, development and test sets. \nThe PHEME dataset provides 2,402 conversations covering nine events - in each fold, one event's conversations are used for testing, and all the rest events are used for training.. Explanation: The first is SemEval-2017 task 8 BIBREF16 dataset. It includes 325 rumorous conversation threads, and has been split into training, development and test sets. These threads cover ten events, and two events of that only appear in the test set. This dataset is used to evaluate both stance classification and veracity prediction tasks.\n\nThe second is PHEME dataset BIBREF48. It provides 2,402 conversations covering nine events. Following previous work, we conduct leave-one-event-out cross-validation: in each fold, one event's conversations are used for testing, and all the rest events are used for training. The evaluation metric on this dataset is computed after integrating the outputs of all nine folds. Note that only a subset of this dataset has stance labels, and all conversations in this subset are already contained in SemEval-2017 task 8 dataset. Thus, PHEME dataset is used to evaluate veracity prediction task.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they split the dataset when training and evaluating their models?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the training set?\n< Explanation: We use the Twitter dataset from the Twitter Corpus (TWC) [1] to train our model. The dataset consists of 100,000 tweets, which are divided into 100,000 training tweets and 100,000 test tweets. The training tweets are used to train the model, and the test tweets are used to evaluate the model. The training tweets are further divided into 10,000 training tweets and 10,000 test tweets. The training tweets and test tweets are\n\nAnswer B:\nSemEval-2017 task 8  dataset includes 325 rumorous conversation threads, and has been split into training, development and test sets. \nThe PHEME dataset provides 2,402 conversations covering nine events - in each fold, one event's conversations are used for testing, and all the rest events are used for training.. Explanation: The first is SemEval-2017 task 8 BIBREF16 dataset. It includes 325 rumorous conversation threads, and has been split into training, development and test sets. These threads cover ten events, and two events of that only appear in the test set. This dataset is used to evaluate both stance classification and veracity prediction tasks.\n\nThe second is PHEME dataset BIBREF48. It provides 2,402 conversations covering nine events. Following previous work, we conduct leave-one-event-out cross-validation: in each fold, one event's conversations are used for testing, and all the rest events are used for training. The evaluation metric on this dataset is computed after integrating the outputs of all nine folds. Note that only a subset of this dataset has stance labels, and all conversations in this subset are already contained in SemEval-2017 task 8 dataset. Thus, PHEME dataset is used to evaluate veracity prediction task.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.08211",
    "question": "How much improvement does their model yield over previous methods?",
    "gold_answer": "",
    "answer_a": "What is the baseline?\n< Explanation: We compare our model with the following baselines: (1) a baseline model that only predicts the rumor veracity, (2) a baseline model that only predicts the rumor stance, (3) a baseline model that predicts the rumor stance and veracity simultaneously, and (4) a baseline model that predicts the rumor stance and veracity simultaneously and then combines the two predictions. The baseline models are trained on the Twitter dataset. The baseline models are trained on the Twitter dataset. The baseline models are trained on the Twitter dataset. The baseline models are trained on the Twitter dataset. The baseline models are",
    "answer_b": "Their model improves macro-averaged F1 by 0.017 over previous best model in Rumor Stance Classification and improves macro-averaged F1 by 0.03 and 0.015 on Multi-task Rumor Veracity Prediction on SemEval and PHEME datasets respectively. Explanation: FLOAT SELECTED: Table 2: Results of rumor stance classification. FS, FD, FQ and FC denote the F1 scores of supporting, denying, querying and commenting classes respectively. \u201c\u2013\u201d indicates that the original paper does not report the metric.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much improvement does their model yield over previous methods?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the baseline?\n< Explanation: We compare our model with the following baselines: (1) a baseline model that only predicts the rumor veracity, (2) a baseline model that only predicts the rumor stance, (3) a baseline model that predicts the rumor stance and veracity simultaneously, and (4) a baseline model that predicts the rumor stance and veracity simultaneously and then combines the two predictions. The baseline models are trained on the Twitter dataset. The baseline models are trained on the Twitter dataset. The baseline models are trained on the Twitter dataset. The baseline models are trained on the Twitter dataset. The baseline models are\n\nAnswer B:\nTheir model improves macro-averaged F1 by 0.017 over previous best model in Rumor Stance Classification and improves macro-averaged F1 by 0.03 and 0.015 on Multi-task Rumor Veracity Prediction on SemEval and PHEME datasets respectively. Explanation: FLOAT SELECTED: Table 2: Results of rumor stance classification. FS, FD, FQ and FC denote the F1 scores of supporting, denying, querying and commenting classes respectively. \u201c\u2013\u201d indicates that the original paper does not report the metric.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.09355",
    "question": "What of the two strategies works best?",
    "gold_answer": "",
    "answer_a": "PKD-Skip. Explanation: Although both strategies achieved improvement over the vanilla KD baseline (see Table TABREF16), PKD-Skip performs slightly better than PKD-Last.",
    "answer_b": "How do they compare?\n< Explanation: We compare the two strategies in the following experiments. The first strategy, PKD-Last, is to learn from the last $k$ layers of the teacher model. The second strategy, PKD-Skip, is to learn from every $k$ layers of the teacher model. The results are shown in Table 1. We observe that the second strategy outperforms the first one, especially for the tasks with a large number of layers. The results also show that the second strategy is more effective than the first one, especially for the tasks with a small number of layers. The second strategy",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat of the two strategies works best?\n\nGround Truth Answer:\n\n\nAnswer A:\nPKD-Skip. Explanation: Although both strategies achieved improvement over the vanilla KD baseline (see Table TABREF16), PKD-Skip performs slightly better than PKD-Last.\n\nAnswer B:\nHow do they compare?\n< Explanation: We compare the two strategies in the following experiments. The first strategy, PKD-Last, is to learn from the last $k$ layers of the teacher model. The second strategy, PKD-Skip, is to learn from every $k$ layers of the teacher model. The results are shown in Table 1. We observe that the second strategy outperforms the first one, especially for the tasks with a large number of layers. The results also show that the second strategy is more effective than the first one, especially for the tasks with a small number of layers. The second strategy\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.09355",
    "question": "What downstream tasks are tested?",
    "gold_answer": "",
    "answer_a": "How are the results evaluated?\n< Explanation: We evaluate the proposed PKD-Last and PKD-Skip on the following tasks: (1) Text Classification (TC), (2) Text Classification with Multi-Task (TC-MT), (3) Text Classification with Multi-Task and Multi-Label (TC-MT-ML), (4) Text Classification with Multi-Task and Multi-Label with Multi-Label (TC-MT-ML-ML), (5) Text Classification with Multi-Task and Multi-Label with Multi-Label and Multi-Label (TC-MT-ML-ML-ML), (",
    "answer_b": "Sentiment Classification. Explanation: We evaluate our proposed approach on Sentiment Classification, Paraphrase Similarity Matching, Natural Language Inference, and Machine Reading Comprehension tasks.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat downstream tasks are tested?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are the results evaluated?\n< Explanation: We evaluate the proposed PKD-Last and PKD-Skip on the following tasks: (1) Text Classification (TC), (2) Text Classification with Multi-Task (TC-MT), (3) Text Classification with Multi-Task and Multi-Label (TC-MT-ML), (4) Text Classification with Multi-Task and Multi-Label with Multi-Label (TC-MT-ML-ML), (5) Text Classification with Multi-Task and Multi-Label with Multi-Label and Multi-Label (TC-MT-ML-ML-ML), (\n\nAnswer B:\nSentiment Classification. Explanation: We evaluate our proposed approach on Sentiment Classification, Paraphrase Similarity Matching, Natural Language Inference, and Machine Reading Comprehension tasks.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2001.05540",
    "question": "How much is BELU score difference between proposed approach and insertion-only method?",
    "gold_answer": "",
    "answer_a": "What is the difference?\n< Explanation: Table 1 shows the results of the proposed model and the insertion-only model on the synthetic translation task. The proposed model achieves a BLEU score of 1.03, which is 0.01 points higher than the insertion-only model. The insertion-only model achieves a BLEU score of 0.98, which is 0.01 points lower than the proposed model. The proposed model achieves a BLEU score of 1.03, which is 0.01 points higher than the insertion-only model. The insertion-only model achieves a BLEU score",
    "answer_b": "deletion model again increases the BLEU score over just the insertion model, by around 2 BLEU points. Explanation: The table below shows that the deletion model again increases the BLEU score over just the insertion model, by around 2 BLEU points.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much is BELU score difference between proposed approach and insertion-only method?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference?\n< Explanation: Table 1 shows the results of the proposed model and the insertion-only model on the synthetic translation task. The proposed model achieves a BLEU score of 1.03, which is 0.01 points higher than the insertion-only model. The insertion-only model achieves a BLEU score of 0.98, which is 0.01 points lower than the proposed model. The proposed model achieves a BLEU score of 1.03, which is 0.01 points higher than the insertion-only model. The insertion-only model achieves a BLEU score\n\nAnswer B:\ndeletion model again increases the BLEU score over just the insertion model, by around 2 BLEU points. Explanation: The table below shows that the deletion model again increases the BLEU score over just the insertion model, by around 2 BLEU points.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1805.00195",
    "question": "what ML approaches did they experiment with?",
    "gold_answer": "",
    "answer_a": "maximum entropy. Explanation: To demonstrate the utility of our annotated corpus, we explore two machine learning approaches for extracting actions and entities: a maximum entropy model and a neural network tagging model. We also present experiments for relation classification.",
    "answer_b": "Explanation: We use a combination of supervised and unsupervised learning approaches to develop a machine learning model for shallow semantic parsing of instructional texts. We use a supervised learning approach to train a neural network to predict the semantic meaning of a text. We use a supervised learning approach to train a neural network to predict the semantic meaning of a text. We use a supervised learning approach to train a neural network to predict the semantic meaning of a text. We use a supervised learning approach to train a neural network to predict the semantic meaning of a text. We use a supervised learning approach to train a neural network to predict the semantic meaning of a text.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat ML approaches did they experiment with?\n\nGround Truth Answer:\n\n\nAnswer A:\nmaximum entropy. Explanation: To demonstrate the utility of our annotated corpus, we explore two machine learning approaches for extracting actions and entities: a maximum entropy model and a neural network tagging model. We also present experiments for relation classification.\n\nAnswer B:\nExplanation: We use a combination of supervised and unsupervised learning approaches to develop a machine learning model for shallow semantic parsing of instructional texts. We use a supervised learning approach to train a neural network to predict the semantic meaning of a text. We use a supervised learning approach to train a neural network to predict the semantic meaning of a text. We use a supervised learning approach to train a neural network to predict the semantic meaning of a text. We use a supervised learning approach to train a neural network to predict the semantic meaning of a text. We use a supervised learning approach to train a neural network to predict the semantic meaning of a text.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1612.02695",
    "question": "What type of attention is used in the recognition system?",
    "gold_answer": "",
    "answer_a": "What is the size of the attention vector?\n< Explanation: The attention mechanism is based on the attention mechanism proposed by Bahdanau et al. (2014). The attention mechanism is based on the attention mechanism proposed by Bahdanau et al. (2014). The attention mechanism is based on the attention mechanism proposed by Bahdanau et al. (2014). The attention mechanism is based on the attention mechanism proposed by Bahdanau et al. (2014). The attention mechanism is based on the attention mechanism proposed by Bahdanau et al. (2014). The",
    "answer_b": "find a set of relevant activations of the listener INLINEFORM0 and summarize them into a context. Explanation: To emit a character the speller uses the attention mechanism to find a set of relevant activations of the listener INLINEFORM0 and summarize them into a context INLINEFORM1 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of attention is used in the recognition system?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the attention vector?\n< Explanation: The attention mechanism is based on the attention mechanism proposed by Bahdanau et al. (2014). The attention mechanism is based on the attention mechanism proposed by Bahdanau et al. (2014). The attention mechanism is based on the attention mechanism proposed by Bahdanau et al. (2014). The attention mechanism is based on the attention mechanism proposed by Bahdanau et al. (2014). The attention mechanism is based on the attention mechanism proposed by Bahdanau et al. (2014). The\n\nAnswer B:\nfind a set of relevant activations of the listener INLINEFORM0 and summarize them into a context. Explanation: To emit a character the speller uses the attention mechanism to find a set of relevant activations of the listener INLINEFORM0 and summarize them into a context INLINEFORM1 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1612.02695",
    "question": "What are the solutions proposed for the seq2seq shortcomings?",
    "gold_answer": "",
    "answer_a": "What are the results of the experiments?\n< Explanation: We propose two solutions to the shortcomings of the seq2seq system: (1) a separate language model that is trained to predict the next character in the sequence, and (2) a trigram language model that is trained to predict the next three characters in the sequence. We compare the performance of these models with a single language model trained to predict the next character in the sequence. We observe that the trigram language model is able to improve the performance of the separate language model, but that the separate language model is able to improve the performance of the trigram language model. We",
    "answer_b": "forbids emitting the EOS token. Explanation: We compare three strategies designed to prevent incomplete transcripts. The first strategy doesn't change the beam search criterion, but forbids emitting the EOS token unless its probability is within a set range of that of the most probable token. This strategy prevents truncations, but is inefficient against omissions in the middle of the transcript, such as the failure shown in Table TABREF17 . Alternatively, beam search criterion can be extended to promote long transcripts.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the solutions proposed for the seq2seq shortcomings?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results of the experiments?\n< Explanation: We propose two solutions to the shortcomings of the seq2seq system: (1) a separate language model that is trained to predict the next character in the sequence, and (2) a trigram language model that is trained to predict the next three characters in the sequence. We compare the performance of these models with a single language model trained to predict the next character in the sequence. We observe that the trigram language model is able to improve the performance of the separate language model, but that the separate language model is able to improve the performance of the trigram language model. We\n\nAnswer B:\nforbids emitting the EOS token. Explanation: We compare three strategies designed to prevent incomplete transcripts. The first strategy doesn't change the beam search criterion, but forbids emitting the EOS token unless its probability is within a set range of that of the most probable token. This strategy prevents truncations, but is inefficient against omissions in the middle of the transcript, such as the failure shown in Table TABREF17 . Alternatively, beam search criterion can be extended to promote long transcripts.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.04745",
    "question": "How much is training speeded up?",
    "gold_answer": "",
    "answer_a": "What is the difference between the two models?\n< Explanation: We compare the performance of the two models on the BIBREF10 dataset. The results are shown in Table TABREF11. We can see that the Pre-LN Transformer with warm-up stage is faster than the original-designed Post-LN Transformer. The difference is even more significant on the BIBREF10 dataset, where the Pre-LN Transformer with warm-up stage is 1.5 times faster than the original-designed Post-LN Transformer. The difference is even more significant on the BIBREF11 dataset, where the Pre-LN Transformer with warm",
    "answer_b": "40% speed-up rate. Explanation: For example, the Post-LN model achieves 1.69 validation loss at 500k updates while the Pre-LN model achieves similar validation loss at 700k updates, which suggests there is a 40% speed-up rate.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much is training speeded up?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the two models?\n< Explanation: We compare the performance of the two models on the BIBREF10 dataset. The results are shown in Table TABREF11. We can see that the Pre-LN Transformer with warm-up stage is faster than the original-designed Post-LN Transformer. The difference is even more significant on the BIBREF10 dataset, where the Pre-LN Transformer with warm-up stage is 1.5 times faster than the original-designed Post-LN Transformer. The difference is even more significant on the BIBREF11 dataset, where the Pre-LN Transformer with warm\n\nAnswer B:\n40% speed-up rate. Explanation: For example, the Post-LN model achieves 1.69 validation loss at 500k updates while the Pre-LN model achieves similar validation loss at 700k updates, which suggests there is a 40% speed-up rate.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2002.04745",
    "question": "What experiments do they perform?",
    "gold_answer": "",
    "answer_a": "How do they measure the results?\n< Explanation: We first compare the performance of the original-designed Post-LN Transformer and the Pre-LN Transformer with the same architecture and hyper-parameters. We also compare the performance of the original-designed Post-LN Transformer with the same architecture and hyper-parameters but without the warm-up stage. We also compare the performance of the original-designed Pre-LN Transformer with the same architecture and hyper-parameters but without the warm-up stage. We also compare the performance of the original-designed Post-LN Transformer with the same architecture and hyper-parameters but without the warm-up stage on the BERT task. We",
    "answer_b": "experiments on two widely used tasks: the IWSLT14 German-to-English (De-En) task and the WMT14 English-to-German (En-De) task. Explanation: Experiments ::: Experiment Settings ::: Machine Translation\nWe conduct our experiments on two widely used tasks: the IWSLT14 German-to-English (De-En) task and the WMT14 English-to-German (En-De) task.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat experiments do they perform?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they measure the results?\n< Explanation: We first compare the performance of the original-designed Post-LN Transformer and the Pre-LN Transformer with the same architecture and hyper-parameters. We also compare the performance of the original-designed Post-LN Transformer with the same architecture and hyper-parameters but without the warm-up stage. We also compare the performance of the original-designed Pre-LN Transformer with the same architecture and hyper-parameters but without the warm-up stage. We also compare the performance of the original-designed Post-LN Transformer with the same architecture and hyper-parameters but without the warm-up stage on the BERT task. We\n\nAnswer B:\nexperiments on two widely used tasks: the IWSLT14 German-to-English (De-En) task and the WMT14 English-to-German (En-De) task. Explanation: Experiments ::: Experiment Settings ::: Machine Translation\nWe conduct our experiments on two widely used tasks: the IWSLT14 German-to-English (De-En) task and the WMT14 English-to-German (En-De) task.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.12984",
    "question": "Which datasets do they evaluate on?",
    "gold_answer": "",
    "answer_a": "How do they evaluate?\n< Explanation: We evaluate our model on two datasets: ASR data and human data. ASR data is collected from the Baidu ASR system. The ASR data is collected from the Baidu ASR system. The ASR data is collected from the Baidu ASR system. The ASR data is collected from the Baidu ASR system. The ASR data is collected from the Baidu ASR system. The ASR data is collected from the Baidu ASR system. The ASR data is collected from the Baidu ASR system. The ASR data is collected from the",
    "answer_b": "NIST02. Explanation: We choose NIST 2006 (NIST06) dataset as our development set, and the NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets as our test sets.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich datasets do they evaluate on?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they evaluate?\n< Explanation: We evaluate our model on two datasets: ASR data and human data. ASR data is collected from the Baidu ASR system. The ASR data is collected from the Baidu ASR system. The ASR data is collected from the Baidu ASR system. The ASR data is collected from the Baidu ASR system. The ASR data is collected from the Baidu ASR system. The ASR data is collected from the Baidu ASR system. The ASR data is collected from the Baidu ASR system. The ASR data is collected from the\n\nAnswer B:\nNIST02. Explanation: We choose NIST 2006 (NIST06) dataset as our development set, and the NIST 2002 (NIST02), 2003 (NIST03), 2004 (NIST04) 2005 (NIST05), and 2008 (NIST08) datasets as our test sets.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1907.12984",
    "question": "Does larger granularity lead to better translation quality?",
    "gold_answer": "",
    "answer_a": "It depends on the model used.. Explanation: For the former model, the increase of discarded tokens results in an obvious decrease in translation quality, but no definite improvement in latency. The latter model can benefit from the increasing of discarding tokens both in translation quality and latency.",
    "answer_b": "How much better?\n< Explanation: We first compare the performance of our model with the state-of-the-art model in the context-aware decoding (CA) and partial decoding (PD) strategies. The results show that the PD strategy is better than the CA strategy in terms of translation quality. The PD strategy is also better than the CA strategy in terms of latency. The PD strategy is also better than the CA strategy in terms of the number of words. The PD strategy is also better than the CA strategy in terms of the number of characters. The PD strategy is also better than the CA strategy in terms of the number of syllables. The",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nDoes larger granularity lead to better translation quality?\n\nGround Truth Answer:\n\n\nAnswer A:\nIt depends on the model used.. Explanation: For the former model, the increase of discarded tokens results in an obvious decrease in translation quality, but no definite improvement in latency. The latter model can benefit from the increasing of discarding tokens both in translation quality and latency.\n\nAnswer B:\nHow much better?\n< Explanation: We first compare the performance of our model with the state-of-the-art model in the context-aware decoding (CA) and partial decoding (PD) strategies. The results show that the PD strategy is better than the CA strategy in terms of translation quality. The PD strategy is also better than the CA strategy in terms of latency. The PD strategy is also better than the CA strategy in terms of the number of words. The PD strategy is also better than the CA strategy in terms of the number of characters. The PD strategy is also better than the CA strategy in terms of the number of syllables. The\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.09247",
    "question": "What is the baseline method?",
    "gold_answer": "",
    "answer_a": "evaluate the original ill-formed question using the automatic metrics. Explanation: As a baseline, we also evaluate the original ill-formed question using the automatic metrics.",
    "answer_b": "What is the baseline method?\n< Explanation: We train a sequence-to-sequence model on the MQR dataset. We use the following baseline methods: (1) a simple model that uses a single LSTM layer with a linear output layer to predict the question; (2) a model that uses a single LSTM layer with a linear output layer to predict the question and a linear output layer to predict the answer; (3) a model that uses a single LSTM layer with a linear output layer to predict the question, a linear output layer to predict the answer, and a linear output layer to predict the answer to the answer; (4)",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the baseline method?\n\nGround Truth Answer:\n\n\nAnswer A:\nevaluate the original ill-formed question using the automatic metrics. Explanation: As a baseline, we also evaluate the original ill-formed question using the automatic metrics.\n\nAnswer B:\nWhat is the baseline method?\n< Explanation: We train a sequence-to-sequence model on the MQR dataset. We use the following baseline methods: (1) a simple model that uses a single LSTM layer with a linear output layer to predict the question; (2) a model that uses a single LSTM layer with a linear output layer to predict the question and a linear output layer to predict the answer; (3) a model that uses a single LSTM layer with a linear output layer to predict the question, a linear output layer to predict the answer, and a linear output layer to predict the answer to the answer; (4)\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.09247",
    "question": "What aspects are used to judge question quality?",
    "gold_answer": "",
    "answer_a": "What are the baseline methods?\n<sep>Answer: The three aspects are: (1) the question is grammatically correct or not; (2) the question is clear or not; (3) the question is interesting or not. The baseline methods are: (1) the question is grammatically correct or not (2) the question is clear or not (3) the question is interesting or not. We use the average of the three aspects as the quality metric. Explanation: We use the average of the three aspects as the quality metric. The three aspects are: (1) the question is grammatically correct or not;",
    "answer_b": "Is the question grammatically correct?. Explanation: To understand the quality of the question rewriting examples in the MQR dataset, we ask human annotators to judge the quality of the questions in the DEV and TEST splits (abbreviated as DEVTEST onward). Specifically, we take both ill-formed and well-formed questions in DEVTEST and ask human annotators to annotate the following three aspects regarding each question BIBREF0:\n\nIs the question grammatically correct?\n\nIs the spelling correct? Misuse of third person singular or past tense in verbs are considered grammatical errors instead of spelling errors. Missing question mark in the end of a question is also considered as spelling errors.\n\nIs the question an explicit question, rather than a search query, a command, or a statement?\n\nThe annotators were asked to annotate each aspect with a binary (0/1) answer. Examples of questions provided to the annotators are in Table TABREF13. We consider all \u201cHow to\u201d questions (\u201cHow to unlock GT90 in Gran Turismo 2?\u201d) as grammatical. Although it is not a complete sentence, this kind of question is quite common in our dataset and therefore we choose to treat it as grammatically correct.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat aspects are used to judge question quality?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the baseline methods?\n<sep>Answer: The three aspects are: (1) the question is grammatically correct or not; (2) the question is clear or not; (3) the question is interesting or not. The baseline methods are: (1) the question is grammatically correct or not (2) the question is clear or not (3) the question is interesting or not. We use the average of the three aspects as the quality metric. Explanation: We use the average of the three aspects as the quality metric. The three aspects are: (1) the question is grammatically correct or not;\n\nAnswer B:\nIs the question grammatically correct?. Explanation: To understand the quality of the question rewriting examples in the MQR dataset, we ask human annotators to judge the quality of the questions in the DEV and TEST splits (abbreviated as DEVTEST onward). Specifically, we take both ill-formed and well-formed questions in DEVTEST and ask human annotators to annotate the following three aspects regarding each question BIBREF0:\n\nIs the question grammatically correct?\n\nIs the spelling correct? Misuse of third person singular or past tense in verbs are considered grammatical errors instead of spelling errors. Missing question mark in the end of a question is also considered as spelling errors.\n\nIs the question an explicit question, rather than a search query, a command, or a statement?\n\nThe annotators were asked to annotate each aspect with a binary (0/1) answer. Examples of questions provided to the annotators are in Table TABREF13. We consider all \u201cHow to\u201d questions (\u201cHow to unlock GT90 in Gran Turismo 2?\u201d) as grammatical. Although it is not a complete sentence, this kind of question is quite common in our dataset and therefore we choose to treat it as grammatically correct.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.09247",
    "question": "What did the human annotations consist of?",
    "gold_answer": "",
    "answer_a": "What was the quality estimate?\n< Explanation: We provide human annotations for a subset of the dataset as a quality estimate. The annotations are based on the human experience of rewriting questions. We provide annotations for 10 domains and 100,000 question pairs. The annotations are based on the following criteria: (1) the question is well-formed, (2) the question is ill-formed, (3) the question is not well-formed but is ill-formed, and (4) the question is not ill-formed. The annotations are based on the human experience of rewriting questions. We provide annotations for 10 domains",
    "answer_b": "Is the question grammatically correct?. Explanation: Specifically, we take both ill-formed and well-formed questions in DEVTEST and ask human annotators to annotate the following three aspects regarding each question BIBREF0:\n\nIs the question grammatically correct?\n\nIs the spelling correct? Misuse of third person singular or past tense in verbs are considered grammatical errors instead of spelling errors. Missing question mark in the end of a question is also considered as spelling errors.\n\nIs the question an explicit question, rather than a search query, a command, or a statement?\n\nThe annotators were asked to annotate each aspect with a binary (0/1) answer.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat did the human annotations consist of?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat was the quality estimate?\n< Explanation: We provide human annotations for a subset of the dataset as a quality estimate. The annotations are based on the human experience of rewriting questions. We provide annotations for 10 domains and 100,000 question pairs. The annotations are based on the following criteria: (1) the question is well-formed, (2) the question is ill-formed, (3) the question is not well-formed but is ill-formed, and (4) the question is not ill-formed. The annotations are based on the human experience of rewriting questions. We provide annotations for 10 domains\n\nAnswer B:\nIs the question grammatically correct?. Explanation: Specifically, we take both ill-formed and well-formed questions in DEVTEST and ask human annotators to annotate the following three aspects regarding each question BIBREF0:\n\nIs the question grammatically correct?\n\nIs the spelling correct? Misuse of third person singular or past tense in verbs are considered grammatical errors instead of spelling errors. Missing question mark in the end of a question is also considered as spelling errors.\n\nIs the question an explicit question, rather than a search query, a command, or a statement?\n\nThe annotators were asked to annotate each aspect with a binary (0/1) answer.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.09247",
    "question": "What characterizes the 303 domains? e.g. is this different subject tags?",
    "gold_answer": "",
    "answer_a": "is this different question tags?\n<blockquote>Question: What characterizes the 303 domains? e.g. is this different subject tags? is this different question tags? </blockquote> The dataset contains 303 domains, which are divided into 100 domains. The domains are divided into 303 domains, which are divided into 100 domains. The domains are divided into 100 domains, which are divided into 303 domains. The domains are divided into 303 domains, which are divided into 100 domains. The domains are divided into 1",
    "answer_b": "sub areas from Stack Exchange data dumps. Explanation: We use 303 sub areas from Stack Exchange data dumps. The full list of area names is in the appendix. We do not include Stack Overflow because it is too specific to programming related questions. We also exclude all questions under the following language sub areas: Chinese, German, Spanish, Russian, Japanese, Korean, Latin, Ukrainian. This ensures that the questions in MQR are mostly English sentences. Having questions from 303 Stack Exchange sites makes the MQR dataset cover a broad range of domains.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat characterizes the 303 domains? e.g. is this different subject tags?\n\nGround Truth Answer:\n\n\nAnswer A:\nis this different question tags?\n<blockquote>Question: What characterizes the 303 domains? e.g. is this different subject tags? is this different question tags? </blockquote> The dataset contains 303 domains, which are divided into 100 domains. The domains are divided into 303 domains, which are divided into 100 domains. The domains are divided into 100 domains, which are divided into 303 domains. The domains are divided into 303 domains, which are divided into 100 domains. The domains are divided into 1\n\nAnswer B:\nsub areas from Stack Exchange data dumps. Explanation: We use 303 sub areas from Stack Exchange data dumps. The full list of area names is in the appendix. We do not include Stack Overflow because it is too specific to programming related questions. We also exclude all questions under the following language sub areas: Chinese, German, Spanish, Russian, Japanese, Korean, Latin, Ukrainian. This ensures that the questions in MQR are mostly English sentences. Having questions from 303 Stack Exchange sites makes the MQR dataset cover a broad range of domains.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.12660",
    "question": "How long is their dataset?",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n< Explanation: The dataset used in this work is the 2016-2017 Nigerian Pidgin Corpus (NPC) which is a corpus of pidgin English. The dataset contains 1,000,000 words and 10,000 sentences. The dataset is available at https://www.kaggle.com/c/nigerian- pidgin-corpus. The dataset is divided into 10000 sentences, each of which is 100 words long. The dataset is further divided into 1000",
    "answer_b": "21214. Explanation: The train set contained 20214 sentence pairs, while the validation contained 1000 sentence pairs.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long is their dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n< Explanation: The dataset used in this work is the 2016-2017 Nigerian Pidgin Corpus (NPC) which is a corpus of pidgin English. The dataset contains 1,000,000 words and 10,000 sentences. The dataset is available at https://www.kaggle.com/c/nigerian- pidgin-corpus. The dataset is divided into 10000 sentences, each of which is 100 words long. The dataset is further divided into 1000\n\nAnswer B:\n21214. Explanation: The train set contained 20214 sentence pairs, while the validation contained 1000 sentence pairs.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.12660",
    "question": "What metrics are used?",
    "gold_answer": "",
    "answer_a": "BLEU score. Explanation: The model with the highest test BLEU score is selected as the best.",
    "answer_b": "How are they calculated?\n< Explanation: We use BLEU as our evaluation metric. BLEU is a widely used metric for evaluating machine translation systems. It is defined as follows: where $B$ is the best translation, $E$ is the actual translation, and $U$ is the universal translation. The higher the BLEU score, the better the translation. We use the following formula to calculate BLEU: where $B$ is the best translation, $E$ is the actual translation, and $U$ is the universal translation. The higher the BLEU score, the better the translation. We use the following formula to",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat metrics are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nBLEU score. Explanation: The model with the highest test BLEU score is selected as the best.\n\nAnswer B:\nHow are they calculated?\n< Explanation: We use BLEU as our evaluation metric. BLEU is a widely used metric for evaluating machine translation systems. It is defined as follows: where $B$ is the best translation, $E$ is the actual translation, and $U$ is the universal translation. The higher the BLEU score, the better the translation. We use the following formula to calculate BLEU: where $B$ is the best translation, $E$ is the actual translation, and $U$ is the universal translation. The higher the BLEU score, the better the translation. We use the following formula to\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.12660",
    "question": "What is the best performing system?",
    "gold_answer": "",
    "answer_a": "A supervised model with byte pair encoding was the best for English to Pidgin, while a supervised model with word-level encoding was the best for Pidgin to English.. Explanation: For the word-level tokenization English to Pidgin models, the supervised model outperforms the unsupervised model, achieving a BLEU score of 17.73 in comparison to the BLEU score of 5.18 achieved by the unsupervised model. The supervised model trained with byte pair encoding tokenization outperforms both word-level tokenization models, achieving a BLEU score of 24.29.",
    "answer_b": "What is the best performing model?\n<sep><solution>Our best performing model is the one with a 98.5% accuracy on the test set. The best performing model is the one with a 98.5% accuracy on the test set. The best performing model is the one with a 98.5% accuracy on the test set. The best performing model is the one with a 98.5% accuracy on the test set. The best performing model is the one with a 98.5% accuracy on the test set. The best performing model is the one with a 9",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the best performing system?\n\nGround Truth Answer:\n\n\nAnswer A:\nA supervised model with byte pair encoding was the best for English to Pidgin, while a supervised model with word-level encoding was the best for Pidgin to English.. Explanation: For the word-level tokenization English to Pidgin models, the supervised model outperforms the unsupervised model, achieving a BLEU score of 17.73 in comparison to the BLEU score of 5.18 achieved by the unsupervised model. The supervised model trained with byte pair encoding tokenization outperforms both word-level tokenization models, achieving a BLEU score of 24.29.\n\nAnswer B:\nWhat is the best performing model?\n<sep><solution>Our best performing model is the one with a 98.5% accuracy on the test set. The best performing model is the one with a 98.5% accuracy on the test set. The best performing model is the one with a 98.5% accuracy on the test set. The best performing model is the one with a 98.5% accuracy on the test set. The best performing model is the one with a 98.5% accuracy on the test set. The best performing model is the one with a 9\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.12660",
    "question": "What tokenization methods are used?",
    "gold_answer": "",
    "answer_a": "word-level. Explanation: We evaluate the performance of supervised and unsupervised neural machine translation models using word-level and the subword-level tokenization of BIBREF3.",
    "answer_b": "What are the results?\nThe results show that the best model is obtained using the tokenization method that uses the word boundaries as the tokenization unit. The best model achieved an accuracy of 88.5% on the test set. The best model achieved an accuracy of 88.5% on the test set. The best model achieved an accuracy of 88.5% on the test set. The best model achieved an accuracy of 88.5% on the test set. The best model achieved an accuracy of 88.5% on the test set. The best model achieved an accuracy of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat tokenization methods are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nword-level. Explanation: We evaluate the performance of supervised and unsupervised neural machine translation models using word-level and the subword-level tokenization of BIBREF3.\n\nAnswer B:\nWhat are the results?\nThe results show that the best model is obtained using the tokenization method that uses the word boundaries as the tokenization unit. The best model achieved an accuracy of 88.5% on the test set. The best model achieved an accuracy of 88.5% on the test set. The best model achieved an accuracy of 88.5% on the test set. The best model achieved an accuracy of 88.5% on the test set. The best model achieved an accuracy of 88.5% on the test set. The best model achieved an accuracy of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.12660",
    "question": "What baselines do they propose?",
    "gold_answer": "",
    "answer_a": "Transformer architecture of BIBREF7. Explanation: All baseline models were trained using the Transformer architecture of BIBREF7. We experiment with both word-level and Byte Pair Encoding (BPE) subword-level tokenization methods for the supervised models.",
    "answer_b": "How do they compare them?\n< Explanation: We propose three different baselines for NMT between English and Nigerian Pidgin. We use the following models:\n\n1. BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF1",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat baselines do they propose?\n\nGround Truth Answer:\n\n\nAnswer A:\nTransformer architecture of BIBREF7. Explanation: All baseline models were trained using the Transformer architecture of BIBREF7. We experiment with both word-level and Byte Pair Encoding (BPE) subword-level tokenization methods for the supervised models.\n\nAnswer B:\nHow do they compare them?\n< Explanation: We propose three different baselines for NMT between English and Nigerian Pidgin. We use the following models:\n\n1. BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF10: BIBREF1\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.08084",
    "question": "What is the size of the dataset?",
    "gold_answer": "",
    "answer_a": "How is it divided?\n< Explanation: The dataset contains 10,000 statements from 1000 debates, each with a target statement and a list of 1000 statements from the larger context. The dataset is divided into 1000 training and 1000 test sets, with 100 statements in each set. The training set is used to train the models, and the test set is used to evaluate the models. The dataset is publicly available at https://github.com/alexanderkryk/DebateRanking.\n\nThe dataset contains 10,0",
    "answer_b": "5,415 sentences. Explanation: We created a new dataset called CW-USPD-2016 (check-worthiness in the US presidential debates 2016) for finding check-worthy claims in context. In particular, we used four transcripts of the 2016 US election: one vice-presidential and three presidential debates. For each debate, we used the publicly-available manual analysis about it from nine reputable fact-checking sources, as shown in Table TABREF7.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the size of the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it divided?\n< Explanation: The dataset contains 10,000 statements from 1000 debates, each with a target statement and a list of 1000 statements from the larger context. The dataset is divided into 1000 training and 1000 test sets, with 100 statements in each set. The training set is used to train the models, and the test set is used to evaluate the models. The dataset is publicly available at https://github.com/alexanderkryk/DebateRanking.\n\nThe dataset contains 10,0\n\nAnswer B:\n5,415 sentences. Explanation: We created a new dataset called CW-USPD-2016 (check-worthiness in the US presidential debates 2016) for finding check-worthy claims in context. In particular, we used four transcripts of the 2016 US election: one vice-presidential and three presidential debates. For each debate, we used the publicly-available manual analysis about it from nine reputable fact-checking sources, as shown in Table TABREF7.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.08084",
    "question": "What models are trained?",
    "gold_answer": "",
    "answer_a": "SVM classifier with an RBF kernel. Explanation: We experimented with two learning algorithms. The first one is an SVM classifier with an RBF kernel. The second one is a deep feed-forward neural network (FNN) with two hidden layers (with 200 and 50 neurons, respectively) and a softmax output unit for the binary classification. We used ReLU BIBREF25 as the activation function and we trained the network with Stochastic Gradient Descent BIBREF26.",
    "answer_b": "What are the results?\n<blockquote>Our experiments show state-of-the-art results, outperforming a strong rivaling system by a margin, while also confirming the importance of the contextual information. The best model achieves 90.5% accuracy on the test set, and the best model achieves 91.1% accuracy on the test set. The best model achieves 91.1% accuracy on the test set, and the best model achieves 91.1% accuracy on the test set. The best model achieves 91.1% accuracy on the test set, and the best model achieves 9",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat models are trained?\n\nGround Truth Answer:\n\n\nAnswer A:\nSVM classifier with an RBF kernel. Explanation: We experimented with two learning algorithms. The first one is an SVM classifier with an RBF kernel. The second one is a deep feed-forward neural network (FNN) with two hidden layers (with 200 and 50 neurons, respectively) and a softmax output unit for the binary classification. We used ReLU BIBREF25 as the activation function and we trained the network with Stochastic Gradient Descent BIBREF26.\n\nAnswer B:\nWhat are the results?\n<blockquote>Our experiments show state-of-the-art results, outperforming a strong rivaling system by a margin, while also confirming the importance of the contextual information. The best model achieves 90.5% accuracy on the test set, and the best model achieves 91.1% accuracy on the test set. The best model achieves 91.1% accuracy on the test set, and the best model achieves 91.1% accuracy on the test set. The best model achieves 91.1% accuracy on the test set, and the best model achieves 9\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.08084",
    "question": "What is the strong rivaling system?",
    "gold_answer": "",
    "answer_a": "ClaimBuster. Explanation: The previous work that is most relevant to our work here is that of BIBREF9, who developed the ClaimBuster system, which assigns each sentence in a document a score, i.e., a number between 0 and 1 showing how worthy it is of fact-checking.",
    "answer_b": "How does it perform?\n< Explanation: The baseline model is a simple neural network that takes as input the target statement and the context, and outputs a score between 0 and 1 indicating the importance of the statement in the context. The baseline model is trained on the dataset of 1000 sentences, and is trained on the context of the 1000 sentences. The baseline model is trained on the context of the 1000 sentences, and is trained on the target statements. The baseline model is trained on the context of the 1000 sentences, and is trained on the target statements.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the strong rivaling system?\n\nGround Truth Answer:\n\n\nAnswer A:\nClaimBuster. Explanation: The previous work that is most relevant to our work here is that of BIBREF9, who developed the ClaimBuster system, which assigns each sentence in a document a score, i.e., a number between 0 and 1 showing how worthy it is of fact-checking.\n\nAnswer B:\nHow does it perform?\n< Explanation: The baseline model is a simple neural network that takes as input the target statement and the context, and outputs a score between 0 and 1 indicating the importance of the statement in the context. The baseline model is trained on the dataset of 1000 sentences, and is trained on the context of the 1000 sentences. The baseline model is trained on the context of the 1000 sentences, and is trained on the target statements. The baseline model is trained on the context of the 1000 sentences, and is trained on the target statements.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.08084",
    "question": "Where are the debates from?",
    "gold_answer": "",
    "answer_a": "How are they structured?\n< Explanation: The dataset is available at https://github.com/alexander-kravchenko/DebateRanking. It consists of 10000 statements from 10000 debates, each consisting of 1000 statements. The statements are in the form of a sentence, and each sentence is a statement of fact. The statements are in the form of a sentence, and each sentence is a statement of fact. The statements are in the form of a sentence, and each sentence is a statement of fact. The statements are in the form of a sentence, and",
    "answer_b": "four transcripts of the 2016 US election: one vice-presidential and three presidential debates. Explanation: We created a new dataset called CW-USPD-2016 (check-worthiness in the US presidential debates 2016) for finding check-worthy claims in context. In particular, we used four transcripts of the 2016 US election: one vice-presidential and three presidential debates. For each debate, we used the publicly-available manual analysis about it from nine reputable fact-checking sources, as shown in Table TABREF7.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhere are the debates from?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they structured?\n< Explanation: The dataset is available at https://github.com/alexander-kravchenko/DebateRanking. It consists of 10000 statements from 10000 debates, each consisting of 1000 statements. The statements are in the form of a sentence, and each sentence is a statement of fact. The statements are in the form of a sentence, and each sentence is a statement of fact. The statements are in the form of a sentence, and each sentence is a statement of fact. The statements are in the form of a sentence, and\n\nAnswer B:\nfour transcripts of the 2016 US election: one vice-presidential and three presidential debates. Explanation: We created a new dataset called CW-USPD-2016 (check-worthiness in the US presidential debates 2016) for finding check-worthy claims in context. In particular, we used four transcripts of the 2016 US election: one vice-presidential and three presidential debates. For each debate, we used the publicly-available manual analysis about it from nine reputable fact-checking sources, as shown in Table TABREF7.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.00479",
    "question": "By how much do they outpeform existing text denoising models?",
    "gold_answer": "",
    "answer_a": "Their F1 score outperforms an existing model by 0.017 on average for the random segmentation experiment.. Explanation: FLOAT SELECTED: Table 2: Left: Evaluation results for named entity typing. Right: Neighbors of character ngrams. Rank r = 1/r = 2: nearest / second-nearest neighbor.",
    "answer_b": "What is the baseline?\n< Explanation: We compare our model to the following baselines: (1) a baseline that uses a simple bag-of-words model, (2) a baseline that uses a simple word embedding model, (3) a baseline that uses a simple word embedding model with a bag-of-words representation, (4) a baseline that uses a simple word embedding model with a bag-of-words representation and a bag-of-words segmentation model, (5) a baseline that uses a simple word embedding model with a bag-of-words representation and a bag-of-words segmentation model, (6) a baseline that",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much do they outpeform existing text denoising models?\n\nGround Truth Answer:\n\n\nAnswer A:\nTheir F1 score outperforms an existing model by 0.017 on average for the random segmentation experiment.. Explanation: FLOAT SELECTED: Table 2: Left: Evaluation results for named entity typing. Right: Neighbors of character ngrams. Rank r = 1/r = 2: nearest / second-nearest neighbor.\n\nAnswer B:\nWhat is the baseline?\n< Explanation: We compare our model to the following baselines: (1) a baseline that uses a simple bag-of-words model, (2) a baseline that uses a simple word embedding model, (3) a baseline that uses a simple word embedding model with a bag-of-words representation, (4) a baseline that uses a simple word embedding model with a bag-of-words representation and a bag-of-words segmentation model, (5) a baseline that uses a simple word embedding model with a bag-of-words representation and a bag-of-words segmentation model, (6) a baseline that\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.00479",
    "question": "On which datasets do they evaluate their models?",
    "gold_answer": "",
    "answer_a": "What are the results?\n< Explanation: We evaluate our model on the following datasets: (1) the Penn Treebank (PTB) BIBREF1 , (2) the Stanford Sentiment Treebank (SST) BIBREF2 , (3) the Stanford Sentiment Treebank with Word Segmentation (SST-W) BIBREF3 , (4) the Stanford Sentiment Treebank with Tokenization (SST-T) BIBREF4 , (5) the Stanford Sentiment Treebank with Word Segmentation and Tokenization (SST-W-T) BIBREF5 , (6) the Stanford",
    "answer_b": "3 gigabyte English Wikipedia corpus. Explanation: We run experiments on INLINEFORM0 , a 3 gigabyte English Wikipedia corpus, and train word2vec skipgram (W2V, BIBREF1 ) and fastText skipgram (FTX, BIBREF2 ) models on INLINEFORM1 and its derivatives.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nOn which datasets do they evaluate their models?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n< Explanation: We evaluate our model on the following datasets: (1) the Penn Treebank (PTB) BIBREF1 , (2) the Stanford Sentiment Treebank (SST) BIBREF2 , (3) the Stanford Sentiment Treebank with Word Segmentation (SST-W) BIBREF3 , (4) the Stanford Sentiment Treebank with Tokenization (SST-T) BIBREF4 , (5) the Stanford Sentiment Treebank with Word Segmentation and Tokenization (SST-W-T) BIBREF5 , (6) the Stanford\n\nAnswer B:\n3 gigabyte English Wikipedia corpus. Explanation: We run experiments on INLINEFORM0 , a 3 gigabyte English Wikipedia corpus, and train word2vec skipgram (W2V, BIBREF1 ) and fastText skipgram (FTX, BIBREF2 ) models on INLINEFORM1 and its derivatives.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.01347",
    "question": "How do they determine demographics on an image?",
    "gold_answer": "",
    "answer_a": "What is the dataset used for?\nThey determine demographics on an image by comparing the age and gender of the face to the age and gender of the person in the image. They then use this information to determine the age and gender of the face in the dataset. The dataset used for this work is the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC) dataset. The ILSVRC dataset contains 15,000 images of 1,000 people, and is divided into 2012 categories. The categories are: \"person\", \"car\", \"truck\", \"",
    "answer_b": "using model driven face detection, apparent age annotation and gender annotation. Explanation: In order to provide demographic annotations at scale, there exist two feasible methods: crowdsourcing and model-driven annotations.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they determine demographics on an image?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the dataset used for?\nThey determine demographics on an image by comparing the age and gender of the face to the age and gender of the person in the image. They then use this information to determine the age and gender of the face in the dataset. The dataset used for this work is the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC) dataset. The ILSVRC dataset contains 15,000 images of 1,000 people, and is divided into 2012 categories. The categories are: \"person\", \"car\", \"truck\", \"\n\nAnswer B:\nusing model driven face detection, apparent age annotation and gender annotation. Explanation: In order to provide demographic annotations at scale, there exist two feasible methods: crowdsourcing and model-driven annotations.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.01347",
    "question": "What is the most underrepresented person group in ILSVRC?",
    "gold_answer": "",
    "answer_a": "How many people are there in the dataset?\nFor the most underrepresented group, we find that 41.62% of faces in ILSVRC appear as female, 1.71% appear as individuals above the age of 60, and males aged 15 to 29 account for 27.11%. We note that the presented model-driven framework is not fair for all intersectional groups, so annotation are subject to bias. We present this work as the starting point for future development of unbiased annotation models and for the study of downstream effects of imbalances in the demographics of ImageNet",
    "answer_b": "people over the age of 60. Explanation: We evaluate the training set of the ILSVRC 2012 subset of ImageNet (1000 synsets) and the `person' hierarchical synset of ImageNet (2833 synsets) with the proposed methodology. Face detections that receive a confidence score of 0.9 or higher move forward to the annotation phase. Statistics for both datasets are presented in Tables TABREF7 and TABREF10 . In these preliminary annotations, we find that females comprise only 41.62% of images in ILSVRC and 31.11% in the `person' subset of ImageNet, and people over the age of 60 are almost non-existent in ILSVRC, accounting for 1.71%.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the most underrepresented person group in ILSVRC?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow many people are there in the dataset?\nFor the most underrepresented group, we find that 41.62% of faces in ILSVRC appear as female, 1.71% appear as individuals above the age of 60, and males aged 15 to 29 account for 27.11%. We note that the presented model-driven framework is not fair for all intersectional groups, so annotation are subject to bias. We present this work as the starting point for future development of unbiased annotation models and for the study of downstream effects of imbalances in the demographics of ImageNet\n\nAnswer B:\npeople over the age of 60. Explanation: We evaluate the training set of the ILSVRC 2012 subset of ImageNet (1000 synsets) and the `person' hierarchical synset of ImageNet (2833 synsets) with the proposed methodology. Face detections that receive a confidence score of 0.9 or higher move forward to the annotation phase. Statistics for both datasets are presented in Tables TABREF7 and TABREF10 . In these preliminary annotations, we find that females comprise only 41.62% of images in ILSVRC and 31.11% in the `person' subset of ImageNet, and people over the age of 60 are almost non-existent in ILSVRC, accounting for 1.71%.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1711.03759",
    "question": "how many sentences did they annotate?",
    "gold_answer": "",
    "answer_a": "100 sentences. Explanation: We extract 100 sentences from CoNLL 2003 English NER BIBREF8 training data, with each sentence containing at least 4 entities. Two undergraduate students without any experience on those tools are invited to annotate those sentences.",
    "answer_b": "< Explanation: The annotation time of the proposed system is 1.5 times that of the existing annotation tools. The reason is that the proposed system can reduce the annotation time by half compared with existing annotation tools. And the annotation time can be further compressed by 16.47\\% through intelligent recommendation. The annotation time of the proposed system is 1.5 times that of the existing annotation tools. The reason is that the proposed system can reduce the annotation time by half compared with existing annotation tools. And the annotation time can be further compressed by 16.47\\% through intelligent recommendation. The annotation",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nhow many sentences did they annotate?\n\nGround Truth Answer:\n\n\nAnswer A:\n100 sentences. Explanation: We extract 100 sentences from CoNLL 2003 English NER BIBREF8 training data, with each sentence containing at least 4 entities. Two undergraduate students without any experience on those tools are invited to annotate those sentences.\n\nAnswer B:\n< Explanation: The annotation time of the proposed system is 1.5 times that of the existing annotation tools. The reason is that the proposed system can reduce the annotation time by half compared with existing annotation tools. And the annotation time can be further compressed by 16.47\\% through intelligent recommendation. The annotation time of the proposed system is 1.5 times that of the existing annotation tools. The reason is that the proposed system can reduce the annotation time by half compared with existing annotation tools. And the annotation time can be further compressed by 16.47\\% through intelligent recommendation. The annotation\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1711.03759",
    "question": "what dataset was used in their experiment?",
    "gold_answer": "",
    "answer_a": "< Explanation: We use the dataset of the 2017 NLP Challenge (NLP 2017) for annotation. The dataset consists of 10000 sentences annotated by 100 annotators. The annotations are divided into 10000 pairs of sentences, each pair is annotated by 2 annotators. The annotations are annotated by 2 annotators in the following order: 1st annotator annotates the first sentence, and 2nd annotator annotates the second sentence. The annotations are annotated by 2 annotators in the following order: 1",
    "answer_b": "CoNLL 2003 English NER. Explanation: We extract 100 sentences from CoNLL 2003 English NER BIBREF8 training data, with each sentence containing at least 4 entities.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat dataset was used in their experiment?\n\nGround Truth Answer:\n\n\nAnswer A:\n< Explanation: We use the dataset of the 2017 NLP Challenge (NLP 2017) for annotation. The dataset consists of 10000 sentences annotated by 100 annotators. The annotations are divided into 10000 pairs of sentences, each pair is annotated by 2 annotators. The annotations are annotated by 2 annotators in the following order: 1st annotator annotates the first sentence, and 2nd annotator annotates the second sentence. The annotations are annotated by 2 annotators in the following order: 1\n\nAnswer B:\nCoNLL 2003 English NER. Explanation: We extract 100 sentences from CoNLL 2003 English NER BIBREF8 training data, with each sentence containing at least 4 entities.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1711.03759",
    "question": "what are the existing annotation tools?",
    "gold_answer": "",
    "answer_a": "< Explanation: The annotation tools are divided into two categories: command line tools and shortcut keys tools. The command line tools are mainly used for collaborative user annotation, while the shortcut keys tools are mainly used for administrator evaluation and analysis. The annotation tools are divided into two categories: command line tools and shortcut keys tools. The command line tools are mainly used for collaborative user annotation, while the shortcut keys tools are mainly used for administrator evaluation and analysis. The annotation tools are divided into two categories: command line tools and shortcut keys tools. The command line tools are mainly used for collaborative user annotation, while the shortcut keys tools are mainly used for",
    "answer_b": "BIBREF2. Explanation: Existing annotation tools BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 mainly focus on providing a visual interface for user annotation process but rarely consider the post-annotation quality analysis, which is necessary due to the inter-annotator disagreement. In addition to the annotation quality, efficiency is also critical in large-scale annotation task, while being relatively less addressed in existing annotation tools BIBREF6 , BIBREF7 .",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the existing annotation tools?\n\nGround Truth Answer:\n\n\nAnswer A:\n< Explanation: The annotation tools are divided into two categories: command line tools and shortcut keys tools. The command line tools are mainly used for collaborative user annotation, while the shortcut keys tools are mainly used for administrator evaluation and analysis. The annotation tools are divided into two categories: command line tools and shortcut keys tools. The command line tools are mainly used for collaborative user annotation, while the shortcut keys tools are mainly used for administrator evaluation and analysis. The annotation tools are divided into two categories: command line tools and shortcut keys tools. The command line tools are mainly used for collaborative user annotation, while the shortcut keys tools are mainly used for\n\nAnswer B:\nBIBREF2. Explanation: Existing annotation tools BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 mainly focus on providing a visual interface for user annotation process but rarely consider the post-annotation quality analysis, which is necessary due to the inter-annotator disagreement. In addition to the annotation quality, efficiency is also critical in large-scale annotation task, while being relatively less addressed in existing annotation tools BIBREF6 , BIBREF7 .\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1810.13414",
    "question": "what ontologies did they use?",
    "gold_answer": "",
    "answer_a": "(see explanation below)\nThe OWL2 ontology is used as the base ontology for the experiments. The ontology is a subset of the OWL3 ontology, which is a subset of the OWL4 ontology. The OWL4 ontology is a subset of the OWL5 ontology, which is a subset of the OWL6 ontology. The OWL6 ontology is a subset of the OWL7 ontology, which is a subset of the OWL8 ontology. The OWL7 ontology is a subset of the OWL8 ontology, which is a subset of the OWL9 ontology. The OWL9 ontology is a",
    "answer_b": "Wine Ontology. Explanation: We used three ontologies: (i) the Wine Ontology, one of the most commonly used examples of owl ontologies; (ii) the m-piro ontology, which describes a collection of museum exhibits, was originally developed in the m-piro project BIBREF27 , was later ported to owl, and accompanies Naturalowl BIBREF11 ; and (iii) the Disease Ontology, which describes diseases, including their symptoms, causes etc.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat ontologies did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\n(see explanation below)\nThe OWL2 ontology is used as the base ontology for the experiments. The ontology is a subset of the OWL3 ontology, which is a subset of the OWL4 ontology. The OWL4 ontology is a subset of the OWL5 ontology, which is a subset of the OWL6 ontology. The OWL6 ontology is a subset of the OWL7 ontology, which is a subset of the OWL8 ontology. The OWL7 ontology is a subset of the OWL8 ontology, which is a subset of the OWL9 ontology. The OWL9 ontology is a\n\nAnswer B:\nWine Ontology. Explanation: We used three ontologies: (i) the Wine Ontology, one of the most commonly used examples of owl ontologies; (ii) the m-piro ontology, which describes a collection of museum exhibits, was originally developed in the m-piro project BIBREF27 , was later ported to owl, and accompanies Naturalowl BIBREF11 ; and (iii) the Disease Ontology, which describes diseases, including their symptoms, causes etc.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1711.04964",
    "question": "How much improvement is given on RACE by their introduced approach?",
    "gold_answer": "",
    "answer_a": "7.3% on RACE-M and 1.5% on RACE-H. Explanation: As shown in the table, on RACE-M, DFN leads to a 7.8% and 7.3% performance boost over GA and Stanford AR, respectively. On RACE-H, the outperformance is 1.5% and 2.7%.",
    "answer_b": "What is the best result reported on RACE?\n< Explanation: The results of the experiments are shown in Table 1. The best results are obtained by DFNs with a single attention strategy, which is the baseline. The results of DFNs with a multi-strategy attention process are shown in Table 2. The results of DFNs with a multi-step reasoning module are shown in Table 3. The results of DFNs with a multi-step reasoning module and a multi-strategy attention process are shown in Table 4. The results of DFNs with a multi-step reasoning module and a multi-strategy attention",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow much improvement is given on RACE by their introduced approach?\n\nGround Truth Answer:\n\n\nAnswer A:\n7.3% on RACE-M and 1.5% on RACE-H. Explanation: As shown in the table, on RACE-M, DFN leads to a 7.8% and 7.3% performance boost over GA and Stanford AR, respectively. On RACE-H, the outperformance is 1.5% and 2.7%.\n\nAnswer B:\nWhat is the best result reported on RACE?\n< Explanation: The results of the experiments are shown in Table 1. The best results are obtained by DFNs with a single attention strategy, which is the baseline. The results of DFNs with a multi-strategy attention process are shown in Table 2. The results of DFNs with a multi-step reasoning module are shown in Table 3. The results of DFNs with a multi-step reasoning module and a multi-strategy attention process are shown in Table 4. The results of DFNs with a multi-step reasoning module and a multi-strategy attention\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.04579",
    "question": "what pruning did they perform?",
    "gold_answer": "",
    "answer_a": "What was the impact on the accuracy?\n< Explanation: We first perform a baseline model without any pruning. We then perform a baseline model with the pruning step. We then perform a baseline model with the pruning step and the best performing model. We then perform a baseline model with the pruning step and the best performing model and the best performing model. We then perform a baseline model with the pruning step and the best performing model and the best performing model and the best performing model. We then perform a baseline model with the pruning step and the best performing model and the best performing model and the best performing model and the best performing model. We then",
    "answer_b": "eliminate spurious training data entries. Explanation: This algorithm applies a pruning step (discussed in Section SECREF3 ) to eliminate spurious training data entries.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat pruning did they perform?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat was the impact on the accuracy?\n< Explanation: We first perform a baseline model without any pruning. We then perform a baseline model with the pruning step. We then perform a baseline model with the pruning step and the best performing model. We then perform a baseline model with the pruning step and the best performing model and the best performing model. We then perform a baseline model with the pruning step and the best performing model and the best performing model and the best performing model. We then perform a baseline model with the pruning step and the best performing model and the best performing model and the best performing model and the best performing model. We then\n\nAnswer B:\neliminate spurious training data entries. Explanation: This algorithm applies a pruning step (discussed in Section SECREF3 ) to eliminate spurious training data entries.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.01116",
    "question": "How do they show that binary paragraph vectors capture semantics?",
    "gold_answer": "",
    "answer_a": "What are the semantics relevant for?\n< Explanation: We show that the binary paragraph vectors are able to capture semantics relevant for various domain-specific documents. We do this by comparing the performance of binary paragraph vectors with autoencoder-based binary codes. We show that the binary paragraph vectors are able to capture the semantics of the documents, and that the autoencoder-based binary codes are not. We also show that the binary paragraph vectors are able to capture the semantics of the documents, and that the autoencoder-based binary codes are not. We show that the binary paragraph vectors are able to capture the semantics of the documents, and that the autoencoder-based binary",
    "answer_b": "They perform information-retrieval tasks on popular benchmarks. Explanation: To assess the performance of binary paragraph vectors, we carried out experiments on three datasets: 20 Newsgroups, a cleansed version (also called v2) of Reuters Corpus Volume 1 (RCV1) and English Wikipedia.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they show that binary paragraph vectors capture semantics?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the semantics relevant for?\n< Explanation: We show that the binary paragraph vectors are able to capture semantics relevant for various domain-specific documents. We do this by comparing the performance of binary paragraph vectors with autoencoder-based binary codes. We show that the binary paragraph vectors are able to capture the semantics of the documents, and that the autoencoder-based binary codes are not. We also show that the binary paragraph vectors are able to capture the semantics of the documents, and that the autoencoder-based binary codes are not. We show that the binary paragraph vectors are able to capture the semantics of the documents, and that the autoencoder-based binary\n\nAnswer B:\nThey perform information-retrieval tasks on popular benchmarks. Explanation: To assess the performance of binary paragraph vectors, we carried out experiments on three datasets: 20 Newsgroups, a cleansed version (also called v2) of Reuters Corpus Volume 1 (RCV1) and English Wikipedia.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1611.01116",
    "question": "Which training dataset do they use?",
    "gold_answer": "",
    "answer_a": "20 Newsgroups. Explanation: To assess the performance of binary paragraph vectors, we carried out experiments on three datasets: 20 Newsgroups, a cleansed version (also called v2) of Reuters Corpus Volume 1 (RCV1) and English Wikipedia",
    "answer_b": "How many documents are in the training set?\n< Explanation: We use the Penn Treebank BIBREF10 corpus as our training set. We use the same corpus for both evaluation and training. We use 1000 documents for training and 1000 for evaluation. We use 10000 documents for training and 10000 for evaluation. We use 100000 documents for training and 100000 for evaluation. We use 1000000 documents for training and 1000000 for evaluation.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich training dataset do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\n20 Newsgroups. Explanation: To assess the performance of binary paragraph vectors, we carried out experiments on three datasets: 20 Newsgroups, a cleansed version (also called v2) of Reuters Corpus Volume 1 (RCV1) and English Wikipedia\n\nAnswer B:\nHow many documents are in the training set?\n< Explanation: We use the Penn Treebank BIBREF10 corpus as our training set. We use the same corpus for both evaluation and training. We use 1000 documents for training and 1000 for evaluation. We use 10000 documents for training and 10000 for evaluation. We use 100000 documents for training and 100000 for evaluation. We use 1000000 documents for training and 1000000 for evaluation.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1807.03674",
    "question": "How long is the dataset?",
    "gold_answer": "",
    "answer_a": "What is the size of the dataset?\n< Explanation: The dataset consists of 1000 death certificates from 2016. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health.",
    "answer_b": "125383. Explanation: This training set contained 125383 death certificates.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long is the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the dataset?\n< Explanation: The dataset consists of 1000 death certificates from 2016. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health. The data were collected from the French Ministry of Health.\n\nAnswer B:\n125383. Explanation: This training set contained 125383 death certificates.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1807.03674",
    "question": "What are the ICD-10 codes?",
    "gold_answer": "",
    "answer_a": "International Classification of Diseases, 10th revision (ICD-10) BIBREF1. Explanation: More precisely, this task consists in automatically coding death certificates using the International Classification of Diseases, 10th revision (ICD-10) BIBREF1 .",
    "answer_b": "How were they normalized?\n< Explanation: The ICD-10 codes are the codes used by the International Classification of Diseases (ICD) to classify diseases and conditions. They are used to identify the diseases and conditions that are relevant to the study. The codes are divided into 10 classes, each with a code. The codes are used to identify the diseases and conditions that are relevant to the study. The codes are divided into 10 classes, each with a code. The codes are used to identify the diseases and conditions that are relevant to the study. The codes are divided into 10 classes, each with a",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the ICD-10 codes?\n\nGround Truth Answer:\n\n\nAnswer A:\nInternational Classification of Diseases, 10th revision (ICD-10) BIBREF1. Explanation: More precisely, this task consists in automatically coding death certificates using the International Classification of Diseases, 10th revision (ICD-10) BIBREF1 .\n\nAnswer B:\nHow were they normalized?\n< Explanation: The ICD-10 codes are the codes used by the International Classification of Diseases (ICD) to classify diseases and conditions. They are used to identify the diseases and conditions that are relevant to the study. The codes are divided into 10 classes, each with a code. The codes are used to identify the diseases and conditions that are relevant to the study. The codes are divided into 10 classes, each with a code. The codes are used to identify the diseases and conditions that are relevant to the study. The codes are divided into 10 classes, each with a\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1811.00127",
    "question": "What media sources do they use?",
    "gold_answer": "",
    "answer_a": "How do they measure the similarity?\n< Explanation: We use the following sources: (1) Twitter, (2) Facebook, (3) Wikipedia, (4) Wikipedia articles, (5) Wikipedia articles with a specific topic, (6) Wikipedia articles with a specific topic, (7) Wikipedia articles with a specific topic, (8) Wikipedia articles with a specific topic, (9) Wikipedia articles with a specific topic, (10) Wikipedia articles with a specific topic, (11) Wikipedia articles with a specific topic, (12) Wikipedia articles with a specific topic, (13) Wikipedia articles with a",
    "answer_b": "Swedish online data from 2018 crawled by Trendiction. Explanation: We built Doc2Vec embeddings BIBREF27 on Swedish online data from 2018 crawled by Trendiction and manually scraped party platforms from the eight parties in parliament and Feministiskt Initiativ (Feminist Initiative).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat media sources do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they measure the similarity?\n< Explanation: We use the following sources: (1) Twitter, (2) Facebook, (3) Wikipedia, (4) Wikipedia articles, (5) Wikipedia articles with a specific topic, (6) Wikipedia articles with a specific topic, (7) Wikipedia articles with a specific topic, (8) Wikipedia articles with a specific topic, (9) Wikipedia articles with a specific topic, (10) Wikipedia articles with a specific topic, (11) Wikipedia articles with a specific topic, (12) Wikipedia articles with a specific topic, (13) Wikipedia articles with a\n\nAnswer B:\nSwedish online data from 2018 crawled by Trendiction. Explanation: We built Doc2Vec embeddings BIBREF27 on Swedish online data from 2018 crawled by Trendiction and manually scraped party platforms from the eight parties in parliament and Feministiskt Initiativ (Feminist Initiative).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1602.07563",
    "question": "What evidence is presented that humans perceive the sentiment classes as ordered?",
    "gold_answer": "",
    "answer_a": "What is the evidence?\n< Explanation: We show that the sentiment classes are ordered in the training set. We use the following measures to quantify the order of the classes: (1) the number of correct predictions for each class, (2) the number of correct predictions for each class and the number of incorrect predictions for each class, and (3) the number of correct predictions for each class and the number of correct predictions for each class. We find that the number of correct predictions for each class is significantly higher than the number of incorrect predictions for each class. The number of correct predictions for each class is also significantly higher than the number",
    "answer_b": "average agreement is 18% higher with INLINEFORM7 than with INLINEFORM8. Explanation: First, lets compare the agreements in terms of two variants of INLINEFORM0 : INLINEFORM1 (interval) and INLINEFORM2 (nominal). The difference between the two measures is that INLINEFORM3 assigns four times higher cost to extreme disagreements (between the negative and positive classes) than INLINEFORM4 . A measure which yields higher agreements hints at the nature of sentiment class ordering as perceived by humans.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat evidence is presented that humans perceive the sentiment classes as ordered?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the evidence?\n< Explanation: We show that the sentiment classes are ordered in the training set. We use the following measures to quantify the order of the classes: (1) the number of correct predictions for each class, (2) the number of correct predictions for each class and the number of incorrect predictions for each class, and (3) the number of correct predictions for each class and the number of correct predictions for each class. We find that the number of correct predictions for each class is significantly higher than the number of incorrect predictions for each class. The number of correct predictions for each class is also significantly higher than the number\n\nAnswer B:\naverage agreement is 18% higher with INLINEFORM7 than with INLINEFORM8. Explanation: First, lets compare the agreements in terms of two variants of INLINEFORM0 : INLINEFORM1 (interval) and INLINEFORM2 (nominal). The difference between the two measures is that INLINEFORM3 assigns four times higher cost to extreme disagreements (between the negative and positive classes) than INLINEFORM4 . A measure which yields higher agreements hints at the nature of sentiment class ordering as perceived by humans.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1602.07563",
    "question": "What size of dataset is sufficiently large for the model performance to approach the inter-annotator agreement?",
    "gold_answer": "",
    "answer_a": "What is the weakest point of different datasets?\n< Explanation: We use the following metrics to measure the quality of training data: inter-annotator agreement (IAA), inter-annotator agreement score (IAS), and inter-annotator agreement score (IAS) for the top 10 models. We use the following thresholds for the IAA and IAS: 0.5, 0.7, and 0.9. We also use the following thresholds for the IAS: 0.5, 0.7, and 0.9. We use the following thresholds for the IAS",
    "answer_b": "around 100,000 annotations. Explanation: The peak performance depends on the inter-annotator agreement and we estimate that around 100,000 annotations are needed.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat size of dataset is sufficiently large for the model performance to approach the inter-annotator agreement?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the weakest point of different datasets?\n< Explanation: We use the following metrics to measure the quality of training data: inter-annotator agreement (IAA), inter-annotator agreement score (IAS), and inter-annotator agreement score (IAS) for the top 10 models. We use the following thresholds for the IAA and IAS: 0.5, 0.7, and 0.9. We also use the following thresholds for the IAS: 0.5, 0.7, and 0.9. We use the following thresholds for the IAS\n\nAnswer B:\naround 100,000 annotations. Explanation: The peak performance depends on the inter-annotator agreement and we estimate that around 100,000 annotations are needed.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1602.07563",
    "question": "Which measures of inter-annotator agreement are used?",
    "gold_answer": "",
    "answer_a": "Krippendorff's Alpha-reliability. Explanation: Krippendorff's Alpha-reliability ( INLINEFORM0 ) BIBREF6 is a generalization of several specialized agreement measures. It works for any number of annotators, and is applicable to different variable types and metrics (e.g., nominal, ordered, interval, etc.).",
    "answer_b": "What is the best agreement measure?\n<blockquote>We use the following measures of inter-annotator agreement: (1) the kappa coefficient, (2) the Cohen's Kappa coefficient, (3) the intraclass correlation coefficient, (4) the intraclass reliability coefficient, (5) the intraclass reliability coefficient, (6) the intraclass reliability coefficient, (7) the intraclass reliability coefficient, (8) the intraclass reliability coefficient, (9) the intraclass reliability coefficient, (10) the intraclass reliability coefficient, (11) the intraclass reliability coefficient, (12) the intr",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich measures of inter-annotator agreement are used?\n\nGround Truth Answer:\n\n\nAnswer A:\nKrippendorff's Alpha-reliability. Explanation: Krippendorff's Alpha-reliability ( INLINEFORM0 ) BIBREF6 is a generalization of several specialized agreement measures. It works for any number of annotators, and is applicable to different variable types and metrics (e.g., nominal, ordered, interval, etc.).\n\nAnswer B:\nWhat is the best agreement measure?\n<blockquote>We use the following measures of inter-annotator agreement: (1) the kappa coefficient, (2) the Cohen's Kappa coefficient, (3) the intraclass correlation coefficient, (4) the intraclass reliability coefficient, (5) the intraclass reliability coefficient, (6) the intraclass reliability coefficient, (7) the intraclass reliability coefficient, (8) the intraclass reliability coefficient, (9) the intraclass reliability coefficient, (10) the intraclass reliability coefficient, (11) the intraclass reliability coefficient, (12) the intr\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1602.07563",
    "question": "What statistical test(s) is used to compare the top classification models?",
    "gold_answer": "",
    "answer_a": "Friedman-Nemenyi test. Explanation: Fig FIGREF43 gives the results of the Friedman-Nemenyi test for the six classifiers trained in this study.",
    "answer_b": "What is the weakest point of the datasets?\n<blockquote>We use the following statistical tests: (1) the Hamming distance between the predicted and the ground truth labels, (2) the F1 score, (3) the Spearman correlation coefficient, (4) the Krippendorff's alpha coefficient, (5) the Krippendorff's tau coefficient, (6) the Krippendorff's g coefficient, (7) the Krippendorff's b coefficient, (8) the Krippendorff's delta coefficient, (9) the Krippendorff's omega coefficient, (10)",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat statistical test(s) is used to compare the top classification models?\n\nGround Truth Answer:\n\n\nAnswer A:\nFriedman-Nemenyi test. Explanation: Fig FIGREF43 gives the results of the Friedman-Nemenyi test for the six classifiers trained in this study.\n\nAnswer B:\nWhat is the weakest point of the datasets?\n<blockquote>We use the following statistical tests: (1) the Hamming distance between the predicted and the ground truth labels, (2) the F1 score, (3) the Spearman correlation coefficient, (4) the Krippendorff's alpha coefficient, (5) the Krippendorff's tau coefficient, (6) the Krippendorff's g coefficient, (7) the Krippendorff's b coefficient, (8) the Krippendorff's delta coefficient, (9) the Krippendorff's omega coefficient, (10)\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.10001",
    "question": "What is the baseline?",
    "gold_answer": "",
    "answer_a": "rule-based unigram matching baseline. Explanation: We also implement a rule-based unigram matching baseline, which takes the entry with highest unigram overlap with the query string to be the top match.",
    "answer_b": "What is the baseline performance?\n<sep><font size=\"2\">Baseline: </font>baseline is a simple chatbot that uses a simple machine learning model to classify the user's intent and then use that intent to search for hotels. </font>baseline is a simple chatbot that uses a simple machine learning model to classify the user's intent and then use that intent to search for hotels. </font>baseline is a simple chatbot that uses a simple machine learning model to classify the user's intent and then use that intent to search for hotels. </font>baseline is a simple chatbot that uses a simple machine learning model",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the baseline?\n\nGround Truth Answer:\n\n\nAnswer A:\nrule-based unigram matching baseline. Explanation: We also implement a rule-based unigram matching baseline, which takes the entry with highest unigram overlap with the query string to be the top match.\n\nAnswer B:\nWhat is the baseline performance?\n<sep><font size=\"2\">Baseline: </font>baseline is a simple chatbot that uses a simple machine learning model to classify the user's intent and then use that intent to search for hotels. </font>baseline is a simple chatbot that uses a simple machine learning model to classify the user's intent and then use that intent to search for hotels. </font>baseline is a simple chatbot that uses a simple machine learning model to classify the user's intent and then use that intent to search for hotels. </font>baseline is a simple chatbot that uses a simple machine learning model\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.10001",
    "question": "How is their NER model trained?",
    "gold_answer": "",
    "answer_a": "Using SpaCy. Explanation: We use SpaCy to train custom NER models.",
    "answer_b": "What is the accuracy of the model?\n<sep><font size=\"2\">Answer: The NER model is trained using a combination of supervised and unsupervised learning. The supervised learning is done using a large corpus of hotel reviews. The unsupervised learning is done using a large corpus of hotel reviews. The model is trained using a combination of supervised and unsupervised learning. The model is trained using a combination of supervised and unsupervised learning. The model is trained using a combination of supervised and unsupervised learning. The model is trained using a combination of supervised and unsupervised learning. The model is trained using",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is their NER model trained?\n\nGround Truth Answer:\n\n\nAnswer A:\nUsing SpaCy. Explanation: We use SpaCy to train custom NER models.\n\nAnswer B:\nWhat is the accuracy of the model?\n<sep><font size=\"2\">Answer: The NER model is trained using a combination of supervised and unsupervised learning. The supervised learning is done using a large corpus of hotel reviews. The unsupervised learning is done using a large corpus of hotel reviews. The model is trained using a combination of supervised and unsupervised learning. The model is trained using a combination of supervised and unsupervised learning. The model is trained using a combination of supervised and unsupervised learning. The model is trained using a combination of supervised and unsupervised learning. The model is trained using\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.10001",
    "question": "How well does the system perform?",
    "gold_answer": "",
    "answer_a": "For NER,  combined entity model achieves the best performance (F1 0.96). For IR, BERT+fine-tuning model achieves TOP-1 Recall 0.895 and Top-3 Recall 0.961.. Explanation: FLOAT SELECTED: TABLE II RESULTS OF NER MODEL",
    "answer_b": "What are the limitations of the system?\n<blockquote>Our system achieved 99.9% accuracy on the test set, which is better than the 99.7% accuracy achieved by the previous system. However, the accuracy of the system is still lower than the accuracy of the previous system. The reason is that the previous system used a large amount of data to train the model, while our system only used a small amount of data. </blockquote>\n<blockquote>Our system achieved 99.9% accuracy on the test set, which is better than the 99.7% accuracy achieved by the previous",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow well does the system perform?\n\nGround Truth Answer:\n\n\nAnswer A:\nFor NER,  combined entity model achieves the best performance (F1 0.96). For IR, BERT+fine-tuning model achieves TOP-1 Recall 0.895 and Top-3 Recall 0.961.. Explanation: FLOAT SELECTED: TABLE II RESULTS OF NER MODEL\n\nAnswer B:\nWhat are the limitations of the system?\n<blockquote>Our system achieved 99.9% accuracy on the test set, which is better than the 99.7% accuracy achieved by the previous system. However, the accuracy of the system is still lower than the accuracy of the previous system. The reason is that the previous system used a large amount of data to train the model, while our system only used a small amount of data. </blockquote>\n<blockquote>Our system achieved 99.9% accuracy on the test set, which is better than the 99.7% accuracy achieved by the previous\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.10001",
    "question": "Where does their information come from?",
    "gold_answer": "",
    "answer_a": "From conversions between users and customer support agents through their partners, and professional annotators creating data.. Explanation: The hotel search is backed by a database of approximately 100,000 cities and 300,000 hotels, populated using data from our partners.",
    "answer_b": "What are the limitations of the data?\n<sep><br/ >Answer: We use the hotel search data from the Travelocity API. We also use the data from the OpenSearch Engine to train our models. We use the OpenSearch Engine to train our models. We use the OpenSearch Engine to train our models. We use the OpenSearch Engine to train our models. We use the OpenSearch Engine to train our models. We use the OpenSearch Engine to train our models. We use the OpenSearch Engine to train our models. We use the OpenSearch Engine to train our models. We use the OpenSearch Engine to train",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhere does their information come from?\n\nGround Truth Answer:\n\n\nAnswer A:\nFrom conversions between users and customer support agents through their partners, and professional annotators creating data.. Explanation: The hotel search is backed by a database of approximately 100,000 cities and 300,000 hotels, populated using data from our partners.\n\nAnswer B:\nWhat are the limitations of the data?\n<sep><br/ >Answer: We use the hotel search data from the Travelocity API. We also use the data from the OpenSearch Engine to train our models. We use the OpenSearch Engine to train our models. We use the OpenSearch Engine to train our models. We use the OpenSearch Engine to train our models. We use the OpenSearch Engine to train our models. We use the OpenSearch Engine to train our models. We use the OpenSearch Engine to train our models. We use the OpenSearch Engine to train our models. We use the OpenSearch Engine to train\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.10001",
    "question": "What intents do they have?",
    "gold_answer": "",
    "answer_a": "What entities do they recognize?\n<sep><font size=\"2\">INTENT</font> The chatbot has the following intents: (1) hotel search, (2) hotel search by city, (3) hotel search by state, (4) hotel search by country, (5) hotel search by language, (6) hotel search by price range, (7) hotel search by room type, (8) hotel search by room type and price range, (9) hotel search by room type and price range and room type and price range and room type and price range and room type and price range and room type and",
    "answer_b": "thanks. Explanation: The intent model processes each incoming user message and classifies it as one of several intents. The most common intents are thanks, cancel, stop, search, and unknown (described in Table TABREF12); these intents were chosen for automation based on volume, ease of classification, and business impact",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat intents do they have?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat entities do they recognize?\n<sep><font size=\"2\">INTENT</font> The chatbot has the following intents: (1) hotel search, (2) hotel search by city, (3) hotel search by state, (4) hotel search by country, (5) hotel search by language, (6) hotel search by price range, (7) hotel search by room type, (8) hotel search by room type and price range, (9) hotel search by room type and price range and room type and price range and room type and price range and room type and price range and room type and\n\nAnswer B:\nthanks. Explanation: The intent model processes each incoming user message and classifies it as one of several intents. The most common intents are thanks, cancel, stop, search, and unknown (described in Table TABREF12); these intents were chosen for automation based on volume, ease of classification, and business impact\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.06919",
    "question": "What datasets are used for this tasks?",
    "gold_answer": "",
    "answer_a": "CNN/DailyMail corpus. Explanation: We conduct our summarization experiments using the anonymized CNN/DailyMail corpus BIBREF20.",
    "answer_b": "How are they evaluated?\n< Explanation: We use the following datasets for our experiments: (1) the Abstractive Summarization dataset (AS) BIBREF1 , which consists of 1000 documents, each of which is a summary of the preceding 100 documents. The documents are labeled as either informative (IF) or informative but irrelevant (IR) based on the content of the documents. The dataset is available at http://www.cs.cornell.edu/~james/abstractive/. (2) the Popularity dataset (PD) BIBREF2 , which consists of 10",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat datasets are used for this tasks?\n\nGround Truth Answer:\n\n\nAnswer A:\nCNN/DailyMail corpus. Explanation: We conduct our summarization experiments using the anonymized CNN/DailyMail corpus BIBREF20.\n\nAnswer B:\nHow are they evaluated?\n< Explanation: We use the following datasets for our experiments: (1) the Abstractive Summarization dataset (AS) BIBREF1 , which consists of 1000 documents, each of which is a summary of the preceding 100 documents. The documents are labeled as either informative (IF) or informative but irrelevant (IR) based on the content of the documents. The dataset is available at http://www.cs.cornell.edu/~james/abstractive/. (2) the Popularity dataset (PD) BIBREF2 , which consists of 10\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.06919",
    "question": "How big are improvements on these tasks?",
    "gold_answer": "",
    "answer_a": "What are the metrics used?\n< Explanation: We evaluate the proposed approach on two different tasks: abstractive summarization and popularity prediction of online petitions. We use the following metrics: BLEU, METEOR, and F1. We also use the following metrics: Recall, Precision, F1, and NDCG. We also report the results of the RST parser on the same tasks. The RST parser is a parser that uses the RST parser to generate the discourse spans. The RST parser is a parser that uses the RST parser to generate the discourse spans. The RST parser is a parser that",
    "answer_b": "Observing that our model generally has better recall (Table TABREF17) and its summaries tend to be more verbose (e.g. second example in Figure FIGREF20). Explanation: Observing that our model generally has better recall (Table TABREF17) and its summaries tend to be more verbose (e.g. second example in Figure FIGREF20), we calculated the average length of generated summaries for PG$+$Cov and M1-latent, and found that they are of length 55.2 and 64.4 words respectively. This suggests that although discourse information helps the summarization model overall (based on consistent improvement in F1), the negative side effect is that the summaries tend to be longer and potentially more repetitive.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow big are improvements on these tasks?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the metrics used?\n< Explanation: We evaluate the proposed approach on two different tasks: abstractive summarization and popularity prediction of online petitions. We use the following metrics: BLEU, METEOR, and F1. We also use the following metrics: Recall, Precision, F1, and NDCG. We also report the results of the RST parser on the same tasks. The RST parser is a parser that uses the RST parser to generate the discourse spans. The RST parser is a parser that uses the RST parser to generate the discourse spans. The RST parser is a parser that\n\nAnswer B:\nObserving that our model generally has better recall (Table TABREF17) and its summaries tend to be more verbose (e.g. second example in Figure FIGREF20). Explanation: Observing that our model generally has better recall (Table TABREF17) and its summaries tend to be more verbose (e.g. second example in Figure FIGREF20), we calculated the average length of generated summaries for PG$+$Cov and M1-latent, and found that they are of length 55.2 and 64.4 words respectively. This suggests that although discourse information helps the summarization model overall (based on consistent improvement in F1), the negative side effect is that the summaries tend to be longer and potentially more repetitive.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1710.10380",
    "question": "Which downstream tasks are considered?",
    "gold_answer": "",
    "answer_a": "semantic relatedness (SICK, BIBREF17 ), paraphrase detection (MSRP, BIBREF19 ), question-type classification (TREC, BIBREF20 ), and five benchmark sentiment and subjective datasets, which include movie review sentiment (MR, BIBREF21 , SST, BIBREF22 ), customer product reviews (CR, BIBREF23 ), subjectivity/objectivity classification (SUBJ, BIBREF24 ), opinion polarity (MPQA, BIBREF25 ), semantic textual similarity (STS14, BIBREF18 ), and SNLI BIBREF13. Explanation: The downstream tasks for evaluation include semantic relatedness (SICK, BIBREF17 ), paraphrase detection (MSRP, BIBREF19 ), question-type classification (TREC, BIBREF20 ), and five benchmark sentiment and subjective datasets, which include movie review sentiment (MR, BIBREF21 , SST, BIBREF22 ), customer product reviews (CR, BIBREF23 ), subjectivity/objectivity classification (SUBJ, BIBREF24 ), opinion polarity (MPQA, BIBREF25 ), semantic textual similarity (STS14, BIBREF18 ), and SNLI BIBREF13 .",
    "answer_b": "What are their performance?\n< Explanation: We evaluate our model on two different downstream tasks: sentence classification and named entity recognition (NER). We use the Penn Treebank (PTB) BIBREF10 and the Stanford Sentiment Treebank (SST) BIBREF11 as our training and evaluation corpora. We use the Stanford N-grams (SNN) BIBREF12 as our sentence representation model. We use the Stanford N-grams (SNN) BIBREF12 as our sentence representation model. We use the Stanford N-grams (SNN) BIBREF12 as our",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich downstream tasks are considered?\n\nGround Truth Answer:\n\n\nAnswer A:\nsemantic relatedness (SICK, BIBREF17 ), paraphrase detection (MSRP, BIBREF19 ), question-type classification (TREC, BIBREF20 ), and five benchmark sentiment and subjective datasets, which include movie review sentiment (MR, BIBREF21 , SST, BIBREF22 ), customer product reviews (CR, BIBREF23 ), subjectivity/objectivity classification (SUBJ, BIBREF24 ), opinion polarity (MPQA, BIBREF25 ), semantic textual similarity (STS14, BIBREF18 ), and SNLI BIBREF13. Explanation: The downstream tasks for evaluation include semantic relatedness (SICK, BIBREF17 ), paraphrase detection (MSRP, BIBREF19 ), question-type classification (TREC, BIBREF20 ), and five benchmark sentiment and subjective datasets, which include movie review sentiment (MR, BIBREF21 , SST, BIBREF22 ), customer product reviews (CR, BIBREF23 ), subjectivity/objectivity classification (SUBJ, BIBREF24 ), opinion polarity (MPQA, BIBREF25 ), semantic textual similarity (STS14, BIBREF18 ), and SNLI BIBREF13 .\n\nAnswer B:\nWhat are their performance?\n< Explanation: We evaluate our model on two different downstream tasks: sentence classification and named entity recognition (NER). We use the Penn Treebank (PTB) BIBREF10 and the Stanford Sentiment Treebank (SST) BIBREF11 as our training and evaluation corpora. We use the Stanford N-grams (SNN) BIBREF12 as our sentence representation model. We use the Stanford N-grams (SNN) BIBREF12 as our sentence representation model. We use the Stanford N-grams (SNN) BIBREF12 as our\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1710.10380",
    "question": "How long are the two unlabelled corpora?",
    "gold_answer": "",
    "answer_a": "Amazon Review dataset BIBREF26 with 142 million sentences, about twice as large as BookCorpus. Explanation: To compare the effect of different corpora, we also trained two models on Amazon Book Review dataset (without ratings) which is the largest subset of the Amazon Review dataset BIBREF26 with 142 million sentences, about twice as large as BookCorpus.",
    "answer_b": "What are the sizes?\n< Explanation: The two corpora are 100,000 and 100,000 words respectively. The first one is a large corpus of English news articles, while the second one is a large corpus of English news articles with a focus on science and technology. The corpus sizes are 100,000 words and 100,000 words respectively. The corpus sizes are 100,000 words and 100,000 words respectively. The corpus sizes are 100,000 words",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow long are the two unlabelled corpora?\n\nGround Truth Answer:\n\n\nAnswer A:\nAmazon Review dataset BIBREF26 with 142 million sentences, about twice as large as BookCorpus. Explanation: To compare the effect of different corpora, we also trained two models on Amazon Book Review dataset (without ratings) which is the largest subset of the Amazon Review dataset BIBREF26 with 142 million sentences, about twice as large as BookCorpus.\n\nAnswer B:\nWhat are the sizes?\n< Explanation: The two corpora are 100,000 and 100,000 words respectively. The first one is a large corpus of English news articles, while the second one is a large corpus of English news articles with a focus on science and technology. The corpus sizes are 100,000 words and 100,000 words respectively. The corpus sizes are 100,000 words and 100,000 words respectively. The corpus sizes are 100,000 words\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.11025",
    "question": "How is the impact of ParityBOT analyzed?",
    "gold_answer": "",
    "answer_a": "Please present your answer in table form.\n<table> <tbody> <tr> <td>Impact of ParityBOT on the number of tweets containing abusive content </td> </tr> <tr> <td>Number of tweets containing abusive content </td> </tr> <tr> <td>Number of tweets containing abusive content per gender </td> </tr> <tr> <td>Number of tweets containing abusive content per gender per year </td> </tr> <tr> <td>Number of tweets containing abusive content per gender per year per year </td> </tr> <tr> <td",
    "answer_b": "interviewing individuals involved in government ($n=5$). Explanation: We evaluated the social impact of our system by interviewing individuals involved in government ($n=5$). We designed a discussion guide based on user experience research interview standards to speak with politicians in relevant jurisdictions BIBREF18.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is the impact of ParityBOT analyzed?\n\nGround Truth Answer:\n\n\nAnswer A:\nPlease present your answer in table form.\n<table> <tbody> <tr> <td>Impact of ParityBOT on the number of tweets containing abusive content </td> </tr> <tr> <td>Number of tweets containing abusive content </td> </tr> <tr> <td>Number of tweets containing abusive content per gender </td> </tr> <tr> <td>Number of tweets containing abusive content per gender per year </td> </tr> <tr> <td>Number of tweets containing abusive content per gender per year per year </td> </tr> <tr> <td\n\nAnswer B:\ninterviewing individuals involved in government ($n=5$). Explanation: We evaluated the social impact of our system by interviewing individuals involved in government ($n=5$). We designed a discussion guide based on user experience research interview standards to speak with politicians in relevant jurisdictions BIBREF18.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.11025",
    "question": "What public online harassment datasets was the system validated on?",
    "gold_answer": "",
    "answer_a": "20194 cleaned, unique tweets identified as either hateful and not hateful from previous research BIBREF22. Explanation: For validation, we found the most relevant features and set an abusive prediction threshold by using a dataset of 20194 cleaned, unique tweets identified as either hateful and not hateful from previous research BIBREF22.",
    "answer_b": "Is it a quantitative or qualitative assessment?\n< Explanation: We validated ParityBOT on two datasets: the 2019 Alberta provincial election dataset and the 2019 Canadian federal election dataset. The Alberta dataset was collected from the Alberta Elections website. The Canadian dataset was collected from the Canadian Elections website. The datasets were collected from 2019. The datasets were collected from the 2019 Alberta and 2019 Canadian elections. The datasets were collected from the 2019 Alberta and 2019 Canadian elections. The datasets were collected from the 2019",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat public online harassment datasets was the system validated on?\n\nGround Truth Answer:\n\n\nAnswer A:\n20194 cleaned, unique tweets identified as either hateful and not hateful from previous research BIBREF22. Explanation: For validation, we found the most relevant features and set an abusive prediction threshold by using a dataset of 20194 cleaned, unique tweets identified as either hateful and not hateful from previous research BIBREF22.\n\nAnswer B:\nIs it a quantitative or qualitative assessment?\n< Explanation: We validated ParityBOT on two datasets: the 2019 Alberta provincial election dataset and the 2019 Canadian federal election dataset. The Alberta dataset was collected from the Alberta Elections website. The Canadian dataset was collected from the Canadian Elections website. The datasets were collected from 2019. The datasets were collected from the 2019 Alberta and 2019 Canadian elections. The datasets were collected from the 2019 Alberta and 2019 Canadian elections. The datasets were collected from the 2019\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.11025",
    "question": "Where do the supportive tweets about women come from? Are they automatically or manually generated?",
    "gold_answer": "",
    "answer_a": "Manualy (volunteers composed them). Explanation: ParityBOT sent positivitweets composed by volunteers.",
    "answer_b": "How do they differ from hateful tweets?\n< Explanation: We found that the bot was able to generate 100% of the tweets it responded to. We also found that the bot was able to generate 100% of the tweets it responded to. We found that the bot was able to generate 100% of the tweets it responded to. We found that the bot was able to generate 100% of the tweets it responded to. We found that the bot was able to generate 100% of the tweets it responded to. We found that the bot was able to generate 1",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhere do the supportive tweets about women come from? Are they automatically or manually generated?\n\nGround Truth Answer:\n\n\nAnswer A:\nManualy (volunteers composed them). Explanation: ParityBOT sent positivitweets composed by volunteers.\n\nAnswer B:\nHow do they differ from hateful tweets?\n< Explanation: We found that the bot was able to generate 100% of the tweets it responded to. We also found that the bot was able to generate 100% of the tweets it responded to. We found that the bot was able to generate 100% of the tweets it responded to. We found that the bot was able to generate 100% of the tweets it responded to. We found that the bot was able to generate 100% of the tweets it responded to. We found that the bot was able to generate 1\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1911.11025",
    "question": "How are the hateful tweets aimed at women detected/classified?",
    "gold_answer": "",
    "answer_a": "Is the classification done automatically or manually?\n< Explanation: The hateful tweets were classified using a machine learning model trained on a dataset of 10,000 tweets. The model was trained on a dataset of 10,000 tweets, which were manually labeled as either hateful or not hateful. The model was trained on a dataset of 10,000 tweets, which were manually labeled as either hateful or not hateful. The model was trained on a dataset of 10,000 tweets, which were manually labeled as either hateful or not hateful. The model was trained on a dataset of",
    "answer_b": "The text analysis models classify a tweet by using, as features, the outputs from Perspective API from Jigsaw BIBREF10, HateSonar BIBREF11, and VADER sentiment models BIBREF12. Explanation: The text analysis models classify a tweet by using, as features, the outputs from Perspective API from Jigsaw BIBREF10, HateSonar BIBREF11, and VADER sentiment models BIBREF12. Perspective API uses machine learning models to score the perceived impact a tweet might have BIBREF10. The outputs from these models (i.e. 17 from Perspective, 3 from HateSonar, and 4 from VADER) are combined into a single feature vector for each tweet (see Appdx SECREF10).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are the hateful tweets aimed at women detected/classified?\n\nGround Truth Answer:\n\n\nAnswer A:\nIs the classification done automatically or manually?\n< Explanation: The hateful tweets were classified using a machine learning model trained on a dataset of 10,000 tweets. The model was trained on a dataset of 10,000 tweets, which were manually labeled as either hateful or not hateful. The model was trained on a dataset of 10,000 tweets, which were manually labeled as either hateful or not hateful. The model was trained on a dataset of 10,000 tweets, which were manually labeled as either hateful or not hateful. The model was trained on a dataset of\n\nAnswer B:\nThe text analysis models classify a tweet by using, as features, the outputs from Perspective API from Jigsaw BIBREF10, HateSonar BIBREF11, and VADER sentiment models BIBREF12. Explanation: The text analysis models classify a tweet by using, as features, the outputs from Perspective API from Jigsaw BIBREF10, HateSonar BIBREF11, and VADER sentiment models BIBREF12. Perspective API uses machine learning models to score the perceived impact a tweet might have BIBREF10. The outputs from these models (i.e. 17 from Perspective, 3 from HateSonar, and 4 from VADER) are combined into a single feature vector for each tweet (see Appdx SECREF10).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.07252",
    "question": "What languages are used for the experiments?",
    "gold_answer": "",
    "answer_a": "Explanation: The experiments are performed on English and Japanese. The English data set is taken from the Corpus of English (COE) and the Japanese data set is taken from the Japanese Corpus (JCO). The data sets are downloaded from the web pages of the respective corpora. The data sets are used to train the neural network language models. The training data sets are used to train the neural network language models. The training data sets are used to train the neural network language models. The training data sets are used to train the neural network language models. The training data sets are used to train the neural network language models. The training data",
    "answer_b": "English. Explanation: In BIBREF33 , significant improvement on neural machine translation (NMT) for an English to French translation task was achieved by reversing the order of input word sequence, and the possible explanation given for this phenomenon was that smaller \"minimal time lag\" was obtained in this way. In my opinion, another possible explanation is that a word in word sequence may more statistically depend on the following context than previous one.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat languages are used for the experiments?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The experiments are performed on English and Japanese. The English data set is taken from the Corpus of English (COE) and the Japanese data set is taken from the Japanese Corpus (JCO). The data sets are downloaded from the web pages of the respective corpora. The data sets are used to train the neural network language models. The training data sets are used to train the neural network language models. The training data sets are used to train the neural network language models. The training data sets are used to train the neural network language models. The training data sets are used to train the neural network language models. The training data\n\nAnswer B:\nEnglish. Explanation: In BIBREF33 , significant improvement on neural machine translation (NMT) for an English to French translation task was achieved by reversing the order of input word sequence, and the possible explanation given for this phenomenon was that smaller \"minimal time lag\" was obtained in this way. In my opinion, another possible explanation is that a word in word sequence may more statistically depend on the following context than previous one.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.07252",
    "question": "What is the caching mechanism?",
    "gold_answer": "",
    "answer_a": "The cache language models are based on the assumption that the word in recent history are more likely to appear again. Explanation: Like word classes, caching is also a common used optimization technique in LM. The cache language models are based on the assumption that the word in recent history are more likely to appear again. In cache language model, the conditional probability of a word is calculated by interpolating the output of standard language model and the probability evaluated by caching, like: INLINEFORM0\n\nwhere, INLINEFORM0 is the output of standard language model, INLINEFORM1 is the probability evaluated using caching, and INLINEFORM2 is a constant, INLINEFORM3 .",
    "answer_b": "How is it implemented?\n< Explanation: The caching mechanism is a simple but effective way to improve the efficiency of neural network language modeling. The basic idea is to store the output of the neural network in a cache and retrieve the output from the cache when the input is needed. The cache is a small memory that is used to store the output of the neural network. The cache is usually a small amount of memory, such as 1000 words. The cache is used to store the output of the neural network in the order of the input word sequence. The cache is updated when the input word sequence is updated. The cache is",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the caching mechanism?\n\nGround Truth Answer:\n\n\nAnswer A:\nThe cache language models are based on the assumption that the word in recent history are more likely to appear again. Explanation: Like word classes, caching is also a common used optimization technique in LM. The cache language models are based on the assumption that the word in recent history are more likely to appear again. In cache language model, the conditional probability of a word is calculated by interpolating the output of standard language model and the probability evaluated by caching, like: INLINEFORM0\n\nwhere, INLINEFORM0 is the output of standard language model, INLINEFORM1 is the probability evaluated using caching, and INLINEFORM2 is a constant, INLINEFORM3 .\n\nAnswer B:\nHow is it implemented?\n< Explanation: The caching mechanism is a simple but effective way to improve the efficiency of neural network language modeling. The basic idea is to store the output of the neural network in a cache and retrieve the output from the cache when the input is needed. The cache is a small memory that is used to store the output of the neural network. The cache is usually a small amount of memory, such as 1000 words. The cache is used to store the output of the neural network in the order of the input word sequence. The cache is updated when the input word sequence is updated. The cache is\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.07252",
    "question": "What language model architectures are examined?",
    "gold_answer": "",
    "answer_a": "FNNLM. Explanation: The rest of this paper is organized as follows: In next section, the basic neural network language models - feed-forward neural network language model (FNNLM), recurrent neural network language model (RNNLM) and long-short term memory (LSTM) RNNLM, will be introduced, including the training and evaluation of these models. In the third section, the details of some important NNLM techniques, including importance sampling, word classes, caching and bidirectional recurrent neural network (BiRNN), will be described, and experiments will be performed on them to examine their advantages and disadvantages separately.",
    "answer_b": "Explanation: The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat language model architectures are examined?\n\nGround Truth Answer:\n\n\nAnswer A:\nFNNLM. Explanation: The rest of this paper is organized as follows: In next section, the basic neural network language models - feed-forward neural network language model (FNNLM), recurrent neural network language model (RNNLM) and long-short term memory (LSTM) RNNLM, will be introduced, including the training and evaluation of these models. In the third section, the details of some important NNLM techniques, including importance sampling, word classes, caching and bidirectional recurrent neural network (BiRNN), will be described, and experiments will be performed on them to examine their advantages and disadvantages separately.\n\nAnswer B:\nExplanation: The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models are described in this section. The basic neural network language models\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1708.07252",
    "question": "What directions are suggested to improve language models?",
    "gold_answer": "",
    "answer_a": "Explanation: The following directions are suggested to improve language models:\n\n1. Use of more complex models: The use of more complex models, such as recurrent neural network (RNN) models, is suggested to improve the performance of neural network language models. The use of RNN models is suggested to improve the performance of neural network language models by using more complex models, such as recurrent neural network (RNN) models, which are more capable of learning long-term dependencies between words in a sequence. The use of RNN models is suggested to improve the performance of neural network language models by using more complex models, such as recurrent neural network (",
    "answer_b": "Improved architecture for ANN, use of linguistical properties of words or sentences as features.. Explanation: First, the architecture showed in Figure FIGREF19 can be used as a general improvement scheme for ANN, and I will try to figure out the structure of changeless neural network for encoder. What's more, word sequences are commonly taken as signals for LM, and it is easy to take linguistical properties of words or sentences as the features of signals.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat directions are suggested to improve language models?\n\nGround Truth Answer:\n\n\nAnswer A:\nExplanation: The following directions are suggested to improve language models:\n\n1. Use of more complex models: The use of more complex models, such as recurrent neural network (RNN) models, is suggested to improve the performance of neural network language models. The use of RNN models is suggested to improve the performance of neural network language models by using more complex models, such as recurrent neural network (RNN) models, which are more capable of learning long-term dependencies between words in a sequence. The use of RNN models is suggested to improve the performance of neural network language models by using more complex models, such as recurrent neural network (\n\nAnswer B:\nImproved architecture for ANN, use of linguistical properties of words or sentences as features.. Explanation: First, the architecture showed in Figure FIGREF19 can be used as a general improvement scheme for ANN, and I will try to figure out the structure of changeless neural network for encoder. What's more, word sequences are commonly taken as signals for LM, and it is easy to take linguistical properties of words or sentences as the features of signals.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1808.07733",
    "question": "What logic rules can be learned using ELMo?",
    "gold_answer": "",
    "answer_a": "1).But   2).Eng  3). A-But-B. Explanation: FLOAT SELECTED: Table 2: Average performance (across 100 seeds) of ELMo on the SST2 task. We show performance on A-but-B sentences (\u201cbut\u201d), negations (\u201cneg\u201d).",
    "answer_b": "What are the limitations of using ELMo?\n< Explanation: We first introduce the concept of logic rules, which are a form of explicit rule-based logic that can be used to classify sentences into categories. We then introduce ELMo, a neural language model that is trained on a large corpus of English sentences. We show that ELMo can learn explicit rules for sentiment classification, and that it can also learn implicit rules that are not explicitly stated in the training data. We then show that ELMo can learn implicit rules that are not explicitly stated in the training data. Finally, we show that ELMo can learn implicit rules",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat logic rules can be learned using ELMo?\n\nGround Truth Answer:\n\n\nAnswer A:\n1).But   2).Eng  3). A-But-B. Explanation: FLOAT SELECTED: Table 2: Average performance (across 100 seeds) of ELMo on the SST2 task. We show performance on A-but-B sentences (\u201cbut\u201d), negations (\u201cneg\u201d).\n\nAnswer B:\nWhat are the limitations of using ELMo?\n< Explanation: We first introduce the concept of logic rules, which are a form of explicit rule-based logic that can be used to classify sentences into categories. We then introduce ELMo, a neural language model that is trained on a large corpus of English sentences. We show that ELMo can learn explicit rules for sentiment classification, and that it can also learn implicit rules that are not explicitly stated in the training data. We then show that ELMo can learn implicit rules that are not explicitly stated in the training data. Finally, we show that ELMo can learn implicit rules\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1902.04094",
    "question": "Which metrics are used for evaluating the quality?",
    "gold_answer": "",
    "answer_a": "What is the difference between the two?\n< Explanation: We use BLEU (Papineni et al., 2002) to evaluate the quality of the generated sentences. We also use BLEU-4 (Papineni et al., 2002) to evaluate the diversity of the generated sentences. We also use BLEU-5 (Papineni et al., 2002) to evaluate the quality of the generated sentences. We also use BLEU-6 (Papineni et al., 2002) to evaluate the diversity of the generated sentences.",
    "answer_b": "BLEU. Explanation: We follow BIBREF18 by computing BLEU BIBREF19 between the generations and the original data distributions to measure how similar the generations are. We use a random sample of 5000 sentences from the test set of WikiText-103 BIBREF20 and a random sample of 5000 sentences from TBC as references.\r\n\r\nWe also evaluate the perplexity of a trained language model on the generations as a rough proxy for fluency. Specifically, we use the Gated Convolutional Language Model BIBREF21 pretrained on WikiText-103.\r\n\r\nFollowing BIBREF22 , we compute self-BLEU: for each generated sentence, we compute BLEU treating the rest of the sentences as references, and average across sentences. Self-BLEU measures how similar each generated sentence is to the other generations; high self-BLEU indicates that the model has low sample diversity.\r\n\r\nWe also evaluate the percentage of $n$ -grams that are unique, when compared to the original data distribution and within the corpus of generations. We note that this metric is somewhat in opposition to BLEU between generations and data, as fewer unique $n$ -grams implies higher BLEU.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich metrics are used for evaluating the quality?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the difference between the two?\n< Explanation: We use BLEU (Papineni et al., 2002) to evaluate the quality of the generated sentences. We also use BLEU-4 (Papineni et al., 2002) to evaluate the diversity of the generated sentences. We also use BLEU-5 (Papineni et al., 2002) to evaluate the quality of the generated sentences. We also use BLEU-6 (Papineni et al., 2002) to evaluate the diversity of the generated sentences.\n\nAnswer B:\nBLEU. Explanation: We follow BIBREF18 by computing BLEU BIBREF19 between the generations and the original data distributions to measure how similar the generations are. We use a random sample of 5000 sentences from the test set of WikiText-103 BIBREF20 and a random sample of 5000 sentences from TBC as references.\r\n\r\nWe also evaluate the perplexity of a trained language model on the generations as a rough proxy for fluency. Specifically, we use the Gated Convolutional Language Model BIBREF21 pretrained on WikiText-103.\r\n\r\nFollowing BIBREF22 , we compute self-BLEU: for each generated sentence, we compute BLEU treating the rest of the sentences as references, and average across sentences. Self-BLEU measures how similar each generated sentence is to the other generations; high self-BLEU indicates that the model has low sample diversity.\r\n\r\nWe also evaluate the percentage of $n$ -grams that are unique, when compared to the original data distribution and within the corpus of generations. We note that this metric is somewhat in opposition to BLEU between generations and data, as fewer unique $n$ -grams implies higher BLEU.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.04979",
    "question": "Are face tracking, identification, localization etc multimodal inputs in some ML model or system is programmed by hand?",
    "gold_answer": "",
    "answer_a": "What is the best model?\n<sep><br>Yes, face tracking, identification, localization etc are multimodal inputs in some ML model or system is programmed by hand. The system uses a combination of face tracking, identification, localization, and speaker identification to identify speakers. The system uses a combination of face tracking, identification, localization, and speaker identification to identify speakers. The system uses a combination of face tracking, identification, localization, and speaker identification to identify speakers. The system uses a combination of face tracking, identification, localization, and speaker identification to identify speakers. The system uses a combination of face tracking, identification, localization,",
    "answer_b": "Face tracking is performed in an automatic tracklet module, face identification is performed by creating a face embedding from the output of a CNN, the embedding is then compared to a gallery of each person's face using a discriminative classifier (SVM) and localization is modelled with a complex angular central Gaussian model. All are merged in a statistical model.. Explanation: The unconstrained meeting scenario involves many challenges, including face occlusions, extreme head pose, lighting conditions, compression artifacts, low resolution due to device-to-person distances, motion blur. Therefore, any individual frame may not contain necessary information. For example, a face may not be detectable in some frames. Even if it is detectable, it may not be recognizable.\n\nTo handle this variability, we integrate information across time using face tracking as implied by our formulation of $P(h | r, V)$, which requires face identification to be performed only at a tracklet level. Our face tracking uses face detection and low-level tracking to maintain a set of tracklets, where each tracklet is defined as a sequence of faces in time that belong to the same person. We use a method similar to that in BIBREF50 with several adaptions to our specific setting, such as exploiting the stationarity of the camera for detecting motion, performing the low-level tracking by color based mean-shift instead of gray-level based normalized correlation, tuning the algorithm to minimize the risk of tracklet mergers (which in our context are destructive), etc. Also, the faces in each tracklet are augmented with attributes such as face position, dimensions, head pose, and face feature vectors. The tracklet set defines $\\mathcal {R}$ of equation (DISPLAY_FORM7).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nAre face tracking, identification, localization etc multimodal inputs in some ML model or system is programmed by hand?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the best model?\n<sep><br>Yes, face tracking, identification, localization etc are multimodal inputs in some ML model or system is programmed by hand. The system uses a combination of face tracking, identification, localization, and speaker identification to identify speakers. The system uses a combination of face tracking, identification, localization, and speaker identification to identify speakers. The system uses a combination of face tracking, identification, localization, and speaker identification to identify speakers. The system uses a combination of face tracking, identification, localization, and speaker identification to identify speakers. The system uses a combination of face tracking, identification, localization,\n\nAnswer B:\nFace tracking is performed in an automatic tracklet module, face identification is performed by creating a face embedding from the output of a CNN, the embedding is then compared to a gallery of each person's face using a discriminative classifier (SVM) and localization is modelled with a complex angular central Gaussian model. All are merged in a statistical model.. Explanation: The unconstrained meeting scenario involves many challenges, including face occlusions, extreme head pose, lighting conditions, compression artifacts, low resolution due to device-to-person distances, motion blur. Therefore, any individual frame may not contain necessary information. For example, a face may not be detectable in some frames. Even if it is detectable, it may not be recognizable.\n\nTo handle this variability, we integrate information across time using face tracking as implied by our formulation of $P(h | r, V)$, which requires face identification to be performed only at a tracklet level. Our face tracking uses face detection and low-level tracking to maintain a set of tracklets, where each tracklet is defined as a sequence of faces in time that belong to the same person. We use a method similar to that in BIBREF50 with several adaptions to our specific setting, such as exploiting the stationarity of the camera for detecting motion, performing the low-level tracking by color based mean-shift instead of gray-level based normalized correlation, tuning the algorithm to minimize the risk of tracklet mergers (which in our context are destructive), etc. Also, the faces in each tracklet are augmented with attributes such as face position, dimensions, head pose, and face feature vectors. The tracklet set defines $\\mathcal {R}$ of equation (DISPLAY_FORM7).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1912.04979",
    "question": "What are baselines used?",
    "gold_answer": "",
    "answer_a": "A diarization system using only face identification and SSL. Explanation: diarization system using only face identification and SSL may be regarded as a baseline as this approach was widely used in previous audio-visual diarization studies BIBREF33, BIBREF34, BIBREF35. The results show that the use of speaker identification substantially improved the speaker attribution accuracy.",
    "answer_b": "How are they evaluated?\n< Explanation: We compare our system with the following baselines:\n\nBaseline 1: a beamformer with a 100% overlap between the speaker and microphone channels, which is the baseline used in the literature. This is the same as the baseline used in the literature.\n\nBaseline 2: a beamformer with a 100% overlap between the speaker and microphone channels, which is the baseline used in the literature. This is the same as the baseline used in the literature.\n\nBaseline 3: a beamformer with a 100% overlap between the speaker and microphone channels, which is",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are baselines used?\n\nGround Truth Answer:\n\n\nAnswer A:\nA diarization system using only face identification and SSL. Explanation: diarization system using only face identification and SSL may be regarded as a baseline as this approach was widely used in previous audio-visual diarization studies BIBREF33, BIBREF34, BIBREF35. The results show that the use of speaker identification substantially improved the speaker attribution accuracy.\n\nAnswer B:\nHow are they evaluated?\n< Explanation: We compare our system with the following baselines:\n\nBaseline 1: a beamformer with a 100% overlap between the speaker and microphone channels, which is the baseline used in the literature. This is the same as the baseline used in the literature.\n\nBaseline 2: a beamformer with a 100% overlap between the speaker and microphone channels, which is the baseline used in the literature. This is the same as the baseline used in the literature.\n\nBaseline 3: a beamformer with a 100% overlap between the speaker and microphone channels, which is\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1712.00733",
    "question": "What are the baselines for this paper?",
    "gold_answer": "",
    "answer_a": "What are the results?\n<sep><font size=\"2\">Answer: The results of the proposed model are competitive with the state-of-the-art results on both the visual question answering task and the open-domain question answering task. The proposed model achieves the state-of-the-art performance on both the visual question answering task and the open-domain question answering task. The proposed model achieves the state-of-the-art performance on both the visual question answering task and the open-domain question answering task. The proposed model achieves the state-of-the-art performance on both the visual question answering task and the open-domain question answering task. The proposed model achieves the state-of-the",
    "answer_b": "LSTM-Att BIBREF7 , a LSTM model with spatial attention. Explanation: We also compare our method with several alternative VQA methods including (1) LSTM-Att BIBREF7 , a LSTM model with spatial attention; (2) MemAUG BIBREF33 : a memory-augmented model for VQA; (3) MCB+Att BIBREF6 : a model combining multi-modal features by Multimodal Compact Bilinear pooling; (4) MLAN BIBREF11 : an advanced multi-level attention model.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the baselines for this paper?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n<sep><font size=\"2\">Answer: The results of the proposed model are competitive with the state-of-the-art results on both the visual question answering task and the open-domain question answering task. The proposed model achieves the state-of-the-art performance on both the visual question answering task and the open-domain question answering task. The proposed model achieves the state-of-the-art performance on both the visual question answering task and the open-domain question answering task. The proposed model achieves the state-of-the-art performance on both the visual question answering task and the open-domain question answering task. The proposed model achieves the state-of-the\n\nAnswer B:\nLSTM-Att BIBREF7 , a LSTM model with spatial attention. Explanation: We also compare our method with several alternative VQA methods including (1) LSTM-Att BIBREF7 , a LSTM model with spatial attention; (2) MemAUG BIBREF33 : a memory-augmented model for VQA; (3) MCB+Att BIBREF6 : a model combining multi-modal features by Multimodal Compact Bilinear pooling; (4) MLAN BIBREF11 : an advanced multi-level attention model.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1712.00733",
    "question": "What VQA datasets are used for evaluating this task? ",
    "gold_answer": "",
    "answer_a": "What are the evaluation metrics used?\n<sep>1. The VQA dataset is taken from the 2017 ImageQA Challenge. The dataset consists of 1000 images and 1000 questions. The questions are generated by randomly selecting 1000 images from the ImageQA dataset and 1000 questions from the QA dataset. The questions are then labeled with the corresponding images and questions. The dataset is divided into training, validation and test sets. The training set is used to train the model, the validation set is used to evaluate the model, and the test set is used to",
    "answer_b": "Visual7W. Explanation: We specifically implement the evaluation on a public benchmark dataset (Visual7W) BIBREF7 for the close-domain VQA task, and also generate numerous arbitrary question-answers pairs automatically to evaluate the performance on open-domain VQA.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat VQA datasets are used for evaluating this task? \n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the evaluation metrics used?\n<sep>1. The VQA dataset is taken from the 2017 ImageQA Challenge. The dataset consists of 1000 images and 1000 questions. The questions are generated by randomly selecting 1000 images from the ImageQA dataset and 1000 questions from the QA dataset. The questions are then labeled with the corresponding images and questions. The dataset is divided into training, validation and test sets. The training set is used to train the model, the validation set is used to evaluate the model, and the test set is used to\n\nAnswer B:\nVisual7W. Explanation: We specifically implement the evaluation on a public benchmark dataset (Visual7W) BIBREF7 for the close-domain VQA task, and also generate numerous arbitrary question-answers pairs automatically to evaluate the performance on open-domain VQA.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1712.00733",
    "question": "How do they model external knowledge? ",
    "gold_answer": "",
    "answer_a": "Word embeddings from knowledge triples (subject, rel, object) from ConceptNet are fed to an RNN. Explanation: In general, each piece of structured knowledge is represented as a triple $(subject, rel, object)$ with $subject$ and $object$ being two entities or concepts, and $rel$ corresponding to the specific relationship between them. In this paper, we adopt external knowledge mined from ConceptNet BIBREF28 , an open multilingual knowledge graph containing common-sense relationships between daily words, to aid the reasoning of open-domain VQA.",
    "answer_b": "What is the dynamic memory network?\n<sep>Answer: We use a dynamic memory network to attend to the large body of facts in the knowledge graph and images, and then perform reasoning over these facts to generate corresponding answers. The dynamic memory network is a type of recurrent neural network that can learn the long-term dependencies between the facts in the knowledge graph and the images. The network consists of a set of memory cells, each of which stores a set of facts and a set of images. The network is trained to learn the dependencies between the facts and images by using the images as the input and the facts as the output. The memory cells",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow do they model external knowledge? \n\nGround Truth Answer:\n\n\nAnswer A:\nWord embeddings from knowledge triples (subject, rel, object) from ConceptNet are fed to an RNN. Explanation: In general, each piece of structured knowledge is represented as a triple $(subject, rel, object)$ with $subject$ and $object$ being two entities or concepts, and $rel$ corresponding to the specific relationship between them. In this paper, we adopt external knowledge mined from ConceptNet BIBREF28 , an open multilingual knowledge graph containing common-sense relationships between daily words, to aid the reasoning of open-domain VQA.\n\nAnswer B:\nWhat is the dynamic memory network?\n<sep>Answer: We use a dynamic memory network to attend to the large body of facts in the knowledge graph and images, and then perform reasoning over these facts to generate corresponding answers. The dynamic memory network is a type of recurrent neural network that can learn the long-term dependencies between the facts in the knowledge graph and the images. The network consists of a set of memory cells, each of which stores a set of facts and a set of images. The network is trained to learn the dependencies between the facts and images by using the images as the input and the facts as the output. The memory cells\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1712.00733",
    "question": "What type of external knowledge has been used for this paper? ",
    "gold_answer": "",
    "answer_a": "What is the size of the knowledge base?\n<sep>Answer: The knowledge base is constructed by combining the knowledge graph and the external knowledge base. The knowledge graph is constructed by the knowledge graph neural network (KGNN) and the external knowledge base is constructed by the external knowledge base network (ENKB). The knowledge graph contains 10,000 entities and 10,000 relations, and the external knowledge base contains 100,000 entities and 100,000 relations. The knowledge base is 1000 times larger than the KGNN. Explanation:",
    "answer_b": "ConceptNet, which contains common-sense relationships between daily words. Explanation: In this paper, we adopt external knowledge mined from ConceptNet BIBREF28 , an open multilingual knowledge graph containing common-sense relationships between daily words, to aid the reasoning of open-domain VQA.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat type of external knowledge has been used for this paper? \n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the size of the knowledge base?\n<sep>Answer: The knowledge base is constructed by combining the knowledge graph and the external knowledge base. The knowledge graph is constructed by the knowledge graph neural network (KGNN) and the external knowledge base is constructed by the external knowledge base network (ENKB). The knowledge graph contains 10,000 entities and 10,000 relations, and the external knowledge base contains 100,000 entities and 100,000 relations. The knowledge base is 1000 times larger than the KGNN. Explanation:\n\nAnswer B:\nConceptNet, which contains common-sense relationships between daily words. Explanation: In this paper, we adopt external knowledge mined from ConceptNet BIBREF28 , an open multilingual knowledge graph containing common-sense relationships between daily words, to aid the reasoning of open-domain VQA.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.07894",
    "question": "What is the proposed algorithm or model architecture?",
    "gold_answer": "",
    "answer_a": "What are the components of the model?\n< Explanation: We propose a novel fusion model that combines content and graph-based features to improve the performance of the classification task. The model consists of two parts: a content-based model and a graph-based model. The content-based model is based on the content-based feature extraction method proposed by [1] and is composed of a word embedding layer and a convolutional neural network (CNN) layer. The graph-based model is based on the graph convolutional neural network (GCN) proposed by [2] and is composed of a graph convolutional layer followed by a fully connected layer. The model is",
    "answer_b": "They combine content- and graph-based methods in new ways.. Explanation: For this purpose, we take advantage of the content- BIBREF14 and graph-based BIBREF10 methods that we previously developed. We propose three different ways to combine them, and compare their performance on a corpus of chat logs originating from the community of a French multiplayer online game.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the proposed algorithm or model architecture?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the components of the model?\n< Explanation: We propose a novel fusion model that combines content and graph-based features to improve the performance of the classification task. The model consists of two parts: a content-based model and a graph-based model. The content-based model is based on the content-based feature extraction method proposed by [1] and is composed of a word embedding layer and a convolutional neural network (CNN) layer. The graph-based model is based on the graph convolutional neural network (GCN) proposed by [2] and is composed of a graph convolutional layer followed by a fully connected layer. The model is\n\nAnswer B:\nThey combine content- and graph-based methods in new ways.. Explanation: For this purpose, we take advantage of the content- BIBREF14 and graph-based BIBREF10 methods that we previously developed. We propose three different ways to combine them, and compare their performance on a corpus of chat logs originating from the community of a French multiplayer online game.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.07894",
    "question": "What fusion methods are applied?",
    "gold_answer": "",
    "answer_a": "How are they evaluated?\n< Explanation: We propose two fusion methods, based on the content and the graph of the conversation, to improve the performance of the classification task. The first one is based on the content of the messages, while the second one is based on the dynamics of the conversation. The content of the messages is represented by the content of the messages, while the dynamics of the conversation is represented by the graph of the conversation. The content of the messages is represented by the content of the messages, while the dynamics of the conversation is represented by the graph of the conversation. The content of the messages is represented by the content of",
    "answer_b": "Early fusion, late fusion, hybrid fusion.. Explanation: FLOAT SELECTED: FIGURE 1 | Representation of our processing pipeline. Existing methods refers to our previous work described in Papegnies et al. (2017b) (content-based method) and Papegnies et al. (2019) (graph-based method), whereas the contribution presented in this article appears on the right side (fusion strategies). Figure available at 10.6084/m9.figshare.7442273 under CC-BY license.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat fusion methods are applied?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow are they evaluated?\n< Explanation: We propose two fusion methods, based on the content and the graph of the conversation, to improve the performance of the classification task. The first one is based on the content of the messages, while the second one is based on the dynamics of the conversation. The content of the messages is represented by the content of the messages, while the dynamics of the conversation is represented by the graph of the conversation. The content of the messages is represented by the content of the messages, while the dynamics of the conversation is represented by the graph of the conversation. The content of the messages is represented by the content of\n\nAnswer B:\nEarly fusion, late fusion, hybrid fusion.. Explanation: FLOAT SELECTED: FIGURE 1 | Representation of our processing pipeline. Existing methods refers to our previous work described in Papegnies et al. (2017b) (content-based method) and Papegnies et al. (2019) (graph-based method), whereas the contribution presented in this article appears on the right side (fusion strategies). Figure available at 10.6084/m9.figshare.7442273 under CC-BY license.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1905.07894",
    "question": "What graph-based features are considered?",
    "gold_answer": "",
    "answer_a": "Coreness Score, PageRank Centrality, Strength Centrality, Vertex Count, Closeness Centrality, Authority Score, Hub Score, Reciprocity, Closeness Centrality. Explanation: FLOAT SELECTED: TABLE 2 | Top features obtained for our 5 methods.",
    "answer_b": "How are they evaluated?\n< Explanation: We propose to use two types of graph-based features: the graph of the conversation (G) and the graph of the conversation\u2019s history (G). The graph of the conversation is obtained by computing the shortest paths between all pairs of nodes in the conversation graph. The graph of the conversation\u2019s history is obtained by computing the shortest paths between all pairs of nodes in the conversation graph, but also between the nodes of the conversation graph and the nodes of the conversation\u2019s history graph. The graph of the conversation is computed by using the shortest paths between all pairs of nodes in the conversation graph. The graph of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat graph-based features are considered?\n\nGround Truth Answer:\n\n\nAnswer A:\nCoreness Score, PageRank Centrality, Strength Centrality, Vertex Count, Closeness Centrality, Authority Score, Hub Score, Reciprocity, Closeness Centrality. Explanation: FLOAT SELECTED: TABLE 2 | Top features obtained for our 5 methods.\n\nAnswer B:\nHow are they evaluated?\n< Explanation: We propose to use two types of graph-based features: the graph of the conversation (G) and the graph of the conversation\u2019s history (G). The graph of the conversation is obtained by computing the shortest paths between all pairs of nodes in the conversation graph. The graph of the conversation\u2019s history is obtained by computing the shortest paths between all pairs of nodes in the conversation graph, but also between the nodes of the conversation graph and the nodes of the conversation\u2019s history graph. The graph of the conversation is computed by using the shortest paths between all pairs of nodes in the conversation graph. The graph of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.05372",
    "question": "How does Overton handles contradictory or incomplete supervision data?",
    "gold_answer": "",
    "answer_a": "Overton learns the accuracy of these sources using ideas from the Snorkel project. Explanation: The labels are tagged by the source that produced them: these labels may be incomplete and even contradictory. Overton models the sources of these labels, which may come human annotators, or from engineer-defined heuristics such as data augmentation or heuristic labelers. Overton learns the accuracy of these sources using ideas from the Snorkel project BIBREF1. In particular, it estimates the accuracy of these sources and then uses these accuracies to compute a probability that each training point is correct BIBREF9. Overton incorporates this information into the loss function for a task; this also allows Overton to automatically handle common issues like rebalancing classes.",
    "answer_b": "< Explanation: Overton provides a set of high-level abstractions that allow developers to build machine learning systems that can handle contradictory or incomplete supervision data. For example, a system that uses Overton to build a recommender system can use the following high-level abstractions:\n\nA system that uses Overton to build a recommender system can use the following high-level abstractions:\n\nA system that uses Overton to build a recommender system can use the following high-level abstractions:\n\nA system that uses Overton to build a recommender system can use the following high-level abstractions:\n\nA system that uses Overton to build a recomm",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow does Overton handles contradictory or incomplete supervision data?\n\nGround Truth Answer:\n\n\nAnswer A:\nOverton learns the accuracy of these sources using ideas from the Snorkel project. Explanation: The labels are tagged by the source that produced them: these labels may be incomplete and even contradictory. Overton models the sources of these labels, which may come human annotators, or from engineer-defined heuristics such as data augmentation or heuristic labelers. Overton learns the accuracy of these sources using ideas from the Snorkel project BIBREF1. In particular, it estimates the accuracy of these sources and then uses these accuracies to compute a probability that each training point is correct BIBREF9. Overton incorporates this information into the loss function for a task; this also allows Overton to automatically handle common issues like rebalancing classes.\n\nAnswer B:\n< Explanation: Overton provides a set of high-level abstractions that allow developers to build machine learning systems that can handle contradictory or incomplete supervision data. For example, a system that uses Overton to build a recommender system can use the following high-level abstractions:\n\nA system that uses Overton to build a recommender system can use the following high-level abstractions:\n\nA system that uses Overton to build a recommender system can use the following high-level abstractions:\n\nA system that uses Overton to build a recommender system can use the following high-level abstractions:\n\nA system that uses Overton to build a recomm\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.05372",
    "question": "What are high level declarative abstractions Overton provides?",
    "gold_answer": "",
    "answer_a": "For over a year, Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.\n<sep>Overton provides a set of declarative abstractions that enable engineers to build, monitor, and improve production machine learning systems. These abstractions are designed to be declarative, meaning that they are expressed in a declarative language, and they are designed to be declarative, meaning that they are expressed",
    "answer_b": "Code-free Deep Learning. Explanation: Overton provides the engineer with abstractions that allow them to build, maintain, and monitor their application by manipulating data files\u2013not custom code.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are high level declarative abstractions Overton provides?\n\nGround Truth Answer:\n\n\nAnswer A:\nFor over a year, Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.\n<sep>Overton provides a set of declarative abstractions that enable engineers to build, monitor, and improve production machine learning systems. These abstractions are designed to be declarative, meaning that they are expressed in a declarative language, and they are designed to be declarative, meaning that they are expressed\n\nAnswer B:\nCode-free Deep Learning. Explanation: Overton provides the engineer with abstractions that allow them to build, maintain, and monitor their application by manipulating data files\u2013not custom code.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.05372",
    "question": "How are applications presented in Overton?",
    "gold_answer": "",
    "answer_a": "What are the challenges they face?\nThey are presented as a set of tasks, each with a set of inputs and outputs. Each task is represented as a set of tasks, each with a set of inputs and outputs. Each task is represented as a set of tasks, each with a set of inputs and outputs. Each task is represented as a set of tasks, each with a set of inputs and outputs. Each task is represented as a set of tasks, each with a set of inputs and outputs. Each task is represented as a set of tasks, each with a set of inputs and outputs. Each task is represented as a set of",
    "answer_b": "Applications are customized by providing supervision in a data file that conforms to the schema. Explanation: The schema is changed infrequently, and many engineers who use Overton simply select an existing schema. Applications are customized by providing supervision in a data file that conforms to the schema, described next.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are applications presented in Overton?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the challenges they face?\nThey are presented as a set of tasks, each with a set of inputs and outputs. Each task is represented as a set of tasks, each with a set of inputs and outputs. Each task is represented as a set of tasks, each with a set of inputs and outputs. Each task is represented as a set of tasks, each with a set of inputs and outputs. Each task is represented as a set of tasks, each with a set of inputs and outputs. Each task is represented as a set of tasks, each with a set of inputs and outputs. Each task is represented as a set of\n\nAnswer B:\nApplications are customized by providing supervision in a data file that conforms to the schema. Explanation: The schema is changed infrequently, and many engineers who use Overton simply select an existing schema. Applications are customized by providing supervision in a data file that conforms to the schema, described next.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.09722",
    "question": "what metrics are used to evaluate the models?",
    "gold_answer": "",
    "answer_a": "modified version of standard precision, recall, and F INLINEFORM0 , as proposed by reschke2014. Explanation: The first is a modified version of standard precision, recall, and F INLINEFORM0 , as proposed by reschke2014.",
    "answer_b": "What is the baseline?\n< Explanation: We use the F1 score as our evaluation metric. We also use the following metrics to evaluate the models:\n\n* Recall: Recall is the proportion of correctly identified events that are actually present in the cluster. Recall is calculated as the proportion of correctly identified events that are actually present in the cluster. Recall is calculated as the proportion of correctly identified events that are actually present in the cluster. Recall is calculated as the proportion of correctly identified events that are actually present in the cluster. Recall is calculated as the proportion of correctly identified events that are actually present in the cluster. Recall is calculated as the proportion of",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat metrics are used to evaluate the models?\n\nGround Truth Answer:\n\n\nAnswer A:\nmodified version of standard precision, recall, and F INLINEFORM0 , as proposed by reschke2014. Explanation: The first is a modified version of standard precision, recall, and F INLINEFORM0 , as proposed by reschke2014.\n\nAnswer B:\nWhat is the baseline?\n< Explanation: We use the F1 score as our evaluation metric. We also use the following metrics to evaluate the models:\n\n* Recall: Recall is the proportion of correctly identified events that are actually present in the cluster. Recall is calculated as the proportion of correctly identified events that are actually present in the cluster. Recall is calculated as the proportion of correctly identified events that are actually present in the cluster. Recall is calculated as the proportion of correctly identified events that are actually present in the cluster. Recall is calculated as the proportion of correctly identified events that are actually present in the cluster. Recall is calculated as the proportion of\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.09722",
    "question": "what are the baselines?",
    "gold_answer": "",
    "answer_a": "Reschke CRF. Explanation: We evaluate on four categories of architecture:\n\nreschke2014 proposed several methods for event extraction in this scenario. We compare against three notable examples drawn from this work:",
    "answer_b": "What are the results?\n<blockquote>Table 1: Results of the baseline models. The first column shows the F$_1$ score, the second column shows the number of correct predictions, and the third column shows the number of incorrect predictions. The fourth column shows the number of correct predictions for the first 1000 words of the text. The fifth column shows the number of correct predictions for the first 1000 words of the text. The sixth column shows the number of correct predictions for the first 1000 words of the text. The seventh column shows the number of correct predictions for the first",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat are the baselines?\n\nGround Truth Answer:\n\n\nAnswer A:\nReschke CRF. Explanation: We evaluate on four categories of architecture:\n\nreschke2014 proposed several methods for event extraction in this scenario. We compare against three notable examples drawn from this work:\n\nAnswer B:\nWhat are the results?\n<blockquote>Table 1: Results of the baseline models. The first column shows the F$_1$ score, the second column shows the number of correct predictions, and the third column shows the number of incorrect predictions. The fourth column shows the number of correct predictions for the first 1000 words of the text. The fifth column shows the number of correct predictions for the first 1000 words of the text. The sixth column shows the number of correct predictions for the first 1000 words of the text. The seventh column shows the number of correct predictions for the first\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.09722",
    "question": "what is the size of the dataset?",
    "gold_answer": "",
    "answer_a": "How is it divided?\n< Explanation: The dataset consists of 1000 news articles, each of which is labeled with a plane crash event. The dataset is divided into 1000 training and 1000 test sets. The training set is used to train the model, and the test set is used to evaluate the model. The training set is further divided into 1000 training and 1000 validation sets. The validation set is used to evaluate the model on unseen data. The dataset is further divided into 1000 training and 1000 validation sets.",
    "answer_b": "80 plane crash events. Explanation: The Stanford Plane Crash Dataset BIBREF15 is a small data set consisting of 80 plane crash events, each paired with a set of related news articles.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat is the size of the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow is it divided?\n< Explanation: The dataset consists of 1000 news articles, each of which is labeled with a plane crash event. The dataset is divided into 1000 training and 1000 test sets. The training set is used to train the model, and the test set is used to evaluate the model. The training set is further divided into 1000 training and 1000 validation sets. The validation set is used to evaluate the model on unseen data. The dataset is further divided into 1000 training and 1000 validation sets.\n\nAnswer B:\n80 plane crash events. Explanation: The Stanford Plane Crash Dataset BIBREF15 is a small data set consisting of 80 plane crash events, each paired with a set of related news articles.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1610.09722",
    "question": "what dataset did they use?",
    "gold_answer": "",
    "answer_a": "Event dataset with news articles. Explanation: The Stanford Plane Crash Dataset BIBREF15 is a small data set consisting of 80 plane crash events, each paired with a set of related news articles.",
    "answer_b": "Explanation: We use the CrashEvent dataset, which contains 1000 news articles, each of which is labeled with a plane crash event. The dataset is available at https://github.com/krishna1991/CRASHEDANET. The dataset contains 1000 news articles, each of which is labeled with a plane crash event. The dataset is available at https://github.com/krishna1991/CRASHEDANET. The dataset contains 1000 news articles, each of which is labeled with a plane crash event. The dataset is available at https",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nwhat dataset did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nEvent dataset with news articles. Explanation: The Stanford Plane Crash Dataset BIBREF15 is a small data set consisting of 80 plane crash events, each paired with a set of related news articles.\n\nAnswer B:\nExplanation: We use the CrashEvent dataset, which contains 1000 news articles, each of which is labeled with a plane crash event. The dataset is available at https://github.com/krishna1991/CRASHEDANET. The dataset contains 1000 news articles, each of which is labeled with a plane crash event. The dataset is available at https://github.com/krishna1991/CRASHEDANET. The dataset contains 1000 news articles, each of which is labeled with a plane crash event. The dataset is available at https\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.05925",
    "question": "How is the quality of the translation evaluated?",
    "gold_answer": "",
    "answer_a": "What metrics are used?\n< Explanation: We evaluate the quality of the translation by using the BLEU score. The BLEU score is a widely used metric for evaluating machine translation systems. It is defined as follows: BLEU = (score + 1) / (score + 1 + number of words in the reference translation). The BLEU score is a measure of the quality of the translation, and it is a weighted average of the individual BLEU scores of the individual words. The higher the BLEU score, the better the translation quality. The BLEU score is calculated for each word in the reference translation and the corresponding translation",
    "answer_b": "They report the scores of several evaluation methods for every step of their approach.. Explanation: FLOAT SELECTED: Table 2: Unsupervised translation results. We report the scores of several evaluation methods for every step of our approach. Except the result that is listed on the last line, all results are under the condition that the translations are post-processed without patch-up.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow is the quality of the translation evaluated?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat metrics are used?\n< Explanation: We evaluate the quality of the translation by using the BLEU score. The BLEU score is a widely used metric for evaluating machine translation systems. It is defined as follows: BLEU = (score + 1) / (score + 1 + number of words in the reference translation). The BLEU score is a measure of the quality of the translation, and it is a weighted average of the individual BLEU scores of the individual words. The higher the BLEU score, the better the translation quality. The BLEU score is calculated for each word in the reference translation and the corresponding translation\n\nAnswer B:\nThey report the scores of several evaluation methods for every step of their approach.. Explanation: FLOAT SELECTED: Table 2: Unsupervised translation results. We report the scores of several evaluation methods for every step of our approach. Except the result that is listed on the last line, all results are under the condition that the translations are post-processed without patch-up.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1908.05925",
    "question": "What are the post-processing approaches applied to the output?",
    "gold_answer": "",
    "answer_a": "What are the pre-processing approaches applied to the input?\n< Explanation: We first apply a pre-processing step to the input data. We use the BIBREF10 to preprocess the input data. The preprocessing step consists of two steps: (1) removing the stop words and (2) removing the stop words from the input sentences. The stop words are defined as: \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cand\u201d, \u201cbut\u201d, \u201cor\u201d, \u201cfor\u201d, \u201cin\u201d, \u201con\u201d, \u201cto\u201d, \u201cfrom\u201d, \u201cwith\u201d, \u201cby\u201d, \u201cin\u201d, \u201con\u201d, \u201cat\u201d, \u201cfor\u201d, \u201cin",
    "answer_b": "Special Token Replacement. Explanation: In the pre-processing, we use the special tokens and to replace numbers that express a specific quantity and date respectively. Therefore, in the post-processing, we need to restore those numbers. We simply detect the pattern and in the original source sentences and then replace the special tokens in the translated sentences with the corresponding numbers detected in the source sentences.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are the post-processing approaches applied to the output?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the pre-processing approaches applied to the input?\n< Explanation: We first apply a pre-processing step to the input data. We use the BIBREF10 to preprocess the input data. The preprocessing step consists of two steps: (1) removing the stop words and (2) removing the stop words from the input sentences. The stop words are defined as: \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cand\u201d, \u201cbut\u201d, \u201cor\u201d, \u201cfor\u201d, \u201cin\u201d, \u201con\u201d, \u201cto\u201d, \u201cfrom\u201d, \u201cwith\u201d, \u201cby\u201d, \u201cin\u201d, \u201con\u201d, \u201cat\u201d, \u201cfor\u201d, \u201cin\n\nAnswer B:\nSpecial Token Replacement. Explanation: In the pre-processing, we use the special tokens and to replace numbers that express a specific quantity and date respectively. Therefore, in the post-processing, we need to restore those numbers. We simply detect the pattern and in the original source sentences and then replace the special tokens in the translated sentences with the corresponding numbers detected in the source sentences.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.03553",
    "question": "How many general qualitative statements are in dataset?",
    "gold_answer": "",
    "answer_a": "background corpus of 400 qualitative knowledge sentences. Explanation: Our contributions are thus (1) the dataset, containing 3864 richly annotated questions plus a background corpus of 400 qualitative knowledge sentences; and (2) an analysis of the dataset, performance of BERT-based models, and a catalog of the challenges it poses, pointing the way towards solutions.",
    "answer_b": "What is the average length of a statement?\n< Explanation: We collected 3864 qualitative statements from the Internet, each consisting of a single sentence. We extracted the main sentence and the main sentence's main clause, and removed any sentences that were too short or too long. We then extracted the main clause and the main clause's main sentence, and removed any sentences that were too short or too long. We then extracted the main sentence and the main sentence's main clause, and removed any sentences that were too short or too long. We then extracted the main clause and the main clause's main sentence, and removed any sentences that",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow many general qualitative statements are in dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nbackground corpus of 400 qualitative knowledge sentences. Explanation: Our contributions are thus (1) the dataset, containing 3864 richly annotated questions plus a background corpus of 400 qualitative knowledge sentences; and (2) an analysis of the dataset, performance of BERT-based models, and a catalog of the challenges it poses, pointing the way towards solutions.\n\nAnswer B:\nWhat is the average length of a statement?\n< Explanation: We collected 3864 qualitative statements from the Internet, each consisting of a single sentence. We extracted the main sentence and the main sentence's main clause, and removed any sentences that were too short or too long. We then extracted the main clause and the main clause's main sentence, and removed any sentences that were too short or too long. We then extracted the main sentence and the main sentence's main clause, and removed any sentences that were too short or too long. We then extracted the main clause and the main clause's main sentence, and removed any sentences that\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.03553",
    "question": "What are state-of-the-art models on this dataset?",
    "gold_answer": "",
    "answer_a": "What are the results?\n<sep><font size=\"2\">Results: </font>Our results show that the state-of-the-art models on QuaRTz are substantially (20%) below human performance, presenting an open challenge to the NLP community. We also show that the models are not able to reason about the properties of the general knowledge, and that the models are not able to reason about the properties of the situated questions. We also show that the models are not able to reason about the properties of the general knowledge, and that the models are not able to reason about the properties of the situated questions. We also show",
    "answer_b": "BERT (IR). Explanation: We report results using two baselines and several strong models built with BERT-large BIBREF9 as follows:\n\n1. Random: always 50% (2-way MC).\n\n2. BERT-Sci: BERT fine-tuned on a large, general set of science questions BIBREF5.\n\n3. BERT (IR): This model performs the full task. First, a sentence $K_i$ is retrieved from $K$ using $Q_i$ as a search query. This is then supplied to BERT as [CLS] $K_i$ [SEP] question-stem [SEP] answer-option [SEP] for each option. The [CLS] output token is projected to a single logit and fed through a softmax layer across answer options, using cross entropy loss, the highest being selected. This model is fine-tuned using QuaRTz (only).\n\n4. BERT (IR upper bound): Same, but using the ideal (annotated) $K_i$ rather than retrieved $K_i$.\n\n5. BERT-PFT (no knowledge): BERT first fine-tuned (\u201cpre-fine-tuned\u201d) on the RACE dataset BIBREF10, BIBREF11, and then fine-tuned on QuaRTz (questions only, no $K$, both train and test). Questions are supplied as [CLS] question-stem [SEP] answer-option [SEP].\n\n6. BERT-PFT (IR): Same as BERT (IR), except starting with the pre-fine-tuned BERT.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat are state-of-the-art models on this dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n<sep><font size=\"2\">Results: </font>Our results show that the state-of-the-art models on QuaRTz are substantially (20%) below human performance, presenting an open challenge to the NLP community. We also show that the models are not able to reason about the properties of the general knowledge, and that the models are not able to reason about the properties of the situated questions. We also show that the models are not able to reason about the properties of the general knowledge, and that the models are not able to reason about the properties of the situated questions. We also show\n\nAnswer B:\nBERT (IR). Explanation: We report results using two baselines and several strong models built with BERT-large BIBREF9 as follows:\n\n1. Random: always 50% (2-way MC).\n\n2. BERT-Sci: BERT fine-tuned on a large, general set of science questions BIBREF5.\n\n3. BERT (IR): This model performs the full task. First, a sentence $K_i$ is retrieved from $K$ using $Q_i$ as a search query. This is then supplied to BERT as [CLS] $K_i$ [SEP] question-stem [SEP] answer-option [SEP] for each option. The [CLS] output token is projected to a single logit and fed through a softmax layer across answer options, using cross entropy loss, the highest being selected. This model is fine-tuned using QuaRTz (only).\n\n4. BERT (IR upper bound): Same, but using the ideal (annotated) $K_i$ rather than retrieved $K_i$.\n\n5. BERT-PFT (no knowledge): BERT first fine-tuned (\u201cpre-fine-tuned\u201d) on the RACE dataset BIBREF10, BIBREF11, and then fine-tuned on QuaRTz (questions only, no $K$, both train and test). Questions are supplied as [CLS] question-stem [SEP] answer-option [SEP].\n\n6. BERT-PFT (IR): Same as BERT (IR), except starting with the pre-fine-tuned BERT.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.03553",
    "question": "How are properties being compared annotated?",
    "gold_answer": "",
    "answer_a": "What is the annotation quality?\n<sep>Answer: The properties are annotated as follows: (1) the property is a noun phrase, (2) the property is a verb phrase, (3) the property is a noun phrase with a verb phrase, (4) the property is a noun phrase with a noun phrase, (5) the property is a noun phrase with a noun phrase with a noun phrase, (6) the property is a noun phrase with a noun phrase with a noun phrase with a noun phrase, (7) the property is a noun phrase with a noun phrase with a noun phrase with a noun phrase with",
    "answer_b": "qualitative relations were manually extracted by the authors from a large corpus. Explanation: First, 400 sentences expressing general qualitative relations were manually extracted by the authors from a large corpus using keyword search (\u201cincrease\u201d, \u201cfaster\u201d, etc.).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nHow are properties being compared annotated?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the annotation quality?\n<sep>Answer: The properties are annotated as follows: (1) the property is a noun phrase, (2) the property is a verb phrase, (3) the property is a noun phrase with a verb phrase, (4) the property is a noun phrase with a noun phrase, (5) the property is a noun phrase with a noun phrase with a noun phrase, (6) the property is a noun phrase with a noun phrase with a noun phrase with a noun phrase, (7) the property is a noun phrase with a noun phrase with a noun phrase with a noun phrase with\n\nAnswer B:\nqualitative relations were manually extracted by the authors from a large corpus. Explanation: First, 400 sentences expressing general qualitative relations were manually extracted by the authors from a large corpus using keyword search (\u201cincrease\u201d, \u201cfaster\u201d, etc.).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.11531",
    "question": "What state-of-the-art tagging model did they use?",
    "gold_answer": "",
    "answer_a": "the Span-Attribute Tagging (SAT) model. Explanation: One of the challenges for model development in this task is the limited amount of training data. To use the data efficiently, we developed the Span-Attribute Tagging (SAT) model which performs inference in a hierarchical manner by identifying the clinically relevant span using a BIO scheme and classifying the spans into specific labels BIBREF19. The span is represented using the latent representation from a bidirectional encoder, and as such has the capacity to capture the relevant context information. This latent contextual representation is used to infer the entity label and the status of the symptom as experienced or not. This is a trainable mechanism to infer status in contrast to ad hoc methods applied in negation tasks in previous work in clinical domain BIBREF20.",
    "answer_b": "What was the performance of the model?\n< Explanation: We used the following state-of-the-art tagging model: BIBREF10. The model was trained on a corpus of 10k clinical notes. The model was trained using the following parameters: 100 epochs, 0.01 learning rate, 100% of the training data used for training, 10% of the training data used for validation, and 10% of the training data used for testing. The model was trained using the following parameters: 100 epochs, 0.01 learning rate,",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat state-of-the-art tagging model did they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nthe Span-Attribute Tagging (SAT) model. Explanation: One of the challenges for model development in this task is the limited amount of training data. To use the data efficiently, we developed the Span-Attribute Tagging (SAT) model which performs inference in a hierarchical manner by identifying the clinically relevant span using a BIO scheme and classifying the spans into specific labels BIBREF19. The span is represented using the latent representation from a bidirectional encoder, and as such has the capacity to capture the relevant context information. This latent contextual representation is used to infer the entity label and the status of the symptom as experienced or not. This is a trainable mechanism to infer status in contrast to ad hoc methods applied in negation tasks in previous work in clinical domain BIBREF20.\n\nAnswer B:\nWhat was the performance of the model?\n< Explanation: We used the following state-of-the-art tagging model: BIBREF10. The model was trained on a corpus of 10k clinical notes. The model was trained using the following parameters: 100 epochs, 0.01 learning rate, 100% of the training data used for training, 10% of the training data used for validation, and 10% of the training data used for testing. The model was trained using the following parameters: 100 epochs, 0.01 learning rate,\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.06136",
    "question": "By how much do they improve upon supervised traning methods?",
    "gold_answer": "",
    "answer_a": "A2C and REINFORCE-joint for joint policy optimization achieve the improvement over SL baseline of 29.4% and  25.7 susses rate,  1.21  AND 1.28 AvgRevard and  0.25 and  -1.34 AvgSucccess Turn Size, respectively.. Explanation: FLOAT SELECTED: Table 2. Evaluation results on the converted DSTC2 dataset.",
    "answer_b": "What is the difference between the two methods?\n< Explanation: We compare our method with the following two methods: (1) supervised training: we train a dialog agent and a user simulator with supervised training, and then optimize the policies of both agents with deep RL. (2) single-agent RL training: we train a dialog agent with single-agent RL training and then optimize the policies of both agents with deep RL. We compare the results of the two methods on the dialog corpus and the task-oriented dialog corpus. The results are shown in Table 1. We can see that our method leads to significant improvements on task success rate and total task",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nBy how much do they improve upon supervised traning methods?\n\nGround Truth Answer:\n\n\nAnswer A:\nA2C and REINFORCE-joint for joint policy optimization achieve the improvement over SL baseline of 29.4% and  25.7 susses rate,  1.21  AND 1.28 AvgRevard and  0.25 and  -1.34 AvgSucccess Turn Size, respectively.. Explanation: FLOAT SELECTED: Table 2. Evaluation results on the converted DSTC2 dataset.\n\nAnswer B:\nWhat is the difference between the two methods?\n< Explanation: We compare our method with the following two methods: (1) supervised training: we train a dialog agent and a user simulator with supervised training, and then optimize the policies of both agents with deep RL. (2) single-agent RL training: we train a dialog agent with single-agent RL training and then optimize the policies of both agents with deep RL. We compare the results of the two methods on the dialog corpus and the task-oriented dialog corpus. The results are shown in Table 1. We can see that our method leads to significant improvements on task success rate and total task\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1709.06136",
    "question": "Which neural network architecture do they use for the dialog agent and user simulator?",
    "gold_answer": "",
    "answer_a": "What are the architectures for the dialog agent and user simulator?\n< Explanation: We use the following architectures for the dialog agent and user simulator:\n\nDialog Agent: We use a recurrent neural network (RNN) with a bidirectional LSTM (BiLSTM) to model the dialog agent. The RNN is trained end-to-end with the dialog corpus. The BiLSTM is trained with a bidirectional LSTM (BiLSTM) model. The BiLSTM is trained with a bidirectional LSTM (BiLSTM) model. The BiLSTM is trained with a bidirectional LSTM (BiLSTM) model. The BiLSTM",
    "answer_b": "LSTM. Explanation: Figure 1 illustrates the design of the dialog agent. The dialog agent is capable of tracking dialog state, issuing API calls to knowledge bases (KB), and producing corresponding system actions and responses by incorporating the query results, which are key skill sets BIBREF26 in conducting task-oriented dialogs. State of the dialog agent is maintained in the LSTM BIBREF35 state and being updated after the processing of each turn.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich neural network architecture do they use for the dialog agent and user simulator?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the architectures for the dialog agent and user simulator?\n< Explanation: We use the following architectures for the dialog agent and user simulator:\n\nDialog Agent: We use a recurrent neural network (RNN) with a bidirectional LSTM (BiLSTM) to model the dialog agent. The RNN is trained end-to-end with the dialog corpus. The BiLSTM is trained with a bidirectional LSTM (BiLSTM) model. The BiLSTM is trained with a bidirectional LSTM (BiLSTM) model. The BiLSTM is trained with a bidirectional LSTM (BiLSTM) model. The BiLSTM\n\nAnswer B:\nLSTM. Explanation: Figure 1 illustrates the design of the dialog agent. The dialog agent is capable of tracking dialog state, issuing API calls to knowledge bases (KB), and producing corresponding system actions and responses by incorporating the query results, which are key skill sets BIBREF26 in conducting task-oriented dialogs. State of the dialog agent is maintained in the LSTM BIBREF35 state and being updated after the processing of each turn.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "2003.08769",
    "question": "Is this done in form of unsupervised (clustering) or suppervised learning?",
    "gold_answer": "",
    "answer_a": "Supervised methods are used to identify the dish and ingredients in the image, and an unsupervised method (KNN) is used to create the food profile.. Explanation: METHODOLOGY\nThe real task lies in converting the image into interpretable data that can be parsed and used. To help with this, a data processing pipeline is built. The details of the pipeline are discussed below. The data pipeline extensively uses the ClarifaiBIBREF8 image recognition model. The 3 models used extensively are:\n\nThe General Model : It recognizes over 11,000 different concepts and is a great all purpose solution. We have used this model to distinguish between Food images and Non-Food images.\n\nThe Food Model : It recognizes more than 1,000 food items in images down to the ingredient level. This model is used to identify the ingredients in a food image.\n\nThe General Embedding Model : It analyzes images and returns numerical vectors that represent the input images in a 1024-dimensional space. The vector representation is computed by using Clarifai\u2019s \u2018General\u2019 model. The vectors of visually similar images will be close to each other in the 1024-dimensional space. This is used to eliminate multiple similar images of the same food item.",
    "answer_b": "How?\n<sep><br/ >Answer: We use a supervised learning approach. We use a supervised learning approach to train a classifier to predict the user's cuisine preference. We use a supervised learning approach to train a classifier to predict the user's cuisine preference. We use a supervised learning approach to train a classifier to predict the user's cuisine preference. We use a supervised learning approach to train a classifier to predict the user's cuisine preference. We use a supervised learning approach to train a classifier to predict the user's cuisine preference. We use a supervised learning approach to train a classifier to predict the user's cuisine preference. We use a",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nIs this done in form of unsupervised (clustering) or suppervised learning?\n\nGround Truth Answer:\n\n\nAnswer A:\nSupervised methods are used to identify the dish and ingredients in the image, and an unsupervised method (KNN) is used to create the food profile.. Explanation: METHODOLOGY\nThe real task lies in converting the image into interpretable data that can be parsed and used. To help with this, a data processing pipeline is built. The details of the pipeline are discussed below. The data pipeline extensively uses the ClarifaiBIBREF8 image recognition model. The 3 models used extensively are:\n\nThe General Model : It recognizes over 11,000 different concepts and is a great all purpose solution. We have used this model to distinguish between Food images and Non-Food images.\n\nThe Food Model : It recognizes more than 1,000 food items in images down to the ingredient level. This model is used to identify the ingredients in a food image.\n\nThe General Embedding Model : It analyzes images and returns numerical vectors that represent the input images in a 1024-dimensional space. The vector representation is computed by using Clarifai\u2019s \u2018General\u2019 model. The vectors of visually similar images will be close to each other in the 1024-dimensional space. This is used to eliminate multiple similar images of the same food item.\n\nAnswer B:\nHow?\n<sep><br/ >Answer: We use a supervised learning approach. We use a supervised learning approach to train a classifier to predict the user's cuisine preference. We use a supervised learning approach to train a classifier to predict the user's cuisine preference. We use a supervised learning approach to train a classifier to predict the user's cuisine preference. We use a supervised learning approach to train a classifier to predict the user's cuisine preference. We use a supervised learning approach to train a classifier to predict the user's cuisine preference. We use a supervised learning approach to train a classifier to predict the user's cuisine preference. We use a\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.13466",
    "question": "What baselines do they compare to?",
    "gold_answer": "",
    "answer_a": "a encoder-decoder architecture with attention incorporating LSTMs and transformers. Explanation: We show that using both ReWE and ReSE can outperform strong state-of-the-art baselines based on long short-term memory networks (LSTMs) and transformers.",
    "answer_b": "What is the performance of the baselines?\n< Explanation: We compare our model to the following baselines: (1) a baseline that uses only the word embeddings (word-only), (2) a baseline that uses only the sentence embeddings (sentence-only), (3) a baseline that uses only the word embeddings and sentence embeddings (word+sentence), (4) a baseline that uses only the word embeddings and sentence embeddings and word+sentence (word+sentence+word+sentence), (5) a baseline that uses only the word embeddings and sentence embeddings and word+sentence and word+sentence+word+sentence (word+sentence",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat baselines do they compare to?\n\nGround Truth Answer:\n\n\nAnswer A:\na encoder-decoder architecture with attention incorporating LSTMs and transformers. Explanation: We show that using both ReWE and ReSE can outperform strong state-of-the-art baselines based on long short-term memory networks (LSTMs) and transformers.\n\nAnswer B:\nWhat is the performance of the baselines?\n< Explanation: We compare our model to the following baselines: (1) a baseline that uses only the word embeddings (word-only), (2) a baseline that uses only the sentence embeddings (sentence-only), (3) a baseline that uses only the word embeddings and sentence embeddings (word+sentence), (4) a baseline that uses only the word embeddings and sentence embeddings and word+sentence (word+sentence+word+sentence), (5) a baseline that uses only the word embeddings and sentence embeddings and word+sentence and word+sentence+word+sentence (word+sentence\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.13466",
    "question": "What training set sizes do they use?",
    "gold_answer": "",
    "answer_a": "How do they measure the improvement?\n< Explanation: We use 10000 training examples for Basque-English, 10000 for Basque-Italian, 10000 for Italian-English, 10000 for Spanish-English, 10000 for Spanish-Italian, 10000 for Portuguese-English, 10000 for Portuguese-Italian, 10000 for Catalan-English, 10000 for Catalan-Italian, and 10000 for Catalan-Portuguese",
    "answer_b": "219,777. Explanation: Four different language pairs have been selected for the experiments. The datasets' size varies from tens of thousands to millions of sentences to test the regularizers' ability to improve translation over a range of low-resource and high-resource language pairs.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat training set sizes do they use?\n\nGround Truth Answer:\n\n\nAnswer A:\nHow do they measure the improvement?\n< Explanation: We use 10000 training examples for Basque-English, 10000 for Basque-Italian, 10000 for Italian-English, 10000 for Spanish-English, 10000 for Spanish-Italian, 10000 for Portuguese-English, 10000 for Portuguese-Italian, 10000 for Catalan-English, 10000 for Catalan-Italian, and 10000 for Catalan-Portuguese\n\nAnswer B:\n219,777. Explanation: Four different language pairs have been selected for the experiments. The datasets' size varies from tens of thousands to millions of sentences to test the regularizers' ability to improve translation over a range of low-resource and high-resource language pairs.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1909.13466",
    "question": "What languages do they experiment with?",
    "gold_answer": "",
    "answer_a": "German. Explanation: Experiments ::: Datasets\nFour different language pairs have been selected for the experiments",
    "answer_b": "How do they measure the quality of machine translation?\n< Explanation: We use the following four language pairs: Basque-English (BxE), Basque-French (BxF), French-English (FxE), and Spanish-English (SxE). We use the following training set sizes: 100k, 200k, 300k, and 400k. We use the following evaluation set sizes: 100k, 200k, 300k, and 400k. We use the following evaluation metrics: BLEU,",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat languages do they experiment with?\n\nGround Truth Answer:\n\n\nAnswer A:\nGerman. Explanation: Experiments ::: Datasets\nFour different language pairs have been selected for the experiments\n\nAnswer B:\nHow do they measure the quality of machine translation?\n< Explanation: We use the following four language pairs: Basque-English (BxE), Basque-French (BxF), French-English (FxE), and Spanish-English (SxE). We use the following training set sizes: 100k, 200k, 300k, and 400k. We use the following evaluation set sizes: 100k, 200k, 300k, and 400k. We use the following evaluation metrics: BLEU,\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.09916",
    "question": "What language model is trained?",
    "gold_answer": "",
    "answer_a": "ULMFiT. Explanation: As our language model we used ULMFiT BIBREF21.",
    "answer_b": "How is it evaluated?\n< Explanation: We used a language model trained on the small high-reliability dataset. We used the same language model to train and evaluate the models on the large low-reliability dataset. We used the same language model to train and evaluate the models on the small high-reliability dataset. We used the same language model to train and evaluate the models on the large low-reliability dataset. We used the same language model to train and evaluate the models on the small high-reliability dataset. We used the same language model to train and evaluate the models on the large low-reliability dataset.",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat language model is trained?\n\nGround Truth Answer:\n\n\nAnswer A:\nULMFiT. Explanation: As our language model we used ULMFiT BIBREF21.\n\nAnswer B:\nHow is it evaluated?\n< Explanation: We used a language model trained on the small high-reliability dataset. We used the same language model to train and evaluate the models on the large low-reliability dataset. We used the same language model to train and evaluate the models on the small high-reliability dataset. We used the same language model to train and evaluate the models on the large low-reliability dataset. We used the same language model to train and evaluate the models on the small high-reliability dataset. We used the same language model to train and evaluate the models on the large low-reliability dataset.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.09916",
    "question": "What machine learning models are considered?",
    "gold_answer": "",
    "answer_a": "What are the results?\n<blockquote>We then used the two datasets to train and test several machine learning models to extract personality from text, including a language model. </blockquote>\nWe used the following models: Naive Bayes, Logistic Regression, Support Vector Machine, Random Forest, and a language model. We then compared the performance of these models on the two datasets. We found that the language model based on the small high-reliability dataset performed better than the random baseline. We also found that the language model based on the large low-reliability dataset performed better than the random baseline. Finally, we found that the models based",
    "answer_b": "RandomForestRegressor. Explanation: Several regression models were tested from the scikit-learn framework BIBREF20, such as RandomForestRegressor, LinearSVR, and KNeighborsRegressor. The Support Vector Machine Regression yielded the lowest MAE and MSE while performing a cross validated grid search for all the models and a range of hyperparameters.",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat machine learning models are considered?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat are the results?\n<blockquote>We then used the two datasets to train and test several machine learning models to extract personality from text, including a language model. </blockquote>\nWe used the following models: Naive Bayes, Logistic Regression, Support Vector Machine, Random Forest, and a language model. We then compared the performance of these models on the two datasets. We found that the language model based on the small high-reliability dataset performed better than the random baseline. We also found that the language model based on the large low-reliability dataset performed better than the random baseline. Finally, we found that the models based\n\nAnswer B:\nRandomForestRegressor. Explanation: Several regression models were tested from the scikit-learn framework BIBREF20, such as RandomForestRegressor, LinearSVR, and KNeighborsRegressor. The Support Vector Machine Regression yielded the lowest MAE and MSE while performing a cross validated grid search for all the models and a range of hyperparameters.\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1910.09916",
    "question": "What is the agreement of the dataset?",
    "gold_answer": "",
    "answer_a": "Answer with content missing: (Table 2): Krippednorff's alpha coefficient for dataset is: Stability -0.26, Extraversion 0.07, Openness 0.36, Agreeableness 0.51, Conscientiousness 0.31. Explanation: The intra-annotator reliability of both datasets $D_\\textrm {\\textit {LR}}$ and $D_\\textrm {\\textit {HR}}$ is shown in Table TABREF21.",
    "answer_b": "How was it created?\n<blockquote>\n<font size=\"2\">The dataset was created by having experts annotate personality traits in a large number of texts from multiple online sources. The dataset was then split into two parts: a low-reliability dataset and a high-reliability dataset. The low-reliability dataset was used to train and test the models, while the high-reliability dataset was used to evaluate the models. </font>\n<font size=\"2\">The dataset was created by having experts annotate personality traits in a large number of texts from multiple online sources. The dataset was then split into two parts: a low",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhat is the agreement of the dataset?\n\nGround Truth Answer:\n\n\nAnswer A:\nAnswer with content missing: (Table 2): Krippednorff's alpha coefficient for dataset is: Stability -0.26, Extraversion 0.07, Openness 0.36, Agreeableness 0.51, Conscientiousness 0.31. Explanation: The intra-annotator reliability of both datasets $D_\\textrm {\\textit {LR}}$ and $D_\\textrm {\\textit {HR}}$ is shown in Table TABREF21.\n\nAnswer B:\nHow was it created?\n<blockquote>\n<font size=\"2\">The dataset was created by having experts annotate personality traits in a large number of texts from multiple online sources. The dataset was then split into two parts: a low-reliability dataset and a high-reliability dataset. The low-reliability dataset was used to train and test the models, while the high-reliability dataset was used to evaluate the models. </font>\n<font size=\"2\">The dataset was created by having experts annotate personality traits in a large number of texts from multiple online sources. The dataset was then split into two parts: a low\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.05160",
    "question": "Which three variants of sequential validation are examined?",
    "gold_answer": "",
    "answer_a": "What is the performance of the sequential methods?\n<blockquote>3. Sequential validation: We use the sequential validation method described in Section 3.1. We use the same training and test sets, and we use the same number of examples in the training and test sets. We use the same number of examples in the training and test sets, and we use the same number of examples in the training and test sets. We use the same number of examples in the training and test sets, and we use the same number of examples in the training and test sets. We use the same number of examples in the training and test sets, and",
    "answer_b": "seq(9:1, 20, equi) - 9:1 training:test ratio, 20 equidistant samples,. Explanation: In this study, we compare three variants of cross-validation and three variants of sequential validation (subsection sec:eval-proc).",
    "label_a": "QA",
    "label_b": "Full",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich three variants of sequential validation are examined?\n\nGround Truth Answer:\n\n\nAnswer A:\nWhat is the performance of the sequential methods?\n<blockquote>3. Sequential validation: We use the sequential validation method described in Section 3.1. We use the same training and test sets, and we use the same number of examples in the training and test sets. We use the same number of examples in the training and test sets, and we use the same number of examples in the training and test sets. We use the same number of examples in the training and test sets, and we use the same number of examples in the training and test sets. We use the same number of examples in the training and test sets, and\n\nAnswer B:\nseq(9:1, 20, equi) - 9:1 training:test ratio, 20 equidistant samples,. Explanation: In this study, we compare three variants of cross-validation and three variants of sequential validation (subsection sec:eval-proc).\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.05160",
    "question": "Which three variants of cross-validation are examined?",
    "gold_answer": "",
    "answer_a": "10-fold, stratified, blocked;. Explanation: First, we apply 10-fold cross-validation where the training:test set ratio is always 9:1. Cross-validation is stratified when the fold partitioning is not completely random, but each fold has roughly the same class distribution. We also compare standard random selection of examples to the blocked form of cross-validation BIBREF16 , BIBREF11 , where each fold is a block of consecutive tweets. We use the following abbreviations for cross-validations:\n\nxval(9:1, strat, block) - 10-fold, stratified, blocked;\n\nxval(9:1, no-strat, block) - 10-fold, not stratified, blocked;\n\nxval(9:1, strat, rand) - 10-fold, stratified, random selection of examples.",
    "answer_b": "What is the performance of the sequential methods?\n< Explanation: We use the following three variants of cross-validation:\n\n1. Sequential cross-validation: We use the same training and test sets, but we select the test set in a sequential manner, i.e., we select the test set from the training set, then the training set from the test set, and so on. This is the most straightforward way to evaluate classifiers in time-ordered data. We use this method to evaluate the performance of the classifiers in the following experiments.\n\n2. Sequential cross-validation with random selection: We use the same training and test sets, but we select the test",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich three variants of cross-validation are examined?\n\nGround Truth Answer:\n\n\nAnswer A:\n10-fold, stratified, blocked;. Explanation: First, we apply 10-fold cross-validation where the training:test set ratio is always 9:1. Cross-validation is stratified when the fold partitioning is not completely random, but each fold has roughly the same class distribution. We also compare standard random selection of examples to the blocked form of cross-validation BIBREF16 , BIBREF11 , where each fold is a block of consecutive tweets. We use the following abbreviations for cross-validations:\n\nxval(9:1, strat, block) - 10-fold, stratified, blocked;\n\nxval(9:1, no-strat, block) - 10-fold, not stratified, blocked;\n\nxval(9:1, strat, rand) - 10-fold, stratified, random selection of examples.\n\nAnswer B:\nWhat is the performance of the sequential methods?\n< Explanation: We use the following three variants of cross-validation:\n\n1. Sequential cross-validation: We use the same training and test sets, but we select the test set in a sequential manner, i.e., we select the test set from the training set, then the training set from the test set, and so on. This is the most straightforward way to evaluate classifiers in time-ordered data. We use this method to evaluate the performance of the classifiers in the following experiments.\n\n2. Sequential cross-validation with random selection: We use the same training and test sets, but we select the test\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.05160",
    "question": "Which European languages are targeted?",
    "gold_answer": "",
    "answer_a": "Albanian, Bulgarian, English, German, Hungarian, Polish, Portughese, Russian, Serbian, Slovak, Slovenian, Spanish, Swedish. Explanation: FLOAT SELECTED: Table 1. Sentiment label distribution of Twitter datasets in 13 languages. The last column is a qualitative assessment of the annotation quality, based on the levels of the self- and inter-annotator agreement.",
    "answer_b": "How many tweets are collected for each language?\n< Explanation: We collected tweets from 13 European languages, and we collected 1.5 million tweets in total. We collected tweets from 13 languages: English, French, German, Spanish, Italian, Portuguese, Polish, Czech, Russian, Greek, Albanian, Turkish, Arabic, and Somali. We collected tweets from 138 languages: English, French, German, Spanish, Italian, Portuguese, Polish, Czech, Russian, Greek, Albanian, Turkish, Arabic, and Somali. We collected tweets from 138 languages: English, French, German",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nWhich European languages are targeted?\n\nGround Truth Answer:\n\n\nAnswer A:\nAlbanian, Bulgarian, English, German, Hungarian, Polish, Portughese, Russian, Serbian, Slovak, Slovenian, Spanish, Swedish. Explanation: FLOAT SELECTED: Table 1. Sentiment label distribution of Twitter datasets in 13 languages. The last column is a qualitative assessment of the annotation quality, based on the levels of the self- and inter-annotator agreement.\n\nAnswer B:\nHow many tweets are collected for each language?\n< Explanation: We collected tweets from 13 European languages, and we collected 1.5 million tweets in total. We collected tweets from 13 languages: English, French, German, Spanish, Italian, Portuguese, Polish, Czech, Russian, Greek, Albanian, Turkish, Arabic, and Somali. We collected tweets from 138 languages: English, French, German, Spanish, Italian, Portuguese, Polish, Czech, Russian, Greek, Albanian, Turkish, Arabic, and Somali. We collected tweets from 138 languages: English, French, German\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  },
  {
    "paper_id": "1803.05160",
    "question": "In what way are sentiment classes ordered?",
    "gold_answer": "",
    "answer_a": "time-ordered. Explanation: The complexity of Twitter data raises some challenges on how to perform such estimations, as, to the best of our knowledge, there is currently no settled approach to this. Sentiment classes are typically ordered and unbalanced, and the data itself is time-ordered. Taking these properties into account is important for the selection of appropriate estimation procedures.",
    "answer_b": "How are they unbalanced?\n< Explanation: The classes are ordered in the following way: positive, negative, neutral. The classes are unbalanced, with 10,000 tweets in the positive class and 10,000 in the negative class. The number of tweets in the neutral class is 10,000. The number of tweets in the training set is 100,000, and the number of tweets in the test set is 10,000. The number of tweets in the training set is 100,000, and",
    "label_a": "Full",
    "label_b": "QA",
    "prompt": "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n\nPlease focus on the content quality \u2014 not formatting \u2014 and follow the instructions below.\n\nQuestion:\nIn what way are sentiment classes ordered?\n\nGround Truth Answer:\n\n\nAnswer A:\ntime-ordered. Explanation: The complexity of Twitter data raises some challenges on how to perform such estimations, as, to the best of our knowledge, there is currently no settled approach to this. Sentiment classes are typically ordered and unbalanced, and the data itself is time-ordered. Taking these properties into account is important for the selection of appropriate estimation procedures.\n\nAnswer B:\nHow are they unbalanced?\n< Explanation: The classes are ordered in the following way: positive, negative, neutral. The classes are unbalanced, with 10,000 tweets in the positive class and 10,000 in the negative class. The number of tweets in the neutral class is 10,000. The number of tweets in the training set is 100,000, and the number of tweets in the test set is 10,000. The number of tweets in the training set is 100,000, and\n\nTasks:\n1. Choose the better answer (A, B, or Tie).\n2. Rate each answer from 1\u20135 based on relevance, correctness, and completeness.\n3. Briefly comment on how different the two answers are.\n4. Briefly comment on how each compares to the ground truth."
  }
]