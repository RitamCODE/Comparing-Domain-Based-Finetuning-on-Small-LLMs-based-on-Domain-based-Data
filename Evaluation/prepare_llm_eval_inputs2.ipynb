{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    return \" \".join(text.strip().lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load test.jsonl (Qasper test set) ===\n",
    "with open(\"raw_dataset/test.jsonl\", \"r\") as f:\n",
    "    papers = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_output(text):\n",
    "    lines = text.strip().splitlines()\n",
    "\n",
    "    # Remove first line if it looks like a repeated question\n",
    "    if lines and lines[0].strip().endswith(\"?\"):\n",
    "        lines = lines[1:]\n",
    "\n",
    "    # Rejoin and normalize whitespace\n",
    "    cleaned = \" \".join(lines).strip()\n",
    "    cleaned = re.sub(r\"[ \\t\\r\\n]+\", \" \", cleaned)\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load QA predictions ===\n",
    "qa_preds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../qa_finetune/qwen_qa_test_prediction_with_ids_and_questions.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        key = (clean(item[\"id\"]), clean(item[\"question\"]))\n",
    "        qa_preds[key] = clean_output(item[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Full-text predictions ===\n",
    "full_preds = {}\n",
    "gold_answers = {}\n",
    "with open(\"../full_text_finetune/full_text_predictions.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        key = (clean(item[\"id\"]), clean(item[\"question\"]))\n",
    "        full_preds[key] = clean_output(item[\"generated_answer\"])\n",
    "        gold_answers[key] = item.get(\"expected_output\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in papers:\n",
    "    paper_id = paper[\"id\"]\n",
    "    qas = paper[\"qas\"]\n",
    "\n",
    "    questions = qas[\"question\"]\n",
    "    answers_list = qas[\"answers\"]\n",
    "\n",
    "    for i in range(len(questions)):\n",
    "        question = questions[i]\n",
    "        answers = answers_list[i]\n",
    "\n",
    "        # # Extract gold answer\n",
    "        # if isinstance(answers, list) and len(answers) > 0:\n",
    "        #     top_answer = answers[0]\n",
    "        #     spans = top_answer.get(\"extractive_spans\", [])\n",
    "        #     gold_answer = spans[0] if spans else top_answer.get(\"free_form_answer\", \"\")\n",
    "        # else:\n",
    "        #     gold_answer = \"\"\n",
    "\n",
    "        key = (clean(paper_id), clean(question))\n",
    "        if key not in qa_preds or key not in full_preds:\n",
    "            continue\n",
    "\n",
    "        answer_qa = qa_preds[key]\n",
    "        answer_full = full_preds[key]\n",
    "        gold_answer = gold_answers[key]\n",
    "\n",
    "\n",
    "        # Randomize order\n",
    "        if random.random() < 0.5:\n",
    "            answer_a, answer_b = answer_qa, answer_full\n",
    "            label_a, label_b = \"QA\", \"Full\"\n",
    "        else:\n",
    "            answer_a, answer_b = answer_full, answer_qa\n",
    "            label_a, label_b = \"Full\", \"QA\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are evaluating two AI-generated answers to a question about a scientific research paper. The answers may differ in length, formality, or style.\n",
    "\n",
    "Please focus on the content quality — not formatting (gold_answers might not always be well formatted) — and follow the instructions below.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Ground Truth Answer:\n",
    "{gold_answer}\n",
    "\n",
    "Answer A:\n",
    "{answer_a}\n",
    "\n",
    "Answer B:\n",
    "{answer_b}\n",
    "\n",
    "Tasks:\n",
    "1. Choose the better answer (A, B, or Tie).\n",
    "2. Rate each answer from 1–5 based on relevance, correctness, and completeness.\n",
    "3. Briefly comment on how different the two answers are.\n",
    "4. Briefly comment on how each compares to the ground truth.\n",
    "\"\"\".strip()\n",
    "\n",
    "        final_dataset.append({\n",
    "            \"paper_id\": paper_id,\n",
    "            \"question\": question,\n",
    "            \"gold_answer\": gold_answer,\n",
    "            \"answer_a\": answer_a,\n",
    "            \"answer_b\": answer_b,\n",
    "            \"label_a\": label_a,\n",
    "            \"label_b\": label_b,\n",
    "            \"prompt\": prompt\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 1150 examples to llm_eval_input.json\n"
     ]
    }
   ],
   "source": [
    "# === Save output ===\n",
    "with open(\"llm_eval_input2.json\", \"w\") as f:\n",
    "    json.dump(final_dataset, f, indent=2)\n",
    "\n",
    "print(f\"✅ Saved {len(final_dataset)} examples to llm_eval_input.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLPenv)",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
